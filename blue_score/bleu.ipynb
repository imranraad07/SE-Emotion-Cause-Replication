{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/imranm3/Desktop/research/SE-Emotion-Cause/venv/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/imranm3/Desktop/research/SE-Emotion-Cause/venv/lib/python3.10/site-packages (from nltk) (2023.5.5)\n",
      "Requirement already satisfied: joblib in /home/imranm3/Desktop/research/SE-Emotion-Cause/venv/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /home/imranm3/Desktop/research/SE-Emotion-Cause/venv/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: click in /home/imranm3/Desktop/research/SE-Emotion-Cause/venv/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /home/imranm3/Desktop/research/SE-Emotion-Cause/venv/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/imranm3/Desktop/research/SE-Emotion-Cause/venv/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/imranm3/Desktop/research/SE-Emotion-Cause/venv/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/imranm3/Desktop/research/SE-Emotion-Cause/venv/lib/python3.10/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/imranm3/Desktop/research/SE-Emotion-Cause/venv/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/imranm3/Desktop/research/SE-Emotion-Cause/venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /home/imranm3/Desktop/research/SE-Emotion-Cause/venv/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/imranm3/Desktop/research/SE-Emotion-Cause/venv/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/imranm3/Desktop/research/SE-Emotion-Cause/venv/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/imranm3/Desktop/research/SE-Emotion-Cause/venv/lib/python3.10/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/imranm3/Desktop/research/SE-Emotion-Cause/venv/lib/python3.10/site-packages (from scikit-learn) (1.10.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/imranm3/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/imranm3/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/imranm3/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/imranm3/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/imranm3/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install nltk\n",
    "%pip install pandas\n",
    "%pip install scikit-learn\n",
    "\n",
    "import csv\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import string \n",
    "import re \n",
    "\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def text_cleaning(text):\n",
    "    text = str (text)\n",
    "\n",
    "    printable = set(string.printable)\n",
    "    text = ''.join(filter(lambda x: x in printable, text))\n",
    "    text = text.replace('\\x00', ' ')  # remove nulls\n",
    "    text = text.replace('\\r', ' ')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n",
    "    text = text.lower()  # Lowercasing\n",
    "\n",
    "    words = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    wn_tags = {'N': wordnet.NOUN, 'V': wordnet.VERB, 'R': wordnet.ADV, 'J': wordnet.ADJ}\n",
    "    # lemmatize each word with its POS tag\n",
    "    lemmatized_words = []\n",
    "    for word, pos in pos_tags:\n",
    "        if pos[0] in wn_tags:\n",
    "            wn_tag = wn_tags[pos[0]]\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word, wn_tag))\n",
    "        else:\n",
    "            lemmatized_words.append(word)\n",
    "    text = ' '.join([str(elem) for elem in lemmatized_words])\n",
    "\n",
    "    words= text.split()\n",
    "    stemmed_words=[porter_stemmer.stem(word=word) for word in words] # Stemming\n",
    "    text = ' '.join([str(elem) for elem in stemmed_words])\n",
    "\n",
    "    text = text.strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n"
     ]
    }
   ],
   "source": [
    "column_comments = []\n",
    "column_gold = []\n",
    "column_chatgpt = []\n",
    "column_gpt4 = []\n",
    "column_flan = []\n",
    "\n",
    "\n",
    "with open('causes.csv', 'r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    next(csv_reader)\n",
    "    idx = 0\n",
    "    for row in csv_reader:\n",
    "        column_comments.append(text_cleaning(row[1]))\n",
    "        column_gold.append(text_cleaning(row[2]))\n",
    "        column_chatgpt.append(text_cleaning(row[4]))\n",
    "        column_gpt4.append(text_cleaning(row[5]))\n",
    "        column_flan.append(text_cleaning(row[6]))\n",
    "\n",
    "print(len(column_gold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.08888888888889\n"
     ]
    }
   ],
   "source": [
    "word_counter = 0\n",
    "for sentence in column_comments:\n",
    "    word_counter = word_counter + len(sentence.split())\n",
    "print(word_counter/len(column_flan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.428888888888889\n"
     ]
    }
   ],
   "source": [
    "word_counter = 0\n",
    "for sentence in column_gold:\n",
    "    word_counter = word_counter + len(sentence.split())\n",
    "print(word_counter/len(column_flan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.846666666666666\n"
     ]
    }
   ],
   "source": [
    "word_counter = 0\n",
    "for sentence in column_gpt4:\n",
    "    word_counter = word_counter + len(sentence.split())\n",
    "print(word_counter/len(column_flan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.642222222222221\n"
     ]
    }
   ],
   "source": [
    "word_counter = 0\n",
    "for sentence in column_chatgpt:\n",
    "    word_counter = word_counter + len(sentence.split())\n",
    "print(word_counter/len(column_flan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.115555555555556\n"
     ]
    }
   ],
   "source": [
    "word_counter = 0\n",
    "for sentence in column_flan:\n",
    "    word_counter = word_counter + len(sentence.split())\n",
    "print(word_counter/len(column_flan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "Average BLEU-1 score: 0.5216223155541132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imranm3/Desktop/research/SE-Emotion-Cause/venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/imranm3/Desktop/research/SE-Emotion-Cause/venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/imranm3/Desktop/research/SE-Emotion-Cause/venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# ChatGPT: Calculate the BLEU score for each row in the columns\n",
    "chatgpt_scores = []\n",
    "scores = []\n",
    "for i in range(len(column_gold)):\n",
    "    score = sentence_bleu([column_gold[i]], column_chatgpt[i], weights=(1.0, 0, 0, 0))\n",
    "    # print(score)\n",
    "    scores.append(score)\n",
    "\n",
    "\n",
    "print(len(scores))\n",
    "\n",
    "avg_score = sum(scores) / len(scores)\n",
    "chatgpt_scores.append(avg_score)\n",
    "print(f\"Average BLEU-1 score: {avg_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "Average BLEU-2 score: 0.48862693512857797\n"
     ]
    }
   ],
   "source": [
    "# ChatGPT: Calculate the BLEU score for each row in the columns\n",
    "scores = []\n",
    "for i in range(len(column_gold)):\n",
    "    score = sentence_bleu([column_gold[i]], column_chatgpt[i], weights=(0.5, 0.5))\n",
    "    # print(score)\n",
    "    scores.append(score)\n",
    "\n",
    "\n",
    "chatgpt_blue_2_scores = scores    \n",
    "\n",
    "\n",
    "print(len(scores))\n",
    "\n",
    "# Calculate the average BLEU score for the columns\n",
    "avg_score = sum(scores) / len(scores)\n",
    "chatgpt_scores.append(avg_score)\n",
    "print(f\"Average BLEU-2 score: {avg_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "Average BLEU-3 score: 0.4666842196987067\n"
     ]
    }
   ],
   "source": [
    "# ChatGPT: Calculate the BLEU score for each row in the columns\n",
    "scores = []\n",
    "for i in range(len(column_gold)):\n",
    "    score = sentence_bleu([column_gold[i]], column_chatgpt[i], weights=(0.33, 0.33, 0.33))\n",
    "    # print(score)\n",
    "    scores.append(score)\n",
    "\n",
    "\n",
    "print(len(scores))\n",
    "\n",
    "# Calculate the average BLEU score for the columns\n",
    "avg_score = sum(scores) / len(scores)\n",
    "chatgpt_scores.append(avg_score)\n",
    "print(f\"Average BLEU-3 score: {avg_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "Average BLEU-4 score: 0.4504333236694316\n"
     ]
    }
   ],
   "source": [
    "# ChatGPT: Calculate the BLEU score for each row in the columns\n",
    "scores = []\n",
    "for i in range(len(column_gold)):\n",
    "    score = sentence_bleu([column_gold[i]], column_chatgpt[i], weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    # print(score)\n",
    "    scores.append(score)\n",
    "\n",
    "\n",
    "print(len(scores))\n",
    "\n",
    "# Calculate the average BLEU score for the columns\n",
    "avg_score = sum(scores) / len(scores)\n",
    "chatgpt_scores.append(avg_score)\n",
    "print(f\"Average BLEU-4 score: {avg_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "Average BLEU-1 score: 0.6373626036317592\n"
     ]
    }
   ],
   "source": [
    "# GPT-4: Calculate the BLEU score for each row in the columns\n",
    "scores = []\n",
    "gpt4_scores = []\n",
    "for i in range(len(column_gold)):\n",
    "    score = sentence_bleu([column_gold[i]], column_gpt4[i], weights=(1.0, 0, 0, 0))\n",
    "    # print(score)\n",
    "    scores.append(score)\n",
    "\n",
    "\n",
    "print(len(scores))\n",
    "\n",
    "# Calculate the average BLEU score for the columns\n",
    "avg_score = sum(scores) / len(scores)\n",
    "gpt4_scores.append(avg_score)\n",
    "print(f\"Average BLEU-1 score: {avg_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "Average BLEU-2 score: 0.5975115667777628\n"
     ]
    }
   ],
   "source": [
    "# GPT-4: Calculate the BLEU score for each row in the columns\n",
    "scores = []\n",
    "for i in range(len(column_gold)):\n",
    "    score = sentence_bleu([column_gold[i]], column_gpt4[i], weights=(0.5, 0.5))\n",
    "    # print(score)\n",
    "    scores.append(score)\n",
    "\n",
    "gpt4_blue_2_scores = scores\n",
    "\n",
    "print(len(scores))\n",
    "\n",
    "# Calculate the average BLEU score for the columns\n",
    "avg_score = sum(scores) / len(scores)\n",
    "gpt4_scores.append(avg_score)\n",
    "print(f\"Average BLEU-2 score: {avg_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "Average BLEU-3 score: 0.5711087979153057\n"
     ]
    }
   ],
   "source": [
    "# GPT-4: Calculate the BLEU score for each row in the columns\n",
    "scores = []\n",
    "for i in range(len(column_gold)):\n",
    "    score = sentence_bleu([column_gold[i]], column_gpt4[i], weights=(0.33, 0.33, 0.33))\n",
    "    # print(score)\n",
    "    scores.append(score)\n",
    "\n",
    "\n",
    "print(len(scores))\n",
    "\n",
    "# Calculate the average BLEU score for the columns\n",
    "avg_score = sum(scores) / len(scores)\n",
    "gpt4_scores.append(avg_score)\n",
    "print(f\"Average BLEU-3 score: {avg_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "Average BLEU-4 score: 0.5543213195547464\n"
     ]
    }
   ],
   "source": [
    "# GPT-4: Calculate the BLEU score for each row in the columns\n",
    "scores = []\n",
    "for i in range(len(column_gold)):\n",
    "    score = sentence_bleu([column_gold[i]], column_gpt4[i], weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    # print(score)\n",
    "    scores.append(score)\n",
    "\n",
    "\n",
    "print(len(scores))\n",
    "\n",
    "# Calculate the average BLEU score for the columns\n",
    "avg_score = sum(scores) / len(scores)\n",
    "gpt4_scores.append(avg_score)\n",
    "print(f\"Average BLEU-4 score: {avg_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "Average BLEU-1 score: 0.5710519279089656\n"
     ]
    }
   ],
   "source": [
    "# flan-alpaca: Calculate the BLEU score for each row in the columns\n",
    "flan_alpaca_scores = []\n",
    "scores = []\n",
    "for i in range(len(column_gold)):\n",
    "    score = sentence_bleu([column_gold[i]], column_flan[i], weights=(1.0, 0, 0, 0))\n",
    "    # print(score)\n",
    "    scores.append(score)\n",
    "\n",
    "\n",
    "print(len(scores))\n",
    "\n",
    "# Calculate the average BLEU score for the columns\n",
    "avg_score = sum(scores) / len(scores)\n",
    "flan_alpaca_scores.append(avg_score)\n",
    "print(f\"Average BLEU-1 score: {avg_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "Average BLEU-2 score: 0.5428681782625324\n"
     ]
    }
   ],
   "source": [
    "# flan-alpaca: Calculate the BLEU score for each row in the columns\n",
    "\n",
    "scores = []\n",
    "for i in range(len(column_gold)):\n",
    "    score = sentence_bleu([column_gold[i]], column_flan[i], weights=(0.5, 0.5))\n",
    "    # print(score)\n",
    "    scores.append(score)\n",
    "\n",
    "flan_blue_2_scores = scores\n",
    "\n",
    "print(len(scores))\n",
    "\n",
    "# Calculate the average BLEU score for the columns\n",
    "avg_score = sum(scores) / len(scores)\n",
    "flan_alpaca_scores.append(avg_score)\n",
    "print(f\"Average BLEU-2 score: {avg_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "Average BLEU-3 score: 0.524756866685228\n"
     ]
    }
   ],
   "source": [
    "# flan-alpaca: Calculate the BLEU score for each row in the columns\n",
    "\n",
    "scores = []\n",
    "for i in range(len(column_gold)):\n",
    "    score = sentence_bleu([column_gold[i]], column_flan[i], weights=(0.33, 0.33, 0.33))\n",
    "    # print(score)\n",
    "    scores.append(score)\n",
    "\n",
    "\n",
    "print(len(scores))\n",
    "\n",
    "# Calculate the average BLEU score for the columns\n",
    "avg_score = sum(scores) / len(scores)\n",
    "flan_alpaca_scores.append(avg_score)\n",
    "print(f\"Average BLEU-3 score: {avg_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "Average BLEU-4 score: 0.5077396989640749\n"
     ]
    }
   ],
   "source": [
    "# flan-alpaca: Calculate the BLEU score for each row in the columns\n",
    "\n",
    "scores = []\n",
    "for i in range(len(column_gold)):\n",
    "    score = sentence_bleu([column_gold[i]], column_flan[i], weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    # print(score)\n",
    "    scores.append(score)\n",
    "\n",
    "\n",
    "print(len(scores))\n",
    "\n",
    "# Calculate the average BLEU score for the columns\n",
    "avg_score = sum(scores) / len(scores)\n",
    "flan_alpaca_scores.append(avg_score)\n",
    "print(f\"Average BLEU-4 score: {avg_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5216223155541132, 0.48862693512857797, 0.4666842196987067, 0.4504333236694316]\n",
      "[0.6373626036317592, 0.5975115667777628, 0.5711087979153057, 0.5543213195547464]\n",
      "[0.5710519279089656, 0.5428681782625324, 0.524756866685228, 0.5077396989640749]\n"
     ]
    }
   ],
   "source": [
    "print(chatgpt_scores)\n",
    "print(gpt4_scores)\n",
    "print(flan_alpaca_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "44.170731707317074\n",
      "5.048780487804878\n",
      "10.097560975609756\n",
      "13.146341463414634\n",
      "22.829268292682926\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "word_counter_comments = 0\n",
    "word_counter_gold = 0\n",
    "word_counter_chatgpt = 0\n",
    "word_counter_gpt4 = 0\n",
    "word_counter_flan = 0\n",
    "\n",
    "for i in range(len(flan_blue_2_scores)):\n",
    "    if flan_blue_2_scores[i] < 0.3 and chatgpt_blue_2_scores[i] < 0.3 and gpt4_blue_2_scores[i] < 0.3:\n",
    "        count = count + 1\n",
    "        word_counter_comments = word_counter_comments + len(column_comments[i].split())\n",
    "        word_counter_gold = word_counter_gold + len(column_gold[i].split())\n",
    "        word_counter_gpt4 = word_counter_gpt4 + len(column_gpt4[i].split())\n",
    "        word_counter_chatgpt = word_counter_chatgpt + len(column_chatgpt[i].split())\n",
    "        word_counter_flan = word_counter_flan + len(column_flan[i].split())\n",
    "print(count)\n",
    "\n",
    "print(word_counter_comments/count)\n",
    "print(word_counter_gold/count)\n",
    "print(word_counter_gpt4/count)\n",
    "print(word_counter_chatgpt/count)\n",
    "print(word_counter_flan/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n",
      "15.261682242990654\n",
      "7.02803738317757\n",
      "7.88785046728972\n",
      "7.794392523364486\n",
      "7.953271028037383\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "word_counter_comments = 0\n",
    "word_counter_gold = 0\n",
    "word_counter_chatgpt = 0\n",
    "word_counter_gpt4 = 0\n",
    "word_counter_flan = 0\n",
    "\n",
    "for i in range(len(flan_blue_2_scores)):\n",
    "    if flan_blue_2_scores[i] > 0.5 and chatgpt_blue_2_scores[i] > 0.5 and gpt4_blue_2_scores[i] > 0.5:\n",
    "        count = count + 1\n",
    "        word_counter_comments = word_counter_comments + len(column_comments[i].split())\n",
    "        word_counter_gold = word_counter_gold + len(column_gold[i].split())\n",
    "        word_counter_gpt4 = word_counter_gpt4 + len(column_gpt4[i].split())\n",
    "        word_counter_chatgpt = word_counter_chatgpt + len(column_chatgpt[i].split())\n",
    "        word_counter_flan = word_counter_flan + len(column_flan[i].split())\n",
    "print(count)\n",
    "\n",
    "print(word_counter_comments/count)\n",
    "print(word_counter_gold/count)\n",
    "print(word_counter_gpt4/count)\n",
    "print(word_counter_chatgpt/count)\n",
    "print(word_counter_flan/count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
