Comment_ID,User,Issue_ID,Date,Comment,Emotion Causes
60172,NoteDancing,1490460205,2023-03-30 15:00:20,@synandi Tensorflow 2.12.0 Can't use GPU on Windows 11?,Can't use GPU on Windows 11?
60174,Thf772,1490394682,2023-03-30 14:20:54,"Sorry, even with the addition of `prefetch` I am still seeing this log message.",I am still seeing this log message.
59984,elfringham,1490303833,2023-03-30 13:26:47,@zyxkad Your problem is not related to the one that this issue refers to. I think you should open a new issue to cover your Mac specific build problem.,"""Your problem is not related to the one that this issue refers to."""
59985,setter43,1490009121,2023-03-30 9:45:41,"I am using Teachable Machine for my audio model. I don't have the option to export in .h5 for audio. Just .js or .tflite. tflite would be compatible with the google coral I am working with. But the sample app Git link just leads to an error and it's hard to find an example where I can take my AI model and just plug and play.
![image](https://user-images.githubusercontent.com/49942725/228797393-af98c7b0-2e74-4cbd-8ec5-18feb5cff7db.png)",I am using Teachable Machine for my audio model. I don't have the option to export in .h5 for audio. Just .js or .tflite. tflite would be compatible with the google coral I am working with. But the sample app Git link just leads to an error and it's hard to find an example where I can take my AI model and just plug and play.
60101,cfields5,1489541438,2023-03-30 1:08:37,@tiruk007 Having the same issue. CUDA 11.8 and cuDNN 8.6,Having the same issue.
60148,trickiwoo,1488863747,2023-03-29 15:40:41,"Thanks @pat749 and @Ruparani777 for pointing out the issue! I checked and found that `enable_v2_behavior` is not needed to trigger this bug, here is the minimized code that triggers Segmentation fault:
```
import tensorflow as tf
@tf.function
def foo():
ta = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)
ta.write(0, tf.constant(0))
return ta.gather([0])
foo()
```
Output:
```
Segmentation fault (core dumped)
```",Segmentation fault (core dumped)
60124,SuryanarayanaY,1488462459,2023-03-29 11:55:07,"The check fail is happening due to different ` input_min=-1`, `input_max=[-1,1]`. As per [API](https://www.tensorflow.org/api_docs/python/tf/raw_ops/QuantizeAndDequantizeV4) both `input_min` and `input_max` should be of same type and size if `range_given=True`.
Please refer to attached [gist](https://colab.research.google.com/gist/SuryanarayanaY/f1ccb35630bfa6b9109bff3071da2815/untitled173.ipynb).","The check fail is happening due to different  input_min=-1,  input_max=[-1,1]. As per [API](https://www.tensorflow.org/api_docs/python/tf/raw_ops/QuantizeAndDequantizeV4) both  input_min and  input_max should be of same type and size if  range_given=True. Please refer to attached [gist](https://colab.research."
60106,fedezocco,1488269198,2023-03-29 9:37:46,"I see. So I am executing 1 and 2 instead of the pip command I mentioned. Then, I proceed with the Segmentation tutorial as usual. Now I get the error below
![Capture](https://user-images.githubusercontent.com/62107909/228492552-77b969f4-754c-41ba-abcc-5d1dc5616122.PNG)","I see. So I am executing 1 and 2 instead of the pip command I mentioned. Then, I proceed with the Segmentation tutorial as usual. Now I get the error below ![Capture](https://user-images.githubusercontent.com/62107909/228492552-77b969f4-754c-41ba-abcc-5d1dc5616122.PNG)"
41898,axeltidemann,1488159628,2023-03-29 8:30:36,@nikitamaia could you please re-open the issue?,Could you please re-open the issue?
59642,reedwm,1487781445,2023-03-29 0:33:58,"@qlzh727, the ClipByValue gradient is commented out and there is a TODO assigned to you from 2018: https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/python/ops/clip_ops.py;l=126;drc=ec69e86e01436b58d53f49b12782a0452bd26c8c. Do you remember why it's commented out?","""The ClipByValue gradient is commented out and there is a TODO assigned to you from 2018"""
60106,Shreyas-SAS,1487638835,2023-03-28 21:50:30,"Hey, @fedezocco @alecamacho @18mdemark, I guess it is a problem with pip installing git.
Please try these commands in order once:
1. `!apt remove git -y` - remove any degraded git module
2. `!apt-get install git -y && git clone https://github.com/tensorflow/examples.git` - Install git first, then clone.
Meanwhile, I will look at possible problems and create a PR if I am able to solve it.",I guess it is a problem with pip installing git.
60118,gaurav00700,1487621968,2023-03-28 21:32:19,"Hi, I also need higher version of Protobuf for onnx package. But tensorflow-macos requires lower version of protobuf. How to deal with such problem. I want to install tensorflow and onnx in same python environment.",I also need higher version of Protobuf for onnx package. But tensorflow-macos requires lower version of protobuf. How to deal with such problem. I want to install tensorflow and onnx in same python environment.
60106,fedezocco,1487595488,2023-03-28 21:09:53,"Good evening,
Since yesterday I am having the same issue when executing the first cell of the TensorFlow Segmentation tutorial: https://www.tensorflow.org/tutorials/images/segmentation. No issues before yesterday. Restarting Colab does not solve the issue for me.
Regards,
Federico","""Having the same issue"""
60109,sachinprasadhs,1487445646,2023-03-28 18:54:51,"@TortoiseHam , Tested build configuration for Tensorflow 2.12 is cuDNN 8.6 and CUDA 11.8. Make sure while installing torch also this should not conflict.
Also. could you please do one more check by creating two different environments and installing `torch` and `Tensorflow` separately and check the internal package dependency versions using `pip list`.
My hunch is that it might be because of version conflicts on package dependencies.","""Version conflicts on package dependencies"""
59984,elfringham,1486903787,2023-03-28 13:36:35,@sdeoras Actually your build error looks more like there was a stale object in the bazel cache that was not rebuilt when it should have been. If this still needs resolving please try again but run 'bazel clean --expunge' first. If you can try 2.11.1 or even better 2.12.0 then that would be good.,stale object in the bazel cache that was not rebuilt when it should have been.
11085,t-kalinowski,1486821988,2023-03-28 12:45:10,I don't think this should be closed until `pip install tensorflow` does the right thing. Not everyone wants to or can use conda.,I don't think this should be closed until pip install tensorflow does the right thing.
60139,AyanmoI,1486783915,2023-03-28 12:20:31,"> I attached comments to the places I had to adjust because of ClangTidy and Linter. It might be easier for you if you revert the revert PR, then you have all those fixes already and can add additional fixes for the build time issues on top.
The reopened PR seemed to have just the last commit and I don't think I can revert the reverted PR. Are u able to do that on your end? Otherwise, I'll manually make the changes.",I attached comments to the places I had to adjust because of ClangTidy and Linter.
59984,elfringham,1486727281,2023-03-28 11:53:53,"@tiruk007 Sorry, I see that the r2.11 branch had a temporary build failure for a while which is what caused the error you found. This build error was not present in 2.11.0 nor is it present in 2.11.1 but was introduced and then fixed in between those two releases.","""This build error was not present in 2.11.0 nor is it present in 2.11.1 but was introduced and then fixed in between those two releases."""
59984,elfringham,1486521062,2023-03-28 9:30:37,@tiruk007 _ComputeOutputShape is not in the 2.11.x source code so I think maybe you were not building 2.11?,_ComputeOutputShape is not in the 2.11.x source code
59984,elfringham,1486519501,2023-03-28 9:29:29,@sdeoras Your original issue looks like you have a bad version of tensorflow_io_gcs_filesystem installed. For building TensorFlow 2.11.x please use tensorflow_io_gcs_filesystem==0.31.0.,"""Your original issue looks like you have a bad version of tensorflow_io_gcs_filesystem installed."""
60106,Shreyas-SAS,1486384139,2023-03-28 7:54:41,@sushreebarsa if you are not working on it then can you please assign the issue to me?,if you are not working on it then can you please assign the issue to me?
60047,akuegel,1486297931,2023-03-28 6:40:14,"> Actually all of it was reverted:
https://github.com/tensorflow/tensorflow/commit/7a4d2e8b1a503a45ff86d40b0387fc832302379f
Why it doesn't show properly merged in this PR, I don't know :-(
Maybe it is easier to create a new PR?","""Why it doesn't show properly merged in this PR, I don't know"""
60047,AyanmoI,1486279647,2023-03-28 6:19:43,It looks like only the last commit got reverted. I don't think the issue (of build failing with CUDA 11.4 and CuDNN 8.2) will have resolved with reverting the last commit only.,I don't think the issue (of build failing with CUDA 11.4 and CuDNN 8.2) will have resolved with reverting the last commit only.
58206,yyoon,1486039254,2023-03-28 0:27:46,"Apologies for the delayed response. Before we jump in to the details of quantization debugger, wanted to do a quick sanity check. Are you sure you're correctly quantizing your inputs before feeding them to the model?
In https://github.com/tensorflow/tensorflow/issues/58206#issuecomment-1416118721 I don't see any code for getting the quantization parameters for the input tensor, which would be causing issues.","""I don't see any code for getting the quantization parameters for the input tensor, which would be causing issues."""
59762,MarkDaoust,1486017154,2023-03-28 0:01:44,"Yeah, it's cool that there's a workaround, but it would be much better if we could find the script that makes these, and fix it.","""It would be much better if we could find the script that makes these, and fix it."""
60106,alecamacho,1485952329,2023-03-27 22:36:42,"Hi, i'm having the same issue, i've already restarted colab, also i tried to install it in a local environment and the same happened","i'm having the same issue, i've already restarted colab, also i tried to install it in a local environment and the same happened."
60062,mraunak,1485812476,2023-03-27 20:20:46,"Hi @Eva-An, please try MSVC 2019 (set BAZEL_VC=C:\Program Files(x86)\Microsoft Visual Studio\2019\Enterprise\VC). I think MSVC 2022 is not supported for the TF build.",I think MSVC 2022 is not supported for the TF build.
59725,sachinprasadhs,1485585386,2023-03-27 17:58:39,"Currently it falls under the limitations of `tf.function()`, you try try wrapping `tf.py_function ` https://www.tensorflow.org/api_docs/python/tf/py_function inside tf.function. If it still fails, you need to execute it outside tf.function which uses eager mode instead of graph mode.","Currently it falls under the limitations of tf.function(), you try try wrapping tf.py_function  https://www.tensorflow.org/api_docs/python/tf/py_function inside tf.function. If it still fails, you need to execute it outside tf.function which uses eager mode instead of graph mode."
60047,AyanmoI,1485442660,2023-03-27 16:22:13,Can we re-open this PR to work on fixing the issues? I don't see a way to re-open this.,I don't see a way to re-open this.
60047,AyanmoI,1485432593,2023-03-27 16:17:48,"> Unfortunately this change needs to be rolled back, it seems it breaks JAX build under CUDA 11.4 and CuDNN 8.2
@akuegel what are the issues? We can work on fixing them and re-merge.","""Unfortunately this change needs to be rolled back, it seems it breaks JAX build under CUDA 11.4 and CuDNN 8.2"""
60047,akuegel,1485425131,2023-03-27 16:12:51,"Unfortunately this change needs to be rolled back, it seems it breaks JAX build under CUDA 11.4 and CuDNN 8.2","""Unfortunately this change needs to be rolled back, it seems it breaks JAX build under CUDA 11.4 and CuDNN 8.2"""
52217,bhack,1485365563,2023-03-27 15:41:08,Yes probably it hard to maintain availability on the same PR since Oct 1 2021.,It hard to maintain availability on the same PR since Oct 1 2021.
59534,mattbahr,1485324526,2023-03-27 15:19:40,"ROCm CI build failure appears to be due to the removal of this constructor from `status.h` in commit: https://github.com/tensorflow/tensorflow/commit/667081b1c3d84c8fee4b5965d5e151158c74d4f8#diff-2497c155e2f97993a2cea18969d80f2d414e2f1b346cf006d8a8b655a8316b1d
Looks like other pull request builds are failing with the same errors.",ROCm CI build failure appears to be due to the removal of this constructor from status.h in commit: https://github.com/tensorflow/tensorflow/commit/667081b1c3d84c8fee4b5965d5e151158c74d4f8#diff-2497c155e2f97993a2cea18969d80f2d414e2f1b346cf006d8a8b655a8316b1d
57060,kun-lu20,1485052699,2023-03-27 12:34:16,Closing this PR since we'll raise another one to fix this issue.,Closing this PR since we'll raise another one to fix this issue.
52217,shkarupa-alex,1484600192,2023-03-27 6:56:21,Can anybody tell the reason why this PR was rejected?,Can anybody tell the reason why this PR was rejected?
59635,mergian,1484538069,2023-03-27 5:56:57,"Thanks for posting the two guides. I agree that using the newer variants with Generator or Stateless should not produce this problem. Still: as long as `tf.random.uniform` is not deprecated, I think the bug should be mentioned in the corresponding documentation: https://www.tensorflow.org/api_docs/python/tf/random/uniform
Also in the `tf.random.uniform` page it is not mentioned that it is strongly discouraged to use these.","I agree that using the newer variants with Generator or Stateless should not produce this problem. Still: as long as tf.random.uniform is not deprecated, I think the bug should be mentioned in the corresponding documentation: https://www.tensorflow.org/api_docs/python/tf/random/uniform Also in the tf.random.uniform page it is not mentioned that it is strongly discouraged to use these."
60119,Evan-0715,1484367575,2023-03-27 2:04:25,"This may be due to the fact that my data is generated from:
def Net():
input = keras.layers.Input(shape=(64, 64, 3), dtype=""float32"")
conv1 = keras.layers.Conv2D(filters=3, kernel_size=3, padding=""same"")
……
conv2 = keras.layers.Conv2D(filters=2, kernel_size=3, padding=""same"") #shape(1,64,64,2)
data = reshape(conv2, [1, 4096, 2])","This may be due to the fact that my data is generated from: def Net(): input = keras.layers.Input(shape=(64, 64, 3), dtype=""float32"") conv1 = keras.layers.Conv2D(filters=3, kernel_size=3, padding=""same"") ...... conv2 = keras.layers.Conv2D(filters=2, kernel_size=3, padding=""same"") #shape(1,64,64,2)"
60119,Evan-0715,1484352858,2023-03-27 1:52:13,"sorry, i tried the following methods to convert the data[Tensor/KerasTensor] to data[Numpy]
1. data_np = data.numpy()
2. data_np = keras.backend.get_val(data)
but neither works. It shows: AttributeError: 'Tensor' object has no attribute 'numpy'. So I can't use scipy.signal.hilbert to implement a Hilbert transform on data[Tensor/KerasTensor].","""I can't use scipy.signal.hilbert to implement a Hilbert transform on data[Tensor/KerasTensor]"""
57052,taimaruuu,1484196698,2023-03-26 19:19:55,I'm also seeing this warning when trying to train a custom image segmentation model with pixellib,I'm also seeing this warning when trying to train a custom image segmentation model with pixellib.
60113,neel-jotaniya,1484094458,2023-03-26 13:18:24,"@pat749 i think it because limitations of floating-point arithmetic used by computers.
Obtaining a very small non-zero loss value even when the actual and predicted values are the same is not a bug, it is a normal behavior and expected in most cases.",obtaining a very small non-zero loss value even when the actual and predicted values are the same
21698,mikel-brostrom,1483927707,2023-03-25 21:37:52,"You won't see INT8 model boosts compared to Float32 on Intel CPUs under 10th gen @MustafaAlahmid . This is because Intel CPUs < 10th gen don't have Intel DLBoost, a specific instruction set (ISA) architecture designed to improve performance of INT8 DL models. This ISA is present in Intel chips from 10th gen onwards. Most certainly, without a specific INT8 ISA the operations get upsampled to Float32.","""Intel CPUs  10th gen don't have Intel DLBoost, a specific instruction set (ISA) architecture designed to improve performance of INT8 DL models."""
60104,mihaimaruseac,1483843707,2023-03-25 14:53:36,"Please always makes PRs from master branch against master branch.
The release branches are only updated when we do patch releases and those are limited. Currently, only the latest release is considered for patch release. This means that r2.8 won't be updated anymore.",The release branches are only updated when we do patch releases and those are limited.
58988,beyarkay,1482937767,2023-03-24 14:55:36,"@SuryanarayanaY Could you please re-open this issue, as the problem is not fixed. I'm having the same issue on a fresh install of tensorflow in a virtual environment",I'm having the same issue on a fresh install of tensorflow in a virtual environment.
59779,egolfbr,1482785754,2023-03-24 13:19:16,I had this error on Ubuntu 22.04 as well as Fedora 37,I had this error on Ubuntu 22.04 as well as Fedora 37.
60092,innat,1482666728,2023-03-24 11:39:44,"@tilakrayal That is because, you're trying to initiate tpu-vm from colab. Colab only provides tpu-node, which initialization process is bit different. And as it's mentioned, I've used tpu-vm (on kaggle) and not tpu-node. Node requires to have data on GCP bucket or something and doesn't work on local file system but tpu-vm does. However, the actual issue can be reproduced in any system.",Trying to initiate tpu-vm from colab.
60092,tilakrayal,1482660304,2023-03-24 11:33:29,"@sachinprasadhs,
I tried to execute the mentioned code on tensoflow v2.9 and nightly & it was failing due to `No OpKernel was registered` error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/22ac50ef65e150184ddce505c2bccaba/invalidargumenterror_unsupported_data_type_for_tpu_string.ipynb).",No OpKernel was registered
60093,mayankagarwals,1482171207,2023-03-24 2:39:31,"@sushreebarsa the problem is we are registering gradient for round function in registry https://github.com/tensorflow/tensorflow/blob/f46ff5e9cfff71a4de98a8aac79d44df27df48f4/tensorflow/python/ops/math_grad.py#L1812 And the logic to add documentation is whether a op is present in registry or not
https://github.com/tensorflow/tensorflow/blob/f46ff5e9cfff71a4de98a8aac79d44df27df48f4/tensorflow/tools/docs/generate2.py#L143 Just need to be sure this is the only use case of registering gradients",registering gradient for round function in registry
59719,reedwm,1482014900,2023-03-23 22:52:54,"`numpy` also measures the device transfer time, which sometimes you want, but you're right this example does not want to measure this, since it's just comparing the performance of `tf.function`.
Perhaps the example with `timeit` should be removed. I don't think the example adds much value since it doesn't show `tf.function` being faster, even if `sync_devices` is added.","numpy also measures the device transfer time, which sometimes you want, but you're right this example does not want to measure this, since it's just comparing the performance of tf.function."
59728,trevor-m,1482009697,2023-03-23 22:45:08,"Hi @reedwm, I spent some time on this but the cast code is a bit confusing.
Using the macros, I keep getting duplicate registration errors or missing instantiations.
I also tried to explicitly only add int32, which compiles, but when running the script it still always uses the CPU. Any idea what I'm missing here: https://github.com/trevor-m/tensorflow/commit/ac9a6b134ac022dd520cf0c3a73a222f0824418d ?",I spent some time on this but the cast code is a bit confusing.
59719,DEKHTIARJonathan,1481947865,2023-03-23 21:38:49,"@reedwm I disagree. We implemented `tf.test.experimental.sync_devices()` precisely for this usecase. Let's not introduce poor/bad practices in the official documentation.
`.numpy()` is NOT a good idea to measure / assess performance. Hence we had to come up with `sync_devices()`",I disagree. We implemented tf.test.experimental.sync_devices() precisely for this usecase. Let's not introduce poor/bad practices in the official documentation. .numpy() is NOT a good idea to measure / assess performance. Hence we had to come up with sync_devices().
59270,mihaimaruseac,1481697596,2023-03-23 18:26:33,"Please don't use ""update "" commit messages. These make it harder to understand at a glance at commit history what the changes are.
https://cbea.ms/git-commit/","""these make it harder to understand at a glance at commit history what the changes are."""
59975,mihaimaruseac,1481695404,2023-03-23 18:24:47,"Please don't use ""update <file>"" commit messages. These make it harder to understand at a glance at commit history what the changes are.
https://cbea.ms/git-commit/","""update file>"" commit messages. These make it harder to understand at a glance at commit history what the changes are. https://cbea.ms/git-commit/."
59853,yishuangP,1481642687,2023-03-23 17:56:29,"Hi guanyu, this seems a tooling issue on tensorflow side. Could you help file a separate issue for this?","""This seems a tooling issue on tensorflow side."""
58067,poulsbo,1481564522,2023-03-23 17:07:42,@gbaned I should not review in Github right? NVIDIA should review here.,I should not review in Github right? NVIDIA should review here.
58368,kulinseth,1481492160,2023-03-23 16:19:43,"> Would anyone have a workable version of ld to share? I can't install XCode 13.x once 14.x is in place with MacOS 13.x is installed..
Hi @feranick this is fixed in https://developer.apple.com/services-account/download?path=/Developer_Tools/Xcode_14.3_beta_2/Xcode_14.3_beta_2.xip.
Can you please check and verify?",I can't install XCode 13.x once 14.x is in place with MacOS 13.x is installed.
59853,heguanyu,1481482841,2023-03-23 16:14:03,"@yishuangP I'm using 3.9.6 Let me update to 3.9.9 and try again. Is there anything I need to configure when running ./configure file? I only select ""yes"" for iOS question, all others are ""No"".","I'm using 3.9.6 Let me update to 3.9.9 and try again. Is there anything I need to configure when running ./configure file? I only select ""yes"" for iOS question, all others are ""No""."
59853,yishuangP,1481453187,2023-03-23 15:58:16,"`This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)`
Hmm what's the python version you have? We have an internal job that runs this script and the job is working fine. It uses python 3.9.9. I think this error is related to some configuration issue.","This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)"
41898,axeltidemann,1481045254,2023-03-23 11:46:51,"The workaround that @ratovarius uses (posted in its entirety on [Stack Overflow](https://stackoverflow.com/questions/63034145/warningtensorflowefficient-allreduce-is-not-supported-for-1-indexedslices#)), does not work for me. I get the following error message:
```ValueError: Disallowed task type found in cluster spec. Allowed types are ('chief', 'worker', 'evaluator', 'ps', None) and the cluster spec is ClusterSpec({'master': ['127.0.0.1:2222']}).```","""ValueError: Disallowed task type found in cluster spec."""
60043,gokulkrishna98,1480644164,2023-03-23 6:00:36,"Yes, I have also tested it on tf2.11, the same issue persists. I believe there has been no change in elementwise parser by commit history in nightly, so I don’t think testing is required there.","I have also tested it on tf2.11, the same issue persists."
60080,npanpaliya,1480621772,2023-03-23 5:25:35,Looks like TF 2.12.0 tag is created from r2.11 branch instead of r2.12. Because r2.12 branch shows correct version in setup.py.,"""Looks like TF 2.12.0 tag is created from r2.11 branch instead of r2.12"""
58994,edwardyehuang,1480590817,2023-03-23 4:36:20,"@mihaimaruseac @mohantym Could you remove the label ""awaiting PR merge"" please?","""awaiting PR merge"""
60024,YuliyaPylypiv,1480379830,2023-03-22 23:24:14,"Hi @synandi,
I'm seeing the same issue again with `tf-nightly 2.13.0.dev20230322` on macos x86 machine
To reproduce:
```
pip install tf-nightly==2.13.0.dev20230322
python -c ""import tensorflow as tf; print(tf.version.VERSION)""
```
Result: ```
Traceback (most recent call last):
File ""<string>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute 'version'
```","""I'm seeing the same issue again with tf-nightly 2.13.0.dev20230322 on macos x86 machine"""
59938,mraunak,1480357019,2023-03-22 22:52:28,"@AndrewVallette try the command: ""bazel clean --expunge"" to clean the bazel cache and then re-rerun the build. I observed you are using --config=mkl, https://github.com/tensorflow/tensorflow/blob/master/.bazelrc#L222 as per the documentation it is yet to be supported on windows. you may need to remove --config=mkl.","""I observed you are using --config=mkl, https://github.com/tensorflow/tensorflow/blob/master/.bazelrc#L222 as per the documentation it is yet to be supported on windows."""
58014,milpuz01,1480244989,2023-03-22 20:52:47,"> Hi @milpuz01 Any update on this PR? Please. Thank you!
HI @gbaned, very sorry for very long delay in addressing review comments. I believe they are addressed with [069cb25](https://github.com/tensorflow/tensorflow/pull/58014/commits/069cb256f1555ec2b1db14da86ee2d8898c7a760)","""very sorry for very long delay in addressing review comments"""
59607,jpienaar,1480060681,2023-03-22 18:24:20,(I don't know why AMD build failed here),I don't know why AMD build failed here.
56352,vufg,1480051069,2023-03-22 18:19:14,@mahmoud-abuzaina i think you need to rebase this PR to the head to trigger the tests,i think you need to rebase this PR to the head to trigger the tests.
59853,yishuangP,1479931590,2023-03-22 16:56:04,Hi @heguanyu sorry I just realized you'll need to use `tflite_custom_cc_library` instead of `tflite_custom_c_library`. I'm sending out another PR for review.,I'm sending out another PR for review.
41891,animesh-wynk,1479659271,2023-03-22 14:24:57,"I am getting a similar issue in tensorflow 2.11.0
[here are more details](https://stackoverflow.com/questions/75813079/error-polling-for-event-status-failed-to-query-event-cuda-error-illegal-addres)",I am getting a similar issue in tensorflow 2.11.0 [here are more details](https://stackoverflow.com/questions/75813079/error-polling-for-event-status-failed-to-query-event-cuda-error-illegal-addres)
59504,georgthegreat,1479376086,2023-03-22 11:16:41,"Well, I can, but I am not sure if this will be of any help.
This is not the first PR which gets stalled and remains unmerged for months.",This is not the first PR which gets stalled and remains unmerged for months.
58996,Sarvesh-Kesharwani,1478950204,2023-03-22 5:44:35,"> This PR is now just adding a single empty line
I regret to inform you that I am currently unable to dedicate enough time to familiarize myself with the required code base for this PR. As a result, I will have to close this PR.",I am currently unable to dedicate enough time to familiarize myself with the required code base for this PR.
59916,yijie-yang,1478627217,2023-03-21 21:45:24,"Hi @gbaned, I'm not authorized to merge this PR. Could you request it from Hexagon team?",I'm not authorized to merge this PR. Could you request it from Hexagon team?
58067,poulsbo,1478486475,2023-03-21 19:45:20,"I'm seeing a failure here:
```
[.../tf2tensorrt/trt_convert_api_test.cc:255]
Expected equality of these values:
::tsl::OkStatus()
Which is: OK
(result.status())
Which is: NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_000_000)
```
Followed by a string of similar failures.
@drivanov, can you repro these problems on your side and address them?",[.../tf2tensorrt/trt_convert_api_test.cc:255] Expected equality of these values: ::tsl::OkStatus() Which is: OK (result.status()) Which is: NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_000_000)
55566,bhack,1478417531,2023-03-21 18:47:03,This is 11 months old. I will invest time again if there are reviewers resources,I will invest time again if there are reviewers resources.
59900,pjpratik,1477695235,2023-03-21 11:46:09,"Hi @TheHellTower, sorry for the inconvenience. We are working on the C example as well. Please feel free to raise a PR if you have the updated code implementation available.","""We are working on the C example as well. Please feel free to raise a PR if you have the updated code implementation."""
59815,milincheta,1477431716,2023-03-21 8:21:45,"Hi @pjpratik API its: Pixel 2 API 30
SDK Android 13 and 10 are installed
besides the error I mentioned on the first message, that now I managed to fix somehow.
I did the #todo however on the code I downloaded some of them were already done...
Now when testing on my mobile device I just see: Fake label and no recognition at all.
By the way I am using my own network that I trained to recognize fruits instead of flowers.","""Now when testing on my mobile device I just see: Fake label and no recognition at all."""
59938,AndrewVallette,1476933641,2023-03-20 21:05:27,"@mraunak Thank you, but this did not solve my issue. Today I tried another build in a fresh environment, just in case, but no luck.","I tried another build in a fresh environment, just in case, but no luck."
59930,elfringham,1475966065,2023-03-20 10:20:10,"@SuryanarayanaY Yes, that matches the failure I see.","""That matches the failure I see."""
59815,pjpratik,1475945112,2023-03-20 10:07:44,"Hi @milincheta, sorry for the inconvenience caused.
Can I know which Android API and SDK version you are using and did you following the [#TODO](https://codelabs.developers.google.com/codelabs/recognize-flowers-with-tensorflow-on-android#6) steps for recognition?
The issue is not reproducible at my end. Can you post the errors you have encountered during the process?
Thanks.","""The issue is not reproducible at my end."""
59960,trickiwoo,1475302660,2023-03-19 15:57:26,"@sushreebarsa @CrazyShenanigan Sorry for the delay. Thanks for providing the workaround for the issue! I tried implementing the workaround you suggested, but it doesn't seem to resolve the problem. It fails in gradient computation with the same error message:
```
TypeError: in user code:
TypeError: _QuantizeAndDequantizeV4GradGrad() takes 2 positional arguments but 4 were given
```","I tried implementing the workaround you suggested, but it doesn't seem to resolve the problem. It fails in gradient computation with the same error message."
60037,ictorv,1475110732,2023-03-19 5:25:40,"```tensorflow 1.14.0``` is not compatible with auto installed ```protobuf``` version.
Step 1: ```pip uninstall protobuf```
Step 2: ```pip install protobuf==30.20.0```
or any other lower version:
https://pypi.org/project/protobuf/#history",tensorflow 1.14.0 is not compatible with auto installed protobuf version.
59914,tatwaichong,1474220723,2023-03-17 18:07:52,"move the new tests back, and delete the new test file.","move the new tests back, and delete the new test file."
60004,samdabadei,1473662709,2023-03-17 11:01:58,Is there no way to change the trainable attribute of a Variable after the first creation? Between the model-init() and the model-compile(). Its possible for layers and models and its quite tedious to change this always in the init when doing transfer learning,Is there no way to change the trainable attribute of a Variable after the first creation? Between the model-init() and the model-compile(). Its possible for layers and models and its quite tedious to change this always in the init when doing transfer learning.
59863,Caolongjie,1473073540,2023-03-17 3:33:19,The issue should be opened for bert project not tf. You can close it.,The issue should be opened for bert project not tf. You can close it.
59534,swachhandl,1472718064,2023-03-16 20:47:45,"Apologies for the late response. I'm not quite sure that it could be happening due to switching out `port::InternalError` with `tsl::errors::Internal`. Are you able to check why `a.opaque()` is a nullptr? And how does it (or whether it even does) map to the input arguments to the op? If we can, we might be able to put a check in the op.",I'm not quite sure that it could be happening due to switching out port::InternalError with tsl::errors::Internal.
59897,enmanuelmag,1472702997,2023-03-16 20:34:20,"Hello! Sorry i forget to comment that I found the problem. The new 40 series GPU needs CUDA 11.8 or higher and cuDNN 8.6 or higher. And that versions aren't available on Conda, so you need to install on system-side or globally","""And that versions aren't available on Conda, so you need to install on system-side or globally."""
22926,pranjalsrv123,1472623838,2023-03-16 19:28:12,"f you want your generated code placed in a specific directory, use an absolute path:
mkdir -p /some/absolute/path
protoc my.proto --go_out=/some/absolute/path","f you want your generated code placed in a specific directory, use an absolute path: mkdir -p /some/absolute/path protoc my.proto --go_out=/some/absolute/path."
59675,SandSnip3r,1472534269,2023-03-16 18:32:55,@pgpetrak I guess you're still the reviewer for this? I'm going to remove myself as reviewer if you can take care of this.,I guess you're still the reviewer for this?
59743,trevor-m,1472408337,2023-03-16 17:31:45,@reedwm Looks like CudnnRNN doesn't support BF16 yet. Should I add a bf16 registration for the op and just cast to float always? I guess the alternative would be to exclude CudnnRNN from the auto mixed precision allowlist for bf16 only - I don't see any place where there is currently a way to have a different list for bf16 vs fp16.,"""I don't see any place where there is currently a way to have a different list for bf16 vs fp16."""
60015,terryheo,1472357484,2023-03-16 17:00:19,"> Is https://github.com/tensorflow/tensorflow/issues/59631 the right issue?
Yes, this is a cherry-pick so I didn't change commit message. It's a breakage on CMake build.","""It's a breakage on CMake build."""
60008,mihaimaruseac,1472328733,2023-03-16 16:43:24,"No template filled in, new user, looks spam","No template filled in, new user, looks spam."
59932,elfringham,1472103749,2023-03-16 14:37:11,"[api_compat_test_fail.zip](https://github.com/tensorflow/tensorflow/files/10991956/api_compat_test_fail.zip)
Attached log of fail with Python pip etc commands at beginning to show that protobuf 4.22.1 was indeed installed into the place that TensorFlow used in its build.
I think the gist is not going to happen, sorry, I only have a free account and its going to hit the runtime limit well before it finishes.","I think the gist is not going to happen, sorry, I only have a free account and its going to hit the runtime limit well before it finishes."
59225,rameezdev,1471253439,2023-03-16 3:41:33,"> Added a PR #59249 for the fix.
Both TensorFlow-2.11.0-cp38-cp38-win_amd64.zip and TensorFlow-2.12.0rc0-cp38-cp38-win_amd64.zip alone don't have a vulnerability
The individual libraries which are having vulnerabilities are tensorflow_io_gcs_filesystem-0.31.0-cp38-cp38-win_amd64.zip and tensorflow_intel-2.11.0-cp38-cp38-win_amd64.whl",The individual libraries which are having vulnerabilities are tensorflow_io_gcs_filesystem-0.31.0-cp38-cp38-win_amd64.zip and tensorflow_intel-2.11.0-cp38-cp38-win_amd64.whl.
59971,reedwm,1471198520,2023-03-16 2:42:33,Can you resolve conflicts? a695fffc660994e892583391d69e4c1b9ce89320 has some minor conflicts.,Can you resolve conflicts? a695fffc660994e892583391d69e4c1b9ce89320 has some minor conflicts.
59853,yishuangP,1470346435,2023-03-15 16:21:53,"We need to update tensorflow/lite/ios/build_frameworks.sh, it is outdated. In the meantime, can you try building each framework manually? For example, the c framework https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/ios/BUILD.apple#L102, and the selected ops framework:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/ios/BUILD.apple#L153","""It is outdated"""
59990,edi-bice,1470112086,2023-03-15 14:30:00,@synandi agree completely but should have made it clearer that this is TF running in TFX/KFP pipelines on Kubeflow accessing its cluster-local Minio storage via S3 protocol etc. Minio is bundled with Kubeflow and at least the gcp-blueprint distribution seems to have it configured without SSL.,TF running in TFX/KFP pipelines on Kubeflow accessing its cluster-local Minio storage via S3 protocol etc. Minio is bundled with Kubeflow and at least the gcp-blueprint distribution seems to have it configured without SSL.
59767,tilakrayal,1469930202,2023-03-15 12:38:48,Closing this as stale. Please reopen if this is still a valid request.,Closing this as stale. Please reopen if this is still a valid request.
52576,fergushenderson,1469838640,2023-03-15 11:32:06,"It looks like Alan Kelly already wrote an update for the RELEASE.md:
* Add int16x8 support for the built-in ops `space_to_batch_nd` and
`batch_to_space_nd`
However, it looks like some tests are failing with this change, in particular //tensorflow/lite/testing:zip_test_batch_to_space_nd","""It looks like some tests are failing with this change, in particular //tensorflow/lite/testing:zip_test_batch_to_space_nd."""
59943,mikcla,1469657410,2023-03-15 9:30:41,"@mraunak I think so to, but I don't how to fix this, I have tried to set system variables, change the path in one of the config files and also tried to change the path in cmd. I am using windows 11 btw.
I’m sorry I don't have everything on hand because I kept moving on with my project, so I took some steps back, and ran simply ran pip install. However I’m willing to try to build from source again some time, but want to have something new to try beforehand.","I have tried to set system variables, change the path in one of the config files and also tried to change the path in cmd."
59943,mraunak,1469523083,2023-03-15 7:58:43,@mikcla @SuryanarayanaY I was able to run the bazel build --config=opt --copt=-march=native //tensorflow/tools/pip_package:build_pip_package successfully for TF 2.11.0 with Bazel 5.3.0 on Windows 10 and for Python 3.10. I think this error is related to the path/environmental variable setup,"""I think this error is related to the path/environmental variable setup."""
48843,nalden141,1469013937,2023-03-14 23:46:55,"my gpu was working in tensorflow but when i deleted spyder and installed it again , some Libraries deleted like numpy and pandas , and gpu doesn't work in tensorflow but it work in pytorch can you help me","""some Libraries deleted like numpy and pandas , and gpu doesn't work in tensorflow but it work in pytorch"""
59861,terryheo,1468932567,2023-03-14 22:32:17,"I also couldn't find a way to get API level in Bazel BUILD rules.
That's why TF's Android builds requires API level 26 or higher.
https://github.com/tensorflow/tensorflow/blob/master/configure.py#L760
If you want to use lower level, you need to modify the file manually.","""I also couldn't find a way to get API level in Bazel BUILD rules."""
59932,elfringham,1467952915,2023-03-14 11:49:04,@SuryanarayanaY You are missing an install of keras and tensorflow-estimator. Or alternatively keras-nightly and tf-estimator-nightly.,"""You are missing an install of keras and tensorflow-estimator. Or alternatively keras-nightly and tf-estimator-nightly."""
59932,SuryanarayanaY,1467946084,2023-03-14 11:43:33,"@elfringham ,
Actually earlier I did not downloaded any protobuf version explicitly. I tried to test the command by installing latest protobuf version (4.22.1) now and the test is failing.The log is attached below for reference.
[#59932.log](https://github.com/tensorflow/tensorflow/files/10967721/59932.log)",I did not downloaded any protobuf version explicitly.
59968,Cranky-cat,1467634070,2023-03-14 8:31:40,"I convert file format without label .txt file, so my conversion will fail.
So I would like to ask how to get the label .txt file of the model.","I convert file format without label .txt file, so my conversion will fail. So I would like to ask how to get the label .txt file of the model."
59968,Cranky-cat,1467628554,2023-03-14 8:27:30,"[train_mobilenet.zip](https://github.com/tensorflow/tensorflow/files/10966111/train_mobilenet.zip)
I used this py to train the model, and converting the trained H5 model file to a tflite model file caused the loss of metadata","I used this py to train the model, and converting the trained H5 model file to a tflite model file caused the loss of metadata."
58749,cantonios,1467086661,2023-03-13 22:55:55,"The issue here is that `uint32.max` is not actually representable in `float32` - and rounds up when converting, from 4294967295 to 4294967300.0. This eventually leads to a cast overflow and undefined behavior - which is why we see different values between CPU and GPU.","The issue here is that uint32.max is not actually representable in float32 - and rounds up when converting, from 4294967295 to 4294967300.0. This eventually leads to a cast overflow and undefined behavior - which is why we see different values between CPU and GPU."
51839,TheBigTicket13,1466851693,2023-03-13 19:43:05,"I have encounter this issue recently. Solution for me was to avoid tf.py_function. I used normal python function instead with eager execution enabled. In this mode it is possible to convert Tensor to numpy with numpy() method
My py_function was inside tf.GradientTape() and yet there was a memory leak","""I have encounter this issue recently"""
59942,taoky,1466682102,2023-03-13 18:22:49,Looks like there's a typo in L33 of this PR: `LLVM_BAZEL_OVERLAY` should be `LLVM_SRC` as overlay is not used here.,looks like there's a typo in L33 of this PR: LLVM_BAZEL_OVERLAY should be LLVM_SRC as overlay is not used here.
59631,Altair-Bueno,1466594927,2023-03-13 17:31:20,"Whops sorry, I meant `tensorflow/tensorflow` *Docker* images. I wasn't specific enough",I wasn't specific enough.
59870,ShashmurinSergey,1466357584,2023-03-13 15:23:00,"@pjpratik But I am not using Conv2d transpose, the error occurs on Conv2d layers with dilation=2, 4, 8.
I am using a Docker container with the latest version of TensorFlow, I also tried using a container with tf-nightly image, but I still get the same error.
Is there any other way to convert Conv2d layer with dilation != 1 without adding BatchToSpaceND and SpaceToBatchND layers?","""I am using a Docker container with the latest version of TensorFlow, I also tried using a container with tf-nightly image, but I still get the same error."""
46356,bujoralexandru,1466340156,2023-03-13 15:13:14,"@tilakrayal,
Thank you for your answer, but we managed to fix this issue. It was a side-effect of some code generation we do for our app, but we configured a single class name wrong and everything started breaking because of that.",We configured a single class name wrong and everything started breaking because of that.
59950,SuryanarayanaY,1465722800,2023-03-13 8:41:47,"@Zaoyee ,
For this case it seems `tf.io.parse_single_sequence_example` not working when `context_features =FixedLenSequenceFeature()` with Sparse input. Workaround seems to use` tf.io.VarLenFeature()` instead of `FixedLenSequenceFeature()`. Could you confirm whether this works for your case or do you have your case to use only with` FixedLenSequenceFeature()` ?","""For this case it seems tf.io.parse_single_sequence_example not working when context_features = FixedLenSequenceFeature() with Sparse input"""
58762,SuryanarayanaY,1465647510,2023-03-13 7:38:41,"Hi @SajjadAemmi ,
It seems the version compatibility between tensorflow and tensorflow-io causing the problem.
Can you try installing tensorflow-io alongwith its compatible tensorflow using the below code.
`!pip install tensorflow-io[tensorflow]`
For me the reported Import error gone with above installation code.Please refer to attached [gist](https://colab.research.google.com/gist/SuryanarayanaY/ad1c1c1af38ca6c5a6a80805751426cb/58762_r2.ipynb).","""It seems the version compatibility between tensorflow and tensorflow-io causing the problem."""
58206,gsirocco,1465247199,2023-03-12 16:56:14,"Hi, looking for any help or suggestions. This is very close to working for our product if only can get the correct answer. The TPU is providing the speed that just doing TF or TFLite can't. It's discouraging that I can't get over this last hurdle.",I can't get over this last hurdle.
59534,mattbahr,1465087968,2023-03-12 4:12:24,"It looks like `a.opaque()` is returning a null pointer. I tried adding the following lines of code before the call to `DoBlasInternalImpl`, and my test error was triggered.
```
const void *test = a.opaque();
if (!test) {
return tsl::errors::Internal(""TEST ERROR TRIGGERED"");
}
```
@swachhandl do you have any ideas on how we might handle this better?",a.opaque() is returning a null pointer.
58867,alanwilter,1464636656,2023-03-10 23:26:06,I tried `pip install tensorflow==2.12.rc1` and still not working with CUDA 12. Which release should see TF working with CUDA 12?,I tried pip install tensorflow==2.12.rc1 and still not working with CUDA 12
22853,janmarxen,1463952714,2023-03-10 15:17:30,"> @2649 to be clear, the existence of ListWrappers is intended behavior. Them causing problems is a bug. Are they causing issues?
When I use dask and my custom models (when subclassing) are being serialized/deserialized, the error Exception: TypeError(""can not serialize '_DictWrapper' object"") is thrown. Are these wrappers not serializable?",Them causing problems is a bug.
58762,SajjadAemmi,1463832005,2023-03-10 13:51:00,"Hi @SuryanarayanaY i import tensorflow_io, but i have same error
![image](https://user-images.githubusercontent.com/42681293/224332891-1ba72ff8-78b7-4928-bf46-80a3257cb63e.png)","""i have same error"""
59739,isolateFeng,1463809340,2023-03-10 13:32:20,"I saw RoboTux's PR https://github.com/tensorflow/tensorflow/pull/59942 had failed merge test, maybe I'll try it (qwq
or could someone give me an alternative method that I can try","I saw RoboTux's PR https://github.com/tensorflow/tensorflow/pull/59942 had failed merge test, maybe I'll try it (qwq or could someone give me an alternative method that I can try."
57099,mohamedma872,1463731305,2023-03-10 12:23:57,TfLiteGpu.isGpuDelegateAvailable(context) always returns false and my phone is google pixel 6 so I don't know what that ? is anyone has the same problem,TfLiteGpu.isGpuDelegateAvailable(context) always returns false and my phone is google pixel 6 so I don't know what that ? is anyone has the same problem.
59926,apmanikandan,1463420141,2023-03-10 8:03:31,"seems you are trying to convert a TensorSpec or a RaggedTensorSpec object into a DType object directly using tf.dtypes.as_dtype() method, which is not possible. the as_dtype() method is used to convert a string or numpy dtype object to a tensorflow DType object. replace your code for d2 and d3 as below , this shall solve the TypeError
...
d2 = type_value0.dtype
d3 = type_value1.dtype
....","""Trying to convert a TensorSpec or a RaggedTensorSpec object into a DType object directly using tf.dtypes.as_dtype() method, which is not possible."""
59888,apmanikandan,1463406714,2023-03-10 7:51:26,"@dengpanyin can you explicitly specify the device type ,so that all the devices in the collective group have the same device type","Can you explicitly specify the device type ,so that all the devices in the collective group have the same device type."
56231,Vrajs16,1463185478,2023-03-10 3:02:47,I am also having this problem. Its kinda of annoying not having autocomplete.,I am also having this problem. Its kinda of annoying not having autocomplete.
58700,cantonios,1462584225,2023-03-09 18:42:41,"We no longer support Windows GPU builds. 2.10 was the last supported release.
https://www.tensorflow.org/install/pip#windows-native",We no longer support Windows GPU builds. 2.10 was the last supported release.
58368,feranick,1462229904,2023-03-09 15:15:36,"As this is now affecting 2 releases (2.11 and the upcoming 2.12), the least that could be done is to put a warning on the release notes.","As this is now affecting 2 releases (2.11 and the upcoming 2.12), the least that could be done is to put a warning on the release notes."
59915,nouiz,1460834406,2023-03-08 20:27:34,"The CI failed, but it doesn't seem related.","The CI failed, but it doesn't seem related."
59724,albertz,1460582082,2023-03-08 17:48:50,"Maybe you misunderstood what I wrote. I did not say that the behavior is unexpected or not documented. I know that it is exactly as expected. But I'm not really asking about `tf.gather` here at all.
I'm asking for a **separate** new `tf.gather_with_fallback` or whatever you want to name it. Or maybe a new **option** for `tf.gather`.
I don't want to change any existing behavior.","""Maybe you misunderstood what I wrote."""
59859,jpienaar,1460566153,2023-03-08 17:40:31,Still see failure tosa/tests/tfl-to-tosa-pipeline.mlir:390:45 and :358,Still see failure tosa/tests/tfl-to-tosa-pipeline.mlir:390:45 and :358.
57982,sampathrajapaksha,1460517003,2023-03-08 17:09:02,"@bgLeon Yes, that's one possible solution which I also thought about. I saw a few people have posted the same issue in StackOverflow and, unfortunately couldn't find any efficient solution yet","""I saw a few people have posted the same issue in StackOverflow and, unfortunately couldn't find any efficient solution yet."""
59724,SuryanarayanaY,1460512659,2023-03-08 17:05:55,"Hi @ albertz,
Unfortunately this can't be considered as Feature now.With invalid data the behaviour is expected and also documented. Please refer the Developer [comment-1439267105](https://github.com/tensorflow/tensorflow/issues/59750#issuecomment-1439267105) regarding similar issue with same Op where it was clearly mentioned that it won't be considered for fix and closed the issue well.","""Unfortunately this can't be considered as Feature now."""
59587,dhruvsreenivas,1460400883,2023-03-08 15:59:01,"Alright, I think it might not be a TensorFlow problem as much as it could just be I'm loading in a LOT of data at once, so memory might be at a premium -- I've tried this with a couple other experiments and it seems that this is what is going on. Sorry for the inconvenience!
For example, I tried something simple where I would delete the batch once I finished updating with it, and that actually worked well with limited memory.","I'm loading in a LOT of data at once, so memory might be at a premium -- I've tried this with a couple other experiments and it seems that this is what is going on."
57982,sampathrajapaksha,1460280224,2023-03-08 14:52:23,"@bgLeon Did you find the solution for this?
i have the same issue and couldn't find a proper solution yet",i have the same issue and couldn't find a proper solution yet.
59763,fede72bari,1459818900,2023-03-08 9:13:50,It turned out that reinstalling everything was not enough; probably there was a synch problem with the updating of the kernel used fo rthe jupyter notebook. So in case other will come into the same troble I suggest to reinstall tensorflow and then create a new kernel for jupyter in the same new Anaconda environment.,reinstalling everything was not enough; probably there was a synch problem with the updating of the kernel used fo rthe jupyter notebook.
59094,johnnkp,1459233542,2023-03-08 2:56:43,"My last comment is, kotlin gradle plugin 1.8+ required Java 11 as target. Sometimes need to freeze its version to 1.7 series.","""kotlin gradle plugin 1.8+ required Java 11 as target"""
38701,bas-aarts,1459100617,2023-03-08 0:56:59,still an issue to date,still an issue to date
59859,jpienaar,1458916760,2023-03-07 21:46:41,"I'm seeing
```
mlir/tosa/tests/tfl-to-tosa-pipeline.mlir:358:45: error: undefined variable: VAL_0
// CHECK: %[[VAL_3:.*]] = ""tosa.rescale""(%[[VAL_0]]) {double_round = false, input_zp = 128 : i32, multiplier = array<i32: 1073741824>, output_zp = 0 : i32, per_channel = false, scale32 = true, shift = array<i32: 30>}
^
```
on internal result.",mlir/tosa/tests/tfl-to-tosa-pipeline.mlir:358:45: error: undefined variable: VAL_0
59855,MartinKlefas,1457957492,2023-03-07 10:52:12,"If I follow the suggestion in the other issue and install with `--no-dependencies` then pip complains about missing dependencies next time I need a package, and the example code won't run.
I'm using the same code on a docker though that came with pre-installed tflite model maker - so I know it's this conda environment, and not the code that's the problem.","""I'm using the same code on a docker though that came with pre-installed tflite model maker - so I know it's this conda environment, and not the code that's the problem."""
59513,ilfrich,1457336883,2023-03-07 1:29:36,Having the latest version of tensorflow didn't solve the issue for me. I had to manually update `h5py` to 3.8.0 (latest as of writing this) and that solved the issue.,Having the latest version of tensorflow didn't solve the issue for me.
59896,mihaimaruseac,1456733783,2023-03-06 18:34:29,"Oh they are, the PR is a clone of an old change. Solving the conflicts shows that there's nothing new.","Oh they are, the PR is a clone of an old change. Solving the conflicts shows that there's nothing new."
59857,DEKHTIARJonathan,1456627997,2023-03-06 17:47:10,"@matthiaskramm could we work at making this not SavedModel specific maybe ?
https://github.com/tensorflow/tensorflow/blob/b737ee48f1de346151b1e32e4f2d8351071e64d7/tensorflow/compiler/mlir/tensorflow/transforms/tf_savedmodel_passes.td#L93-L113","""could we work at making this not SavedModel specific maybe ?"""
55639,johnlundberg,1456503761,2023-03-06 16:50:37,I'm seeing the same issue with PopulationCount,I'm seeing the same issue with PopulationCount.
45216,ElectricalMichael,1456487807,2023-03-06 16:40:49,"As of now, this issue is still not fixed, correct? In my case, I want to deploy the model on an FPGA and therefore need to export it to a valid .h5 file. For this (and the quantization step), I need to execute the model with correct behaviour regarding the groups arguments, therefore the above solution does not work for me.
Is there any progress? @mohantym @manish-p-gupta","""Is there any progress?"""
59887,mihaimaruseac,1456408291,2023-03-06 16:01:25,This seems like a recent regression,This seems like a recent regression.
45446,PureTryOut,1456259351,2023-03-06 14:38:02,"Note that since then libexecinfo has been removed from Alpine Linux. They had technical reasons but I don't recall them exactly, it's been a while since I looked at this.",libexecinfo has been removed from Alpine Linux.
59828,GatGit12,1456007365,2023-03-06 12:01:44,"@learning-to-play Sorry, I meant native Windows GPU (CUDA) support in my post.
Windows CPU only is unfortunately not an alternative due to the massive increase in compute time.",I meant native Windows GPU (CUDA) support in my post. Windows CPU only is unfortunately not an alternative due to the massive increase in compute time.
38658,tilakrayal,1455923528,2023-03-06 11:04:03,"@leeyeetonn,
As per the documentation, whenever we are trying to use dtype=tf.int32 , it throws a **TypeError** if you try to clip an int to a float value ([tf.cast](https://www.tensorflow.org/api_docs/python/tf/cast) the input to float first) which was intended.
![image](https://user-images.githubusercontent.com/81610181/223092890-98ccf5a5-339b-4f38-add6-390d5ba267ce.png)
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/clip_ops.py","""TypeError"""
59762,zarzaro,1455888190,2023-03-06 10:39:02,"Hi, i got the same issue on Win10 and Visual Studio 2019 and the prebuild Tensorflow lib in Version 2.11 (https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-2.11.0.zip). Also tried older versions of TF like 2.6 and there is the same problem. There is no tensorflow/c/tf_buffer.h. The file doesnt exist.
I downloaded the linux version and here the tf_buffer.h exists.
Please help :(","""I got the same issue on Win10 and Visual Studio 2019 and the prebuild Tensorflow lib in Version 2.11"""
59525,froody,1455521439,2023-03-06 6:17:45,"@sachinprasadhs which recent commit? Looking at master I still see code in the format below, which leaves output_tensor uninitialized when `data_format == ""NDHWC""`
```
ITensorProxyPtr output_tensor;
if (data_format == ""NCHW"") {
...
}
if (data_format == ""NHWC"") {
...
}
params->outputs->push_back(TRT_TensorOrWeights(output_tensor));
```","data_format == ""NDHWC"""
59879,david507huang,1455314332,2023-03-06 2:01:48,Still the same errors. I tried converting the model to frozen graph and then to tflite. But the final tflite model outputs incorrect output and runs very slowly.,I tried converting the model to frozen graph and then to tflite. But the final tflite model outputs incorrect output and runs very slowly.
59578,guscaldas,1454784582,2023-03-04 15:54:22,"> I encountered the same issue a few weeks ago. It was solved by reducing the batch size. I had to go from a batch size of 16 to 4 with 10gb of vram.
I am facing the same error, but I could not solve it by reducing the batch size, even to 1. The curious thing is that this error occurs only on Linux, not on Windows.","I encountered the same issue a few weeks ago. It was solved by reducing the batch size. I had to go from a batch size of 16 to 4 with 10gb of vram. I am facing the same error, but I could not solve it by reducing the batch size, even to 1."
59710,SuperSecureHuman,1454379802,2023-03-04 4:14:53,"I was not able to reproduce the issue again, I'll create a new issue if I get it again.",I was not able to reproduce the issue again
59534,mattbahr,1454356200,2023-03-04 3:19:50,"> Did you try investigating where the error is originating from? I'm not sure why it isn't being propagated with your changes in place.
I was on the wrong track comparing those two functions in `lstm_ops.cc` and `lstm_ops_gpu.cu.cc`. The internal error is originating here:
https://github.com/tensorflow/tensorflow/blob/4139a2ada21479f923a850bf2708dfe077a22254/tensorflow/core/kernels/rnn/blas_gemm.cc#L52-L56",I was on the wrong track comparing those two functions in lstm_ops.cc and lstm_ops_gpu.cu.cc. The internal error is originating here: https://github.com/tensorflow/tensorflow/blob/4139a2ada21479f923a850bf2708dfe077a22254/tensorflow/core/kernels/rnn/blas_gemm.cc#L52-L56.
59818,sayakpaul,1454339671,2023-03-04 2:24:13,"Not even related @gaikwadrahul8. The examples you referenced refer to training, whereas my issue is related to XLA-compiling an off-the-shelf function (in this case which involves a model). > tf.config.optimizer.set_jit(True) or tf.config.optimizer.set_jit('autoclustering')
These two are applied when there's XLA compilation involved in the optimization process, but that is not the case here.",Not even related
59841,WenTheProgrammer,1453901088,2023-03-03 18:03:28,"Hi @NiharJani2002 The only difference between your suggestion and my code is that you divided the image by 255 after converting the numpy array into a tensor: `image = tf.convert_to_tensor(array, dtype=tf.float32) / 255`. I tested this method, and obtained prediction results that are way off from what I obtained using `tf.convert_to_tensor`. It seems like images should not be divided by 255 after tensor conversion.","""I tested this method, and obtained prediction results that are way off from what I obtained using tf.convert_to_tensor."""
59709,priyansh4320,1453663889,2023-03-03 15:03:16,"@tiruk007 ok, now can you please provide me solution to resolve this error . i am not able to move forward in my project.","ok, now can you please provide me solution to resolve this error . i am not able to move forward in my project."
23561,Weili17,1453363877,2023-03-03 11:07:56,"> I think the `-config=c++1z/17` will add `-stdlib=libc++` to the params, which is only recognized by `Clang`, is there some solution to `custom_op` issue when building with GCC 7 and C++17 in Ubuntu 16.04?
> I have similar issue when compiling my custom op with C++17. I did try to compile TF 1.14 with the mentioned flags: `--config=c++1z`, but got hit by compilation errors. Any ideas?
I met the same problem, did you solve it?","""I have similar issue when compiling my custom op with C++17"""
59849,josemalope,1453206237,2023-03-03 9:10:39,"Hi @tiruk007,
The message is expected since we are using CPU only (we already switched to tensorflow-cpu).
However, I created the ticket to point out the inconsistency between the warning message: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
and the level at which it is logged: ERROR
Regards,
Jose","""The message is expected since we are using CPU only"""
59772,shkarupa-alex,1453053957,2023-03-03 6:41:38,"> Till then could you please test it against the published tested configurations as below.
@sachinprasadhs , my test with stable release and cuda 11.2 is here https://github.com/tensorflow/tensorflow/issues/59772#issuecomment-1440047780
// ptxas warnings and DWConv is 2x slower","""ptxas warnings and DWConv is 2x slower"""
59828,adamcrume,1452860970,2023-03-03 2:26:11,"https://github.com/tensorflow/tensorflow/blob/90fda24ac68fffb9d6c3ca8db0268d47b6404735/README.md#official-builds just says ""Status Temporarily Unavailable"", nothing about it being permanently removed. Dropping GPU support on Windows wasn't widely announced. I see https://github.com/tensorflow/tensorflow/releases/tag/v2.11.0 says ""For using TensorFlow GPU on Windows, you will need to install TensorFlow in WSL2"", but that's not even listed under Breaking Changes (which this definitely is!).","""Dropping GPU support on Windows wasn't widely announced"""
52977,JaydenKing32,1452842374,2023-03-03 2:10:27,"I understand that the OCR results may not work that well, but it really shouldn't crash on me. Also I've tried out Google ML Kit, but it wasn't suitable for my use case.
I should clarify that I'm not worried about this issue right now, since I'm currently not working on the project that required the use of OCR.","OCR results may not work that well, but it really shouldn't crash on me."
59837,gzmkl,1452595395,2023-03-02 21:49:59,"I am not sure if my recent two commits (unblock 2 unit tests) caused the ""ARM CI / build (3.10) (pull_request)"" failure.
I could not find much useful info from ""Details""","I am not sure if my recent two commits (unblock 2 unit tests) caused the ""ARM CI / build (3.10) (pull_request) failure"""
59838,SuryanarayanaY,1452158414,2023-03-02 16:27:35,"Hi @trickiwoo , Thanks for your time for reporting this issue.
Basically the issue is coming due to tf.cast which is called by this API. With CPU `tf.cast(np.nan, tf.int32)` returns the min value of `tf.int32` i.e `-2147483648`. But with GPU `tf.cast(np.nan, tf.int32)` casting to `'0'`. This may be due to inconsistent behaviour with `nan` which is an undefined value. The result is coming from `pywrap_tfe.TFE_Py_FastPathExecute`. This type of behaviour expected with garbage data.","""This type of behaviour expected with garbage data."""
59861,shelper,1452002258,2023-03-02 14:53:51,"i am not a bazel guy, played with it a little bit, but i didnt figure out how to access this api_level variable in this bazel file. I do have api_level defined in my root WORKSPACE file as below, but just dont know how can i get it ```
android_ndk_repository(
name=""androidndk"",
path=""/path/to/android-ndk-r21"",
api_level=21,
)
```",api_level = 21
59865,boocheck,1451555716,2023-03-02 9:25:03,* i have NOT reproduced with tf nightly,i have NOT reproduced with tf nightly.
58493,JW1992,1451023470,2023-03-01 23:28:11,Closing it as the underlying cause should have been fixed in [#58369](https://github.com/tensorflow/tensorflow/issues/58369#issuecomment-1373394109). As a verification I cannot reproduce it using the latest version,Closing it as the underlying cause should have been fixed in [#58369](https://github.com/tensorflow/tensorflow/issues/58369#issuecomment-1373394109) as a verification I cannot reproduce it using the latest version.
59675,kaixih,1450966441,2023-03-01 22:44:50,"Basically, there are two changes of this PR: (1) update the download url and (2) update the patch file.
Then, I checked the above automatically created PR https://github.com/openxla/xla/pull/1589, and saw one test ""XLA Linux GPU"" fails.
It seems that the above PR only fetches the change (2) and I don't know how to make it to apply the change (1) as well (I guess it uses a different way to download the zip rather than `workspace2.bzl`.).
@reedwm Can you help or redirect?","""I guess it uses a different way to download the zip rather than workspace2.bzl."""
23689,Leci37,1450169431,2023-03-01 13:39:48,The TFlite models have to have metadata in correct order https://techzizou.com/build-android-app-for-custom-object-detection-using-tf2/#tf2_step20,The TFlite models have to have metadata in correct order.
59815,milincheta,1449672078,2023-03-01 9:23:35,"https://codelabs.developers.google.com/codelabs/recognize-flowers-with-tensorflow-on-android#0
not to mention that once you finish all the steps the whole thing doesn't work :(","""not to mention that once you finish all the steps the whole thing doesn't work"""
56112,zape-aat,1449368836,2023-03-01 5:22:18,"> This error still happens to me. I used tensorflow 2.8.0 [screenshot](https://drive.google.com/file/d/1MxlfuCEFY_CWXKwCVNgNWUIfb9nCymdS/view?usp=sharing) @muja555 @sachinprasadhs Do you have any ideas how to fix it?
I found the recommendation_model_launcher_keras in this repository and it worked! clone this repository instead of the example one.
https://github.com/niap123/ml",I used tensorflow 2.8.0 [screenshot](https://drive.google.com/file/d/1MxlfuCEFY_CWXKwCVNgNWUIfb9nCymdS/view?usp=sharing)
59794,shelper,1449354982,2023-03-01 5:05:08,"I had a different but similar issue that builds for tflite but failed for GPU delegates
It is the final linkage step that raise an error stating `can't find -lnativewindow`..
I remember it worked before but then I pulled the latest commits and forgot where I was before. ...",I had a different but similar issue that builds for tflite but failed for GPU delegates
59084,tilakrayal,1449344508,2023-03-01 4:56:21,"@dmc1778,
I tried to execute the mentioned code on tf-nightly(2.13.0-dev20230228), the crash did not happen when invalid input was provided to ragged_tensor_to_variant. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/a39039cba5adacc7a775d5cf0bd1b1a3/untitled1000.ipynb) and also please find the reference of the ubuntu22.04.
![Screenshot 2023-03-01 10 16 02 AM](https://user-images.githubusercontent.com/81610181/222048309-5e9896b0-deb0-489e-b384-e59a8ec4f617.png)","""The crash did not happen when invalid input was provided to ragged_tensor_to_variant"""
55267,LXP-Never,1449253690,2023-03-01 2:53:47,"I also encountered this problem. From tensorflow 2.8 to 2.11, this problem still exists. Does the official plan to make any fixes?","I also encountered this problem. From tensorflow 2.8 to 2.11, this problem still exists. Does the official plan to make any fixes?"
59534,swachhandl,1449155553,2023-03-01 1:01:30,"> I get the internal error when I run the on the GPU, but no error is produced when I run the same code on the CPU.
Did you try investigating where the error is originating from? I'm not sure why it isn't being propagated with your changes in place.","I get the internal error when I run the on the GPU, but no error is produced when I run the same code on the CPU."
59521,saimanohar1999,1448616703,2023-02-28 17:53:38,"@terryheo still i am facing issues in generating file for sam9x60 which is ARM926 based board , help me in solving this.","still i am facing issues in generating file for sam9x60 which is ARM926 based board , help me in solving this."
58665,alankelly,1447997661,2023-02-28 11:14:02,"Sorry but as I said, the error messages in most cases but this specific case would serve to only confuse users.",The error messages in most cases but this specific case would serve to only confuse users.
59804,reedwm,1446962323,2023-02-27 19:44:00,@philipphack Can you please fix merge conflicts?,Can you please fix merge conflicts?
39479,csuter,1446760930,2023-02-27 17:39:44,"TFP is maintained by a separate group from TF proper. The TFP Layers are not under active development, so we're not planning on implementing this. We welcome PRs on the [TFP github repo](https://github.com/tensorflow/probability); these layers can be finicky to work with, though.",TFP is maintained by a separate group from TF proper.
59815,milincheta,1446533993,2023-02-27 15:25:02,@pjpratik I think its an error on the tutorial. Should be corrected/clarified?,I think its an error on the tutorial. Should be corrected/ clarified?
58400,tucan9389,1446152236,2023-02-27 11:18:31,"Hi @SaoirseARM
We found several failure tests. Could you check it with the following commands and fix those?
```shell
bazel test //tensorflow/compiler/mlir/lite/quantization/lite:quantize_model_test
```
```shell
bazel test //tensorflow/lite/kernels:transpose_conv_test
```","""We found several failure tests. Could you check it with the following commands and fix those?"""
59299,dgscharan,1445482695,2023-02-26 22:17:53,"Hi @SuryanarayanaY @1515236544hit the mirrored worker strategy worked, however the multimirrored strategy is not working. Can you please help me with it ? i have configured accordingly the documentation but i still when i run it on multiple workers, the load was not getting distributed. rather, each worker node is running twice the same code.","""i have configured accordingly the documentation but i still when i run it on multiple workers, the load was not getting distributed. rather, each worker node is running twice the same code."""
59521,saimanohar1999,1445361945,2023-02-26 13:26:27,Thank you...but I tried doing that still i was unable get the expected file... @terryheo,I tried doing that still i was unable get the expected file...
58674,eyadmba,1445136854,2023-02-25 14:58:55,"I'm seeing the same issue. Same environment as OP.
I worked around the issue by just installing tensorflow using pip, and the only thing it did actually install was `tensorflow_intel-2.11.0-cp39-cp39-win_amd64.whl`","I'm seeing the same issue. Same environment as OP. I worked around the issue by just installing tensorflow using pip, and the only thing it did actually install was tensorflow_intel-2.11.0-cp39-cp39-win_amd64.whl."
59117,SuryanarayanaY,1444998558,2023-02-25 5:22:36,Reopening the issue as Bot couldn't remove stall label.,Bot couldn't remove stall label.
59685,matthiaskramm,1444496622,2023-02-24 21:10:26,"I can't find a pass that would already do this (take the exported_names and create a wrapper function for each). The one that comes closest is `tensorflow/compiler/mlir/tfrt/transforms/lower_saved_model.cc`, but it duplicates the function(s) in question which I don't think we should.
Will write a new pass and hook up to `tf-lower-to-mlprogram-and-hlo`.","I can't find a pass that would already do this (take the exported_names and create a wrapper function for each). The one that comes closest is tensorflow/compiler/mlir/tfrt/transforms/lower_saved_model.cc, but it duplicates the function(s) in question which I don't think we should. Will write a new pass and hook up to tf-lower-to-mlprogram-and-hlo."
38877,klanderson,1444388548,2023-02-24 20:08:15,"When you run the colab above (TF 2.11) it still is reproducing the issue
Also, the [documentation ](https://www.tensorflow.org/api_docs/python/tf/keras/backend)doesn't mention in_train_phase anymore. Is it deprecated?","""When you run the colab above (TF 2.11) it still is reproducing the issue"""
59606,jpienaar,1443869494,2023-02-24 15:42:21,I'll check later but the error was around a non-integer/float type being queried from wrong attribute. Could you check with assertions enabled? (I think for this tool you could still build the debug version).,"""I'll check later but the error was around a non-integer/float type being queried from wrong attribute."""
59794,jyh2378,1442977802,2023-02-24 7:43:27,"It seems that the error caused by below code in `tensorflow/tensorflow/lite/delegates/gpu/BUILD`:
```
ios_static_framework(
name = ""tensorflow_lite_gpu_framework"",
hdrs = [
""metal_delegate.h"",
""metal_delegate_internal.h"",
],
minimum_os_version = ""11.4"",
deps = ["":metal_delegate""],
)
``` When I disable this code, the problem is gone. But I think it need to be checked.","ios_static_framework( name = ""tensorflow_lite_gpu_framework"", hdrs = [""metal_delegate.h"", ""metal_delegate_internal.h"", ), minimum_os_version = ""11.4"", deps = ["":metal_delegate""], "
58758,cantonios,1442900252,2023-02-24 7:10:22,"This is behaving as expected. There isn't enough input data to satisfy any ""valid"" pooling region without padding, so there's nothing that can be done. If you want padding, it needs to be specific in the input parameters by setting `padding=SAME'.","""This is behaving as expected."""
59708,Jerry-Ge,1442624435,2023-02-24 0:36:53,"There is actually no implementation of the `dtype.isUnsigned()` [Reference](https://mlir.llvm.org/doxygen/classmlir_1_1quant_1_1QuantizedType.html) . Simply using getStorageTypeMin/Max for much cleaner implementation.
Jerry","""There is actually no implementation of the dtype.isUnsigned()"""
59276,grantjensen,1442548263,2023-02-23 23:05:58,"Hi all, I just ran through the instructions of https://github.com/Bahar-BM/test_openCL with cmake version=3.25.1 & NDK version=22.0.7026061. I ran into the following error when I ran `make`:
[openCL_inf_error_output.txt](https://github.com/tensorflow/tensorflow/files/10819243/openCL_inf_error_output.txt)
Please advise. Perhaps a different NDK version needs to be used?","""I ran into the following error when I ran make:"""
59606,jpienaar,1442478106,2023-02-23 21:52:08,"I see a failure on tensorflow/compiler/mlir/tosa/tests:tfl-to-tosa-pipeline.mlir.test , could you check locally?","I see a failure on tensorflow/compiler/mlir/tosa/tests:tfl-to-tosa-pipeline.mlir.test , could you check locally?"
56937,rposts,1442411853,2023-02-23 20:49:30,Closing - no response.,No response.
59787,wecing,1442369365,2023-02-23 20:10:43,"A better solution might be to clamp to `[-1, 1]` (assuming `clamp(NaN, [-1, 1])` is still `NaN`), then round (away from zero).","""A better solution might be to clamp to [-1, 1] (assuming clamp(NaN, [-1, 1]) is still NaN), then round (away from zero)"""
59678,AndreasMadsen,1442344533,2023-02-23 19:51:20,@google-ml-butler The stalling is not my responsibility and I don't see a reason why this issue should be closed. Maybe you can remove the `stat:awaiting response` label to avoid confusion.,"""stalling"""
59752,rubbberrabbit,1441165361,2023-02-23 3:16:06,"I find this problem still exists in the latest docker images tensorflow/tensorflow:nightly-gpu or in the enviroment using pip to install tf-nightly in my own computer(RTX3090 CUDA 11.6/8.6 Ubuntu 18.04), I am not sure it may be a version release issue. It was confirmed on colab that this issue has been fixed with latest nightly version.","I find this problem still exists in the latest docker images tensorflow/tensorflow:nightly-gpu or in the enviroment using pip to install tf-nightly in my own computer(RTX3090 CUDA 11.6/8.6 Ubuntu 18.04), I am not sure it may be a version release issue. It was confirmed on colab that this issue has been fixed with latest nightly version."
58983,cusniwtt,1440945298,2023-02-22 22:49:00,@zexelon Can you describe environment that you use? I use tensorflow-macos 2.9 on Macbook air M2 has the same problem. I'm stuck with this MaskRCNN.,I'm stuck with this MaskRCNN.
59777,MarkDaoust,1440666325,2023-02-22 19:22:48,"I see the TODO(numpy 1.24) in the setup.py file.
Darn, your other packages won't work with np 1.23?","I see the TODO(numpy 1.24) in the setup.py file. Darn, your other packages won't work with np 1.23?"
59777,bcm-at-zama,1440592396,2023-02-22 18:38:39,"Arg, it still fails:
`Because tensorflow (2.12.0rc0) depends on numpy (>=1.22,<1.24)`
ie, tf doesn't want the new numpy.","Arg, it still fails: Because tensorflow (2.12.0rc0) depends on numpy (>=1.22,1.24)"
59743,yufang67,1440002216,2023-02-22 13:17:21,"@SuryanarayanaY I dont think mixed_bfloat16 is for TPUs only (A100 support bfloat16). As more and more layers support bfloat16 on GPU, i would like to test the gain for the whole model. And the above activations and layers are blockers. thats why i open this feature request. if i understand you correctly, do you means that there is no plan to support bfloat16 for LSTM layer ?
Thanks","""And the above activations and layers are blockers."""
59774,elfringham,1439905742,2023-02-22 12:01:38,"I believe that this may be masking some unit test failures as well.
//tensorflow/python/kernel_tests/proto:decode_proto_op_test
//tensorflow/python/util/protobuf:protobuf_compare_test
//tensorflow/tools/api/tests:api_compatibility_test
These fail with protobuf > 3.20.3",I believe that this may be masking some unit test failures as well.
59773,elfringham,1439788615,2023-02-22 10:35:24,This does not happen with protobuf==3.20.3 installed. However the default install currently will be protobuf==4.22.0,This does not happen with protobuf==3.20.3 installed. However the default install currently will be protobuf==4.22.0.
59743,SuryanarayanaY,1439775669,2023-02-22 10:26:29,"Hi @yufang67 ,
`bfloat16` data type specifically designed for TPUs only and on GPUs this wont work and hence throwing user error as `ValueError:Value passed to parameter 'input' has DataType bfloat16 not in list of allowed values: float16, float32, float64` which is intended behaviour.
Please use `mixed_bfloat16` for TPUs environment only. For GPUs you should use `mixed_float16` .
Thanks!","""ValueError:Value passed to parameter 'input' has DataType bfloat16 not in list of allowed values: float16, float32, float64"""
59499,Wanzizhu,1439273110,2023-02-22 0:35:51,"Close this PR, as it's same to [PR59766](https://github.com/tensorflow/tensorflow/pull/59766)","Close this PR, as it's same to [PR59766](https://github.com/tensorflow/tensorflow/pull/59766)"
59633,mraunak,1439223860,2023-02-21 23:27:22,"Hi @gbaned, this file ( tensorflow/python/keras/engine/training.py) is getting called when the user is executing his code in the issue mentioned which resides inside the TensorFlow. I am not sure how fixing in Keras repo would fix this issue.","""I am not sure how fixing in Keras repo would fix this issue."""
59765,elfringham,1439182976,2023-02-21 22:38:13,"Changing the installed version of the Python protobuf package did not make a difference, 3.20.1, 3.20.3, 4.21.9, 4.21.12 all the same failure. This is perhaps not surprising as the tests are c++ based.","Changing the installed version of the Python protobuf package did not make a difference, 3.20.1, 3.20.3, 4.21.9, 4.21.12 all the same failure."
59765,elfringham,1439181698,2023-02-21 22:36:45,The start of these failures was bisected to https://github.com/tensorflow/tensorflow/commit/84f40925e929d05e72ab9234e53c729224e3af38,The start of these failures was bisected to https://github.com/tensorflow/tensorflow/commit/84f40925e929d05e72ab9234e53c729224e3af38.
59221,AnandInguva,1439144204,2023-02-21 21:59:13,"Is Tensorflow 2.12 gonna support Protobuf>=4.21.0?
I am working on updating protobuf dependency in Apache Beam but Apache Beam has a dependency on Tensorflow. So we are not able to make the update of Protobuf on tensorflow==2.11.0",I am working on updating protobuf dependency in Apache Beam but Apache Beam has a dependency on Tensorflow. So we are not able to make the update of Protobuf on tensorflow==2.11.0.
59390,zichuan-wei,1439077241,2023-02-21 20:53:18,"1 is correct, for 2, there are some complexities, the original model in keras is taking float32 as input & output, as such ""my_weight"" has to be float32 to enable the correct execution in keras. as such, when we go to 3, it has to be treated as float even in the quantized execution, because how the variable tensor is initialized. My understanding is that if you enforce ""my_weight"" as a int8, then it's no longer a valid graph in keras.","""my_weight"" has to be float32 to enable the correct execution in keras."
58749,cantonios,1438884877,2023-02-21 17:54:40,"We make no guarantees that CPU and GPU results are identical, especially for garbage data. The input doesn't crash, so it's not a security issue. Error checking is expensive.
The GPU result is flushing all results to the max value (essentially saturating the input). We could potentially do the same on CPU. I wouldn't say it's a requirement though.","""The GPU result is flushing all results to the max value (essentially saturating the input)"""
58934,rivershah,1438685079,2023-02-21 15:34:04,Please reopen. The proposed solution does not work for TPU pods. Please try this issue on a pod (I am unable to provide a colab example for pods). This issue should work with `TPUStrategy`,The proposed solution does not work for TPU pods. Please try this issue on a pod (I am unable to provide a colab example for pods). This issue should work with TPUStrategy.
59732,JackTemaki,1437490130,2023-02-20 19:58:31,"Opening an `ipython` session with just:
```python3
In [1]: import tensorflow as tf
In [2]: x = tf.constant(1)
```
Is sufficient. This will block the GPU and show the difference in memory allocation in both the log message and in `nvidia-smi`.
I also tried `export TF_FORCE_GPU_ALLOW_GROWTH=true` but this did not change anything.","""This will block the GPU and show the difference in memory allocation in both the log message and in nvidia-smi."""
58032,johnnynunez,1437359378,2023-02-20 17:38:03,"> Seems that TF [v2.12](https://github.com/tensorflow/tensorflow/releases/tag/v2.12.0-rc0) (currently pre-release) is compatible with Python 3.11.
but not with cuda 11.8/12 and cudnn 8.8... :(",:(
59696,taylora-mitre,1437099009,2023-02-20 14:20:46,"@gaikwadrahul8 Yes, that's correct. I am expecting the train and validation loss to be the same, since I'm using identical datasets. Or, if not perfectly identical, at least very close -- I am not expecting them to differ by several orders of magnitude as in your gist and mine","I am expecting the train and validation loss to be the same, since I'm using identical datasets. Or, if not perfectly identical, at least very close -- I am not expecting them to differ by several orders of magnitude as in your gist and mine."
59595,RoboTux,1436952077,2023-02-20 12:42:22,"1. Delete bazel cache
2. Run `bazel build --config=dbg -c opt tensorflow/compiler/mlir:tf-opt`
3. ls bazel-out/*-opt/bin/external/llvm-project/llvm/lib/Target
Which for me shows AArch64, AMDGPU, ARM and X86. I have a WIP patch that allow me to select the targets I want enabled and have for instance only AArch64.","""I have a WIP patch that allow me to select the targets I want enabled and have for instance only AArch64.."""
59722,albertz,1436614003,2023-02-20 9:23:39,"Thanks for creating the gist.
However, as I said, it is crucial to run it on Apple M1 hardware to reproduce it. In your gist, I see that you use an Nvidia GPU.
Also, you did not exactly use the commits I specified, although this probably should not matter.","""You did not exactly use the commits I specified, although this probably should not matter."""
59721,pjpratik,1436545681,2023-02-20 8:33:35,"@vishnukvmd I have tried to build in MacOS with the command provided and I have received different error. Please refer the below screenshot. Thanks!
<img width=""1037"" alt=""Screenshot 2023-02-20 at 1 23 32 PM"" src=""https://user-images.githubusercontent.com/118897289/220045388-e4f9f5cf-2dda-437d-86fe-5d0e4434af27.png"">",I have tried to build in MacOS with the command provided and I have received different error.
59713,cevheck,1436537093,2023-02-20 8:25:45,"I've debugged the issue further and the origin of the slow results was due to the usage of lists. The resulting behaviour remains weird however, being the increase in computation time in each new call of the model.call. A quick and easy fix was to replace the lists by tf.constants.
list = [0, 0.8217, 1.03, 5.12, 5.775, 5.938, 14.7207]
becomes
list = tf.constant([0, 0.8217, 1.03, 5.12, 5.775, 5.938, 14.7207])",I've debugged the issue further and the origin of the slow results was due to the usage of lists.
59735,CuiDonCai,1436224595,2023-02-20 2:39:23,"Standalone code `y = tf.keras.layers.Cropping1D(cropping=(-1,0))(x)` parameter should be -1",-1
58881,svenstaro,1435993940,2023-02-19 13:47:20,This isn't upstreamed. Why is this closed? Current master branch still doesn't compile with gcc 12.,"""Why is this closed?"""
41009,ZGJY95,1435951777,2023-02-19 10:34:49,"I got same error with my code. And I fixed it with adding following code before `model.fit`.
```
model.build([None, 32,32,3])
model.call(tf.keras.Input(shape=(32,32,3)))
```",I got same error with my code. And I fixed it with adding following code before model.fit.
59692,turgut-baba,1435942877,2023-02-19 9:54:43,"@pjpratik On a side note, I've found some pre-compiled versions of the library on the internet from 2021, it runs just fine on it's own but when I use it inside my project, which uses threading on android, it gives out an error. (I've set `setnumThreads` to 2.). I assume this issue is addressed in the newer releases but since I can't compile it on my own I'm stuck. Also: If I compile the .so files with NDK 19, can I use that in my NDK 25 > project?","I've found some pre-compiled versions of the library on the internet from 2021, it runs just fine on it's own but when I use it inside my project, which uses threading on android, it gives out an error. (I've set setnumThreads to 2.) I assume this issue is addressed in the newer releases but since I can't compile it on my own I'm stuck. Also: If I compile the .so files with NDK 19, can I use that in my NDK 25 > project?"
58866,Rishi-11-2,1435713597,2023-02-18 16:41:18,"With tensorflow-directml plugin , I was able to make tensorflow utilise AMD GPU . However when I tried to run LSTM , it gave an error","""I was able to make tensorflow utilise AMD GPU . However when I tried to run LSTM , it gave an error."""
58866,Rishi-11-2,1435713349,2023-02-18 16:40:01,"The problem is that I used a direct-ml plugin so tensorflow can utilise the AMD GPU . But I don't know why , cuda LSTM is being executed which is not supported by AMD GPU and therefore this error is being thrown","I don't know why , cuda LSTM is being executed which is not supported by AMD GPU and therefore this error is being thrown."
58866,bakikocabasa,1435699079,2023-02-18 15:22:29,"it is really annoying, there is no such source to find a solution for this error. I keep having it. What I guess is that it might be library conflict problem that causes this one.",I keep having it.
59300,mihaimaruseac,1434882969,2023-02-17 16:30:11,This seems to be an issue in TF Probability.,This seems to be an issue in TF Probability.
58762,SuryanarayanaY,1434497058,2023-02-17 11:17:08,"Hi @SajjadAemmi ,
The Error coming from this piece of [code](https://github.com/keras-team/keras/blob/r2.11/keras/utils/audio_dataset.py#L171-176).
It is expecting `tensorflow_io` imported as `tfio`.
Could you try adding the import statement like below and confirm whether it works for you ?
`import tensorflow_io as tfio
`",Error coming from this piece of code.
59521,saimanohar1999,1434142524,2023-02-17 6:01:00,"thank you @sachinprasadhs i already gone through the references which you shared, but the issue is tflite_runtime-2.13.0-cp38-cp38-linux_x86_64.whl is the file i am getting but its seems to be 64-bit , even i gave armhf while building its giving 64-bit","i already gone through the references which you shared, but the issue is tflite_runtime-2.13.0-cp38-cp38-linux_x86_64.whl is the file i am getting but its seems to be 64-bit , even i gave armhf while building its giving 64-bit."
59506,nyadla-sys,1433973434,2023-02-17 1:35:01,"I have yet to find support for rsqrt int16 on TFLM (TensorFlow Lite micro).
Refer the below link for more details
https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/kernels/elementwise.cc#L61
bool IsRsqrtSupportedType(const TfLiteType type) {
return type == kTfLiteFloat32 || type == kTfLiteInt8;
}",I have yet to find support for rsqrt int16 on TFLM (TensorFlow Lite micro)
59511,innat,1433956388,2023-02-17 1:13:55,"Set it true but still gives error.
```
tf.config.set_soft_device_placement(
    enabled
)
```
How to place on CPU devices (colab or kaggle)? ```
tf.config.list_logical_devices('TPU')
```
Only returns the available tpu node. I also tried `tf.device('CPU'):`, but no luck.","tf.config.set_soft_device_placement( enabled )  How to place on CPU devices (colab or kaggle)?  tf.config.list_logical_devices('TPU')  Only returns the available tpu node. I also tried tf.device('CPU'):, but no luck."
59511,jszaday,1433938736,2023-02-17 0:49:33,"This program exhibits two behaviors presently unsupported by TPUs:
1) Variables with unknown shapes.
a) ""TPUs do not have registered OpKernel support for [tf.raw_ops.]Shape.""
2) Variables whose shapes change over time.
This error is appearing due to the first behavior, but resolving that will induce the second.
My guidance would be to rewrite this program to avoid these behaviors, if possible. For example, you could place these variables on the host CPU.","""TPUs do not have registered OpKernel support for [tf.raw_ops.]Shape."""
59472,sachinprasadhs,1433871769,2023-02-16 23:19:01,"I don't think subclassing a `tf.Tensor` is doable in Tensorflow with the existing design, since Tensorflow tensors are immutable.
I would go with the suggestion mentioned here https://github.com/tensorflow/tensorflow/issues/59472#issuecomment-1408120513","I don't think subclassing a tf.Tensor is doable in Tensorflow with the existing design, since Tensorflow tensors are immutable. I would go with the suggestion mentioned here https://github.com/tensorflow/tensorflow/issues/59472#issuecomment-1408120513."
59390,zichuan-wei,1433759918,2023-02-16 21:52:03,"Hi @MalekItani Looking at your code, I think this is expected behaviour. The input and weight to the graph are both float32, and since every time after the execution, you're writing the results back to ""My_weight"" we have to do the dequant and quant to ensure the datatype and value are still valid. If you didn't need to write back to the weight, I would recommend you use tf.const instead for weight. Please let us know this is a valid fix for you","""Looking at your code, I think this is expected behaviour."""
59117,DEKHTIARJonathan,1433726712,2023-02-16 21:18:03,Do not close the issue. @qqfish didn't reply !,"""@qqfish didn't reply"""
59704,nrwahl2,1433707160,2023-02-16 20:59:34,"Would this be undefined by the specification anyway? The [tf.image.convert_image_dtype](https://www.tensorflow.org/api_docs/python/tf/image/convert_image_dtype) documentation says floating-point values are expected to be in the range `[0, 1)`. If so, then I'd expect inconsistent behavior and perhaps a warning message for out-of-range values.","""Would this be undefined by the specification anyway?"""
58998,mihaimaruseac,1433605811,2023-02-16 19:30:17,This has been suprseeded by internal code changes and also #59703,This has been suprseeded by internal code changes and also #59703
59715,DEKHTIARJonathan,1433604177,2023-02-16 19:28:46,@rjpower seems like your change in 8a216d2c29881745c9851dd3e22b11eca56b96c3 introduced a bug,"""Your change in 8a216d2c29881745c9851dd3e22b11eca56b96c3 introduced a bug"""
59703,mihaimaruseac,1433465031,2023-02-16 17:38:03,"Internally the imports get rewritten based on where the code is situated in the Google monorepo and the type of the code (OSS -> third_party). Plus, code linters want the imports sorted.","""Internally the imports get rewritten based on where the code is situated in the Google monorepo and the type of the code (OSS -> third_party). Plus, code linters want the imports sorted."""
55418,mihaimaruseac,1433321146,2023-02-16 16:01:23,The docker containers will be moved in a different direction. I think we should instead close this.,The docker containers will be moved in a different direction.
59666,turgut-baba,1432725693,2023-02-16 8:50:51,"I've deleted all the files, git clone again, checkout to the branch you linked, did everything the same, same result.","I've deleted all the files, git clone again, checkout to the branch you linked, did everything the same, same result."
59534,mattbahr,1432504728,2023-02-16 4:44:33,"@swachhandl I confirmed that when run on the CPU, no error is thrown, but I'm having a hard time finding a way to get the GPU code to behave in same way. Would it be valid to mark this test case to run only on GPU?",I'm having a hard time finding a way to get the GPU code to behave in same way.
59619,DEKHTIARJonathan,1432444248,2023-02-16 3:26:00,@k-w-w we tried to register an OP that can define or not an optional `func`.,"""We tried to register an OP that can define or not an optional func.."""
59506,Burton2000,1431667310,2023-02-15 16:45:45,"Getting the same as @nyadla-sys when trying to convert the same model.
From [here](https://www.tensorflow.org/mlir/tfl_ops#tflrsqrt_mlirtflrsqrtop) it seems that int16 should be supported for inference on RSQRT though.",Getting the same as @nyadla-sys when trying to convert the same model.
59407,mihaimaruseac,1431551222,2023-02-15 15:29:31,Still waiting on file name changes,Still waiting on file name changes.
58676,DrChrisLevy,1431264088,2023-02-15 12:04:47,"> Its still being looked into. Does the workaround suggested here - https://github.com/tensorflow/tensorflow/issues/58676#issuecomment-1341147862 work for you until we have an resolution?
It would be a pretty big refactor to do this. Also correct me if I'm wrong but aren't TF datasets more for setting up efficient pipelines to utilize CPU and GPU when training models etc. Here we are just talking about inference and serving models.",Its still being looked into.
59675,pgpetrak,1430291135,2023-02-14 19:49:22,v0.8 is not available at https://github.com/NVIDIA/cudnn-frontend at the main branch. I cannot proceed until it has been updated.,v0.8 is not available at https://github.com/NVIDIA/cudnn-frontend at the main branch.
59617,mihaimaruseac,1430241208,2023-02-14 19:04:56,Need to be removed from CI too. This PR is not sufficient.,Need to be removed from CI too. This PR is not sufficient.
59685,jpienaar,1430204771,2023-02-14 18:38:28,@matthiaskramm for visibility (can't seem to assign you),Can't seem to assign you.
59355,kenfranko,1430188800,2023-02-14 18:27:27,In this case there is no security issue. What occurs is that -67794891775896 overflows to 1166991464. The output shape of unsorted_segment_sum is then (1166991464) which for a float64 is approximately 8GB of memory. This leads to OOM if there is less than 8GB of memory available.,-67794891775896 overflows to 1166991464.
59355,sachinprasadhs,1430188782,2023-02-14 18:27:26,"@dmc1778 , This is the intended behavior, as per the discussion this behavior is because of `-67794891775896` overflows to `1166991464`. The output shape of `unsorted_segment_sum` is then (`1166991464`) which for a `float64` is approximately 8GB of memory. This leads to OOM if there is less than 8GB of memory available.
In some colab runtimes this succeeds and others it fails due to different amounts of memory available.",67794891775896 overflows to 1166991464
59671,AndreasMadsen,1430067671,2023-02-14 16:54:18,@pjpratik I think you should add the `comp:xla` label too. As this bug only occurs when XLA is enabled.,comp:xla label too. As this bug only occurs when XLA is enabled.
59671,AndreasMadsen,1430065672,2023-02-14 16:52:45,"In this gist, I have removed the dependency on `transformers` and a large amount of irrelevant code:
V1: https://gist.github.com/AndreasMadsen/dc5785b5a55bf740c555b2fb5cdab1db (~600 LOC)
V2: https://gist.github.com/AndreasMadsen/5fdaa8431929e25cf3a990f234f88a8c (update, more code removed. ~450 LOC)
V3: https://gist.github.com/AndreasMadsen/2590423c055bc47a932a449a5161bac7 (update, more code removed. 209 LOC)
I have tried to reduce the code further, but this causes the bug to disappear.","I have tried to reduce the code further, but this causes the bug to disappear."
20690,ngjunlong9651,1429078061,2023-02-14 3:59:49,"> did any one resolve import error issue? ![Screen Shot 2023-02-14 at 12 15 41 AM](https://user-images.githubusercontent.com/46056249/218595820-1a3dd8af-d8ba-4b98-9866-682833191806.png)
I have this issue as well. Even after changing the async -> async1",I have this issue as well. Even after changing the async -> async1.
59636,mihaimaruseac,1428874146,2023-02-13 23:50:22,We cannot accept PRs on versions that are out of life.,We cannot accept PRs on versions that are out of life.
59247,vufg,1428804274,2023-02-13 22:44:03,There isn't any updates for 3 weeks on fixing the failed test case (added from this PR). So will close this PR for now. Feel free to reopen when the test is fixed. Thanks,"""There isn't any updates for 3 weeks on fixing the failed test case (added from this PR). So will close this PR for now."""
59273,terryheo,1428766170,2023-02-13 22:14:29,Updating build toolchain makes a regression. I've reverted the change and will revisit this.,Updating build toolchain makes a regression.
58270,mihaimaruseac,1428479396,2023-02-13 18:52:31,"2.9 is out of life, no longer updated. Always try to fix on master branch, test if the issue reproduces with nightly.","2.9 is out of life, no longer updated."
59666,turgut-baba,1428011081,2023-02-13 14:15:24,I've tried building with `bazel build -c opt //tensorflow/lite:libtensorflowlite.so --fat_apk_cpu=arm64-v8a` but now it says `<path to .so> is not compatible with aarch64linux` . I couldn't build with `bazel build -c opt //tensorflow/lite:libtensorflowlite.so --cpu=arm64-v8a` because I got `cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'arm64-v8a'` error.,I've tried building with bazel build -c opt //tensorflow/lite:libtensorflowlite.so --fat_apk_cpu=arm64-v8a but now it says path to .so> is not compatible with aarch64linux . I couldn't build with bazel build -c opt //tensorflow/lite:libtensorflowlite.so --cpu=arm64-v8a because I got cc_
59654,thewizardnet,1427251867,2023-02-13 2:46:36,"Still don't work ""'Gravity' object has no attribute '_set_hyper'""","""Gravity"" object has no attribute '_set_hyper'."
58270,jphwang212,1427173206,2023-02-13 0:08:53,"I am attempting to reproduce the environment. I am using a virtual machine arm64 ubuntu 20.04. When doing a pip install for tensorflow==2.9,
```python
ERROR: Could not find a version that satisfies the requirement tensorflow==2.9.0 (from versions: 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0)
ERROR: No matching distribution found for tensorflow==2.9.0
```
Before moving forward, am I allowed to use a later version like 2.10?
Thank you!","python ERROR: Could not find a version that satisfies the requirement tensorflow==2.9.0 (from versions: 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0) "
59654,thewizardnet,1426951133,2023-02-12 6:02:38,"Thank you, sir, but still not working.","""still not working"""
59597,alkatar21,1426680062,2023-02-11 9:58:01,"@dburian @sachinprasadhs I think the Issue here will probably solved with https://github.com/microsoft/pylance-release/issues/3937? Which has the start of https://github.com/python/typeshed/issues/7144 as an underlying reason.
I mean tensorflow has other autocompletion problems, but at least the apearance of nothing working with pyright version 1.1.292 is probably caused by that.","I mean tensorflow has other autocompletion problems, but at least the apearance of nothing working with pyright version 1.1.292 is probably caused by that."
57956,JXRiver,1426255313,2023-02-10 19:40:56,"Looks like the pywrap_gradient_exclusions.cc test still fails with
```
tensorflow/python/eager/pywrap_gradient_exclusions.cc:420:4: error: too many initializers for 'std::__array_traits<{anonymous}::OpIndexInfo, 365>::_Type' {aka '{anonymous}::OpIndexInfo [365]'}
420 | }};
| ^
```
It is unclear to me what is the cause. Maybe sync to master and rerun the script?",looks like the pywrap_gradient_exclusions.cc test still fails
59440,mihaimaruseac,1426001781,2023-02-10 15:49:57,(that's the reason I closed this as not actionable),That's the reason I closed this as not actionable.
59431,codingzebra333,1425702947,2023-02-10 11:50:15,"Hey, I tried different approach and got ValueError: Could not open tflite model. Here is the code: https://colab.research.google.com/drive/14zMCm0XaszVrUoWd9bMvBjSXSdPvSWuw#scrollTo=o3lYXU2CwMoG","""I tried different approach and got ValueError: Could not open tflite model."""
57956,philipphack,1425036794,2023-02-10 1:04:39,I updated goldens and the gradient exclusions. I still see a problem with the doctest which doesn't seem to be caused by this change.,I updated goldens and the gradient exclusions. I still see a problem with the doctest which doesn't seem to be caused by this change.
59626,carloshmorales,1424914191,2023-02-09 22:20:47,"We ran into this as well. Intuitively, it looks like audio generation in general will run into perceptual issues at 8 bits.","Intuitively, it looks like audio generation in general will run into perceptual issues at 8 bits."
43648,Ranazzi,1424650329,2023-02-09 18:38:21,Having this same trouble using Tensorflow 2.8.3,Having this same trouble using Tensorflow 2.8.3.
57936,hhb,1423705391,2023-02-09 6:33:26,For anyone came here: this is a duplicate of https://github.com/tensorflow/tensorflow/issues/58368. Probably an issue in `ld`.,ld.
59622,YuliyaPylypiv,1423485404,2023-02-09 1:35:48,"Closing, tensorflow_datasets 3.1.0 is too old","Closing, tensorflow_datasets 3.1.0 is too old."
59615,nfelt,1423182354,2023-02-08 20:13:18,"This can't be merged until we actually have published a 2.12 release of TensorBoard. We're working on it right now and it should be ready later today, but please do not merge this until we let you know it's ready.","""We're working on it right now and it should be ready later today, but please do not merge this until we let you know it's ready."""
59514,mihaimaruseac,1423013948,2023-02-08 17:50:55,Commit needs to be cherrypicked to 2.12 branch first,Commit needs to be cherrypicked to 2.12 branch first.
59327,joeyearsley,1422797303,2023-02-08 15:31:49,"Tried in v2.11 and allowing cuda to see the 2nd GPU, but neither worked.","Tried in v2.11 and allowing cuda to see the 2nd GPU, but neither worked."
59327,joeyearsley,1422731102,2023-02-08 15:00:13,"I've tried setting numerous `XLA_PYTHON_CLIENT_MEM_FRACTION` values from 2.0 to 16.0, the issue still persists.","I've tried setting numerous XLA_PYTHON_CLIENT_MEM_FRACTION values from 2.0 to 16.0, the issue still persists."
59435,tomytsai1,1421452820,2023-02-07 21:07:56,"The tests ""Code Check - Changed Files"" and ""Py+CPP Test Suite - Ubuntu CPU, Python 3.9"" seemed to be stuck... Will see how to restart them for validating the change.","The tests ""Code Check - Changed Files"" and ""Py+CPP Test Suite - Ubuntu CPU, Python 3.9"" seemed to be stuck..."
59163,mihaimaruseac,1421352085,2023-02-07 19:48:07,"Looks like an issue on cleanup side.
@learning-to-play",Looks like an issue on cleanup side.
59179,mihaimaruseac,1421351506,2023-02-07 19:47:34,Typical OOM,Typical OOM
59325,mihaimaruseac,1421349509,2023-02-07 19:45:44,not actionable.,not actionable.
59341,mihaimaruseac,1421349284,2023-02-07 19:45:31,"Typical OOM, not a vuln","Typical OOM, not a vuln."
59359,mihaimaruseac,1421345345,2023-02-07 19:41:49,"Clear OOM, not a vuln","Clear OOM, not a vuln."
59365,mihaimaruseac,1421344623,2023-02-07 19:41:11,Closing as fixed. No credit,No credit.
59401,mihaimaruseac,1421338691,2023-02-07 19:35:48,"Fixed, no credit.","""No credit"""
59415,mihaimaruseac,1421337406,2023-02-07 19:34:37,"@tilakrayal this is a vulnerability report (which should not be on GitHub, anyway). @learning-to-play","""this is a vulnerability report"""
59439,mihaimaruseac,1421336429,2023-02-07 19:33:40,"Not actionable, closing. Please report using the right channels.","Not actionable, closing. Please report using the right channels."
59506,nyadla-sys,1421287059,2023-02-07 18:53:53,"@mohantym I used PTQ(post training quantization )with representative data set and generated int8 model,however it is producing wrong results,Soon I will share colab notebook here","I used PTQ(post training quantization )with representative data set and generated int8 model,however it is producing wrong results."
59583,mihaimaruseac,1421265968,2023-02-07 18:35:24,"Internal error is a valid error message if arguments are invalid.
In this case, you are trying to run a kernel on a device that does not support it with the given types","""Internal error"""
59594,mihaimaruseac,1421261726,2023-02-07 18:31:42,"Please use better commit messages and PR titles.
https://cbea.ms/git-commit/","""Please use better commit messages and PR titles."""
57591,poulsbo,1420978255,2023-02-07 15:36:48,"@pjannaty
Hi Pooya, could you please review (or mark as reviewed by NVIDIA) this PR? If I review it here in Github, then I will need to a find a separate reviewer on the internal Google side, which will slow things down. It's best if I do the reviews on the Google side, and NVIDIA staff (yourself or Nathan representing NVIDIA) handle the Github side.","If I review it here in Github, then I will need to a find a separate reviewer on the internal Google side, which will slow things down."
59521,saimanohar1999,1420499597,2023-02-07 9:59:51,the Pintos repo mentioned above dosent have any .whl file for ARM9 32-bit architecture.,dosent have any .whl file for ARM9 32-bit architecture.
59592,mengran1234,1420483280,2023-02-07 9:47:42,"before above error, it happened
ERROR: --experimental_link_static_libraries_once=false :: Unrecognized option: --experimental_link_static_libraries_once=false
.so I delete experimental_link_static_libraries_once option","""Error: --experimental_link_static_libraries_once=false :: Unrecognized option: --experimental_link_static_libraries_once=false .so I delete experimental_link_static_libraries_once option."""
59450,mihaimaruseac,1419973834,2023-02-07 0:11:40,Closing as reported wrongly. Not a vuln.,Closing as reported wrongly.
59573,JaviBonilla,1418700062,2023-02-06 8:33:36,"I think it is related to the hostlist expansion, fix here #58033, because your nodelist is:
```
node[01-04]
```
But Tensorflow got:
```
node1, node2, node3 and node4.
```
Instead of:
```
node01, node02, node03 and node04.
```","I think it is related to the hostlist expansion, fix here #58033."
59574,tatwaichong,1418660871,2023-02-06 8:03:12,need to work with a counterpart change in mlir: https://reviews.llvm.org/D143312,need to work with a counterpart change in mlir: https://reviews.llvm.org/D143312
58966,mbarnes13,1418491209,2023-02-06 4:18:05,"Hi @rchao. Unfortunately I don't have the cycles to contribute a fix right now. FYI, I'm also FTE at Google. Mostly using this to track the issue.","""Unfortunately I don't have the cycles to contribute a fix right now."""
59077,pozil,1418209526,2023-02-05 18:34:28,"Hi, I'm facing the same issue: `pip install tflite-model-maker` is fetching nightly builds non-stop.
macOS Ventura 13.1.
pip 23.0 running on python 3.10
I'm using venv for the install (I don't want to use a global install)",I'm facing the same issue: pip install tflite-model-maker is fetching nightly builds non-stop. macOS Ventura 13.1. pip 23.0 running on python 3.10 I'm using venv for the install (I don't want to use a global install)
57679,johnnynunez,1417653651,2023-02-05 12:12:49,"> i don't understand the export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/python3.8/site-packages/tensorrt/ part. could you please tell me exactly which path i have to export and where,
because you install cuda and tensorrt prebinaries from anaconda. Conda prefix is where your SO path.
Ubuntu is like /home/user/...
windows is like C:\ProgramFiles\...
You must search where is your anaconda folder","""I don't understand the export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/python3.8/site-packages/tensorrt/ part."""
36360,ofiryaish,1416828418,2023-02-04 19:10:04,"@jsimsa Since `tf.data.experimental.TFRecordWriter` IS DEPRECATED, tf.io.TFRecordWriter should be the only way to create TFRecords, and it does not scale well. Is there any solution for @SmileTM question?","""IS DEPRECATED, tf.io.TFRecordWriter should be the only way to create TFRecords, and it does not scale well."""
49725,mlukianska,1416795634,2023-02-04 16:35:22,"For me worked this code:
**!pip install tensorflow-gpu==2.4.1** (!pip install tensorflow==2.4.1 - this didn't work, it didn't downgrade the version)
import tensorflow as tf
print(tf.__version__)","""This didn't work, it didn't downgrade the version"""
59423,johnthagen,1416737498,2023-02-04 12:17:56,"@terryheo Should 3.10 wheels have been published for `tflite-runtime-nightly` after this was merged? I don't see them. Does something else also need to be updated to get them published?
- https://pypi.org/project/tflite-runtime-nightly/2.13.0.dev20230203/#files",I don't see them. Does something else also need to be updated to get them published?
46976,tsdeng,1416663846,2023-02-04 5:07:05,This problem still persist. Can someone help reopen this issue?,This problem still persist.
59563,ke1ding,1416651405,2023-02-04 3:49:40,"Eigen test also breaks after eigen update.
Before:
Executed 4597 out of 4623 tests: 4050 tests pass, 26 fail to build and 547 fail locally.
After:
Executed 2640 out of 4629 tests: 2227 tests pass, 1989 fail to build and 413 fail locally.","""Eigen test also breaks after eigen update"""
36072,atronchi,1416549215,2023-02-04 0:13:49,"I was suffering this same problem using Tensorflow 2.11, Driver Version: 470.57.02, CUDA Version: 11.4, when using Adam with `amsgrad=True` and `batch_size=2**14` on an 8GPU host using tf.distribute.MirroredStrategy. Using normal Adam with `amsgrad=False` had no issues. Wanted to point out here for me a simple workaround to make `amsgrad=True` work was setting `jit_compile=False`, perhaps suggesting that this issue is at least sometimes related to this feature.","I was suffering this same problem using Tensorflow 2.11, Driver Version: 470.57.02, CUDA Version: 11.4, when using Adam with amsgrad=True and batch_size=2**14 on an 8GPU host using tf.distribute.MirroredStrategy. Using normal Adam with amsgrad=False had no issues. Wanted to point out here for me a simple workaround to make amsgrad=True"
57956,philipphack,1416326861,2023-02-03 19:45:54,NNTest and TPUEmbeddingForServingTest were still calling `embedding_lookup_ragged`. The other test failures in the log don't seem to be related to this change.,The other test failures in the log don't seem to be related to this change.
59442,mihaimaruseac,1416265982,2023-02-03 18:50:15,I just went over these stale reports as the team responsible with security (my former team) has failed to react to them in time.,"""my former team"" has failed to react to them in time."
59442,mihaimaruseac,1416260809,2023-02-03 18:44:52,"This is not a real vulnerability. Closing.
@dmc1778 please stop posting vulns on GitHub. Please consult SECURITY.md for how to properly and ethically disclose them.",This is not a real vulnerability. Closing.
59349,mihaimaruseac,1416259096,2023-02-03 18:42:58,"This is a vulnerability that got fixed. Since it was not reported on the proper channels, no credit is given. @dmc1778 please stop posting vulns on GitHub. Please consult SECURITY.md for how to properly and ethically disclose them.","""since it was not reported on the proper channels, no credit is given."""
59497,mihaimaruseac,1416258446,2023-02-03 18:42:14,"3 commits for a single line change? Can you please merge the commits in just one?
In general, we don't want to insert all and every links to the README. There are too many and some don't have the same quality as others. It takes too much time to evaluate the materials, so it is better to only include links that Google has vetted. As such, I don't think this PR is worthwhile.",3 commits for a single line change?
59441,mihaimaruseac,1416256704,2023-02-03 18:40:31,"This is a vulnerability that needs fixing.
@sushreebarsa Given that this is a vulnerability, don't be too eager to close it (by adding the label).
@dmc1778 please stop posting vulns on GitHub. Please consult SECURITY.md for how to properly and ethically disclose them.","""Don't be too eager to close it (by adding the label)"""
59124,mihaimaruseac,1416255644,2023-02-03 18:39:26,"This is a vulnerability that needs fixing. Please make sure the stalleness labels are not added as that will result in the issue getting closed before team finds time to look at the vulns (which should not be reported here)
@dmc1778 please stop posting vulns on GitHub. Please consult SECURITY.md for how to properly and ethically disclose them.","""Please make sure the stalleness labels are not added as that will result in the issue getting closed before team finds time to look at the vulns (which should not be reported here"")"""
59404,mihaimaruseac,1416254327,2023-02-03 18:38:02,"This is a vulnerability that needs fixing.
@sushreebarsa Colab doesn't always report crashes/vulnerability issues.
@dmc1778 please stop posting vulns on GitHub. Please consult SECURITY.md for how to properly and ethically disclose them.",Colab doesn't always report crashes/vulnerability issues.
59177,mihaimaruseac,1416253417,2023-02-03 18:37:00,"Closing as fixed vuln.
@dmc1778 please stop posting vulns on GitHub. Please consult SECURITY.md for how to properly and ethically disclose them.",Closing as fixed vuln.
59444,mihaimaruseac,1416252742,2023-02-03 18:36:14,"Closing as not a vuln
@dmc1778 please stop posting vulns on GitHub. Please consult SECURITY.md for how to properly and ethically disclose them.",Closing as not a vuln
59368,mihaimaruseac,1416251473,2023-02-03 18:34:52,"This is a vulnerability that needs fixing.
@tilakrayal Given that this is a vulnerability, don't be too eager to close it (by adding the label).
@dmc1778 please stop posting vulns on GitHub. Please consult SECURITY.md for how to properly and ethically disclose them.","""Don't be too eager to close it (by adding the label)"""
59410,mihaimaruseac,1416251248,2023-02-03 18:34:36,"Closing as not a vuln.
@dmc1778 please stop posting vulns on GitHub. Please consult SECURITY.md for how to properly and ethically disclose them.",Closing as not a vuln.
59084,mihaimaruseac,1416249122,2023-02-03 18:32:22,"@tilakrayal this is potentially a vulnerability. Don't add the awaiting response tag as that closes the issue before it gets fixed given team does not notice these.
Please test with nightly, not just last release.
@dmc1778 Please stop posting vulenrabilities on GitHub page. It is not the usual procedure for reporting these.","""Don't add the awaiting response tag as that closes the issue before it gets fixed given team does not notice these."""
59509,smarthmaster,1415821557,2023-02-03 12:43:16,it is also slow,it is also slow
59509,smarthmaster,1415129426,2023-02-03 6:38:29,"yeah, actually the dataset was not in correct format also can you help because it takes hours for 500000lines of data to be processed on my laptop with intel i5 with average speed of 2.12ghz. is there a workaround for this","""actually the dataset was not in correct format"""
59413,luckeyca,1415040429,2023-02-03 5:45:21,"I think the problem is with cudnn, even the latest 8.7 does not support cuda 12 yet, only up to 11.8.","I think the problem is with cudnn, even the latest 8.7 does not support cuda 12 yet, only up to 11.8."
59302,mdfaijul,1414531719,2023-02-03 0:05:23,There seem to be no way to set loading tf_type dialect before mlir-pdll executes. Closing this PR.,No way to set loading tf_type dialect before mlir-pdll executes.
59398,aaudiber,1414381492,2023-02-02 21:14:23,"`from_generator` datasets are not checkpointable due to their dependency on a Python runtime. That being said, it would be good to improve the error message to something clearer.","from_generator datasets are not checkpointable due to their dependency on a Python runtime. That being said, it would be good to improve the error message to something clearer."
59491,Colabnoob,1414335083,2023-02-02 20:30:01,"I just discovered I trained my model using tensorflow 1 so I think the model is incompatible in the latest tensorflow, if so I apologise for the inconvenience I caused. I can understand it’s hard to diagnose the error. But the model works in HIFI-GAN so I’m really not sure what is the issue here.","I just discovered I trained my model using tensorflow 1 so I think the model is incompatible in the latest tensorflow, if so I apologise for the inconvenience I caused."
59117,DEKHTIARJonathan,1413873938,2023-02-02 14:55:44,"@SuryanarayanaY if it's a ""DEBUG INFO"" can you move this message to a DEBUG log. This message has nothing to do at INFO level as you clearly states in the new message (`DEBUG INFO ...`)?
CC: @reedwm","""if it's a ""DEBUG INFO"" can you move this message to a DEBUG log"""
59520,DonghakPark,1413622883,2023-02-02 11:58:15,"@gbaned there was failed ci.
Is there anything to do?",there was failed ci.
59122,tilakrayal,1413622634,2023-02-02 11:58:02,"@sachinprasadhs,
I was able to reproduce the issue on tensorflow [v2.11](https://colab.research.google.com/gist/tilakrayal/4212b2c3d4ede31976079649f401eef1/untitled841_gpu.ipynb) and nightly and the error was Python process crashing. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/8f3c41bc045d1a3d65f5f692b764f9ca/untitled914.ipynb).",I was able to reproduce the issue on tensorflow [v2.11](https://colab.research.google.com/gist/tilakrayal/4212b2c3d4ede31976079649f401eef1/untitled841_gpu.ipynb) and nightly and the error was Python process crashing.
59293,mikolajpabiszczak,1413265039,2023-02-02 7:24:30,"Hi, @gaikwadrahul8 I have the update ready, but I have not submitted it yet. As per TF contributing guidelines I am waiting on the CLA clearing of the company I work for to do that. Sorry for the delay.
```
tf-gpu latest b21fe88ebb3f 9 days ago 6.63GB
tf-gpu-new latest 8e74c99520d2 9 days ago 6.52GB
tf-cpu latest ec6067c9710e 9 days ago 2.08GB
tf-cpu-new latest 6b92ad24e155 9 days ago 1.76GB
```
GPU images are only ~0.1GB smaller, but for CPU the decrease is bigger: ~ 0.3GB.","I have the update ready, but I have not submitted it yet. As per TF contributing guidelines I am waiting on the CLA clearing of the company I work for to do that."
59491,mihaimaruseac,1413040496,2023-02-02 1:59:13,"The issue is that you are mixing different versions of TF between TF and other packages installed in your ecosystem.
This is not really a TF issue, you need to make sure your Python environment (local, in colab, in Jupyter, does not really matter) contains a consistent set of dependencies. Look at the output of `pip list` for example.
In any case, since this is not a TF issue, this should be closed",mixing different versions of TF between TF and other packages installed in your ecosystem.
59342,mihaimaruseac,1413029496,2023-02-02 1:40:46,"@pjpratik vulnerabilities occur when invalid arguments are being sent to APIs. So your reply here is wrong, this is an actual issue that must be handled to make sure it is not a vulnerability with negative effects.","""vulnerabilities occur when invalid arguments are being sent to APIs."""
59366,mihaimaruseac,1413027179,2023-02-02 1:37:16,"@pjpratik vulnerabilities occur when invalid arguments are being sent to APIs. So your reply here is wrong.
@nimashiri this is not a vulnerability, you're receiving a user error. Please consult SECURITY.md before sending new reports as very few of them are really actionable (and they should have been submitted via private channels)","""vulnerabilities occur when invalid arguments are being sent to APIs."""
59362,mihaimaruseac,1413025783,2023-02-02 1:35:06,@pjpratik vulnerabilities occur when invalid arguments are being sent to APIs. So your reply here is wrong. This is an actual issue. Please don't be too eager to get issues closed for wrong reasons.,"""vulnerabilities occur when invalid arguments are being sent to APIs."""
59340,mihaimaruseac,1413025318,2023-02-02 1:34:25,"@synandi vulnerabilities occur when invalid arguments are being sent to APIs. So your reply here is wrong.
@nimashiri this is not a vulnerability, it looks like the code is hanging based on your output. Please consult SECURITY.md before sending new reports as very few of them are really actionable (and they should have been submitted via private channels)","""vulnerabilities occur when invalid arguments are being sent to APIs."""
59325,mihaimaruseac,1413024697,2023-02-02 1:33:26,"@pjpratik this is a segfault, needs to be further investigated, not necessarily caused by OOM. Don't be over eager in pushing issues towards closure","This is a segfault, needs to be further investigated, not necessarily caused by OOM. Don't be over eager in pushing issues towards closure."
59400,mihaimaruseac,1413022898,2023-02-02 1:30:54,"@synandi vulnerabilities occur when invalid arguments are being sent to APIs. So your reply here is wrong.
@nimashiri this is not a vulnerability, you're receiving a user error. Please consult SECURITY.md before sending new reports as very few of them are really actionable (and they should have been submitted via private channels)","""vulnerabilities occur when invalid arguments are being sent to APIs."""
59351,mihaimaruseac,1413019915,2023-02-02 1:29:10,"@synandi : wrong answer. The termination of the process could have other causes, this needs further investigation to make sure it is not a vulnerability",wrong answer
59355,mihaimaruseac,1413016499,2023-02-02 1:27:20,@synandi this is the wrong answer.This is a vulnerability that needs investigating and fixing. Your wrong reply just made the issue go into the stalling pipeline and then it would have been closed without an actual fix.,"""Your wrong reply just made the issue go into the stalling pipeline and then it would have been closed without an actual fix."""
59412,mihaimaruseac,1413012774,2023-02-02 1:24:24,Closing as duplicate of already fixed issue.,Closing as duplicate of already fixed issue.
59059,mihaimaruseac,1413009384,2023-02-02 1:18:47,"Nope, the overflow is an issue but @SuryanarayanaY gave a wrong answer.","Nope, the overflow is an issue but @SuryanarayanaY gave a wrong answer."
59453,mihaimaruseac,1413006910,2023-02-02 1:14:55,"Closing as duplicate report.
@nimashiri please see SECURITY.md regarding how to report vulnerabilities.",Closing as duplicate report.
59411,mihaimaruseac,1413005106,2023-02-02 1:12:21,"Closing as duplicate report.
@nimashiri please make sure to always test against latest release and nightly. Please don't automate opening issues, instead use private reporting media. See SECURITY.md.",Closing as duplicate report.
59447,mihaimaruseac,1413003938,2023-02-02 1:10:48,Closing as duplicate report,Closing as duplicate report.
59517,gsirocco,1412964539,2023-02-02 0:31:52,"Attaching the saved model zip file again, as I don't think it got attached above.
[qam_modulator.zip](https://github.com/tensorflow/tensorflow/files/10563521/qam_modulator.zip)","Attaching the saved model zip file again, as I don't think it got attached above."
58738,sachinprasadhs,1412930585,2023-02-02 0:05:07,"@LIONEFAN , as per the commit mentioned here https://github.com/tensorflow/tensorflow/commit/52992fc29f00fc743e07e16067f6418af6c489ff, I don't think there is any workaround to generate trace.json.gz file other than the solution mentioned above. Even the Test cases has been moved out in the above change.
![image](https://user-images.githubusercontent.com/73069040/216196724-d631275c-b7e5-4fd3-988e-5ea74a3c4df3.png)",I don't think there is any workaround to generate trace.json.gz file other than the solution mentioned above. Even the Test cases has been moved out in the above change.
59390,MalekItani,1412909489,2023-02-01 23:38:44,"I opened [this](https://discuss.tensorflow.org/t/tfliteconverter-adds-de-quantization-blocks-before-and-after-operations-on-a-weight-variable/14479) discussion on the TensorFlow forum. It doesn't seem to be getting any attention. Do you know if the behavior I'm getting is expected? If so, how can I fix it?","I opened [this](https://discuss.tensorflow.org/t/tfliteconverter-adds-de-quantization-blocks-before-and-after-operations-on-a-weight-variable/14479) discussion on the TensorFlow forum. It doesn't seem to be getting any attention. Do you know if the behavior I'm getting is expected? If so, how can I fix it?"
45155,spaced,1412830807,2023-02-01 22:33:33,tflite/java (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java) is not the same as tensorflow/java. please reopen,tflite/java (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java) is not the same as tensorflow/java.
59496,nskocabey,1412628261,2023-02-01 19:45:00,"@DURGESH716 Your proposal does **NOT** solve the problem on my machine.
I installed tf-nightly==2.12.0.dev20230201 and can confirm that the issue is resolved there.","""Your proposal does **NOT** solve the problem on my machine."""
59491,Colabnoob,1412559571,2023-02-01 18:56:46,"sorry about that, in the google colab the tensorflow version is 1.15.3 however I have been using tensorflow 2.11.0. When I use tensorflow 1.15.3 in google colab, it says google colab doesn't support tensorflow 1. It also says error tensorflow cannot find a matching distribution for tensorflow 1.15.3 and suggests me to use the latest version of tensorflow, so I have been using tensorflow 2.11.0 for now.","""google colab doesn't support tensorflow 1"""
59506,nyadla-sys,1412539485,2023-02-01 18:38:33,"I attempted to convert to a full int8 encoder model, but it produced inaccurate results. That's why I want to try using int16 activations and int8 weights instead","I attempted to convert to a full int8 encoder model, but it produced inaccurate results."
58451,nyadla-sys,1412525703,2023-02-01 18:26:54,I apologize for bothering you frequently. I do not have any other options for generating the full int8 model of Whisper apart from relying on the TFLite Converter. Do you have an ETA for this feature to be available?,I apologize for bothering you frequently.
59491,Colabnoob,1412517508,2023-02-01 18:20:05,"So I have successfully updated the Hparams.py file and unfortunately it's still saying the same error. ""AttributeError What hyperparameters did you want me to change in the hparams.py file? AttributeError: module. 'tensorflow._api.v2.config' has no attribute 'HParams' is there any information I can provide to help you get a better understanding? Thanks","""AttributeError: module. 'tensorflow._api.v2.config' has no attribute 'HParams'"""
52853,mihaimaruseac,1412221285,2023-02-01 15:10:50,Closing due to https://github.com/tensorflow/tensorflow/commit/84f40925e929d05e72ab9234e53c729224e3af38,Closing due to https://github.com/tensorflow/tensorflow/commit/84f40925e929d05e72ab9234e53c729224e3af38.
59491,DURGESH716,1411424923,2023-02-01 3:55:42,"Hello @Colabnoob, Adding to the @mihaimaruseac points,
The error message you are encountering suggests that the code you are trying to run uses the `tf.contrib` module, which has been deprecated and removed in TensorFlow 2.x.
To resolve this issue, you need to update the code to use a non-deprecated API. In this case, it seems like the code is using the `tf.contrib.training.HParams` class, which can be replaced by the `tf.config.HParams` class.","""The error message you are encountering suggests that the code you are trying to run uses the tf.contrib module, which has been deprecated and removed in TensorFlow 2.x."""
59438,chenmoneygithub,1411360338,2023-02-01 2:23:42,"@kulinseth Sorry I forgot to include the context for it. Currently we are seeing an issue that the new Keras optimizer runs very slowly on M1 mac, so we need the profiling tool to do a deep dive. Your help will be very much appreciated! We are struggling now without the visual tool.","""We are struggling now without the visual tool."""
58806,mihaimaruseac,1411229330,2023-01-31 23:50:21,Closing since PRs should be against master branch,Closing since PRs should be against master branch.
59334,mihaimaruseac,1411229062,2023-01-31 23:49:57,"Since 2.10 is no longer getting patched, closing this one.","Since 2.10 is no longer getting patched, closing this one."
58022,ymwangg,1410992264,2023-01-31 20:05:46,I suspect there's a race condition between the main stream and BFC allocator asynchronous deallocation as well while debugging the ViT model. More details can be found here https://github.com/pytorch/xla/issues/4541.,I suspect there's a race condition between the main stream and BFC allocator asynchronous deallocation as well while debugging the ViT model.
59153,cheshire,1410735613,2023-01-31 16:54:07,"> Attempting to fetch value instead of handling error NOT_FOUND: no CUDA devices found
You probably need to add gpu_plugin as a dependency.",Attempting to fetch value instead of handling error NOT_FOUND: no CUDA devices found
59091,plooney,1409530034,2023-01-30 23:44:35,@sushreebarsa could you repoen please,Could you repoen please?
59381,mihaimaruseac,1409493676,2023-01-30 23:05:44,"This is valid behavior, you are requesting a large computation.
@nimashiri please report vulnerabilities outside of GitHub. Either via OSS VRP or via the security form. It seems the vulenrabilities reported on GitHub have a tendency of being mistreated by the first responders to these issues. Plus, it's not really good to report vulnerabilities in public directly.","""It seems the vulenrabilities reported on GitHub have a tendency of being mistreated by the first responders to these issues."""
59439,mihaimaruseac,1409492665,2023-01-30 23:04:50,"@tiruk007 vulnerabilities arise when invalid arguments are passed. So replying with ""you are holding it wrong"" is not valid.
@nimashiri please report vulnerabilities outside of GitHub. Either via OSS VRP or via the security form. It seems the vulenrabilities reported on GitHub have a tendency of being mistreated by the first responders to these issues. Plus, it's not really good to report vulnerabilities in public directly.","""vulenrabilities reported on GitHub have a tendency of being mistreated by the first responders to these issues."""
59353,mihaimaruseac,1409491897,2023-01-30 23:04:13,"This is valid behavior, user sees an error from Python, not a crash/vulnerability.
@nimashiri please report vulnerabilities outside of GitHub. Either via OSS VRP or via the security form. It seems the vulenrabilities reported on GitHub have a tendency of being mistreated by the first responders to these issues. Plus, it's not really good to report vulnerabilities in public directly.","""It seems the vulenrabilities reported on GitHub have a tendency of being mistreated by the first responders to these issues."""
59397,mihaimaruseac,1409490856,2023-01-30 23:03:18,"@gaikwadrahul8 vulnerabilities arise when invalid arguments are passed. So replying with ""you are holding it wrong"" is not valid.
@nimashiri please report vulnerabilities outside of GitHub. Either via OSS VRP or via the security form. It seems the vulenrabilities reported on GitHub have a tendency of being mistreated by the first responders to these issues. Plus, it's not really good to report vulnerabilities in public directly.","""vulenrabilities reported on GitHub have a tendency of being mistreated by the first responders to these issues."""
59443,mihaimaruseac,1409488203,2023-01-30 23:00:49,"(finally, please always test against latest TF release, preferably also against latest nightly; this way duplicate reports won't be made)","""finally, please always test against latest TF release, preferably also against latest nightly; this way duplicate reports won't be made"""
59443,mihaimaruseac,1409487579,2023-01-30 23:00:06,"@nimashiri please report vulnerabilities outside of GitHub.
Also, please post code and error directly on Github issue if using this form.","Please report vulnerabilities outside of GitHub. Also, please post code and error directly on Github issue if using this form."
59492,Colabnoob,1409316719,2023-01-30 20:54:01,Ok is it possible the colab code can be redone to be compatible with Tensorflow 2? I tried renewing the code and unfortunately the problem persists,I tried renewing the code and unfortunately the problem persists.
59416,mihaimaruseac,1409114633,2023-01-30 18:28:21,"@synandi Vulnerabilities are exploited by sending in invalid arguments.
This issue should not be autoclosed. The vulnerability in it should be resolved","""Vulnerabilities are exploited by sending in invalid arguments."""
59448,mihaimaruseac,1409110953,2023-01-30 18:25:33,"@nimashiri Please post code snippet and error report in the issue itself, not hidden behind a link. Links can go stale, the contents behind the link are not accessible on a search in the github interface.
Also, I think it is better to use other media to report vulnerabilities to TF, not public issues.","""Links can go stale, the contents behind the link are not accessible on a search in the github interface."""
59122,mihaimaruseac,1409107738,2023-01-30 18:23:19,"@sushreebarsa https://github.com/tensorflow/tensorflow/issues/59350#issuecomment-1399378528 does not apply. That comment was about user receiving a valid Python error, whereas here (and in several places where the same canned response was used) the user sees the Python process crashing.
CC @learning-to-play","""The Python process crashing"""
50147,mihaimaruseac,1409096390,2023-01-30 18:16:08,Closing due to https://github.com/tensorflow/tensorflow/commit/84f40925e929d05e72ab9234e53c729224e3af38 Please reopen if that does not fix.,Closing due to https://github.com/tensorflow/tensorflow/commit/84f40925e929d05e72ab9234e53c729224e3af38
59494,mihaimaruseac,1409091940,2023-01-30 18:13:16,"CC @learning-to-play @pak-laura Is the advisory wrong?
In any case, @yili731 TF 2.9 is out of SLO.","Is the advisory wrong? In any case, @yili731 TF 2.9 is out of SLO."
59489,vam-google,1409031094,2023-01-30 17:30:16,"These nightly releases are indeed broken:
2.12.0.dev20230129
2.12.0.dev20230128
The lates one (2.12.0.dev20230130) seems healthier. Although it still misses py311 and py310 wheels.","""It still misses py311 and py310 wheels."""
59169,SuryanarayanaY,1408966601,2023-01-30 16:46:40,"The issue got resolved in tf-nightly (2.12.0-dev20230130).Executed the code multiple times and got expected error.Please refer to attached snapshot below.Hence the issue not reopening.
<img width=""1503"" alt=""Screenshot 2023-01-30 at 10 05 44 PM"" src=""https://user-images.githubusercontent.com/116063290/215539424-6d658630-7292-4857-a347-5327345792f9.png"">
@nimashiri FYIP. If anybody still observed check fail please confirm and the issue can be reopened.",Executed the code multiple times and got expected error.
58768,Flamefire,1408301591,2023-01-30 9:52:15,"> I was also getting this error for ppc64le build and then `--define=tflite_with_xnnpack=false` allowed me to avoid it but now I am getting build errror [a]. Any idea what might be wrong here? Note I am trying to build TF 2.11.0
> Yes this is a bug in Eigen. I was able to solve it by including https://gitlab.com/libeigen/eigen/-/commit/886aad136111eeeb7604e1d17f62efcc4d824568 as a patch for the `eigen_archive`-`tf_http_archive`","""I was also getting this error for ppc64le build and then --define=tflite_with_xnnpack=false allowed me to avoid it but now I am getting build errror [a]. Any idea what might be wrong here?"""
59487,innat,1407725852,2023-01-29 17:42:10,"From keras team cc. @ianstenbit
> This seems like a constraint of tf.distribute that we can't work around fully in the Keras train_step, so from the Keras POV I think there's nothing to be done
cc. @rchao
> this appears not supported by tf.distribute at this time, and I would recommend filing an issue on tf.distribute if you would like such support.","This seems like a constraint of tf.distribute that we can't work around fully in the Keras train_step, so from the Keras POV I think there's nothing to be done."
59468,mihaimaruseac,1407422785,2023-01-28 15:29:55,"Closing spam issue (second time it happens, same person)","Closing spam issue (second time it happens, same person)"
59419,tomjrtsmith,1407324177,2023-01-28 7:55:29,"Hi, is there any way to diagnose this error? It seems to be persistent, we've removed unused tensorflow random seed references from code and it still occurs.","""It seems to be persistent, we've removed unused tensorflow random seed references from code and it still occurs."""
57956,JXRiver,1407156056,2023-01-27 22:38:14,"Saw the following error in macos presubmit:
```
In file included from tensorflow/core/kernels/fill_empty_rows_op.cc:16:
In file included from ./tensorflow/core/kernels/fill_empty_rows_op.h:19:
./tensorflow/core/framework/op_kernel.h:24:10: fatal error: 'absl/time/time.h' file not found
#include ""absl/time/time.h""
^~~~~~~~~~~~~~~~~~
```
Trying to figure out why.
Meanwhile, could you sync this PR to the head? Thanks!","Trying to figure out why. Meanwhile, could you sync this PR to the head? Thanks!"
59413,luckeyca,1407091700,2023-01-27 21:28:13,"Did you try your suggested procedure with CUDA 12? @SuryanarayanaY When I change the cudatoolkit=11.2 to cudatoolkit=12.0, it says there is no such package","""When I change the cudatoolkit=11.2 to cudatoolkit=12.0, it says there is no such package."""
59413,luckeyca,1407090246,2023-01-27 21:26:09,"Hi @SuryanarayanaY, my question is related to CUDA 12, yet your conda example is using cuda 11.2.","""My question is related to CUDA 12, yet your conda example is using cuda 11.2.."""
50952,mark-r-anderson,1407044695,2023-01-27 20:31:36,"Can confirm that I still experience this behaviour in all versions of TensorFlow I have tried after 2.4 (including 2.5, 2.8, and most recently 2.11). Have there been any updates on this at all? Or is there an easy fix or way to disable this warning?","""I still experience this behaviour in all versions of TensorFlow I have tried after 2.4"""
58769,cantonios,1406913218,2023-01-27 18:20:40,"> Hi @cantonios ,
> > I found the `Windows Bazel` test showed `Internal CI infrastructure error`. I think it's not related to this code change, right?
> > Thank you!
I don't think so - it looks like it was manually aborted. We're still waiting for other reviewers internally to approve.","""We're still waiting for other reviewers internally to approve."""
58384,mihaimaruseac,1406780248,2023-01-27 16:54:01,Closing as this fixes just one letter and keeps failing CI.,Closing as this fixes just one letter and keeps failing CI.
59470,aakash-chakraborty1995,1406325145,2023-01-27 10:44:13,Need to rewrite the classifier.py script in raspberry pi implementation of pose classification.,Need to rewrite the classifier.py script in raspberry pi implementation of pose classification.
59469,sanjail3,1406113501,2023-01-27 7:17:37,"It seems that you are trying to use the experimental module from keras.optimizers which is not available in this version of Tensorflow.
This module was introduced in Tensorflow 2.4 and it is not available in 2.6.0.
As the error message suggests, you might want to use the optimizers that are available in the tf.keras.optimizers module or you can upgrade your Tensorflow version to the latest version.","""Trying to use the experimental module from keras.optimizers which is not available in this version of Tensorflow"""
56435,vsbc2010,1405854503,2023-01-27 0:27:24,"bazel build //tensorflow/lite/c:tensorflowlite_c.dll -c opt --config=mkl_threadpool --define build_with_mkl_dnn_v1_only=true
But still not include oneDNN.","""But still not include oneDNN."""
59463,smuzaffar,1405600924,2023-01-26 20:19:10,"According to https://github.com/google/XNNPACK/issues/4207, `ppc64le` is not supported by XNNPack and they have no plans to add the support so does this mean we can not build Tensorflow for ppc64le? Is there any way to disabled it for TF build?",ppc64le is not supported by XNNPack and they have no plans to add the support so does this mean we can not build Tensorflow for ppc64le? Is there any way to disabled it for TF build?
59463,smuzaffar,1404740130,2023-01-26 9:26:29,just to add that I am able to build `tensorflow 2.11.0` for `aarch64 and x86_64` with `gcc 11.2` on `AlmaLinux 8` it only fails for ppc64le,I am able to build tensorflow 2.11.0 for aarch64 and x86_64 with gcc 11.2 on AlmaLinux 8 it only fails for ppc64le.
42660,EvenOldridge,1404357184,2023-01-25 23:28:47,@VoVAllen @sushreebarsa is there any traction here to fix this on the TF side? We've had a workaround in place in Merlin for a while but we're overhauling that system and it's a pain to maintain.,"""overhauling that system and it's a pain to maintain."""
59437,penpornk,1404142764,2023-01-25 19:46:04,"[MacOS CPU Python3](https://source.cloud.google.com/results/invocations/709f8779-1cf0-4c10-a8cd-fc06ffcddd52/targets) failures seem unrelated to this PR:
```
clang++: error: no such file or directory: 'tensorflow/compiler/mlir/lite/sparsity/sparsify_model.cc'
clang++: error: no input files
```","""Failures seem unrelated to this PR"""
59283,sachinprasadhs,1404105657,2023-01-25 19:11:03,"`num_classes` can be anywhere between 1 to n, including the first example given in the document which shows num_classes as 3. Below is the example for `num_classes=2 `.
```
metric = tfa.metrics.F1Score(num_classes=2, threshold=0.5)
y_true = np.array([[1,1],
[0,1],
[0,1]], np.int32)
y_pred = np.array([[1,0.6],
[0.2,0.7],
[0.6,0.2]], np.float32)
metric.update_state(y_true, y_pred)
result = metric.result()
result.numpy()
array([0.6666667, 0.8 ], dtype=float32)
```",num_classes=2
59454,jackgle,1403192319,2023-01-25 7:19:22,"`x = conv(inputs, training=False)` makes the losses approximately equal.
As explained here: https://keras.io/guides/transfer_learning/ > When you unfreeze a model that contains BatchNormalization layers in order to do fine-tuning, you should keep the BatchNormalization layers in inference mode by passing training=False when calling the base model. Otherwise the updates applied to the non-trainable weights will suddenly destroy what the model has learned.","""x = conv(inputs, training=False)"""
59306,jenkspt,1402979487,2023-01-25 1:55:44,I'm finding that using the generator is very very slow. It seems like the use of a memory mapped array could be a very useful special case of `tf.data.Dataset.from_tensor_slices`.,I'm finding that using the generator is very very slow.
59117,reedwm,1402908928,2023-01-25 1:09:58,"b63d9a4ec0887cee07665b2bbfba5abf0cdd5891 makes it more clear the log is not an error. It does not remove it, however. I think removing this is hard, since it sometimes provides useful context for other errors, but I don't know the context behind it.",b63d9a4ec0887cee07665b2bbfba5abf0cdd5891
58966,mbarnes13,1402870207,2023-01-25 0:21:42,"Hi Sachin. The first epoch is correct. The issue is that on subsequent epochs:
If `validation_steps <= len(dataset)`: Restarts validation at beginning of dataset.
If `validation_steps > len(dataset)`: Performs no validation. This is the unexpected behavior. It should restart at the beginning of the dataset.","""This is the unexpected behavior."""
58769,cantonios,1402743450,2023-01-24 21:56:31,"The macos one looks like a failing test: `math_ops:approx_topk_test_cpu`. Looks like this is currently failing at head, so not related to this change. The arm CI looks like a bad pip setup. That's not blocking though, so it's okay.
Update: we see your change internally now, but it spans three different teams needing three different approvals (me for TF, someone from MLIR, and someone from SavedModel). I'll try to get it in before branch cut today, but no guarantees.",The macos one looks like a failing test: math_ops:approx_topk_test_cpu.
59343,synandi,1402469248,2023-01-24 19:21:18,"@sushreebarsa, I was able to replicate the issue in [TF v2.10](https://colab.sandbox.google.com/gist/synandi/c853a7c8d51e9412152c988f70c3475a/59343_2-10.ipynb) and [TF v2.11](https://colab.sandbox.google.com/gist/synandi/eaf3157cad81ea56a6c7751492618aca/59343_2-11.ipynb) in colab. Could you please check this issue?",I was able to replicate the issue in [TF v2.10](https://colab.sandbox.google.com/gist/synandi/c853a7c8d51e9412152c988f70c3475a/59343_2-10.ipynb) and [TF v2.11](https://colab.sandbox.google.com/gist/synandi/eaf3157cad81ea56a6c7751
58393,penpornk,1402465279,2023-01-24 19:18:19,"The added test failed internally, so I don't think this PR will make it into TF 2.12. But I'll follow up on this with the tfg team soon. Sorry again for the long delay!","The added test failed internally, so I don't think this PR will make it into TF 2.12."
59367,synandi,1402219067,2023-01-24 16:19:55,"Hi @nimashiri , the input tensor must be either 1D or 2D. You are trying to pass uint64 tensor to the API which is not supported. It supports int32 and int64. Also, the size attribute should be a non-negative integer. Kindly refer to [DenseBincount](https://www.tensorflow.org/api_docs/python/tf/raw_ops/DenseBincount) API. Please find the gist of working code [here](https://colab.sandbox.google.com/gist/synandi/d3c6e6dc94846bb09c5fa3f92681a0c7/59367.ipynb). Thank you!","""Trying to pass uint64 tensor to the API which is not supported"""
59328,penpornk,1401811276,2023-01-24 11:50:21,Closing this PR since the same change already went in through https://github.com/tensorflow/tensorflow/pull/59315,Closing this PR since the same change already went in through https://github.com/tensorflow/tensorflow/pull/59315.
57214,janpfeifer,1401568296,2023-01-24 8:51:14,"hi @mohantym , I haven't had the opportunity to use it again so I haven't tried it.
But from what I was told the Go API is no longer maintained and a friend who is actually trying to use it had to patch the current API, and is planning to wrap their TensorFlow needs in a small C code, and create their own Go API for that.
It's quite sad actually, that TensorFlow is dropping the support for various languages outside python.","""It's quite sad actually, that TensorFlow is dropping the support for various languages outside python."""
59135,zndr27,1401446363,2023-01-24 6:19:52,I'm having the same issue,I'm having the same issue.
59029,jprabhas,1401105719,2023-01-23 22:39:53,"@cheshire , the tests on this PR (CodeCheck and Py+Cpp) seem to be stuck. They have been running since Friday, 01/20. Could you please check what's going on with these tests ?",tests on this PR (CodeCheck and Py+Cpp) seem to be stuck.
59213,annop-w,1400998820,2023-01-23 21:22:31,@penpornk Looks like the pipeline is broken. All red for other PRs with the same error.,Looks like the pipeline is broken.
59101,nluehr,1400768477,2023-01-23 18:10:32,@rainwoodman is the merge of this PR blocked by something?,is the merge of this PR blocked by something?
59213,annop-w,1400675198,2023-01-23 17:00:44,The ARM CI pipeline failed even before running the tests https://github.com/tensorflow/tensorflow/actions/runs/3988056012/jobs/6839402805#step:5:16929,The ARM CI pipeline failed even before running the tests.
59213,annop-w,1400600145,2023-01-23 16:07:36,"@penpornk I have been trying to reproduce the failure locally, but without success so far. Could we try running the CI pipeline once more ?","I have been trying to reproduce the failure locally, but without success so far."
59416,synandi,1399969635,2023-01-23 8:32:46,"Hi @nimashiri,
You are sending invalid inputs to the `sparse_concat`. Please note that
- The first argument of `sparse_concat` should be a list of at least two 2-D Tensor objects with type `int64`. - The third argument(shapes): The shape of a tensor must be a tuple of non-negative integers, but you are passing negative integers to it. Please find the gist of working code [here](https://colab.sandbox.google.com/gist/synandi/74792a6ec1c2b1f460a9773087ca7f14/59416.ipynb). Thank you!","""You are sending invalid inputs to the sparse_concat."""
59350,mihaimaruseac,1399378528,2023-01-22 1:28:43,This is not an issue. You are sending invalid inputs to the API and the API returns a custom error back,"""You are sending invalid inputs to the API and the API returns a custom error back."""
59325,dmc1778,1399304676,2023-01-21 18:19:21,"> Please can share the code that caused the error in the commets so we can furthur proceed.
I am using a fuzzer that generate test cases, I don't have control over its test case generation. The reason I uploaded into google drive is that you need data file along with the source code to run the test case. I can't upload data here.","I am using a fuzzer that generate test cases, I don't have control over its test case generation. The reason I uploaded into google drive is that you need data file along with the source code to run the test case. I can't upload data here.."
58206,LukeBoyer,1399166794,2023-01-21 3:37:17,"Hi @gsirocco, I can confirm I can reproduce this by running the supplied code. Just to confirm, are you receiving this same warning?
`tensorflow/lite/python/convert.py:789: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.`","""I can confirm I can reproduce this by running the supplied code. Just to confirm, are you receiving this same warning?"""
53550,hakimus2k,1398655398,2023-01-20 16:48:28,"Same here! Recently with python 3.9, I was unable o install because the requirement scann==1.2.6, cannot be installed, pip doesnt found it! My pip versios is the lastest.
Before that, I tryied with Python 3.10, it was a nightmare! it falls in a loop installing ""niggtly versions"", eating disk space. I read that it could be caused by the Python version.
With Pythoh 3.8 env, I get stuck with a ""METADATA file not found"" (but exists).
Maybe I should try python 3.6?","""I read that it could be caused by the Python version."""
59318,trevor-m,1397735420,2023-01-19 23:20:13,"> Eek, that's a lot of changes. Just double-checking: do we have a sense of whether these all work with rocm as well?
Hi @cantonios, there is no actual change happening in this PR. This is just moving the call to `TF_CALL_bfloat16` from each individual op to the `TF_CALL_GPU_NUMBER_TYPES` macro.
If there was no problems with rocm before there should be no effect.","""That's a lot of changes. Just double-checking: do we have a sense of whether these all work with rocm as well?"""
59029,jprabhas,1397409293,2023-01-19 18:13:33,"> Where is this previous comment?
I couldnt find that comment either. Could you add it again, @cheshire ?","I couldnt find that comment either. Could you add it again, @cheshire ?"
58806,mihaimaruseac,1397378844,2023-01-19 17:49:41,Can you please make this against master branch?,Can you please make this against master branch?
59310,cantonios,1397214976,2023-01-19 16:03:54,"Rather than add a warning in the documentation, it would be better to fix the underlying issue. Can you file an issue instead?","Rather than add a warning in the documentation, it would be better to fix the underlying issue. Can you file an issue instead?"
59278,penpornk,1396577757,2023-01-19 8:01:37,PR https://github.com/tensorflow/tensorflow/pull/59253 already went in. Could you please help resolve merge conflict?,"""Resolve merge conflict?"""
58451,nyadla-sys,1396323619,2023-01-19 1:25:53,"I still see below issue with tf-nighty
0x00007fffcb614553 in mlir::quant::QuantizedType::getExpressedType() const () from ~whisper-tflite/venv/lib/python3.10/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so",I still see below issue with tf-nighty 0x00007fffcb614553 in mlir::quant::QuantizedType::getExpressedType() const () from whisper-tflite/venv/lib/python3.10/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so.
58966,sachinprasadhs,1396191299,2023-01-18 22:40:23,"When `validation_steps' > len(Dataset)` as per the existing implementation where it runs validation for first epoch is proper. Otherwise, it would increase the evaluation time when the validation is performed on each epoch for the above scenario.","""validation_steps"" > len(Dataset)"
52125,henryiii,1387703891,2023-01-18 20:03:28,"You can't precompile wheels for FreeBSD and upload them to PyPI. Last I checked, there's not a proper BSD compatibility guarantee (either forward or between BSDs, from what I understand) to allow a ""ManyBSD"" wheel (like manylinux & GLIBC, or even musllinux with MUSL), so binaries cannot be provided for it by anyone on PyPI - this is not tensorflow specific. Currently the only option is to go with some sort of BSD packaging system other than pip.","""Currently the only option is to go with some sort of BSD packaging system other than pip."""
52125,probonopd,1387684861,2023-01-18 19:51:10,"Thanks for checking @mohantym. Well, a (for me) satisfactory solution would be a working `pip install tensorflow ` on FreeBSD. Apparently this would require tensorflow to be compiled by someone in the TensorFlow team. All other solutions mean that TensorFlow on FreeBSD is not fully supported on the same level as it is on Linux, Windows, and the Mac. Compiling it from source is error-prone and needs a lot of time, resources, and skills.",Apparently this would require tensorflow to be compiled by someone in the TensorFlow team.
59292,sachinmuradi,1387444050,2023-01-18 17:22:17,"> Could you please help fix the [PyLint error](https://github.com/tensorflow/tensorflow/actions/runs/3949051506/jobs/6759788729)?
> > ```
> tensorflow/dtensor/python/tests/test_util.py:278:0: C0301: Line too long (81/80) (line-too-long)
> ```
@penpornk Pushed the Fix for pylint error.","""PyLint error"""
59123,sushreebarsa,1387394184,2023-01-18 17:00:03,"@nimashiri Sorry for the late response!
I tried to execute the provided code and colab is crashing during execution.
Please check this [gist ](https://colab.research.google.com/gist/sushreebarsa/3e44c83f10a0e242e6c88958b0386e7d/59123.ipynb)and confirm the same?
Thank you!","""I tried to execute the provided code and colab is crashing during execution."""
59293,mihaimaruseac,1387267143,2023-01-18 15:36:32,Please send PRs with size difference too.,"""Please send PRs with size difference too."""
44916,mmarouen,1387249717,2023-01-18 15:24:37,"same thing could/should be added to the ""iOS"" macro, in the tflite CMake Line 444 under:
`if(CMAKE_SYSTEM_NAME MATCHES ""Android"")
list(APPEND TFLITE_PROFILER_SRCS
${TFLITE_SOURCE_DIR}/profiling/atrace_profiler.cc
)`
Similarly a ""iOS"" loop needs to be added
`elseif(CMAKE_SYSTEM_NAME MATCHES ""iOS"")
list(APPEND TFLITE_PROFILER_SRCS
${TFLITE_SOURCE_DIR}/profiling/atrace_profiler.cc
)`","""same thing could/should be added to the ""iOS"" macro, in the tflite CMake Line 444 under: if(CMAKE_SYSTEM_NAME MATCHES ""Android"") list(APPEND TFLITE_PROFILER_SRCS $TFLITE_SOURCE_DIR/profiling/atrace_profiler.cc ) Similarly a ""iOS"" loop needs to be added elseif(CMAKE_SYSTEM_NAME MATCHES ""iOS"")"
59029,cheshire,1387227908,2023-01-18 15:10:16,@reedwm Seems the previous comment about a request to change the variable name is not addressed?,Seems the previous comment about a request to change the variable name is not addressed?
59292,penpornk,1387064793,2023-01-18 13:21:02,"Could you please help fix the [PyLint error](https://github.com/tensorflow/tensorflow/actions/runs/3949051506/jobs/6759788729)?
```
tensorflow/dtensor/python/tests/test_util.py:278:0: C0301: Line too long (81/80) (line-too-long)
```",tensorflow/dtensor/python/tests/test_util.py:278:0: C0301: Line too long (81/80) (line-too-long)
59282,cheshire,1386896599,2023-01-18 11:22:34,Could you add a test which crashes it?,Could you add a test which crashes it?
51071,Fremih,1386678593,2023-01-18 8:38:33,"Since May 2022 no new version was added [here](https://zenodo.org/record/6574269). As stated above, v2.9.1 is the most recent version. Shall I open a new issue for this bug?",Since May 2022 no new version was added
47309,mmehedin,1386598070,2023-01-18 7:20:58,"> Was able to replicate this issue in TF 2.6.0-dev20210527 ,please check the gist [here](https://colab.research.google.com/gist/sushreebarsa/7ea54b7baba812fee361ce0561d0cd8a/untitled13.ipynb#scrollTo=PA0JdhGTgy8j)...Thanks !
Replace
loss = tf.reduce_mean(loss, keepdims=True)
by
loss = tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, keepdims=True))(loss)
While this worked for the smaller examples, I still got the same error in the full model MaskRCNN. There must be additional issues.","""I still got the same error in the full model MaskRCNN."""
59258,FlyWong,1386283547,2023-01-18 0:29:15,"![tfLitePredictionResults](https://user-images.githubusercontent.com/10432466/213037199-d7cc2bf4-0316-4ab4-83c4-4022afdfd239.JPG)
Hi @pjpratik , may I know which is the class index you are referring too? also, I tried to call argMax on OutputTensor, but there is no such function:
![argMax Err](https://user-images.githubusercontent.com/10432466/213047678-4c5d5f35-e7a5-47fc-9489-f6490e218997.JPG)","I tried to call argMax on OutputTensor, but there is no such function:"
59037,arfaian,1386272230,2023-01-18 0:15:57,"Building the whl for armv6 should be fixed with 63c028fe7ba4d82c0ad9156533acaddaad377049 and was broken for some time due to an outdated toolchain. @Athuliva you can sync to HEAD and run the following command:
```sh
make -C tensorflow/lite/tools/pip_package docker-build TENSORFLOW_TARGET=rpi0
```",Building the whl for armv6 should be fixed with 63c028fe7ba4d82c0ad9156533acaddaad377049 and was broken for some time due to an outdated toolchain.
58925,aaudiber,1386163935,2023-01-17 22:23:22,"The issue with the original repro is that it doesn't set the `reshuffle_each_iteration` argument to [shuffle](https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly#shuffle). By default `shuffle` will reshuffle, so you get a different order each iteration.
You can [save dataset iterators into checkpoints](https://www.tensorflow.org/guide/checkpoint#create_the_checkpoint_objects), so that they can be restored to the exact state.",The issue with the original repro is that it doesn't set the reshuffle_each_iteration argument to shuffle.
59277,snadampal,1386016333,2023-01-17 20:34:02,"Not ready for merge yet.
ARM CI build passed but it's not testing with TF_ONEDNN_ASSUME_FROZEN_WEIGHTS=1
I see few unit test failures when I tested locally with the above flag. Will fix and update the PR.",ARM CI build passed but it's not testing with TF_ONEDNN_ASSUME_FROZEN_WEIGHTS=1
56852,k-w-w,1385834571,2023-01-17 18:14:12,"Sorry for extremely late reply. Use `strategy.run` to call the model, or use `model.fit`. Here is the guide: https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_custom_training_loops
Only model creation should be done inside of the distribution scope.",Sorry for extremely late reply.
58014,cantonios,1385731582,2023-01-17 16:57:55,"@milpuz01 is this still active? If not, we should close it.","""If not, we should close it."""
35160,maingoh,1385384654,2023-01-17 12:56:23,Don't loose your time @tomchen1000 they won't investigate.,They won't investigate.
59229,SuryanarayanaY,1385084305,2023-01-17 9:26:06,"@kamil5b ,
The CUDA Driver version (528.02) may not compatible with the CUDA library i.e CUDA-11.2 here. You may try to install Driver version compatible with CUDA 11.2 library from [here](https://www.nvidia.com/Download/index.aspx). For example Driver Version 470.161.03 may be suitable for CUDA 11.2-11.4. Please try this and let us know if it works.",The CUDA Driver version (528.02) may not compatible with the CUDA library i.e CUDA-11.2 here.
59273,DURGESH716,1384442030,2023-01-16 18:58:05,oops Please Take care of indentation spaces !!!,oops Please Take care of indentation spaces !
59263,pjpratik,1383962053,2023-01-16 12:11:07,Closing this as spam. Will be reopened once the template is updated.,Closing this as spam.
59240,chalakprana,1383924978,2023-01-16 11:41:53,"@alanpurple no support for VS 2022 yet, possible attach full error screen
https://stackoverflow.com/questions/73129804/cannot-build-tensorflow-on-windows
https://www.tensorflow.org/install/source_windows#install_visual_c_build_tools_2019",No support for VS 2022 yet
59048,akuegel,1383738443,2023-01-16 9:29:27,@cheshire I think my review comment was not addressed yet. The PR as-is will fail when we try to merge it.,I think my review comment was not addressed yet.
59250,DURGESH716,1382366731,2023-01-13 20:43:47,"Hello @binbinxue, Storing layers in a list and accessing them through indexing can lead to issues with the gradients being nearly zero, which can prevent the model from learning. This is likely because the layers in the list are not being properly tracked by TensorFlow's automatic differentiation system, which is used to compute gradients during training.","Storing layers in a list and accessing them through indexing can lead to issues with the gradients being nearly zero, which can prevent the model from learning."
58763,reedwm,1382331275,2023-01-13 20:03:25,Sorry for the delays in merging. This is failing internal tests and we are still trying to figure out why. We hope to have it merged soon.,delays in merging
59055,OleksiiZaderykhinCapgemini,1382228860,2023-01-13 18:29:32,"> Seems like a few of the functions here are creating domains, should these go into the domains folder?
@fcoUnda I think the `domains` folder is for reusable domains. But if the domain is specific to one file and used only in this file it is better to place it near its usage. So we will not clatter the `domains` folder. But it is my opinion and could be a matter of taste.","a few of the functions here are creating domains, should these go into the domains folder?"
58022,ymwangg,1382218433,2023-01-13 18:17:47,Can someone from Google take this over? I tried several test cases but non of them can capture this bug in tensorflow.,I tried several test cases but non of them can capture this bug in tensorflow.
56647,adis300,1381858755,2023-01-13 13:35:42,"> I have signed the CLA multiple times. I am pretty sure the recheck feature doesn't work for me. It show's me ""400. That’s an error.
That’s all we know."" with a Google logo.
I noticed as a Googler you could help me to rescan this pull request from the page.
""Googlers: Go [here](https://goto.google.com/prinfo/github.com%2Ftensorflow%2Ftensorflow%2Fpull%2F56647) to view more details and manage scans for this pull request.""","I have signed the CLA multiple times. I am pretty sure the recheck feature doesn't work for me. It show's me ""400. That’s an error. That’s all we know."" with a Google logo. I noticed as a Googler you could help me to rescan this pull request from the page. ""Googlers: Go [here](https://goto.google.com/prinfo/github.com%2Ftensorflow%2Ftensorflow%2Fpull%2"
59242,pjpratik,1381540963,2023-01-13 9:35:28,"@clime When attempting to remove a function, a segmentation fault may occur if the garbage collector in the child process is unable to properly manage the TensorFlow objects inherited from the parent process. Further, `gc.disable()`may cause memory leak issue. Can you please provide any reproducible code to further expedite the trouble shooting process?","attempting to remove a function, a segmentation fault may occur if the garbage collector in the child process is unable to properly manage the TensorFlow objects inherited from the parent process."
58809,xiaogangzhu,1381425872,2023-01-13 7:36:54,"Hi,
drop_reinder=True will not cause error. However, it will lose some data which is not what we want to see. Is there any other solutions? I think it is a bug of tensorflow and if you guys can fix it?","""I think it is a bug of tensorflow and if you guys can fix it?"""
59211,josephrocca,1381386108,2023-01-13 6:46:49,"@mohantym Ah, sorry, I should have mentioned that @Ferev [showed me](https://github.com/google/jax/issues/13416#issuecomment-1379383626) the `allow_custom_ops` workaround on the JAX repo, but there were concerns expressed that something is potentially going wrong here - seems unusual that `Case` is a custom op:
> For some reason this is interpreted as a custom op. Will ask the team investigate why this is the case. I agree this is weird behavior for `Case`.","""I agree this is weird behavior for Case."""
35160,tomchen1000,1381069794,2023-01-12 22:36:00,"I'm still seeing a similar high RAM usage on Tensorflow 2.10.1 which is compiled without the option of -D_GLIBCXX_USE_CXX11_ABI=0. I think we should reopen this issue, although we have a workaround, which is to compile TF with the option of -D_GLIBCXX_USE_CXX11_ABI=0.",I'm still seeing a similar high RAM usage on Tensorflow 2.10.1 which is compiled without the option of -D_GLIBCXX_USE_CXX11_ABI=0.
58973,chenmoneygithub,1380866070,2023-01-12 18:53:33,"`deepcopy` is not supported due to the restriction on distribution strategy. you can actually bypass the issue by nullifying the `self._distribution_strategy`, i.e.,
```
strategy = old_optimizer._distribution_strategy
old_optimizer._distribution_strategy = None
new_optimizer = deepcopy(old_optimizer)
new_optimizer._distribution_strategy = strategy
```
Also we will not deprecate the old optimizer, but not adding new features to it.",deepcopy is not supported due to the restriction on distribution strategy.
59215,JonasAmrich,1380815316,2023-01-12 18:19:44,"@sachinprasadhs Thanks for this confirmation. The situation is a bit unfortunate as keras optimizers are JIT compiled by default (sice 3ad70f6cab8912860abb1b94c004e84b1114a6f7) and check_numerics therefore doesn't work out of the box on GPU. The workaround (explicitly disabling jit_compile on optimizer) is straightforward, however it took me some time to realize.. Would it make sense to add a note to docs?","""The situation is a bit unfortunate as keras optimizers are JIT compiled by default"""
53765,Flamefire,1380204771,2023-01-12 11:42:53,This seems to be currently broken. Testing with TF 2.9.1 and Abseil 20220623.1 I get undefined references due to missing `cordz_functions` library e.g. `cordz_should_profile_slow` and even fails before due Bazel files referencing `hash_testing` which isn't included either.,I get undefined references due to missing cordz_functions library e.g. cordz_should_profile_slow and even fails before due Bazel files referencing hash_testing which isn't included either.
59035,SuryanarayanaY,1379996232,2023-01-12 8:52:40,"Hi @maifeeulasad ,
The requested feature may not be possible inside the Model.However your intention is to use GPU you can use the context of tf.device('GPU') like below where the code within this context runs under GPU.
```
with tf.device('GPU'):
model = tf.keras.models.Sequential()
model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
model.add(StringLayer())
```",The requested feature may not be possible inside the Model.However your intention is to use GPU you can use the context of tf.device('GPU') like below where the code within this context runs under GPU.
59235,mihaimaruseac,1379782280,2023-01-12 4:02:35,"nothing filled in, closing as spam","nothing filled in, closing as spam"
53146,zichuan-wei,1379510017,2023-01-11 21:31:25,"still can't reproduce the error on the latest tip, running XNNPACK on 1 - 4 threads results in: 39.441ms, 19.834ms, 14.066ms and 11.099ms respectively, matching expected behaviour",still can't reproduce the error on the latest tip
58676,pati-dev,1379256974,2023-01-11 17:43:28,"@sampathweb, @sagunb, @sachinprasadhs - We are seeing this issue for TF 2.11 and we also use poetry in production. Is there a TAT on this issue as this looks like a major issue with the newer TF versions.","""We are seeing this issue for TF 2.11 and we also use poetry in production. Is there a TAT on this issue as this looks like a major issue with the newer TF versions."""
59224,mohantym,1378561846,2023-01-11 10:48:43,Sorry! Closing as duplicate to PR #58753 .,Closing as duplicate to PR #58753 ..
59186,Rishikesh2338,1378555715,2023-01-11 10:43:40,Can you please assign me this work.,Can you please assign me this work.
56947,penpornk,1377625938,2023-01-10 17:43:15,[ARM CI failures](https://github.com/tensorflow/tensorflow/actions/runs/3883911445/jobs/6626246458) seem to be already failing tests (7 tests; no new failures).,"""ARM CI failures"""
58071,penpornk,1377625335,2023-01-10 17:42:47,[ARM CI failures](https://github.com/tensorflow/tensorflow/actions/runs/3856922764/jobs/6623077289) seem to be already failing tests (7 tests; no new failures).,[ARM CI failures](https://github.com/tensorflow/tensorflow/actions/runs/3856922764/jobs/6623077289) seem to be already failing tests (7 tests; no new failures).
55411,penpornk,1377624767,2023-01-10 17:42:23,[ARM CI failures](https://github.com/tensorflow/tensorflow/actions/runs/3883926311/jobs/6626240719) seem to be already failing tests (7 tests; no new failures).,[ARM CI failures](https://github.com/tensorflow/tensorflow/actions/runs/3883926311/jobs/6626240719) seem to be already failing tests (7 tests; no new failures).
59192,dmc1778,1377588381,2023-01-10 17:13:10,"> @tilakrayal Security issues sometimes manifest by running the code multiple times. If you run the same cell multiple times you will get the session crashing.
Exactly, another case is multiple security issues relates only for one specific API.","""multiple security issues relates only for one specific API."""
59192,mihaimaruseac,1377487535,2023-01-10 15:59:26,@tilakrayal Security issues sometimes manifest by running the code multiple times. If you run the same cell multiple times you will get the session crashing.,"""Security issues sometimes manifest by running the code multiple times."""
58640,rohan100jain,1377347791,2023-01-10 14:15:42,"To be safe one option would be to hollow out the kernel implementation and just throw an error. That could also lead to a fair amount of code deletion?
We leave it like that for one release and next release if no one complains, we can then do the full scale deletion",To be safe one option would be to hollow out the kernel implementation and just throw an error. That could also lead to a fair amount of code deletion?
58640,rohan100jain,1377346911,2023-01-10 14:15:02,"Do we expect these ops to ever show up in a SavedModel? If not, I think its fine to remove them.","Do we expect these ops to ever show up in a SavedModel? If not, I think its fine to remove them."
59203,joker-eph,1376572865,2023-01-10 1:02:31,Seems like the check should just be relaxed to not be so picky about the precision,The check should just be relaxed to not be so picky about the precision.
58851,nouiz,1376397021,2023-01-09 22:09:40,"Any update? I tried locally, now it build, but without the CUDA backend included:
```
2023-01-09 22:03:33.475237: I tensorflow/compiler/xla/service/platform_util.cc:72] platform Host present but no XLA compiler available:
could not find registered compiler for platform Host -- was support for that platform linked in?
2023-01-09 22:03:33.475295: F tensorflow/compiler/xla/client/client_library.cc:127] Non-OK-status: client_status.status() status: NOT_F
OUND: no platforms found
```","I tried locally, now it build, but without the CUDA backend included: "
58248,apivovarov,1376328450,2023-01-09 21:16:26,"@sushreebarsa @jkr26 I do not think the comment gives exact solution to TF-TRT users on how to return fast TF-TRT inference back.
It seems that the only working solution which we have now is to revert commit 612a531",I do not think the comment gives exact solution to TF-TRT users on how to return fast TF-TRT inference back. It seems that the only working solution which we have now is to revert commit 612a531.
56783,gaurides,1376151022,2023-01-09 19:12:46,"Adding @ezhulenev back to the review, not sure how he was removed","Adding @ezhulenev back to the review, not sure how he was removed."
59117,pavanimajety,1375998444,2023-01-09 17:32:46,"@mohantym I am able to consistently reproduce with the nightly build or any commit from almost first week of December. I believe it doesn't matter that the issue is not reproducible in 2.11, since I am on the latest releases for TF, Cuda and Cudnn.",I am able to consistently reproduce with the nightly build or any commit from almost first week of December.
59192,mihaimaruseac,1375946498,2023-01-09 16:56:23,This is serious and will need fixing,This is serious and will need fixing.
59081,dev0x13,1375778839,2023-01-09 15:17:09,"I confirm the issue. The exact same auxiliary script wrapping Bazel calls (with `--config=monolithic`) producing different libtensorflow_cc.so artifacts for 2.9.3 and 2.10.1: the former is actually monolithic and the latter depends on libtensorflow_framework.so.
Also with `--config=monolithic` I experience the same errors OP does.
I can provide more details on my setup if needed.",The exact same auxiliary script wrapping Bazel calls (with --config=monolithic) producing different libtensorflow_cc.so artifacts for 2.9.3 and 2.10.1: the former is actually monolithic and the latter depends on libtensorflow_framework.so. Also with --config=monolithic I experience the same errors OP does.
58995,Neizvestnyj,1375761856,2023-01-09 15:07:23,"> tensorflow-cpu
As I said, I know, that directml plugin work only on `tensorflow-cpu==2.10.0` I have indicated this above, I already installed Microsoft Visual C++, and I cant install `cudatoolkit` and etc, because I only have AMD GPU.","I have indicated this above, I already installed Microsoft Visual C++, and I cant install cudatoolkit and etc, because I only have AMD GPU."
59069,sidbab94,1375607058,2023-01-09 13:12:48,"Thanks for the suggestion @mohantym. But the environment I'm working on is _heavily_ dependant on TF1.x, I will revisit this once I am able to reproduce the same on TF 2.x.",_heavily_ dependant on TF1.x
56231,swap-10,1375258802,2023-01-09 8:26:03,"This still seems to be an issue with TF 2.11.0 Python 3.9.4 and VSCode
`from tensorflow.keras import applications`
generates a warning that `tensorflow.keras` could not be resolved
Is it just not recommended to use this form of import now and rather use
`import tensorflow as tf; from tf.keras import ...` ?",from tensorflow.keras import applications generates a warning that tensorflow.keras could not be resolved
59174,dmc1778,1375132459,2023-01-09 5:48:12,"> @nimashiri I tried to replicate the issue in tf-nightly and faced a different outcome. Could you please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/b60efc6c4782d3f1bc719f596eedbc7a/59174.ipynb) and confirm the same? Thank you!
This issue is for 2.10.",I tried to replicate the issue in tf-nightly and faced a different outcome.
59086,SuryanarayanaY,1375097678,2023-01-09 4:32:36,"@nimashiri ,
I tried to execute the same code few times again and i can't get the illegal memory error.Please refer to attached snapshot.
<img width=""1512"" alt=""Screenshot 2023-01-08 at 12 38 27 PM"" src=""https://user-images.githubusercontent.com/116063290/211241317-fb853e62-eae9-4e76-a57f-19077b3e4a9a.png"">
As the behaviour is not observed now shall we close the issue now ? Please feel free to report if you find the behaviour again.Awaiting your confirmation.Thank you!",I tried to execute the same code few times again and i can't get the illegal memory error.
58657,rsanthanam-amd,1374843475,2023-01-08 14:00:54,"When I make this change, I get some other error so I am currently debugging that.","""I get some other error so I am currently debugging that."""
59161,dmc1778,1374569000,2023-01-07 18:37:58,I think I have already reported this bug.,I think I have already reported this bug.
58995,SuryanarayanaY,1374389403,2023-01-07 6:10:38,"@Neizvestnyj ,
Sorry,I couldn't notice earlier you are using DirectML plugin.For windows native Tensorflow GPU supported for TF 2.10 Versions.Anything TF>=2.11v will not support GPU on windows native.
Can you try this setting `tf.config.set_soft_device_placement(True)` and let us know if it works.","""I couldn't notice earlier you are using DirectML plugin.For windows native Tensorflow GPU supported for TF 2.10 Versions.Anything TF>=2.11v will not support GPU on windows native."""
56661,otavio-silva,1374104548,2023-01-06 20:36:56,"@gowthamkpr I understand, but TensorFlow should deal with memory in a more efficient manner.
Memory growth should be enabled by default and it should recognize at least 80% of the device memory.","""I understand, but TensorFlow should deal with memory in a more efficient manner."""
59104,hugo-mrc,1373621495,2023-01-06 13:24:56,"Also got the error by specifying `mirrored_strategy = tf.device('/gpu:1')`
Any idea please ? @mohantym @SuryanarayanaY",mirrored_strategy = tf.device('/gpu:1')
58901,SJaffa,1373554466,2023-01-06 12:13:42,"I didn't respond over my Christmas holiday so this has been marked stale but I'll note here in case anyone has the same problem later:
To resolve this we did `export LD_LIBRARY_PATH=/usr/local/lib` when installing on Mac instead of `sudo ldconfig /usr/local/lib`",I didn't respond over my Christmas holiday so this has been marked stale
59094,johnnkp,1373236553,2023-01-06 7:14:45,Downgrade AGP doesn't help. Errors still occur.,Downgrade AGP doesn't help. Errors still occur.
58813,akuegel,1373191858,2023-01-06 6:18:58,"I am back from vacation, but not the person who needs to approve it internally.","I am back from vacation, but not the person who needs to approve it internally."
59105,ted1995,1373184280,2023-01-06 6:07:13,#58668 i remove TensorFlowLiteSelectTfOps with the answer in this peoblem,"""i remove TensorFlowLiteSelectTfOps with the answer in this peoblem"""
57804,ianlewis,1373141600,2023-01-06 4:50:24,"> So, this is blocked because our docker username is encoded in secrets instead of directly in the workflow. Will need to fix this first.
https://github.com/slsa-framework/slsa-github-generator/pull/1409 should unblock this once it's merged and released.","""Our docker username is encoded in secrets instead of directly in the workflow."""
58678,jameshilliard,1372921257,2023-01-05 23:23:16,Anything holding this up from being merged? I see a `Google internal checks FAILED for runs with create time 2022-12-30T16:36:16.327881750Z.` test failure but there aren't any failure details visible.,I see a Google internal checks Failed for runs with create time 2022-12-30T16:36:16.327881750Z. test failure but there aren't any failure details visible.
58677,jameshilliard,1372920645,2023-01-05 23:22:22,Anything holding this up from being merged? I see a `Google internal checks FAILED for runs with create time 2022-12-30T01:04:31.307953960Z.` test failure but there aren't any failure details visible.,I see a Google internal checks Failed for runs with create time 2022-12-30T01:04:31.307953960Z. test failure but there aren't any failure details visible.
59111,nluehr,1372651997,2023-01-05 19:40:54,This fix doesn't seem to be sufficient with the latest commits in the master branch. I need to investigate the test failures I'm seeing on A100 more closely. Closing this PR for now.,I need to investigate the test failures I'm seeing on A100 more closely.
59078,cantonios,1372642352,2023-01-05 19:30:41,"Visible to who? I'm pushing through a fix for this on our end. The XLA error message was slightly different, causing the `*_xla_gpu` test to fail.","The XLA error message was slightly different, causing the *_xla_gpu test to fail."
47285,sachinprasadhs,1372591229,2023-01-05 18:37:36,"`tf.keras.backend` API is not suggested for public use, most of the functions which are not listed fall under legacy code.
Users are encouraged to use alternative Tensorflow APIs as the direct usage.","API is not suggested for public use, most of the functions which are not listed fall under legacy code. Users are encouraged to use alternative Tensorflow APIs as the direct usage."
57679,mshavliuk,1372539282,2023-01-05 17:50:04,@learning-to-play I believe it is closed by mistake - the issue still persists,I believe it is closed by mistake - the issue still persists.
59104,hugo-mrc,1372516715,2023-01-05 17:27:54,"Hello @mohantym Thanks for your answer.
I also tried with this explicit statement but also got the same error.
Unfortunately, I can't try with tf>2.2 cause my gpus only work with ROCm<=3.5 on my machine.",I also tried with this explicit statement but also got the same error.
44777,Bengt,1372303247,2023-01-05 14:39:58,"Under Ubuntu 18.04 with TensorFlow 2.4.0, I needed to link the libcusolver in my Python 3.8 virtual environment managed by tox:
```bash
ln -s /usr/local/cuda/lib64/libcusolver.so.11 .tox/py38-cuda/lib/python3.8/site-packages/tensorflow/python/libcusolver.so.10
```","""Under Ubuntu 18.04 with TensorFlow 2.4.0, I needed to link the libcusolver in my Python 3.8 virtual environment managed by tox: bash ln -s /usr/local/cuda/lib64/libcusolver.so.11 .tox/py38-cuda/lib/python3.8/site-packages/tensorflow/python/libcusolver.so.10 """
59104,hugo-mrc,1372162061,2023-01-05 12:36:14,"The issue is probably related to synchronized computing because when running with: `mirrored_strategy = tf.distribute.MirroredStrategy(devices=[""/gpu:1""])` the program doesn't appear to crash ever.
Tried as well with 2 GPUs with `tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.ReductionToOneDevice())` but this caused the same error previously described","The issue is probably related to synchronized computing because when running with: mirrored_strategy = tf.distribute.MirroredStrategy(devices=[""/gpu:1""]) the program doesn't appear to crash ever. Tried as well with 2 GPUs with tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.ReductionToOneDevice())"
59104,hugo-mrc,1372112452,2023-01-05 11:42:14,"Other exemple below at t=385:
2023-01-05 12:40:34.394719: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Invalid argument: Input to reshape is a tensor with 1 values, but the requested shape has 64
[[{{node gradient_tape/mean_squared_error/Reshape}}]]
[[div_no_nan/allreduce/CollectiveReduce/_294]]","""Input to reshape is a tensor with 1 values, but the requested shape has 64"""
59072,SuryanarayanaY,1371817401,2023-01-05 6:02:33,Closing the issue as it is duplicate of #58809,Closing the issue as it is duplicate of #58809
58958,Lotte1990,1371476267,2023-01-04 21:57:26,@sachinprasadhs I cannot upgrade to 2.6.0 and beyond because my models behave differently and I do need the changes made in 2.5.2 and 2.5.3. Please upload these versions.,I cannot upgrade to 2.6.0 and beyond because my models behave differently and I do need the changes made in 2.5.2 and 2.5.3.
58539,tatwaichong,1371276460,2023-01-04 18:29:05,"For PReLU, this change performs rescale on the tensor of input and alpha before and after the multiplication. My colleague @Tai78641 found out my previous patch didn't consider zero point in the operation, so the incorrect result happens in certain numeric ranges.","""My colleague @Tai78641 found out my previous patch didn't consider zero point in the operation, so the incorrect result happens in certain numeric ranges."""
59077,RonaldoSCouto,1371224887,2023-01-04 17:36:11,"Hi,
I update from pip 22.0.2 to pip 22.3.1, purge pip cache and ran again.
I had stopped installation on tf_nightly-2.12.0.dev20221229 (7 days).
Nothing changed. Same situation. Looking for tf_nightly.","I update from pip 22.0.2 to pip 22.3.1, purge pip cache and ran again. I had stopped installation on tf_nightly-2.12.0.dev20221229 (7 days). Nothing changed. Same situation. Looking for tf_nightly."
58248,sushreebarsa,1371178136,2023-01-04 16:54:41,"@apivovarov I tried to replicate the issue on colab using tf-nightly, and faced `RuntimeError: Tensorflow has not been built with TensorRT support`. Please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/c6be14b1375c204f48de99e30c521d04/58248.ipynb) and confirm the same?
Thank you!","""RuntimeError: Tensorflow has not been built with TensorRT support"""
6971,VenkateshSoni,1370877843,2023-01-04 12:38:49,"Can we open this issue again?
I am facing same issue on AWS Sagemaker Notebook - 32GB RAM.
Dimension of my dataset is 408 * 241390, whenever use the initializer command -
self.sess.run(tf.global_variables_initializer())
it shoes me this error - Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT",I am facing same issue on AWS Sagemaker Notebook - 32GB RAM.
59074,rajivhockey97,1370594269,2023-01-04 8:06:03,@sarataylor2000 I tried replicating the issue and i got meaningful results. Could you please provide me a gist so that i can verify the garbage output which you are receiving ?,I tried replicating the issue and i got meaningful results. Could you please provide me a gist so that i can verify the garbage output which you are receiving ?
58665,mihaimaruseac,1370387156,2023-01-04 1:19:24,"It got pulled in the Google system (see the 2 copybara checks). However, it seems internal build fails, probably some system is looking for the same error message (Hyrum's Law).
Someone will need to look at it internally and try to fix.","""It seems internal build fails, probably some system is looking for the same error message (Hyrum's Law)"""
56647,mihaimaruseac,1370386507,2023-01-04 1:17:56,"> > Please sign CLA. We shouldn't have reviewed before that
> > I signed the CLA a few months ago. Clicked ""update check"" and google showed me 404 error.
> > I tried again today, and still 404. You can help me on rechecking the commit as a Googler?
If you are not currently covered under a CLA, please visit https://cla.developers.google.com/. Once you've signed, follow the ""New Contributors"" link at the bottom of this page to update this check.","""I tried again today, and still 404."""
59068,hengruo,1370218371,2023-01-03 20:59:01,"@nitins17 I had tried 11.8 + 8.7 and got the same error. Do I still need to try 11.8 + 8.6?
Combinations I tried: cuda 11.8 + cuDNN 8.7, cuda 11.4 + cuDNN 8.7, cuda 11.2 + cuDNN 8.1.",I had tried 11.8 + 8.7 and got the same error.
58867,cantonios,1369969377,2023-01-03 16:26:30,"> Hi @cantonios Can you please review this PR ? Thank you!
I added my review, and there was no response.","I added my review, and there was no response."
55394,zhaozheng09,1369929570,2023-01-03 15:52:14,"> Unfortunately you can do nothing :-( This is still blocked by an internal test failure. There were two test failures, one of them I can fix myself by increasing the allowed epsilon value for numerical differences. The other test is in a code base where I don't have access to, so I contacted the team to ask them whether they can check what can be done.
Thanks for your hard work.","""Unfortunately you can do nothing"""
55560,MorphSeur,1369912265,2023-01-03 15:37:40,"I got the same issue.
Any help please!!",I got the same issue.
57371,sanguinariojoe,1369884153,2023-01-03 15:14:13,"Same problem here, with ROCM 5.4.1. It fails with GCC 12.2.0 but works with GCC 11.3.0","Same problem here, with ROCM 5.4.1. It fails with GCC 12.2.0 but works with GCC 11.3.0."
58769,joker-eph,1369882788,2023-01-03 15:13:04,"It's waiting on an internal reviewer to come back from vacations, should get done ""soon"".","waiting on an internal reviewer to come back from vacations, should get done ""soon""."
57128,tilakrayal,1369598215,2023-01-03 10:19:10,"@sachinprasadhs,
I was able to reproduce the issue on tensorflow [v2.9](https://colab.research.google.com/gist/tilakrayal/fc3bb093faa07f69c6e82f49cb1efdd9/untitled831.ipynb), [v2.11](https://colab.research.google.com/gist/tilakrayal/01952d120576ddb7108abad1625fbadb/untitled832.ipynb) and [nightly](https://colab.research.google.com/gist/tilakrayal/2075154ea5c2a250e9399e97ea0ba2f3/untitled833.ipynb). Kindly find the gist.",I was able to reproduce the issue on tensorflow [v2.9](https://colab.research.google.com/gist/tilakrayal/fc3bb093faa07f69c6e82f49cb1efdd9/untitled831.ipynb)
56647,adis300,1369339151,2023-01-03 2:39:28,"> Please sign CLA. We shouldn't have reviewed before that
I signed the CLA a few months ago. Clicked ""update check"" and google showed me 404 error.
I tried again today, and still 404. You can help me on rechecking the commit as a Googler?","""update check"" and ""google showed me 404 error"""
58032,tux-o-matic,1369253720,2023-01-02 22:48:12,"@jdcrunchman , for one thing, no hardware accelerated training since Apple's last ""sort of working"" TF release (`tensorflow-macos`) is stuck on 2.9 and that won’t run with Python 3.11.
So you'll have to wait for a `tensorflow-cpu` release working on Python 3.11 which might be faster thanks to the faster CPython effort on 3.11 but will purely run on the CPU cores of your M1 Mac.","No hardware accelerated training since Apple's last ""sort of working"" TF release (tensorflow-macos) is stuck on 2.9 and that won’t run with Python 3.11."
58995,Neizvestnyj,1368782792,2023-01-02 9:42:18,"> @Neizvestnyj ,
> I observed you installed tensorflow-cpu==2.10 and it seems you are trying to use GPU also. Since Windows native can support GPU for TF <=2.10 v could you uninstall tensorflow and then try removing cpu tag with `pip install tensorflow==2.10` and let us know if problem still persists. Its incorrect, directml support only cpu version, now its 2.10.0, tensorflow for gpu is depracted","Its incorrect, directml support only cpu version, now its 2.10.0, tensorflow for gpu is depracted."
45930,Dayeong-An,1368735022,2023-01-02 8:24:01,HI.. still having a same issue..,still having a same issue
58995,SuryanarayanaY,1368670476,2023-01-02 5:37:56,"@Neizvestnyj ,
I observed you installed tensorflow-cpu==2.10 and it seems you are trying to use GPU also. Since Windows native can support GPU for TF <=2.10 v could you uninstall tensorflow and then try removing cpu tag with `pip install tensorflow==2.10` and let us know if problem still persists.","""I observed you installed tensorflow-cpu==2.10 and it seems you are trying to use GPU also."""
59050,GuyPozner,1368668869,2023-01-02 5:29:44,"You are using a different machine which has an A100 and might have different cpus and different amount of cpus. The issue is that bfloat16 is an optimization that should make training faster not slower, and in your case it is also slower by ~20x.","""Using a different machine which has an A100 and might have different cpus and different amount of cpus"""
59050,GuyPozner,1368173868,2022-12-31 6:39:48,"The issue is not a regression from 2.10, the issue is that on an A10 gpu, bfloat16 should be atleast as fast.","The issue is not a regression from 2.10, the issue is that on an A10 gpu, bfloat16 should be atleast as fast."
58996,edwardyehuang,1368099803,2022-12-30 21:45:59,"> I think the overflow fixes have to be in C++, not in Python
Agree. The example below is also dangerous.
```
import tensorflow as tf
a = 2147483647
a = tf.convert_to_tensor(a, tf.int32)
print(a) # tf.Tensor(2147483647, shape=(), dtype=int32)
b = a + tf.constant(1, tf.int32)
print(b) # tf.Tensor(-2147483648, shape=(), dtype=int32)
```","I think the overflow fixes have to be in C++, not in Python"
57805,kaique-ryan-santos-chagas,1368073704,2022-12-30 20:03:38,I have the same problem when i try to install the tensorflow on Python 3.11 project. ![image](https://user-images.githubusercontent.com/59677362/210108006-6e8a740f-1d98-425a-a90e-9ea596385c50.png),I have the same problem when i try to install the tensorflow on Python 3.11 project.
58982,mihaimaruseac,1368005020,2022-12-30 16:41:34,"TF 2.3 was released before Python 3.9. Dependencies it contains are also not able to work with Python 3.9
On the other hand, you are compiling from source and if you look at the error message closely this is what is happening to `scipy` too, pip tries to compile it from source. You could succeed if you ensure you have the needed environment to do that, but this is no longer a TF issue.
Hence, closing.",TF 2.3 was released before Python 3.9. Dependencies it contains are also not able to work with Python 3.9
58995,Neizvestnyj,1367954857,2022-12-30 14:47:20,"> I have the same problem. You can try to replace Adam optimizer to another one. This problem only cause on this optimizer, avoid to use it and the problems will be solved temporarily
The Stable Diffusion model is already trained and there is no way to change the optimizer",The Stable Diffusion model is already trained and there is no way to change the optimizer.
58995,Yigremacheweshetu,1367903411,2022-12-30 12:52:08,"I have the same problem. You can try to replace Adam optimizer to another one. This problem only cause on this optimizer, avoid to use it and the problems will be solved temporarily",I have the same problem.
58221,mihaimaruseac,1367720168,2022-12-30 4:35:49,"There are no changes when this gets imported. This is because internal autoformatter is different.
Closing PR and associated issue.",No changes when this gets imported. This is because internal autoformatter is different. Closing PR and associated issue.
56137,bperseghetti,1367544980,2022-12-29 19:44:27,"Looks like it won't build on those as they use 3.10.9
python:3.10-buster fails:
![image](https://user-images.githubusercontent.com/10233412/209990270-28f1b2fd-96e2-440c-a94a-513e4ea3c83e.png)
python:3.10-bullseye fails:
![image](https://user-images.githubusercontent.com/10233412/209990199-78b3e93f-e8ab-478d-8c36-762371977b6e.png)",Looks like it won't build on those as they use 3.10.9
58106,maxcopeland,1367516475,2022-12-29 18:43:00,"Appears that the commit referenced in the sec advisory avoids the check fail in their listed test case, but not for the one on this issue.","Appears that the commit referenced in the sec advisory avoids the check fail in their listed test case, but not for the one on this issue."
56137,bperseghetti,1367462158,2022-12-29 16:55:44,"So, building it from 20.04 with deadsnakes for 3.10 fails, even when trying anything greater than the minimum matching python3.10 capable numpy:
![image](https://user-images.githubusercontent.com/10233412/209984641-be23e35e-5e20-4c74-a76e-21785b3fb2db.png)","So, building it from 20.04 with deadsnakes for 3.10 fails, even when trying anything greater than the minimum matching python3.10 capable numpy: ![image](https://user-images.githubusercontent.com/10233412/209984641-be23e35e-5e20-4c74-a76e-21785b3fb2db.png)"
53469,mihaimaruseac,1367452458,2022-12-29 16:37:17,This keeps failing internal tests. Let me see if I can quickly generate a fix,This keeps failing internal tests.
52464,SamuelMarks,1367451049,2022-12-29 16:34:47,"@mihaimaruseac It's not stale, looking at master your latest commit to that file still has the wrong type: https://github.com/tensorflow/tensorflow/blob/dd79e90/tensorflow/compiler/jit/increase_dynamism_for_auto_jit_pass.cc#L80","""It's not stale, looking at master your latest commit to that file still has the wrong type"""
57832,msvbg,1367425730,2022-12-29 15:53:58,"The second occurrence of `#include <malloc.h>` seems to have been replaced by `#include <stdlib.h>`. Now there's only one mention of ""malloc.h"" left, for Linux builds only. Maybe not worth merging anymore. I have rebased the branch either way.","The second occurrence of #include malloc.h> seems to have been replaced by #include stdlib.h>. Now there's only one mention of ""malloc.h"" left, for Linux builds only. Maybe not worth merging anymore. I have rebased the branch either way."
58285,bhack,1367385871,2022-12-29 14:53:45,"Here we have multiple values that we have not set (-1).
The PR is editable on your side so we could differentiate the number of days x labels. We need to set something to replace `-1`
E.g. `contribution welcome` could have a different threshold from `awaiting response` etc..",awaiting response
58978,freedomtan,1367295121,2022-12-29 12:45:57,"> Should this also check that the ranks are equal?
@mihaimaruseac I guess NO, because NNAPI does support broadcast (rank=1), see [documentation of NNAPI's ADD op](https://developer.android.com/ndk/reference/group/neural-networks#group___neural_networks_1ggaabbe492c60331b13038e39d4207940e0ad681988001e5f8ab73230a311f4ab034)","""I guess NO, because NNAPI does support broadcast (rank=1), see [documentation of NNAPI's ADD op](https://developer.android.com/ndk/reference/group/neural-networks#group___neural_networks_1ggaabbe492c60331b13038e39d4207940e0ad681988001e5f8ab73230a311f4ab034)"""
58264,johnthagen,1367276848,2022-12-29 12:11:17,"@bperseghetti See - https://github.com/tensorflow/tensorflow/issues/56137#issuecomment-1342705829
We're basically blocked on either DeadSnakes Ubuntu 18.04 3.10 `armhf` builds being fixed, which some of us in the community tried to help with but we're unsuccessful, OR bumping the Docker image builds for tflite-runtime up to Ubuntu `20.04`.
As a community member, any help with these would be greatly appreciated as this is a big blocker for many of us.","""We're basically blocked on either DeadSnakes Ubuntu 18.04 3.10 armhf builds being fixed, which some of us in the community tried to help with but we're unsuccessful, OR bumping the Docker image builds for tflite-runtime up to Ubuntu 20.04."""
33312,mihaimaruseac,1367078981,2022-12-29 5:18:43,Closing stale PR. Should be reopened when modular filesystem is fully implemented.,Closing stale PR.
42903,mihaimaruseac,1367078577,2022-12-29 5:17:10,Closing stale PR. Please reopen if still needed and wanting to work on it.,Closing stale PR. Please reopen if still needed and wanting to work on it.
54352,mihaimaruseac,1367076575,2022-12-29 5:10:32,"Closing as blocked by protobuf update. When protobuf updates this will also be updated, so the PR will be obsolete",Closing as blocked by protobuf update.
54519,mihaimaruseac,1367075775,2022-12-29 5:08:18,Closing as it stalled. Please open a new one when coming back to this,Closing as it stalled.
52217,mihaimaruseac,1366842044,2022-12-28 18:29:24,@gbaned another one which had the old CLA and failed to import. Needed to force trigger new CLA scan and then it works,"""had the old CLA and failed to import"""
53005,mihaimaruseac,1366837185,2022-12-28 18:19:42,@gbaned This fails because CLA check did not pass before. I triggered a force scan now,"""CLA check did not pass before"""
44658,Ryandry1st,1366343524,2022-12-28 3:39:21,"I see this has been waiting for two years... any chance of an update? Since it was first raised there are now even more complex datasets that can take advantage of TF and the documentation even suggests that it should work exactly as the numpy implementation, yet it does not support complex data types at the moment.
Honestly not sure why this was never adopted or at least commented on.","""Why this was never adopted or at least commented on."""
57798,Co1lin,1366016939,2022-12-27 16:11:12,"Hi @sachinprasadhs! I understand on GPU, this out-of-bound index results in undefined behavior, but I don't think it's a safe behavior because the program **is aborted**, the process is **crashed**, and the CoLab service is **stopped** due to this error. As a public API, it should raise exception at Python level, or just return something, but not let the process crash.","""I understand on GPU, this out-of-bound index results in undefined behavior, but I don't think it's a safe behavior because the program **is aborted**, the process is **crashed**, and the CoLab service is **stopped** due to this error."""
40801,jall,1365912178,2022-12-27 13:44:19,"@pjpratik the company that owns that GCP account shut down this year!
I think this issue should be reproducible with any gzipped file in any account, but I don't have access to a GCP account to help out",the company that owns that GCP account shut down this year
44515,Qrox,1365893784,2022-12-27 13:14:50,"Hi mohantym,
The result in 2.11 is the same as described in the OP so this is not fixed. Though I should have made it clearer that the result was produced after creating two virtual GPUs using the following code:
```
gpus = tf.config.list_physical_devices('GPU')
tf.config.set_logical_device_configuration(
gpus[0],
[tf.config.LogicalDeviceConfiguration(memory_limit=1024),
tf.config.LogicalDeviceConfiguration(memory_limit=1024)])
```
I've updated the OP to clarify it.","The result in 2.11 is the same as described in the OP so this is not fixed. Though I should have made it clearer that the result was produced after creating two virtual GPUs using the following code:  gpus = tf.config.list_physical_devices('GPU') tf.config.set_logical_device_configuration(gpus[0], [tf.config.LogicalDeviceConfiguration(memory_limit=1024), tf.config.LogicalDeviceConfiguration("
58636,deepyapws,1365663155,2022-12-27 7:02:27,"> i think you need to add
`-DTFLITE_KERNEL_TEST=on`
in order for TFLITE_HOST_TOOLS_DIR to be used by cmake, Currently the TFLITE_HOST_TOOLS_DIR variable is not used by Cmake is generating the makefile",Currently the TFLITE_HOST_TOOLS_DIR variable is not used by Cmake is generating the makefile.
58817,seanshpark,1365602062,2022-12-27 4:21:31,"@joker-eph , @nutsiepully doesn't seem to have time so can someone else take the review?","""doesn't seem to have time"""
59015,mihaimaruseac,1365275972,2022-12-26 16:28:39,"Please only report vulnerabilities for __the latest__ version of TF. Old versions cannot be patched, and in this case, it looks like someone already patched these, so your report on old versions can only result in duplicated work.
Also check your report for duplicates against previous versions at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/README.md","""It looks like someone already patched these, so your report on old versions can only result in duplicated work."""
59005,mihaimaruseac,1365275832,2022-12-26 16:28:07,"Using all available RAM is not an issue that we should spend time on, since in this case this is similar to user calling `malloc(a_very_huge_number)` and then expecting this to work.
There's a difference if a small input can generate a large memory allocation, but this doesn't seem to be case","Using all available RAM is not an issue that we should spend time on, since in this case this is similar to user calling malloc(a_very_huge_number) and then expecting this to work."
59012,mihaimaruseac,1365275322,2022-12-26 16:26:26,"Please only report vulnerabilities for __the latest__ version of TF. Old versions cannot be patched, and in this case, it looks like someone already patched these, so your report on old versions can only result in duplicated work.
Also check your report for duplicates against previous versions at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/README.md
Please reopen if this occurs on 2.11, preemptively closing now as all other reports on 2.4 seem to have been resolved.","""Please only report vulnerabilities for __the latest__ version of TF."""
59010,mihaimaruseac,1365274867,2022-12-26 16:25:06,"Please only report vulnerabilities for __the latest__ version of TF. Old versions cannot be patched, and in this case, it looks like someone already patched these, so your report on old versions can only result in duplicated work.
Also check your report for duplicates against previous versions at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/README.md
Please reopen if still present in 2.11, closing as all other similar issues so far have been false positives.","""Please only report vulnerabilities for __the latest__ version of TF."""
59000,mihaimaruseac,1365273892,2022-12-26 16:22:09,"No. Please only report vulnerabilities for __the latest__ version of TF. Old versions cannot be patched, and in this case, it looks like someone already patched these, so your report on old versions can only result in duplicated work.
Also check your report for duplicates against previous versions at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/README.md","No. Please only report vulnerabilities for __the latest__ version of TF. Old versions cannot be patched, and in this case, it looks like someone already patched these, so your report on old versions can only result in duplicated work. Also check your report for duplicates against previous versions at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/README.md."
59004,mihaimaruseac,1365272936,2022-12-26 16:19:31,"This seems invalid. Affected version is old, out of support.","This seems invalid. Affected version is old, out of support."
58999,mihaimaruseac,1365272147,2022-12-26 16:17:25,This seems invalid. Log message does not show claimed crash. Affected version is old.,This seems invalid. Log message does not show claimed crash. Affected version is old.
53750,mihaimaruseac,1365268729,2022-12-26 16:10:58,Someone needs to manually shepherd this inside as it touches code managed by copybara (inside it looks different),"""Manually shepherd this inside as it touches code managed by copybara (inside it looks different"")"""
33030,vermouth1992,1365218358,2022-12-26 14:19:14,I have to unsubscribe from this thread. Final suggestion: maybe it's time to switch.,I have to unsubscribe from this thread.
58997,Mussil,1365120033,2022-12-26 12:00:42,"I'm not sure which GPU is used in [gpu](https://colab.research.google.com/gist/tiruk007/ee6739fcbdbffc42476c77301eaf9511/58997gpu.ipynb) , I faced this issue on A100 GPU in particular. Anyway upgrade to TF v2.11 didn't help.","I'm not sure which GPU is used in [gpu](https://colab.research.google.com/gist/tiruk007/ee6739fcbdbffc42476c77301eaf9511/58997gpu.ipynb) , I faced this issue on A100 GPU in particular."
53750,georgthegreat,1365107876,2022-12-26 11:28:05,"@gbaned, is there a chance for this PR to be accepted and merged?
I have rebased this PR twice and it is almost one year old as of today.
If it is not going to be merged, I suggest closing this PR.","I have rebased this PR twice and it is almost one year old as of today. If it is not going to be merged, I suggest closing this PR."
58938,TZeng20,1365061862,2022-12-26 10:17:26,"Hi @SuryanarayanaY, Thanks for your reply, however that is not what I'm looking for. I was looking to use the default tf.keras.Model.fit with a custom train step, like [here](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit). The code you have attached does not use the default model.fit method. So I may not have access to `global_batch_size`, and also not sure if model.fit uses tf.distribute.ReduceOp.SUM to aggregate gradients of each replica.","""I was looking to use the default tf.keras.Model.fit with a custom train step, like [here](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit). The code you have attached does not use the default model.fit method."""
58368,freedomtan,1364678435,2022-12-25 12:59:50,@nitins17 FYR. Setting `DEVELOPER_DIR=/Applications/Xcode.app/Contents/Developer` doesn't work for me on macOS 13.1 + Xcode 14.2 (either M1 or x86_64).,Setting DEVELOPER_DIR=/Applications/Xcode.app/Contents/Developer doesn't work for me on macOS 13.1 + Xcode 14.2 (either M1 or x86/64).
58983,zexelon,1363538968,2022-12-23 2:02:55,"@tiruk007 Unfortunately I am unable to properly build a test case to show this as the issue is only exposed within the mask-rcnn code. That said, I have started over from a clean slate and now this issue no longer appears. I think its an issue somewhere in the mix between Nvidia CUDA/cuDNN versions, tensorflow and keras versioning but the error is in no way clear as to where the problem is. I am okay to close this as I cannot reproduce this issue currently with a fresher setup.",I am unable to properly build a test case to show this as the issue is only exposed within the mask-rcnn code.
58970,cheshire,1363206067,2022-12-22 18:19:58,"> there are a number of tests in tensorflow that have one or more sanitizers disabled, when they shouldn't, i.e. noasan or nomsan tags
Sure, but then I don't think this patch would solve it given that otherwise all XLA tests would have to have noasan?
We've discussed it a bit more, why not instead write
```
static Thread* t = tsl::Env::Default()->StartThread(....)
```
I think this should be enough to silence asan, and the resource usage would be the same?",noasan or nomsan tags
58958,Lotte1990,1362597887,2022-12-22 9:17:04,"@SuryanarayanaY The docker images are not listed in the link you provided. 2.5.0 and 2.5.1 are present, but 2.5.2 and 2.5.3 are missing. Please add these docker images. I cannot upgrade beyond these versions because my models behave differently from 2.6 onwards.",The docker images are not listed in the link you provided.
58933,MDSchechtman,1361806824,2022-12-21 18:15:51,"> Can you provide an alternate / temporary link for the GPU .zip file? I don't want to build/install this from scratch, but just need the valid zip file so I can reference some of the included DLL files.
Note sure about how to get the latest version, but the previous can be downloaded by changing the url: https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-windows-x86_64-2.10.0.zip",Can you provide an alternate / temporary link for the GPU .zip file?
58952,LuFinch,1361340648,2022-12-21 13:54:56,I have tried `tensorflow.compat.v1.disable_v2_behavior()` but it does not work and throw the same error.,I have tried tensorflow.compat.v1.disable_v2_behavior() but it does not work and throw the same error.
58752,SuryanarayanaY,1361044398,2022-12-21 9:09:45,"Hi @YaoJiayi ,
Sorry for delayed response.
The API `tf.random.stateless_uniform` for `float` dtypes seems working fine with the default range [0,1) only. Please refer attached [gist](https://colab.sandbox.google.com/gist/SuryanarayanaY/6784c0a6b2ae5936faea3e1106d99e2a/58752-gpu.ipynb).","""The API tf.random.stateless_uniform for float dtypes seems working fine with the default range [0,1] only"""
58881,perestoronin,1360957833,2022-12-21 7:38:24,"Same issues about undefined reference to CSRSparseMatrixTranspose:
- https://github.com/tensorflow/tensorflow/issues/43234 - https://github.com/tensorflow/tensorflow/issues/57371",Same issues about undefined reference to CSRSparseMatrixTranspose: - https://github.com/tensorflow/tensorflow/issues/43234 - https://github.com/tensorflow/tensorflow/issues/57371.
58933,krsummersBA,1360716248,2022-12-21 2:18:16,"Can you provide an alternate / temporary link for the GPU .zip file? I don't want to build/install this from scratch, but just need the valid zip file so I can reference some of the included DLL files.",Can you provide an alternate / temporary link for the GPU .zip file?
58967,nitins17,1360619142,2022-12-21 0:59:00,Closed as I still need to add configs for Python 3.11 in `tensorflow/tools/toolchains/remote_config/configs.bzl`,I still need to add configs for Python 3.11 in tensorflow/tools/toolchains/remote_config/configs.bzl.
58857,impjdi,1360437506,2022-12-20 23:11:59,"I don't have an AMD GPU to test this out. It could be OpenCL driver bug, or our shaders may be relying on undefined behavior of mobile GPUs that don't translate to your AMD GPU. The only thing I would try out is using MAX PRECISION as top inference priority to make sure it's not the FP16 getting in the way. Regardless of the outcome, this is beyond our level of support.","I don't have an AMD GPU to test this out. It could be OpenCL driver bug, or our shaders may be relying on undefined behavior of mobile GPUs that don't translate to your AMD GPU. The only thing I would try out is using MAX PRECISION as top inference priority to make sure it's not the FP16 getting in the way. Regardless of the outcome, this is beyond our level of support."
58851,nouiz,1360151782,2022-12-20 20:32:16,"`//tensorflow/compiler/xla/tools:run_hlo_module` is also broken.
Can the build of one of those 2 tools (or both) be included in the CI?
It isn't great to have our basic tools broken for over a week now.",//tensorflow/compiler/xla/tools:run_hlo_module is also broken.
58843,mihaimaruseac,1359831997,2022-12-20 17:20:05,There is no TF 2.13 released yet.,No TF 2.13 released yet.
40614,mihaimaruseac,1359692423,2022-12-20 16:35:52,The issue here is not about Windows.,The issue here is not about Windows.
58768,Flamefire,1359412840,2022-12-20 14:03:06,"Further investigation showed that this (`__subpackages__`) wasn't the exact issue. But it is this: https://github.com/tensorflow/tensorflow/blob/8a254397ff86a5a2efc0b51ef2eaee1059299095/tensorflow/lite/delegates/xnnpack/BUILD#L128
As ""PowerPC is not supported in XNNPACK"" this means now TF is not supported on PowerPC as the build will fail. A fix might be to use a better default for `tflite_with_xnnpack` which depends on the architecture.","Further investigation showed that this (__subpackages__) wasn't the exact issue. But it is this: https://github.com/tensorflow/tensorflow/blob/8a254397ff86a5a2efc0b51ef2eaee1059299095/tensorflow/lite/delegates/xnnpack/BUILD#L128 As ""PowerPC is not supported in XNNPACK"" this means now TF is not supported on PowerPC as the build will"
58768,Flamefire,1359047186,2022-12-20 9:12:25,Any update here? The referenced commit doesn't seem to made it into any branch of XNNPACK so this needs to be fixed from this repo,The referenced commit doesn't seem to made it into any branch of XNNPACK so this needs to be fixed from this repo.
58809,xiaogangzhu,1358834145,2022-12-20 4:43:49,Hi I tired TF2.11 and cuda11.2 and the issue still present.,I tired TF2.11 and cuda11.2 and the issue still present.
50937,Reginaekwere,1358568779,2022-12-19 23:19:53,"> Is the issue resolved?
No",No.
50925,gowthamkpr,1358362285,2022-12-19 21:05:55,"Yes, make sure you use a clean environment as saving become tricky and you can run into these errors which are hard to debug","""saving become tricky and you can run into these errors which are hard to debug."""
33030,Neizvestnyj,1357733904,2022-12-19 14:12:48,Have save bug on `tensorflow==2.10.0` when I use Stable Diffusions models,tensorflow==2.10.0
58747,RocaVincent,1357731133,2022-12-19 14:10:55,@tilakrayal The code is too complexe and I haven't managed to create a simple reproducer. But I tried the same code with the [classical pip install](https://www.tensorflow.org/install/pip) and the error disappeared.,The code is too complexe and I haven't managed to create a simple reproducer.
57104,carlthome,1357441338,2022-12-19 10:39:35,"Any update on this from Googlers? This is a pretty tricky annoyance in our CI/CD workflows on GitHub Actions to Vertex and Dataflow, since `gcloud` just works while TensorFlow Datasets (`tfds` CLI and `tfds.load`) does not without jumping through hurdles.","pretty tricky annoyance in our CI/CD workflows on GitHub Actions to Vertex and Dataflow, since gcloud just works while TensorFlow Datasets (tfds CLI and tfds.load) does not without jumping through hurdles."
58740,twerkmeister,1357426868,2022-12-19 10:26:13,"Hey @SuryanarayanaY, this seems indeed to be limited to some windows versions. One of them is windows server 2019, which is what I tested on initially and the system we at rasa run our CI for windows. I just created a fresh windows server 2022 instance on gcloud and it didn't have the error. I also tested again on the Windows server 2019 instance installing with pip tensorflow-cpu==2.10.0 as you did, and I got the same error again.",I just created a fresh windows server 2022 instance on gcloud and it didn't have the error.
58823,FranciszekNowak,1356824214,2022-12-18 15:41:59,"Thanks, I think I am able to write code with full integer quantization now.
But how would you refer to the rest of the problems mentioned above (for example: python interpreter memory placement or persisting increase in memory)?
Please help, I am hopeless without external advice.","""How would you refer to the rest of the problems mentioned above (for example: python interpreter memory placement or persisting increase in memory)?"""
58665,mihaimaruseac,1356591612,2022-12-18 0:45:33,"Can you make one single commit with the change in the first file? 7 commits for just one single file change are a little bit too much.
You can squash all commits to a single one too if you don't want to open a separate PR.",7 commits for just one single file change are a little bit too much.
33131,innat,1354907218,2022-12-16 14:10:55,"@mohantym cc @aohan237 The item assignment is still not included. So, it doesn't matter to re-check with current release (tensorflow==2.11).
@aohan237 Could you please check it https://github.com/tensorflow/community/pull/433 and share your thoughts there? (direct [link](https://github.com/tensorflow/community/blob/a0896d7f6062a937208ebdf6c3c7ad6304babb63/rfcs/20221020-tensor-indexed-updates.md))","The item assignment is still not included. So, it doesn't matter to re-check with current release (tensorflow==2.11)."
58875,AntouanK,1354608503,2022-12-16 11:42:33,"So, I managed to build it successfully.
![image](https://user-images.githubusercontent.com/4569111/208090788-c450f978-0b29-4cae-a257-a9c15dcb7864.png)
Not sure what the next step is.
Running that executable doesn't do anything.
![image](https://user-images.githubusercontent.com/4569111/208090908-871a2164-d0b9-48df-a1e0-b0ec75790363.png)",Not sure what the next step is. Running that executable doesn't do anything.
58875,AntouanK,1354559743,2022-12-16 11:03:28,"@mohantym got an error on this step
```
!bazel build -c opt --define MEDIAPIPE_DISABLE_GPU=1 mediapipe/examples/desktop/hand_tracking:hand_tracking_cpu
```
![image](https://user-images.githubusercontent.com/4569111/208084600-f54b6bd9-f20b-4f99-ba23-4fa11b4e19ea.png)
I got those files, not sure how to tell bazel about it
![image](https://user-images.githubusercontent.com/4569111/208085940-4d798a94-1b8e-4bbc-af82-dc6fdf3df4e3.png)
**EDIT**
adding `--copt -I/usr/include/opencv4` solved that issue",got an error on this step
58875,AntouanK,1354543722,2022-12-16 10:47:45,"@mohantym I opened the notebook you mentioned and I connected it to my local runtime.
On the 3rd step I get an error
![image](https://user-images.githubusercontent.com/4569111/208082005-09fe93c7-c5bc-4dd8-8903-79350fa172e6.png)
EDIT :
I had to add `--no-check-certificate` to the command.",I opened the notebook you mentioned and I connected it to my local runtime. On the 3rd step I get an error ![image](https://user-images.githubusercontent.com/4569111/208082005-09fe93c7-c5bc-4dd8-8903-79350fa172e6.png)
58738,tilakrayal,1354345761,2022-12-16 7:53:12,"@LIONEFAN,
Sorry for the delay. I tried on tensorflow v2.10 to reproduce the issue but I was not able to fetch the files which are mentioned. Kindly find the [gist](https://colab.research.google.com/gist/tilakrayal/a78bb5afe17b093e69cb489d2281cf12/2-1000000.ipynb) and the image for the reference.
![image](https://user-images.githubusercontent.com/81610181/208050074-5c7115bc-fdaf-47f1-b923-e989b9bb1877.png)",I tried on tensorflow v2.10 to reproduce the issue but I was not able to fetch the files which are mentioned.
58675,akuegel,1354319162,2022-12-16 7:18:27,"> @trisolaran @akuegel Can one of you approve again this PR? It was already approved. I only fixed some builds issues after the approval.
I approved it 3 days ago, and I don't see another commit by you after that. So I guess we pulled in the latest state? I had to do a few more fixes, but should be merged soon.","I had to do a few more fixes, but should be merged soon."
58675,nouiz,1353874075,2022-12-15 23:44:21,"@trisolaran @akuegel Can one of you approve again this PR?
It was already approved. I only fixed some builds issues after the approval.",Can one of you approve again this PR? It was already approved. I only fixed some builds issues after the approval.
58720,reedwm,1353659679,2022-12-15 20:18:48,"Also note, I submitted 5d27ce5a27e5123266f5872b3a25514f0c9453fe which causes even more merge conflicts. I will address. We made some similar changes to various helper functions, but since I was aware of internal tests I ensured they all passed. It was easier to submit my change first then revert the equivalent changes you made.","""It was easier to submit my change first then revert the equivalent changes you made."""
58720,reedwm,1353598411,2022-12-15 19:23:44,There are merge conflicts and internal test failures. I will fix and merge. Please don't update the PR to address merge conflicts since it will revert the internal test failure fixes I am doing.,merge conflicts and internal test failures.
44369,impjdi,1353473279,2022-12-15 17:43:18,"No, our OpenGL delegate doesn't accommodate per-op profiling (IIUC it's about per-shader profiling not being a part of the GL ES API, and only available as an extension which can be buggy / inaccurate for each vendor; Metal or OpenCL on the other hand have per-shader profiling in their API) and therefore we don't have an equivalent profiler for OpenGL.","""IIUC it's about per-shader profiling not being a part of the GL ES API, and only available as an extension which can be buggy / inaccurate for each vendor; Metal or OpenCL on the other hand have per-shader profiling in their API"")"
57630,bhack,1353336265,2022-12-15 16:11:57,@MichaelHudgins Let me know when you are ready so that I can reopen this old contribution. Currently the build time/required computing resources is one of the biggest bottleneck to the non enterprise OSS contribution in the repo.,"""Currently the build time/required computing resources is one of the biggest bottleneck to the non enterprise OSS contribution in the repo."""
54352,mihaimaruseac,1353315364,2022-12-15 15:59:49,This should happen at the same time as upgrading protobuf. That one is blocked.,This should happen at the same time as upgrading protobuf. That one is blocked.
58851,chr1sj0nes,1353072494,2022-12-15 13:32:33,"> @chr1sj0nes Could you take look at this issue? seems that the broken is related to your commits to `tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc`
Apologies for the slow reply. I can't see how my commit might have caused this error. @d0k 's theory looks a lot more likely to me.",I can't see how my commit might have caused this error.
23545,gowthamkpr,1352366394,2022-12-14 23:43:04,"@IFeelBloated As mentioned [here](https://github.com/tensorflow/tensorflow/pull/23596#issuecomment-454192185), this is not possible. Please let me know. Thanks!","As mentioned [here](https://github.com/tensorflow/tensorflow/pull/23596#issuecomment-454192185), this is not possible. Please let me know. Thanks!"
57928,brnorris03,1352329615,2022-12-14 22:57:11,"Yes, I have been using the standalone `mlir-hlo` repo, although it has its own CMake build issues (but easier for me to work around). I think if cmake support is not maintainable in the tensorflow tree, then it may be better to just remove it altogether.","""I think if cmake support is not maintainable in the tensorflow tree, then it may be better to just remove it altogether."""
58851,nouiz,1352100692,2022-12-14 20:15:33,"> @mohantym I tried to reproduce the issue on Colab but got a different output. Please find the gist [here](https://colab.research.google.com/gist/tiruk007/9ffa05e9d87cd8f19d51065ec8ecad6f/untitled41.ipynb) for reference. Could you please look into this.
Note, the colab checkout TF 2.11. The issue reported here is about the master branch, not the last release.",I tried to reproduce the issue on Colab but got a different output.
58522,janbernloehr,1351964069,2022-12-14 18:47:32,We unfortunately cannot use tf.function in our scenario because we call into a python library. We also had no issues with this until TF 2.10 was released.,"""We also had no issues with this until TF 2.10 was released."""
42138,mihaimaruseac,1351961713,2022-12-14 18:46:21,"Until modular TF is brought back, these updates are just treading water. We cannot take the PR in until modular TF work is being done and until then there will be conflicts and staleness.
It might be better to close it and revisit later, given there is no way to mark this work as paused. Or, maybe add a ""paused"" label?","Until modular TF is brought back, these updates are just treading water."
58522,JXRiver,1351955487,2022-12-14 18:44:50,"This may be related to `py_function`. py_function requires GIL and is generally not recommended to be used in tf.data except for experimental purpose. See the limitation part of https://www.tensorflow.org/api_docs/python/tf/py_function.
Did you try use `tf.funciton` instead of `py_function`?",py_function. py_function requires GIL and is generally not recommended to be used in tf.data except for experimental purpose. See the limitation part of https://www.tensorflow.org/api_docs/python/tf/py_function. Did you try use tf.funciton instead of py_function?.
58825,brandjon,1351772684,2022-12-14 17:00:04,"It looks like 5.4 will go ahead without adding support for `public`. As best as we can tell, it also seems like TF is the only significant case where this has impacted migration. So at the moment we're not planning on backporting this, given that there is the workaround of using `//visibility:public` in place of a package group.",5.4 will go ahead without adding support for public.
55498,mihaimaruseac,1351549482,2022-12-14 14:46:43,"Can you rebase this on master again, please?","Can you rebase this on master again, please?"
58763,sergeykozub,1351476256,2022-12-14 14:18:55,"> Can you run Google internal performance tests first?
I've run the internal benchmarks, and there are no regressions (and no improvements though, probably the new code path is not triggered in the existing benchmarks).",Can you run Google internal performance tests first?
58858,DerryFitz,1351153612,2022-12-14 11:52:46,"Also, I can replicate the bug back as far as tf 2.6, it seems to be ok in tf 2.5, but I really need to use more modern versions of tf for other features in my actual model","I can replicate the bug back as far as tf 2.6, it seems to be ok in tf 2.5, but I really need to use more modern versions of tf for other features in my actual model."
6341,babinos87,1350716024,2022-12-14 9:27:23,"For me it works when I create an `__init__.py` file inside `<venv_location>/lib/python3.10/site-packages/protobuf-4.21.11-py3.10.egg/google`.
But I don't think this is an acceptable solution. I shouldn't have to create any files manually. Other things mentioned (e.g. re-installing packages) do not work for me.
All I do is to install `google-ads`, which installs all required packages. Not a tensor flow issue, but it seems a google one.","""Not a tensor flow issue, but it seems a google one."""
58090,akuegel,1350693733,2022-12-14 9:14:48,"> @rsanthanam-amd Can you please change your fix so that instead of setting GPU count to 0, you change the MaybeCreateNcclCommunicator() method to also not create a NCCL manager if the device_filter doesn't include any GPU?","Can you please change your fix so that instead of setting GPU count to 0, you change the MaybeCreateNcclCommunicator() method to also not create a NCCL manager if the device_filter doesn't include any GPU?"
58762,SajjadAemmi,1350648211,2022-12-14 8:45:29,"@krrishdholakia i installed Tensorflow-IO, but same error","""i installed Tensorflow-IO, but same error"""
56420,guweixin,1350528894,2022-12-14 7:21:16,@sssstudyy still not solved.,still not solved
55303,API92,1350048895,2022-12-13 23:32:45,"I am using TPUStrategy. There is no TPUEstimator in my code. The word ""TPUEstimator"" only appears in the exception message thrown by TensorFlow in what I consider to be a bug.","I am using TPUStrategy. There is no TPUEstimator in my code. The word ""TPUEstimator"" only appears in the exception message thrown by TensorFlow in what I consider to be a bug."
58870,shawnwang18,1349203366,2022-12-13 17:31:52,"@cheshire I did not find a create new discussion option in https://github.com/openxla/xla/discussions, is there some permission issue?","I did not find a create new discussion option in https://github.com/openxla/xla/discussions, is there some permission issue?"
58840,kp3393,1348454789,2022-12-13 12:33:05,"Hello @mohantym,
I see where the problem is. You are still at macOS Monterey (your platform is **macOS-12.6.1-arm64-arm-64bit**). Please, refer to my problem description. I have mentioned my platform to be **macOS-13.0.1-arm64-arm-64bit**. If you update to macOS Ventura, there is a high possibility that you will face the same issues.","""If you update to macOS Ventura, there is a high possibility that you will face the same issues."""
44918,pjpratik,1348186974,2022-12-13 10:58:54,"@edgarbc !
When original h5 model is used on TF 2.11 using [this](https://github.com/tensorflow/tensorflow/issues/44918#issuecomment-730525525), It throws shape mismatch error. Could you please refer this [gist](https://colab.research.google.com/gist/pjpratik/77f19ce813bd3371185064cb32787ab6/44918.ipynb). The original h5 model is taken from [here](https://www.dropbox.com/s/gb78ssrnv94puk9/mymodel.h5?dl=0) and tested on a random image with (640,480,3) shape to run the gist.","""It throws shape mismatch error"""
58849,krrishdholakia,1347870410,2022-12-13 7:38:06,"It looks like you are running out of memory when you are trying to allocate a tensor with shape [2,3,1030,1030] and type float. To solve this issue, try reducing the size of the tensor or using a smaller batch size. You may also want to reduce the number of layers in your model or try to use a lighter weight model - clerkie (https://clerkie.co/)",running out of memory
58866,Rishi-11-2,1347863469,2022-12-13 7:33:36,I have AMD GPU so CUDA Toolkit wont be compatible with it. My question is why LSTM using cuda operations?,I have AMD GPU so CUDA Toolkit wont be compatible with it.
58852,gaohuacq,1347751620,2022-12-13 5:03:30,"> @gaohuacq Can you please refer through [this](https://github.com/keras-team/keras/issues/12625#issuecomment-668814501) comment and let us know if that helps. Thank you.
I have tested this, but it doesn't work.","I have tested this, but it doesn't work."
58368,feranick,1347436695,2022-12-12 22:34:52,"> Hi guys, Xcode [14.2 RC](https://developer.apple.com/news/releases/?id=12072022h) is out. Maybe it will help you?
Thanks. Just tried (with both TF 2.11.0 and TF2.10) and it is still broken. Log attached (it's similar to the previous). [log.txt](https://github.com/tensorflow/tensorflow/files/10212480/log.txt)","""Just tried (with both TF 2.11.0 and TF2.10) and it is still broken."""
58786,dpm16,1347042322,2022-12-12 18:36:08,The error only happens on windows and works on linux. It does work on the Jupyter notebook but not an any Windows machine running it.,The error only happens on windows and works on linux.
58675,nouiz,1346963460,2022-12-12 17:59:59,"I updated this PR with another commit to fix the build on some platform.
I mixed NCCL and NVTX macro.",I updated this PR with another commit to fix the build on some platform. I mixed NCCL and NVTX macro.
58261,mihaimaruseac,1346931003,2022-12-12 17:33:15,"https://github.com/tensorflow/tensorflow/blob/de8c87d351456d5f0d6fcf6b6d3c7d5e63c2b701/tensorflow/core/kernels/rnn/gru_ops.cc#L32-L35
`OP_REQUIRES_OK` has a similar semantic, if the status is no `Ok` the macro finishes execution of the kernel and returns the invalid `Status` back to the user.",OP_REQUIRES_OK
58441,cheshire,1346455254,2022-12-12 13:02:40,"Adding a huge number of ROCM-specific tests is not great, could we generalize existing ones instead?","Adding a huge number of ROCM-specific tests is not great, could we generalize existing ones instead?"
58746,roserg,1346111959,2022-12-12 8:55:48,"Conv2DTranspose supported on gpu.
Not supported Pack and Shape that you have in the first case.",Not supported
58851,shawnwang18,1345819296,2022-12-12 3:28:06,Seems that recent commits to `tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc` has broken the build? bazel build --verbose_failures //tensorflow/compiler/xla/tools:replay_computation_gpu,"""Recent commits to tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc has broken the build"""
57928,brnorris03,1345283292,2022-12-10 15:05:13,"The specific problem has not been addressed, namely the `add_subdirectory(stablehlo)` line, given that there is no `stablehlo` subdirectory in `tensorflow/compiler/xla/mlir_hlo` (it is in the top-level `third_party` directory).
https://github.com/tensorflow/tensorflow/blob/4a1b6f1b37b992aa9c74683fb1f2af55832812c8/tensorflow/compiler/xla/mlir_hlo/CMakeLists.txt#L171","The specific problem has not been addressed, namely the add_subdirectory(stablehlo) line, given that there is no stablehlo subdirectory in tensorflow/compiler/xla/mlir_hlo (it is in the top-level third_party directory)."
58832,meteorcloudy,1344268731,2022-12-09 12:46:46,Looks like tf_runtime needs to be fixed first.,Looks like tf_runtime needs to be fixed first.
44306,pjpratik,1343932616,2022-12-09 7:04:22,@sayakpaul I was trying to reproduce this in TF 2.11. I got data access forbidden error. Can you please check [this](https://colab.research.google.com/gist/pjpratik/49028e7ef71fb197678dcb4d7013b677/albert_keras.ipynb) gist and help me with this. Thank you.,I was trying to reproduce this in TF 2.11
17353,purvang3,1343550358,2022-12-08 23:49:26,"> I am trying deepamc_maskrcnn and having the same issues. https://github.com/tensorflow/models/tree/master/official/projects/deepmac_maskrcnn ![image](https://user-images.githubusercontent.com/103889847/175985384-d9c8cc1b-2d86-47a0-baba-2defd9d6ada8.png) ![image](https://user-images.githubusercontent.com/103889847/175985604-030f72d5-ed00-48c2-aec5-59fc58bdaf33.png)
I am using same architecture and getting similar error. Did you find solution @Devvvi1 ?",I am using same architecture and getting similar error.
58544,elgraniti,1343175439,2022-12-08 18:42:47,"Same problems here.
I'm using:
Red Hat Enterprise Linux Server release 7.9 (Maipo)
TF = 2.10.0
python = 3.10 cuDNN/8.1.1.33
CUDA-11.2.2
Do I need to upgrade CUDA? or cuDNN
In older versions of tf I dont have this problem","""Same problems here"""
58805,essandess,1342938519,2022-12-08 15:54:42,"I tried setting `bazel build --linkopt=""-fuse-ld=${prefix}/bin/ld""` but this also fails with the same error.","I tried setting bazel build --linkopt=""-fuse-ld=$prefix/bin/ld"" but this also fails with the same error."
58522,thoomi,1342773825,2022-12-08 13:54:43,Setting the `TF_RUN_EAGER_OP_AS_FUNCTION` to `false` helps and solves the issue for TF 2.10. However this flag was removed in TF 2.11 and we are still running into the same problem there.,Setting the TF_RUN_EAGER_OP_AS_FUNCTION to false helps and solves the issue for TF 2.10 However this flag was removed in TF 2.11 and we are still running into the same problem there.
58808,moonlightnexus,1342661294,2022-12-08 12:32:54,@SuryanarayanaY I cannot report this to you now because I have to downgrade tensorflow to 2.10 for doing my work earliest I'll definitely come back next week after upgrading to tensorflow==2.11 thanks,I cannot report this to you now because I have to downgrade tensorflow to 2.10 for doing my work earliest
57679,jthibaut,1342230732,2022-12-08 8:00:19,"I have the following message after the last command:
`bash: /home/jacquin/miniconda3/etc/conda/activate.d/env_vars.sh: Aucun fichier ou dossier de ce type
`
Translated:
`bash: /home/jacquin/miniconda3/etc/conda/activate.d/env_vars.sh: No file or no directory of tthis type
`",bash: /home/jacquin/miniconda3/etc/conda/activate.d/env_vars.sh: Aucun fichier ou dossier de ce type
58816,shkarupa-alex,1342172957,2022-12-08 7:03:12,+1 waiting for actual cuda version support,waiting for actual cuda version support
58774,rajutechie,1341328779,2022-12-07 17:36:33,"@sushreebarsa Now I can install TensorFlow using the new conda env, but I am still facing the issue with tensorflow.keras module import. Tensorflow 2.11.0, Anacondo3 and Mac OS Ventura M1
<img width=""657"" alt=""image"" src=""https://user-images.githubusercontent.com/68769797/206251827-8b4c1ff4-4e32-4807-b2a1-114a188fe98f.png"">",I am still facing the issue with tensorflow.keras module import.
57756,ronan-keane,1341132734,2022-12-07 15:27:01,"Your response makes no sense to me. The issue has nothing to do with loops. The issue is with tensor out of scope. If you look at the fix I used to make this code work, it seems extremely clear to me that it is a bug in tensorflow's jit compilation. If you think that is wrong, please explain why.","""The issue has nothing to do with loops. The issue is with tensor out of scope."""
58674,twerkmeister,1341124755,2022-12-07 15:21:19,"Hey @SuryanarayanaY, any idea how I can test this right now? Since this is about the wheel thats published on pypi, I would need to new wheel with the change to be published, right? So I'll probably have to wait until the next micro release ~","""I'll probably have to wait until the next micro release"""
58522,janbernloehr,1340938377,2022-12-07 13:05:51,@bhack @sagunb : we cannot update from tensorflow 2.9 because of this bug. any updates on this when this will be resolved?,"""We cannot update from tensorflow 2.9 because of this bug."""
57914,freedomtan,1340930107,2022-12-07 12:59:12,"> @freedomtan I suspect this is the problem now, thanks for reminding me.
Yes, I am pretty sure that's the ld problem (or maybe compiler+ld problem). It's not an M1 only issue. I ran into same problems on MacBook Pro 2018 + Ventura + Xcode 14.1",I am pretty sure that's the ld problem (or maybe compiler+ld problem). It's not an M1 only issue. I ran into same problems on MacBook Pro 2018 + Ventura + Xcode 14.1.
58808,moonlightnexus,1340919607,2022-12-07 12:49:34,"Thanks; got it, wish if installing 2.11 would have been less time consuming, as simple as pip install --upgrade tensorflow
tell me if I should close this issue ?","""wish if installing 2.11 would have been less time consuming, as simple as pip install --upgrade tensorflow tell me if I should close this issue ?"""
58808,moonlightnexus,1340861599,2022-12-07 11:56:56,"![issue](https://user-images.githubusercontent.com/68702919/206173111-2dbaa2ea-797c-422a-b427-f05e8d4a0711.jpg)
oh! I missed to attach this images",I missed to attach this images.
57679,jthibaut,1340675154,2022-12-07 9:51:00,"How do you do to instal TF 2.9 ?
I have the same issues but I don't succeed in fixing them !",I have the same issues but I don't succeed in fixing them !
57671,jthibaut,1340670220,2022-12-07 9:46:47,"I have the same issue: Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7
But I am lost; I tried to follow the instructions and it doesn't work.
Anyone could help me ? I am not an IT specialist.
Regards",I am lost; I tried to follow the instructions and it doesn't work.
56464,mihaimaruseac,1340070105,2022-12-06 22:04:42,Unfortunately nothing could be done until #53234 or `cc_shared_library` migration is fully done,cc_shared_library migration is fully done.
57880,sachinprasadhs,1340058832,2022-12-06 21:53:28,"We intentionally don't check on GPU, and this is usually documented. On GPU, the check is not worth the extra kernel launch required. Garbage in, garbage out. The calculation has no meaning regardless.
Specifically, quoting from the docs for `scatter_nd`:
> Note that on CPU, if an out of bound index is found, an error is returned. On GPU, if an out of bound index is found, the index is ignored.","""The calculation has no meaning regardless."""
58675,nouiz,1340052288,2022-12-06 21:46:22,"Sorry for the churn, this need a new approval as I updated again the last commit.
I missed one line in the last commit, so if_nccl wasn't available.
This changed between the 2 branches I'm working on and I didn't expected that.","I missed one line in the last commit, so if_nccl wasn't available."
58676,kanvi-nervana,1339935468,2022-12-06 20:00:17,"@DrChrisLevy Can you please share the TF-version you tried?
I used TF-2.10 and did not see the issue with the env var set to false. However, I see the issue in TF-2.11 again and on further investigation found that the env var was removed from TF-2.11+ with this [commit](https://github.com/tensorflow/tensorflow/commit/0bbe98a93de2b19dcd4fb0f67fcdea9995c1e1db)
Can you please test TF-2.10?
cc: @sagunb","""Can you please test TF-2.10?"""
58791,dengpanyin,1339852629,2022-12-06 19:14:58,"@mohantym Sorry, unable to share the code publicly due to company policy. Can we schedule a live debugging session if that help?",unable to share the code publicly due to company policy
57679,markub3327,1339772160,2022-12-06 18:07:40,"I'm waiting for the stable release of 2.12.0, there will be TensorRT 8 working.
Until that, I'm using TF 2.9.
TF 2.10 and 2.11 are terrible releases.","Until that, I'm using TF 2.9. TF 2.10 and 2.11 are terrible releases."
56868,rahulbatra85,1339695155,2022-12-06 17:08:57,"> Is it possible to do the check in a cleaner way? E.g. SkipTest based on platform?
@cheshire I am doing it based on platform, but this is the only way I was able to get platform info. Some other fields don't differentiate between GPU platforms like CUDA or ROCm","I am doing it based on platform, but this is the only way I was able to get platform info. Some other fields don't differentiate between GPU platforms like CUDA or ROCm."
58775,brilliant-ember,1339383236,2022-12-06 13:21:54,In general this exception is raised when there's less timeframes than label length. When it is raised that means that the ctc model is not training for that sample. But sometimes you get cases where the label is just too short so it stops the training on an exception while it could have been just a small warning.,"""This exception is raised when there's less timeframes than label length"""
58676,penpornk,1338659876,2022-12-06 2:54:03,"> @DrChrisLevy I am an engineer from Intel and was able to reproduce this issue on TF2.10+. This [commit](https://github.com/tensorflow/tensorflow/commit/c1286fe88e14146a47) which changes the default value of the env var TF_RUN_EAGER_OP_AS_FUNCTION to true seems to be the causing the leak. As a work around, could you please try setting the env var to false at runtime. e.g. TF_RUN_EAGER_OP_AS_FUNCTION=false python test_script.py
Cc'ing @sagunb, the author of the commit.","""This [commit](https://github.com/tensorflow/tensorflow/commit/c1286fe88e14146a47) which changes the default value of the env var TF_RUN_EAGER_OP_AS_FUNCTION to true seems to be the causing the leak"""
58791,cybergeekgyan,1338408598,2022-12-06 0:07:44,"@dengpanyin How can I assign this issue to myself as I also don't have the privileges on the assignee task.
@tiruk007 please look into the matter","""I also don't have the privileges on the assignee task."""
58742,angerson,1338350760,2022-12-05 23:28:58,"While I appreciate the work you've done here, I prefer not to accept this change because:
- These are ""officially supported"" TensorFlow docker containers, and it's not our goal to include community / third-party support as well
- The complicated ""partials"" system is a nightmare to maintain, and we're discussing deprecation of these containers in favor of a different Docker recommendation (but no concrete plans yet).","- The complicated ""partials"" system is a nightmare to maintain"
58676,kanvi-nervana,1338276396,2022-12-05 22:38:17,"@DrChrisLevy I am an engineer from Intel and was able to reproduce this issue on TF2.10+. This [commit](https://github.com/tensorflow/tensorflow/commit/c1286fe88e14146a47) which changes the default value of the env var TF_RUN_EAGER_OP_AS_FUNCTION to true seems to be the causing the leak. As a work around, could you please try setting the env var to false at runtime. e.g. TF_RUN_EAGER_OP_AS_FUNCTION=false python test_script.py","""This [commit](https://github.com/tensorflow/tensorflow/commit/c1286fe88e14146a47) which changes the default value of the env var TF_RUN_EAGER_OP_AS_FUNCTION to true seems to be the causing the leak"""
58734,trevor-m,1338200092,2022-12-05 21:31:22,"> Hi @trevor-m, Google internal checks failed. I fixed it. Could you please sync? Sorry for the inconvenience.
Thanks @learning-to-play! I synced the PR but it looks like the internal checks are failing again. Let me know if I can help at all.","""It looks like the internal checks are failing again."""
58675,nouiz,1338033552,2022-12-05 19:32:18,This PR broke the build on CPU. I'll fix it locally and update it. Not ready to be approved.,This PR broke the build on CPU.
58658,nouiz,1338007042,2022-12-05 19:16:44,"I do not think this is a regression. I already saw such case where older GPUs where vectorized. So I suppose this isn't a regression.
Vectorization wasn't as important as it is right now, so adding new vectorization on old GPU the impact won't be high on them.
Those are older GPUs. Maxwell was released February 2014. Kepler even before that and is deprecated.
So everything make me think this is a very low priority new feature.","I already saw such case where older GPUs where vectorized. So I suppose this isn't a regression. Vectorization wasn't as important as it is right now, so adding new vectorization on old GPU the impact won't be high on them. Those are older GPUs. Maxwell was released February 2014. Kepler even before that and is deprecated. So everything make me think this is a very low priority new feature."
58658,Artem-B,1337904980,2022-12-05 18:27:04,"This is certainly not expected. `.v2.f32` instructions should've been generated for Maxwell and even Kepler. Disablingthe test may be just papering over a real issue.
If you manage to extract LLVM IR from the failing case, I can take a look.",.v2.f32 instructions should've been generated for Maxwell and even Kepler.
30180,blackCmd,1336892215,2022-12-05 7:46:29,"> model = tf.saved_model.load(export_dir)
> concrete_func = model.signatures[
> tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]
> concrete_func.inputs[0].set_shape([None, 1280, 720, 3])
> converter = TFLiteConverter.from_concrete_functions([concrete_func])
not working. It said,
`ValueError: Dimension 1 in both shapes must be equal, but are 3 and 320. Shapes are [1,3,320,320] and [1,320,320,3].`","""ValueError: Dimension 1 in both shapes must be equal, but are 3 and 320."""
58734,learning-to-play,1336648541,2022-12-05 2:29:36,"Hi @trevor-m, Google internal checks failed. I fixed it. Could you please sync? Sorry for the inconvenience.","""Google internal checks failed"""
58760,MPolaris,1336171688,2022-12-03 14:30:47,"Hi @mohantym!
Thanks for your response. I think this model is very simple and only owns two input nodes.
It's very confusing for ```tf.lite.TFLiteConverter``` is not support quantization for multiple input model?",I think this model is very simple and only owns two input nodes. It's very confusing for tf.lite.TFLiteConverter is not support quantization for multiple input model?
33312,sagunb,1335590218,2022-12-02 17:49:20,"Could you try syncing to head please, it looks like there are some conflicts.","Could you try syncing to head please, it looks like there are some conflicts."
58747,RocaVincent,1335282217,2022-12-02 14:09:27,"@tilakrayal Thanks for your answer, I already tried to limit GPU memory growth with `set_memory_growth` and `set_logical_device`. But it didn't solve the problem.
I have a little precision to bring about this bug. When I replace the `scipy.ndimage.rotate` call in the `numpy_function` by a simple matrix multiplication, there is no bug. Hence, the bug comes from the combination of `scipy.ndimage.rotate` and `numpy_function`.","""I have a little precision to bring about this bug."""
57477,talyz,1335200330,2022-12-02 13:04:49,"@gbaned I was waiting for a reponse from @terryheo, see above.","I was waiting for a reponse from @terryheo, see above."
58715,alebldn,1335191653,2022-12-02 12:59:29,"Kind sir, thanks for your reply.
When using @tf.function on the gpu I already do ignore the warnings and it works fine, it does train decently. But, unfortunately, when commenting @tf.function the errors that show up don't allow me to execute the code further. (in the repo I posted two examples)","""Unfortunately, when commenting @tf.function the errors that show up don't allow me to execute the code further."""
45651,Morpheushealer73,1335154806,2022-12-02 12:18:44,Any progress on this? Currently have a project about implementing Mask R-CNN into android but I have no idea how to do inference with my converted tflite file,Currently have a project about implementing Mask R-CNN into android but I have no idea how to do inference with my converted tflite file.
57679,wern0r,1334992729,2022-12-02 9:42:03,Same issue here with tensorflow 2.11.0 and TensorRT 8.5.1.7,Same issue here with tensorflow 2.11.0 and TensorRT 8.5.1.7.
56985,krunalvaghani,1334868590,2022-12-02 7:47:36,"> Several threads exist about this issue. My two work-arounds: (1) Use tf.compat.v1.lite.TFLiteConverter (2) Sort output indices For details see at #57043
But use of tf.compat.v1.lite.TFLiteConverter, will not support all the ops that is in latest tensorflow version.",Several threads exist about this issue.
58556,sachinprasadhs,1334211813,2022-12-01 18:59:38,"@angerson , Could you please look into this, it seems like the python versions 3.9 and 3.10 are not included in the published docker images, unlike the version which we publish for the `tf-nightly-gpu` where python version is included from 3.7 - 3.10 here https://pypi.org/project/tf-nightly-gpu/2.12.0.dev20221201/#files.","python versions 3.9 and 3.10 are not included in the published docker images, unlike the version which we publish for the tf-nightly-gpu where python version is included from 3.7 - 3.10 here https://pypi.org/project/tf-nightly-gpu/2.12.0.dev20221201/#files..>"
58748,triumph-wangyuyang,1333726142,2022-12-01 12:57:53,"The same problem exists with tf.random.uniform
CPU test:
import tensorflow as tf
with tf.device('/CPU'):
a = tf.random.uniform([1, 35, 12, 64], dtype=tf.float32)
b = tf.random.uniform([5, 1], minval=-256, maxval=257, dtype=tf.int64)
out = tf.gather_nd(a, b)
GPU test:
import tensorflow as tf
with tf.device('/GPU:0'):
a = tf.random.uniform([1, 35, 12, 64], dtype=tf.float32)
b = tf.random.uniform([5, 1], minval=-256, maxval=257, dtype=tf.int64)
out = tf.gather_nd(a, b)",The same problem exists with tf.random.uniform
58748,triumph-wangyuyang,1333711367,2022-12-01 12:43:16,"The same problem exists with tf.random.uniform.
CPU test:
import tensorflow as tf
with tf.device('/CPU'):
m = tf.keras.layers.AveragePooling1D(data_format=""channels_first"")
input = tf.random.uniform([3, 2, 6], dtype=tf.float32)
out = m(input)
GPU test:
import tensorflow as tf
with tf.device('/GPU:0'):
m = tf.keras.layers.AveragePooling1D(data_format=""channels_first"")
input = tf.random.uniform([3, 2, 6], dtype=tf.float32)
out = m(input)",The same problem exists with tf.random.uniform.
58748,triumph-wangyuyang,1333706188,2022-12-01 12:38:15,"The same problem exists with tf.keras.layers.AvgPool1D
CPU test:
import tensorflow as tf
with tf.device('/CPU'):
m = tf.keras.layers.AvgPool1D(data_format=""channels_first"")
input = tf.random.uniform([2, 2, 6], dtype=tf.float32)
out = m(input)
GPU test:
import tensorflow as tf
with tf.device('/GPU:0'):
m = tf.keras.layers.AvgPool1D(data_format=""channels_first"")
input = tf.random.uniform([2, 2, 6], dtype=tf.float32)
out = m(input)",The same problem exists with tf.keras.layers.AvgPool1D.
58748,triumph-wangyuyang,1333680429,2022-12-01 12:15:36,"The same problem exists with tf.keras.losses.sparse_categorical_crossentropy.
CPU test:
import tensorflow as tf
with tf.device('/CPU'):
a = [1024, 0]
b = [0.05, 0.95, 0], [0.1, 0.8, 0.1]
out = tf.keras.losses.sparse_categorical_crossentropy(a, b)
print(out)
GPU test:
import tensorflow as tf
with tf.device('/GPU:0'):
a = [1024, 0]
b = [0.05, 0.95, 0], [0.1, 0.8, 0.1]
out = tf.keras.losses.sparse_categorical_crossentropy(a, b)
print(out)",The same problem exists with tf.keras.losses.sparse_categorical_crossentropy.
58748,triumph-wangyuyang,1333655632,2022-12-01 11:58:19,"The same problem exists with tf.losses.sparse_categorical_crossentropy
CPU test:
import tensorflow as tf
with tf.device('/CPU'):
a = [-3, 2]
b = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
out = tf.metrics.sparse_categorical_crossentropy(a, b)
GPU test:
import tensorflow as tf
with tf.device('/GPU:0'):
a = [-3, 2]
b = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
out = tf.metrics.sparse_categorical_crossentropy(a, b)",The same problem exists with tf.losses.sparse_categorical_crossentropy.
41225,digital-idiot,1333328344,2022-12-01 7:33:48,"@pjpratik [keras.layers.Masking](https://keras.io/api/layers/core_layers/masking#masking-class) is supported in a limited number of layers. Unlike `LSTM`, the `Convolution` layers do not have an option to enable masking. Would it be possible to enable masking for `Convolution` layers by wrapping them in a custom layer like the `ReLU` example in your link?","""[keras.layers.Masking](https://keras.io/api/layers/core_layers/masking#masking-class) is supported in a limited number of layers"""
51870,donglinz,1333078844,2022-12-01 2:42:22,"Hi guys thank you for your discussion. I am facing the same issue when using TF-TRT converting model >=2GB. Do we have any plan to support this in the future? As in 2022, it is very common to have a model with more than 2GB parameters.
This limitation is also blocking convert_variables_to_constants_v2 when I try to convert variables to constant in a saved model.
@DEKHTIARJonathan @sanjoy @bixia1 @mohantym","""I am facing the same issue when using TF-TRT converting model >= 2GB"""
58739,cantonios,1332492564,2022-11-30 17:17:49,"> @cantonios one of these days it could be nice if we could directly generate these from the ops def GT.
Yes, I know. They are generated for some ops - just not ones that have their own python docstrings. Otherwise, a check to detect when the docs are stale would be useful.","""It could be nice if we could directly generate these from the ops def GT."""
58740,twerkmeister,1332335995,2022-11-30 15:20:02,"I tested the proposed change on windows, and I am getting the same error as before. All the provided snippets work on linux. On windows, however, the `model.load_weights(tf_model_file)` makes the operations that follow it fail.","I tested the proposed change on windows, and I am getting the same error as before."
58741,pfk-beta,1332011655,2022-11-30 11:31:59,"Ok, so I have read source code: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc#L314
It looks like, documentation is different than source code, or someone has made a mistake. In command description is written: ""espace colon with backslash"", but in source code ""colon is escaped with colon"".","""In command description is written: ""espace colon with backslash"", but in source code ""colon is escaped with colon""."
58734,learning-to-play,1331663048,2022-11-30 5:29:18,"Hi @trevor-m, I see the following two warnings. Could you please fix?
- There is already a load from ""//third_party/tensorflow/core/platform:build_config_root.bzl"" on line 25. Please merge all loads from the same origin into a single one.
- Symbol ""if_static"" has already been loaded on line 27. Please remove it.","""I see the following two warnings. Could you please fix?"""
56518,rr191211,1331591400,2022-11-30 3:13:31,"> Well, I've uploaded my code in rvm_tflite2.zip. I tried to add your code to mine, but it reports: ""/rvm_tflite_2/main.cc:58: error: undefined reference to `tflite::NnApiDelegate()'
> > I am not sure if the needed .h files or .lib are included. [rvm_tflite_2.zip](https://github.com/tensorflow/tensorflow/files/8953825/rvm_tflite_2.zip)
I am facing this issue with TF 2.11, anyone could help? Thanks.","""I am not sure if the needed .h files or .lib are included."""
58206,terryheo,1331512774,2022-11-30 1:04:15,@gsirocco you've changed output type to int8 so it has int8 outputs [-128 127 127 ... -48 -48 127] instead of float outputs [0. 1. 0. ... 1. 0. 1.]. Could you share what wrong with this?,"""changed output type to int8"""
57873,mihaimaruseac,1331481784,2022-11-30 0:12:18,You can attach the docstring to a new symbol in the Python file and remove it from the proto,can attach the docstring to a new symbol in the Python file and remove it from the proto>
57645,sampathweb,1331460421,2022-11-29 23:41:32,"I was not suggesting that. I was suggesting that reduce the batch size to 1/2 of what's used in training, but keep the function as `model.predict(large_dataset, batch_size=0.5*train_batch_size)`.
Alternatively, you could chunk the dataset into smaller dataset, but still much larger than what fits in memory. For example, if you have 1M files, create a dataset with 200K files and call `model.predict` on it.",I was not suggesting that.
57645,elbaro,1331456051,2022-11-29 23:36:18,"Do you mean `model.predict` is preferred over `predict_on_batch` even for a single minibatch? (2 is always better than 3)?
```
model.predict(dataset_as_large_as_memory, verbose=0, batch_size=batch_size_used_in_training) # 1
model.predict(batch_as_large_as_memory, verbose=0) # 2
model.predict_on_batch(batch_as_large_as_memory) # 3
```","""Do you mean model.predict is preferred over predict_on_batch even for a single minibatch?"""
57645,elbaro,1331412015,2022-11-29 22:41:28,"Having the same performance issue in prediction.
Unlike `model.fit(large_dataset)`, we cannot use `model.predict(large_dataset)` as the result doesn't fit into the memory.
```
def predict_generator(dataset):
for batch in dataset:
for result in keras_model.predict_on_batch(batch): # very slow, x3 faster with eager disabled
yield result
```
@sampathweb Given TF Dataset, what do you suggest for the code like the above?",Having the same performance issue in prediction.
58731,mihaimaruseac,1331231556,2022-11-29 20:04:25,"This is not true, has not been true since TF 2.0.
If you look on PyPI, `tensorflow` and `tensorflow-gpu` packages are identical, except the name","This is not true, has not been true since TF 2.0. If you look on PyPI, tensorflow and tensorflow-gpu packages are identical, except the name."
58731,sniafas,1331190325,2022-11-29 19:32:39,"Hi @mihaimaruseac when compiling from source with CUDA support the built pip wheel should have the following format
```
tensorflow-gpu-2.0.1-cp36-cp36m-linux_x86_64.whl
tensorflow-gpu-2.10.0-cp38-cp38-linux_x86_64.whl
tensorflow-gpu-2.10.0-cp39-cp39-linux_x86_64.whl
```
and when not, this format
```
tensorflow-2.11.0-cp39-cp39-linux_x86_64.whl
tensorflow-2.11.0rc0-cp38-cp38-linux_x86_64.whl
```
For some reason the configuration above does not produce the first format",For some reason the configuration above does not produce the first format.
58377,jurahul,1331013867,2022-11-29 17:23:51,"Overall it seems this is attempting to add tuple support for all-gather and reduce-scatter as well as add a optional dummy token input to the all-gather, the purpose of which is unclear.
I think we should split this into 2 PRs, one for tuple support and discuss support for token types in all-gather separately before adding it.","attempting to add tuple support for all-gather and reduce-scatter as well as add a optional dummy token input to the all-gather, the purpose of which is unclear."
58718,dilverse,1330860017,2022-11-29 15:51:19,"@mihaimaruseac The idea was for the docker images to pick the tensorflow-io support, since applications like tensorboard with object storage or other file system support is purely dependent on that package. This adds complexity to the deployment as described in the issue [#58710](https://github.com/tensorflow/tensorflow/issues/58710), what would be your recommended approach?","The idea was for the docker images to pick the tensorflow-io support, since applications like tensorboard with object storage or other file system support is purely dependent on that package. This adds complexity to the deployment as described in the issue [#58710](https://github.com/tensorflow/tensorflow/issues/58710), what would be your recommended approach?"
57679,markub3327,1330796052,2022-11-29 15:08:16,Probably yes. I have the same issue with TF 2.11.0 and TensorRT 8.5.1-1+cuda11.8.,I have the same issue with TF 2.11.0 and TensorRT 8.5.1-1+cuda11.8.
58718,mihaimaruseac,1330766766,2022-11-29 14:49:53,I think the plan is for users to manually install the TF IO wheel. We don't want `pip install tensorflow` to install all and every package that exists in TF ecosystem.,I think the plan is for users to manually install the TF IO wheel. We don't want pip install tensorflow to install all and every package that exists in TF ecosystem.
47170,bhack,1330429433,2022-11-29 10:41:59,"If it is not planned for the ""old"" bridge with the new bridge `tf.config.experimental.enable_mlir_bridge()` is probably https://github.com/tensorflow/tensorflow/issues/52030
As I see
```python
error: 'tf.TensorListReserve' op unknown tensor list element shape
```","""old"" bridge with the new bridge tf.config.experimental.enable_mlir_bridge() is probably https://github.com/tensorflow/tensorflow/issues/52030 As I see python error: 'tf.TensorListReserve' op unknown tensor list element shape ."
57671,MemoonaTahira,1330417328,2022-11-29 10:31:50,"Same question, I have tensorrt version 8.5.1.7 installed with cudatoolkit=11.2, cudnn=8.1, python=3.10 and tensorflow=2.11, and tensorflow comaplains it cannot find `Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7:` even though libnvinfer.so.8 is installled inside the tensorrt folder and in the path. To install tensorrt 7, I will need to downgrade to python 3.8 and I don't want to do that right now. Is there any other fix?","""Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: even though libnvinfer.so.8 is installled inside the tensorrt folder and in the path."""
58705,mihaimaruseac,1329366151,2022-11-28 16:15:01,"This is somehow too late for TF 2.11, the final release happened ~2 weeks ago.","This is somehow too late for TF 2.11, the final release happened 2 weeks ago."
38054,nwatab,1329353833,2022-11-28 16:06:47,"> For my part it was an SSL problem whenever I ran TensorFlow on an ec2 instance (curl 77 errors in the logs), I temporarily fixed it by setting `S3_VERIFY_SSL=0`.
Not fixed in my case.
_tensorflow.python.framework.errors_impl.UnknownError: {{function_node __wrapped__ReadFile_device_/job:localhost/replica:0/task:0/device:CPU:0}} : curlCode: 77, Problem with the SSL CA cert (path? access rights?) [Op:ReadFile]_","For my part it was an SSL problem whenever I ran TensorFlow on an ec2 instance (curl 77 errors in the logs), I temporarily fixed it by setting S3_VERIFY_SSL=0. Not fixed in my case."
58658,nouiz,1329267061,2022-11-28 15:09:16,"I updated the PR. It was passing here, but failed CI compilation. It should be fixed now.",Failed CI compilation.
58623,8700nd,1329065455,2022-11-28 13:09:37,"Hello @madhavarora2002, 1. I think we might need to add them all to AndroidManifest. What I was saying is it's very hard to manually find out what customized libs each manufacturers use and make our app support them
2. Currently I get with Pixel 4 Snapdragon 855
lightning fp16 CPU: 25~ GPU: 30+
thunder int8 CPU: 15~ GPU: 20~ NNAPI: 20~","""It's very hard to manually find out what customized libs each manufacturers use and make our app support them"""
56458,ghylander,1328888232,2022-11-28 11:02:29,"Hi,
I'm not sure if it's somehow related, but I too was experiencing segmentation faults when attempting to train my model using tf2.9
After a month-long process of checking NVidia drivers, installed libraries, and a long list of etc, I tried to download a docker container with tf2.11 and was able to train with no segmentation faults, using almost the exact same code (only change necessary was changing `tf.keras.optimizers.Adam()` to `tf.optimizers.Adam()`)","""I'm not sure if it's somehow related, but I too was experiencing segmentation faults when attempting to train my model using tf2.9"""
58140,rrajkumar1990,1328536196,2022-11-28 4:53:59,"<img width=""706"" alt=""TF_PERFORMANCE_ANALYSIS"" src=""https://user-images.githubusercontent.com/50878243/204196468-2aefa454-f1d9-4156-ad3e-ad3ca5a582fa.png"">
Hi @SuryanarayanaY , Please see the above performance numbers based on my tests. we see a performance decrease and memory increase as we go up the version
Also please use the base weights /pb files from : http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz .","""We see a performance decrease and memory increase as we go up the version"""
58706,madhavarora2002,1328282533,2022-11-27 16:20:44,"https://github.com/tensorflow/examples/pull/324/commits/bcae5ee17e77d866a11151b351191b09f1115cb8
TF team have marked a to do for fixing gpu in the android version a year and half back. could you look into this, as it's making movenet unsuable for android with low fps!","""TF team have marked a to do for fixing gpu in the android version a year and half back."""
57696,sserfasm,1328274143,2022-11-27 15:49:24,"I am trying to run a tensorflow lite model in the background (while the app is closed) quite frequently (once every few minutes). After the app has run for several hours I got this error message signal 11 (SIGSEGV), code 1 (SEGV_MAPERR). I am aware that there are other people who have received similar issues, but after trying every solution I could [find](https://thealightmotionapk.com/alight-motion-pro-apk/) , I think that this might be a different issue.",I am trying to run a tensorflow lite model in the background (while the app is closed) quite frequently (once every few minutes)
58690,jjjules,1328223693,2022-11-27 11:13:23,Hey! The code you checked doesn't correspond to what's happening when you run the code because it is a more recent version. You're running version 2.4.1 so the corresponding code can be found [here](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/distribute/tpu_strategy.py#L1335). As you can see there's no check on the `rank` in this code. There may be something wrong with your `train` input that causes `np.ndim` to return `None`.,"""The code you checked doesn't correspond to what's happening when you run the code because it is a more recent version."""
56795,madhavarora2002,1328075171,2022-11-26 16:26:15,"@impjdi , can we get the team who developed movenet to look into this? This issue is preventing it's usage with gpu on android!",This issue is preventing it's usage with gpu on android!
58693,triumph-wangyuyang,1328069108,2022-11-26 15:47:17,"tf.math.floor,tf.math.exp also has such a situation, the official type of the parameter x must be bfloat16, half, float32, float64, etc. can run the code successfully.
code: import tensorflow as tf
with tf.device('/CPU'):
x = tf.random.uniform([6, 1], minval=-256, maxval=257, dtype=tf.float16)
res = tf.floor(x)
print(res)","""The official type of the parameter x must be bfloat16, half, float32, float64, etc. can run the code successfully."""
58693,triumph-wangyuyang,1328060730,2022-11-26 14:58:20,"tf.math.cumsum also has such a situation, the official type of the parameter x must be float32, float64, int64, int32, uint8, uint16, int16, int8, complex64, complex128, qint8, quint8, qint32, half, but uint32, etc. can run the code successfully.
code: x = tf.cast(tf.random.uniform([2, 4], minval=0, maxval=2, dtype=tf.int32), dtype=tf.uint32)
axis = 1
results[""res_cpu""] = tf.math.cumsum(x,axis=axis)","""tf.math.cumsum also has such a situation, the official type of the parameter x must be float32, float64, int64, int32, uint8, uint16, int16, int8, complex64, complex128, qint8, quint8, qint32, half, but uint32, etc. can run the code successfully."""
56795,tempstudio,1327944214,2022-11-26 0:54:48,"On the topic of movenet vs blazepose:
Blazepose runs at ~30FPS with NNAPI and ~45FPS with GPU
Movenet runs at ~40FPS with NNAPI and crashes with GPU
So we should be able to get ~60FPS with Movenet and GPU, but we cannot.","""So we should be able to get 60FPS with Movenet and GPU, but we cannot."""
33255,Kheem-Dh,1327928858,2022-11-25 23:28:31,"I am working on sentiBert :
https://github.com/WadeYin9712/SentiBERT
I am facing this issue, again and again, writing two files each one having 6GB and 3GB NumPy binary files.
using torch==1.1.0 torchvision==0.3.0
Can anyone help me with that?
Thanks ![Screenshot 2022-11-25 152329](https://user-images.githubusercontent.com/77248954/204062847-2764f526-e5cf-4044-96d6-d5425f1405c2.png)","I am facing this issue, again and again, writing two files each one having 6GB and 3GB NumPy binary files. using torch==1.1.0 torchvision==0.3.0 Can anyone help me with that? Thanks ![Screenshot 2022-11-25 152329](https://user-images.githubusercontent.com/77248954/204062847-2764f526-e5cf-4044-96d6-d5425f1405c2.png)"
56927,rogeriog,1327625131,2022-11-25 15:23:50,"I was having this problem to use a package with GPU, and the preceding solutions did not work. I could solve it by just downgrading tensorflow-gpu to 2.10.1.","I was having this problem to use a package with GPU, and the preceding solutions did not work."
58621,svobora,1327463101,2022-11-25 13:11:54,The same warning/error happens to my nets using Dropout in Ubuntu 22.04 with TF 2.11.0 when using GPU. Did not happen with 2.10.0. Cannot confirm on Windows since GPU support was dropped in latest version of TF. Right now it seems it does not affect performance.,The same warning/error happens to my nets using Dropout in Ubuntu 22.04 with TF 2.11.0 when using GPU.
58547,cool-man-vk,1327417249,2022-11-25 12:30:19,"Yes @mohantym.
`pip install tensorflow==1.0.0`
> This is the command which I've tried. And installed the tf succesfully.
> But when I use in python ,
> it arises this error.","""But when I use in python , it arises this error."""
58653,vimal-polymagelabs,1327381036,2022-11-25 11:50:32,Closing since this shape inference is not needed as the ResourceGatherOp gets decomposed into ReadVariableOp.,"""Closing since this shape inference is not needed as the ResourceGatherOp gets decomposed into ReadVariableOp."""
58654,mengran1234,1326992060,2022-11-25 3:41:11,"-march=armv8.2-a+dotprod -mfpu=neon-fp-armv8
these option is set in xnnpack code.
https://github.com/google/XNNPACK/issues/3905#event-7879829849
this reply is 32-bit iOS is no longer supported.",-march=armv8.2-a+dotprod -mfpu=neon-fp-armv8 these option is set in xnnpack code.
58602,twerkmeister,1326627053,2022-11-24 15:58:56,"Hey @SuryanarayanaY, we noticed that the released arm packages for tensorflow 2.10 and 2.11 are broken, or rather empty, which leads to the described problem of not being able to import tensorflow. You can see this on the [pypi downloadable files page for tf 2.10.1](https://pypi.org/project/tensorflow/2.10.1/#files). You see that all the aarch64 packages have a size of only 1.9 kb","""The released arm packages for tensorflow 2.10 and 2.11 are broken, or rather empty, which leads to the described problem of not being able to import tensorflow."""
57738,plooney,1326622745,2022-11-24 15:56:13,"I am having this issue with 2.11. Tensorflow crashes with no error message in model.fit
Code works fine in 2.9
I am using the Docker image",I am having this issue with 2.11.
58618,simdax,1326607089,2022-11-24 15:40:37,"Yes I can confirm that. One of the problem is linked to the ""if_mobile"" option in bazel. All the sources built with it failed.
I think TF is assuming building for IOS need to be TFlite, which is not my case ?","""if_mobile"" option in bazel. All the sources built with it failed. I think TF is assuming building for IOS need to be TFlite, which is not my case ?."
58622,sonfire186,1326037337,2022-11-24 7:05:23,"@mohantym `interpreter.resizeInput`
java.lang.OutOfMemoryError: Failed to allocate a 144384016 byte allocation with 25165824 free bytes and 68MB until OOM, target footprint 356191760, growth limit 402653184","interpreter.resizeInput java.lang.OutOfMemoryError: Failed to allocate a 144384016 byte allocation with 25165824 free bytes and 68MB until OOM, target footprint 356191760, growth limit 402653184."
53702,tyoungroy,1326004735,2022-11-24 6:17:47,"is there any update?
well, i got the same issues with tensorflow-lite version 2.8.1. I found an option TFLITE_ENABLE_FLEX in tensorflow-lite 2.5.3 but the option was gone from 2.6.1
I took the part of TFLITE_ENABLE_FLEX in tensorflow 2.5.3 and applied them to 2.8.1. It still doesn't work.
anyone share your tips if you got progress?",I took the part of TFLITE_ENABLE_FLEX in tensorflow 2.5.3 and applied them to 2.8.1. It still doesn't work.
58658,cheshire,1325191428,2022-11-23 14:50:02,Seems like vectorization is broken? I'm fine to skip.,Seems like vectorization is broken.
58140,rrajkumar1990,1324555939,2022-11-23 5:08:41,"> Also I have doubt why you are using TF 1.x version when TF 2.x is available with pretrained models with much Easier and flexible API to handle Transfer learning?
Also I don't see any direct weights available for Retinanet Model, We have 2.x weights for base Resnet only . Can you please point me if you have the pretrained weights for Retinanet in 2.x",I have doubt why you are using TF 1.x version when TF 2.x is available with pretrained models with much Easier and flexible API to handle Transfer learning?
46659,aicheung,1324444067,2022-11-23 1:40:51,"It is not fixed. I tried using your Colab sample and my own local code (I am using 2.9.1 and Py 3.7.15 locally). Still the same error.
> @mg262 Seems the issue is fixed in latest TF Nightly. 2.6 version.When executed the code I am not facing any error. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/50b2a38a537b0932e957fc3181c8306e/untitled92.ipynb).",I tried using your Colab sample and my own local code (I am using 2.9.1 and Py 3.7.15 locally). Still the same error.
58648,agramesh1,1324380396,2022-11-22 23:57:51,CC @kaixih this failure seems to be due to this PR https://github.com/tensorflow/tensorflow/pull/58159,CC @kaixih this failure seems to be due to this PR
58590,misterBart,1323387185,2022-11-22 9:48:49,"I see an automatic comment with ""Some check were not successful"" and ""Merging is blocked"". I checked the Details of the failed checks, but I cannot pinpoint them to my added code. Do I still need to do something or should I simply wait? This is my first contribution to Tensorflow code, hence my question.","""Some check were not successful"" and ""Merging is blocked"""
51728,mohantym,1323019834,2022-11-22 4:10:59,"@benoitdescamps !
It still has not been addressed in[ 2.10 version](https://github.com/tensorflow/tensorflow/blob/r2.10/tensorflow/python/framework/sparse_tensor.py).",It still has not been addressed in[ 2.10 version](https://github.com/tensorflow/tensorflow/blob/r2.10/tensorflow/python/framework/sparse_tensor.py).
58465,SuryanarayanaY,1323015642,2022-11-22 4:06:31,"Hi @Urkchar ,
I think we are still missing some thing here.Please find the error in the attached [gist](https://colab.research.google.com/gist/SuryanarayanaY/f76d1667758f46bf7d9f5a7ad0698f13/58465-r1.ipynb).
`NameError: name 'get_output_sequence_length' is not defined`.","""I think we are still missing some thing here."""
58611,DwayneDuane,1322493538,2022-11-21 18:40:39,"@tiruk007, Tensorflow Lite v 2.11.0 was just released, but I get the same build error. I will close this ticket and file a new one for V2.11.0. I tried https://www.tensorflow.org/lite/guide/build_cmake but it doesn't help. Please see the new ticket (https://github.com/tensorflow/tensorflow/issues/58636) for more details.","""I will close this ticket and file a new one for V2.11.0"""
58093,elfringham,1322158219,2022-11-21 14:38:15,My latest findings on this issue is that it was initially reproduced using gcc 9.3.1. I have now updated to gcc 10.2.1 and can no longer reproduce it.,I have now updated to gcc 10.2.1 and can no longer reproduce it.
58544,SuryanarayanaY,1322002412,2022-11-21 12:37:51,"Hi @rohitreddy21122000,
As you are using Tf2.4 with Latest versions of Ubuntu there might arise incompatibility issues.Could you please Upgrade TF to new version like 2.10 and let us know if problem still persists.Request you go to similar [issue-54286](https://github.com/tensorflow/tensorflow/issues/54286) where issue solved by upgrading TF to newer versions.","""Incompatibility issues"""
57661,samrajkodai,1321878277,2022-11-21 11:04:15,"hi @HebaGamalElDin i did'nt, i tried many methods ,but that issue is not solved.","i did'nt, i tried many methods ,but that issue is not solved."
57914,freedomtan,1321751470,2022-11-21 9:25:11,"Xcode 14.x come with ""problematic"" linker (ld), cf. https://github.com/tensorflow/tensorflow/issues/58368#issuecomment-1321729076","""problematic"" linker (ld)"
58606,SuryanarayanaY,1321643828,2022-11-21 8:10:55,I could able to replicate the issue in google colab but colab stopped at epoch 858 due to longer training time taken.But still the Expected behaviour captured in logs.Please find attached gist [here](https://colab.research.google.com/gist/SuryanarayanaY/f69d51ac0fd2c3b7a53833ac8b06a736/58606.ipynb#scrollTo=LWN099X70JnO).,I could able to replicate the issue in google colab but colab stopped at epoch 858 due to longer training time taken.But still the Expected behaviour captured in logs.Please find attached gist [here](https://colab.research.google.com/gist/SuryanarayanaY/f69d51ac0fd2c3b7a53833ac8b06a736/58606.ipynb#scrollTo=LWN099X70Jn
58610,sryu1,1320874688,2022-11-19 12:24:24,"So, in conclusion:
`from keras.models import load_model` does not work with native windows and tensorflow 2.11
It works with tensorflow-cpu OR tensorflow 2.11 with a conda environment or other sorts of similar.","""does not work with native windows and tensorflow 2.11"""
58610,sryu1,1320804096,2022-11-19 5:06:31,"Ah, ok. Well, if I'm using tensorflow-cpu, it should work, right? It seems like it doesn't though, it gives back the same error...","""It seems like it doesn't though, it gives back the same error."""
57914,sun1638650145,1320786912,2022-11-19 3:59:58,"Hi, because I contributed to the build of `tensorflow-text` on Apple silicon, but now we [found an issue with `tensorflow-macos 2.10`](https://github.com/tensorflow/text/issues/823#issuecomment-1251596272), and `tensorflow-macos` is not open source. So the current idea is to build a `tensorflow` from source code.","""We found an issue with tensorflow-macos 2.10"""
58610,sryu1,1320725330,2022-11-19 1:20:52,"Also, I noticed the tf 2.10 tag, for tensorflow 2.10, `import keras.models import load_model` works but not for 2.11","I noticed the tf 2.10 tag, for tensorflow 2.10, import keras.models import load_model works but not for 2.11."
58610,sryu1,1320605555,2022-11-18 22:35:53,"I've already tried it previously and it didn't work ```
Traceback (most recent call last):
File ""c:/Users/Noah Ryu/Coding/AI/Teachable Machine/Guesser.py"", line 1, in <module>
from tensorflow.keras.models import load_model
ModuleNotFoundError: No module named 'tensorflow.keras'
```","""I've already tried it previously and it didn't work"""
26278,ppwwyyxx,1320550292,2022-11-18 21:35:06,"For more context, I understand that the results would match if I resize `[[6, 7], [11, 12]]` into 4x4 using TFv1's `resize_images(align_corners=True)`. However, `align_corners=True` was removed for good after #6720 and don't exist in TFv2. That's why this issue is a follow up of #6720 that we're still using the weird (and bad for many models) alignment behavior.",align_corners=True was removed for good after #6720 and don't exist in TFv2
547,I-Thompson,1320446470,2022-11-18 19:34:17,"Many parts of tf not working with DTensor yet.
tf.linalg.solve is my show-stopper at the moment. This is done by tf.MatrixSolve
As is the failure of dtensor.relayout to distribute complex64. It does float32 and float64, but not yet complex of any kind. This is done by DTensorAllScatter.",Many parts of tf not working with DTensor yet.
58416,reedwm,1320386924,2022-11-18 18:29:59,"BF16 L2Loss is only supported on the GPU, but the BiasAdd BF16 tests in this PR run with both CPU and GPU. So it's still failing.
I would either use cast the L2 Loss input to FP32 or only run these tests with BF16 if there is a GPU.","BF16 L2Loss is only supported on the GPU, but the BiasAdd BF16 tests in this PR run with both CPU and GPU. So it's still failing. I would either use cast the L2 Loss input to FP32 or only run these tests with BF16 if there is a GPU."
57883,cheshire,1319925254,2022-11-18 12:19:07,"This is implementation-defined behavior, so unlikely to have a fix (at least on the XLA side).","This is implementation-defined behavior, so unlikely to have a fix (at least on the XLA side)."
57000,jondo,1319916782,2022-11-18 12:10:32,"Ah, sorry, it's [here](https://www.tensorflow.org/install/source#gpu). Please update it to TF 2.11.0.","""It's [here](https://www.tensorflow.org/install/source#gpu). Please update it to TF 2.11.0."""
58416,reedwm,1319148902,2022-11-17 20:14:18,"No need to rebase. But this is failing with the environment variable `TF2_BEHAVIOR=1` (you can run with `--test_env=TF2_BEHAVIOR=1` to reproduce the failure). The issue is `bias_op_base.py` calls l2_loss, which doesn't yet support BF16. Probably the easiest way to fix is to cast the input of l2_loss to fp32 when bf16 is used.","""The issue is bias_op_base.py calls l2_loss, which doesn't yet support BF16"""
39378,cowwoc,1319048934,2022-11-17 18:37:34,I don't think you understand what Java Module (JPMS) is. It's not enough to say that Tensorflow/Java can run under a JVM. You'll need to create a new application that uses JPMS and import TensorFlow in your `module-info.java` and check for errors at compile-time and runtime.,I don't think you understand what Java Module (JPMS) is.
57086,ra9hur,1318418096,2022-11-17 10:26:48,"To me, neither the stable version nor nightly version was working. However, installations on colab would go through fine. Ended up checking python version on colab, created a virtual env with the same python version as in colab. Could install successfully.","""Neither the stable version nor nightly version was working."""
39922,RoopendraSingh,1318346744,2022-11-17 9:34:46,"> @ssagkriotis I too facing issue,
How to run another instance of the whole script with the index number set to 1?","I too facing issue, How to run another instance of the whole script with the index number set to 1?"
58586,afq984,1317945663,2022-11-17 1:56:50,my 2c: I think we should reuse https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/generate-pc.sh instead of duplicating the shell code a separate file in lite/ (thesorflow_cc is not in the cc directory either),duplicating the shell code a separate file in lite/ (thesorflow_cc is not in the cc directory either)
46602,MichaelHudgins,1317859729,2022-11-17 0:16:56,Seems like one of the first things we do is check why we use the net.cc PickUnusedPortOrDie in some places and directly use python_port_picker in others. After a quick look it seems like at least in the failure above it was in one of the instances where python_port_picker was used directly.,"""After a quick look it seems like at least in the failure above it was in one of the instances where python_port_picker was used directly."""
48205,ch3rub1m,1317604950,2022-11-16 20:08:37,"I have met exactly the same bug, all is right when using GPU but nan in CPU, tf v2.9.2.
I found the bug always occurs following two Conv2D layers, but I don't know why.","I have met exactly the same bug, all is right when using GPU but nan in CPU, tf v2.9.2."
58402,roserg,1317529380,2022-11-16 19:04:37,Should be fixed after this commit https://github.com/tensorflow/tensorflow/commit/78cbae184cb2cd9541fb21074c3420b930c682ad,"""should be fixed after this commit"""
55743,stewartmiles,1317490892,2022-11-16 18:32:12,"Hi everyone, thanks for approving but it appears merge is still blocked. I can't see the copybara failure - of course - and the AMD ROCm build - which has been failing forever - is still broken.",I can't see the copybara failure - of course - and the AMD ROCm build - which has been failing forever - is still broken.
58523,cantonios,1317430984,2022-11-16 17:59:01,"Looks like the CUDA solver does actually raise an internal error. Not sure why it's not surfacing in colab.
@reedwm internally I get an `InvalidArgumentError`, notifying that the solver received a bad info status here: https://github.com/tensorflow/tensorflow/blob/0ba21962fe3e1f78063812ae9cc475ab928163bf/tensorflow/core/util/cuda_solvers.cc#L239
There's been an effort to return `NaN`s instead of crashing on these linalg failures. So maybe there's a general solution that can be implemented.",CUDA solver does actually raise an internal error.
58520,hellofinch,1316874353,2022-11-16 11:48:41,"@SuryanarayanaY Yeah, I have seen those issues. But none of them can satisfy me, and I don't see anyone who can handle it. It's OK for me to close the issue.
Does that mean Tensorflow does not long support the verbs? Or are there some way else can do?
One last question, I see a [repo](https://github.com/tensorflow/networking) containing verbs, MPI, and some thing else. Is that solution?","""But none of them can satisfy me, and I don't see anyone who can handle it."""
58488,optiluca,1316617074,2022-11-16 8:49:10,"Same issue over here. It seems to me that S3 tensorboard support has never worked in recent history, courtesy of the issue @greper describes.","""It seems to me that S3 tensorboard support has never worked in recent history, courtesy of the issue @greper describes."""
55697,st--,1316518143,2022-11-16 7:40:06,"I've signed an Individual CLA, but your form does not let me fill in my GitHub user-name (it wrongly invalidates my username, ""Invalid GitHub login""), so it seems to not associate my CLA with my commit. I'm afraid I can't see what *I* can do about this. Please let me know once the form has been updated to accept my username (`st--`) as valid.","""Invalid GitHub login"""
49520,mihaimaruseac,1316317927,2022-11-16 4:26:55,"This is too old, might need a new refactor if it is still an issue.","This is too old, might need a new refactor if it is still an issue."
57055,ProfDoof,1316073568,2022-11-16 0:21:56,So I just had this issue as well and I think that the error message needs to be changed to indicate that the XLA compiler (is that the right term) is not using the environment variable CUDA_DIR but is instead using the XLA variable. My own issue would have been solved much faster if I had known that.,"""My own issue would have been solved much faster if I had known that."""
57549,sachinprasadhs,1316045762,2022-11-15 23:54:26,"In colab, with CPU runtime the code is getting executed without any error, attached gist [here](https://colab.sandbox.google.com/gist/sachinprasadhs/6c28fd2530d18727ad72d5af4c0f0f5c/57549.ipynb) for reference. In M1 mac, I'm getting different error. Note: I have installed tensorflow-metal to use GPU in my instance.
<img width=""1549"" alt=""image"" src=""https://user-images.githubusercontent.com/73069040/202049142-4fa25ca3-9377-4ad6-81dd-ec79ccd880fa.png"">","""In M1 mac, I'm getting different error"""
58587,vipeano,1314981741,2022-11-15 8:49:17,"A workaround to this bug is not to use tf.random.uniform() as in the code below
```
max_steps=1
g = tf.random.Generator.from_non_deterministic_state()
def condition(index, random_num):
return index < max_steps
def body(index, random_num):
return index+1, g.uniform([1])
@tf.function def function():
index = tf.constant(0, dtype=""int8"")
a=g.uniform([1]) index, random_num=tf.while_loop(condition,body,[index, tf.zeros([1],dtype='float32')])
return a, random_num
a, b=function()
print(a, b) ```","""A workaround to this bug is not to use tf.random.uniform() as in the code below"""
37729,wookayin,1314910216,2022-11-15 7:43:50,"Can we have this reopened and tracked somehow? I understand why the stalebot was added for such a large repository, but personally I don't like stale-bot closing and killing a long-wanted job suddenly... :| And the ml-butler bot doesn't distinguish the 'completed' and 'not planned' modes in Github issues.","I understand why the stalebot was added for such a large repository, but personally I don't like stale-bot closing and killing a long-wanted job suddenly... :| And the ml-butler bot doesn't distinguish the 'completed' and 'not planned' modes in Github issues."
58534,deadsoul44,1314856234,2022-11-15 6:56:03,"I tried XLA_FLAGS like this one by one:
```
os.environ[""XLA_FLAGS""]=""--xla_gpu_strict_conv_algorithm_picker=false""
os.environ[""XLA_FLAGS""]=""--xla_gpu_autotune_level=0""
```
If I am setting them correctly, in both cases, GPU is not used and CPU is used instead.","I tried XLA_FLAGS like this one by one:  os.environ[""XLA_FLAGS""]=""--xla_gpu_strict_conv_algorithm_picker=false"" os.environ[""XLA_FLAGS""]=""--xla_gpu_autotune_level=0""  if I am setting them correctly, in both cases, GPU is not used and CPU is used instead.."
58538,bhack,1314651616,2022-11-15 1:58:26,"> But I could not find proper documentation on above interactive_graphviz command.
Yes no doc currently. It is only in the usage and in the source code.",I could not find proper documentation on above interactive_graphviz command.
56696,cantonios,1314119666,2022-11-14 17:25:33,"@KingsleyLiu-NV no I think that's a Keras issue. Keras is a layer on top of TF, so TF's internals shouldn't need to know anything about keras. TF only deals directly with Tf tensor types (`Tensor`, `SparseTensor`, `RaggedTensor`, etc...)",I think that's a Keras issue.
55394,cheshire,1314075752,2022-11-14 16:56:22,"> I don't know much about XLA, I don't think my single test is suitable for embedding Exhaustive32BitOrLessUnaryTest.
> input of Exhaustive32BitOrLessUnaryTest must be int type, but I need a float or double input .
Saw this in email, don't see it in GH UI somehow. It looks float to me, why does it need to be an int type?","""I don't know much about XLA, I don't think my single test is suitable for embedding Exhaustive32BitOrLessUnaryTest."""
58522,bhack,1313994846,2022-11-14 16:07:21,I've tried some runs but I cannot reproduce it. Probably you could `set_shape` in you py_function. As py_function has some limits (see Doc) you could explore also what I've suggested in https://github.com/tensorflow/tensorflow/issues/58448#issuecomment-1313695458,I've tried some runs but I cannot reproduce it. Probably you could set_shape in you py_function. As py_function has some limits (see Doc) you could explore also what I've suggested in https://github.com/tensorflow/tensorflow/issues/58448#issuecomment-1313695458.
21526,ZYX-MLer,1313088867,2022-11-14 4:41:05,"tf.raw_ops.MatrixInverse is not supported in tflite, while Op BatchMatrixInverse is not available in GraphDef version 1205. How can i calculate the inverse of the matrix in tflite?","tf.raw_ops.MatrixInverse is not supported in tflite, while Op BatchMatrixInverse is not available in GraphDef version 1205"
58534,bhack,1312762716,2022-11-13 15:56:48,"> As I said, it is dead slow while running in colab also.
I was just replying to your claim:
> Besides, it is running in Kaggle env. It shouldn't be env/installation issue
Actually I cannot reproduce your original error on a Colab env how you are going to exclusive that it is not an env issue if we cannot reproduce it on Colab?","Besides, it is running in Kaggle env. It shouldn't be env/ installation issue Actually I cannot reproduce your original error on a Colab env how you are going to exclusive that it is not an env issue if we cannot reproduce it on Colab?"
58534,deadsoul44,1312747369,2022-11-13 14:44:03,"As I said, it is dead slow while running in colab also.",It is dead slow while running in colab also.
50741,hosford42,1312605094,2022-11-13 0:36:52,"I have a custom keras layer that uses a custom gradient function. It worked fine for a previous project, but now I'm trying to reuse the layer and I'm getting this error. The only potentially meaningful difference I can see is that I have multiple undefined batch dims in the model I'm building now. (I'm using the layer on the results of a convolution.) Could that impact the dispatching somehow?","""I have a custom keras layer that uses a custom gradient function. It worked fine for a previous project, but now I'm trying to reuse the layer and I'm getting this error."""
43916,hosford42,1312603367,2022-11-13 0:22:54,So tf.custom_gradient is incompatible with eager execution? This is bad news! I need both for my use case.,So tf.custom_gradient is incompatible with eager execution? This is bad news! I need both for my use case.
58534,deadsoul44,1312421015,2022-11-12 9:14:26,"The same version of the code works with 2D conv. Besides, it is running in Kaggle env. It shouldn't be env/installation issue.
Those XLA flags make the code running as slow as in CPU.",Those XLA flags make the code running as slow as in CPU.
58515,cantonios,1312072903,2022-11-11 18:56:29,"nm, I double-checked the Eigen source, and it's because we implement negate as (0 - x), so we get this behavior even in no-fast-math mode. (+0 - -0) is +0. I'll change that on the Eigen end. Will take time to propagate.","nm, I double-checked the Eigen source, and it's because we implement negate as (0 - x), so we get this behavior even in no-fast-math mode. (+0 - -0) is +0. I'll change that on the Eigen end. Will take time to propagate.."
57224,sachinprasadhs,1312067608,2022-11-11 18:48:59,This merge would conflict with the commit https://github.com/tensorflow/tensorflow/commit/5984ea373e8804386fa60cf57eb6a18005c02b56. I'm closing the issue.,This merge would conflict with the commit https://github.com/tensorflow/tensorflow/commit/5984ea373e8804386fa60cf57eb6a18005c02b56. I'm closing the issue.
58538,bhack,1311819302,2022-11-11 15:24:08,"Isn't `--hlo_text`?
https://github.com/tensorflow/tensorflow/blob/3e3f1ba6dbefcb7643a22409e242b0f0e32e71c5/tensorflow/compiler/xla/tools/interactive_graphviz.cc#L106",Isn't --hlo_text?
58448,bhack,1311662328,2022-11-11 12:50:22,"> I tried to download the dataset and code from the mentioned Kaggle link and was facing a different issue while execution.
Cause you missed to copy the first cell from the user's Kaggle Colab:
`!pip install natsort`",Cause you missed to copy the first cell from the user's Kaggle Colab: !pip install natsort.
57943,bhpatray,1311221333,2022-11-11 4:35:47,"Still Same issue even in latest 2.10.0
Any Update ?",Still Same issue even in latest 2.10.0 Any Update ?
29075,MMingabc,1311136420,2022-11-11 2:07:29,"> I think this bug is extremely affecting user decisions in NLP deployment. Everything is fine for me when I am aware of this and use a warm-up variable length trick to avoid this slow bug.
Hi, dathudeptrai
I am a faithful user of your TensorFlowTTS. A lot of my code is based on your project. I am facing this exact problem when inferencing on GPUs. Do we have a better solution now? I am a little sad hearing you are stiring to PyTorch. Looking forwards to owesome updates in TensorFlowTTS.",I am a faithful user of your TensorFlowTTS. A lot of my code is based on your project. I am facing this exact problem when inferencing on GPUs. Do we have a better solution now?
34942,Owaiskhan9654,1311129803,2022-11-11 1:54:29,"Same issue with TensorFlow 2.8 in the Kaggle kernel.
dataset = dataset.shuffle(32)
Tried methods mentioned earlier and none of these seems to work......
![image](https://user-images.githubusercontent.com/47840160/201245117-3961c42d-c2c2-4975-9c78-250e56bd551f.png)
![image](https://user-images.githubusercontent.com/47840160/201245726-53adcd14-bed6-4a3e-9c62-2ff3d909f744.png)",Tried methods mentioned earlier and none of these seems to work.
58498,tensorbuffer,1311022712,2022-11-10 23:06:59,"Just to confirm, even if I use bazel build, I can't compile hexagon delegate for Linux ARM (like this: bazel build -c opt --config elinux_aarch64 ...) ?",I can't compile hexagon delegate for Linux ARM (like this: bazel build -c opt --config elinux_aarch64 ...) ?
58453,bhack,1310864678,2022-11-10 20:36:50,"Your env varriable is correctly set to pass the condition
https://github.com/tensorflow/tensorflow/blob/1d34295f45965c7090039773dea44c25ac00ec00/third_party/clang_toolchain/cc_configure_clang.bzl#L9-L19
But I suppose that as we are not testing this in the CI the download part is quite unmaintained. Last commit and clang updated hash -> Sept. 2019
https://github.com/tensorflow/tensorflow/blob/master/third_party/clang_toolchain/download_clang.bzl","""But I suppose that as we are not testing this in the CI the download part is quite unmaintained."""
58453,bhack,1310854616,2022-11-10 20:26:39,I see that `ERROR: Config value 'download_clang' is not defined in any .rc file` disappeared now but still not using the new one.,I see that ERROR: Config value 'download_clang' is not defined in any .rc file disappeared now but still not using the new one.
58422,cantonios,1310663142,2022-11-10 17:46:10,"> * can you tell me the steps or point me to the right documentation from where I can know how to compile my changes in my local system?
https://www.tensorflow.org/install/source
> * I have tried to update my signature on the CLA form but it still showing failed. please let me know where might be the issue?
https://github.com/tensorflow/tensorflow/pull/58422/checks?check_run_id=9261642567","""I have tried to update my signature on the CLA form but it still showing failed. please let me know where might be the issue"""
58453,feranick,1310656095,2022-11-10 17:39:45,"Compilation fails just the same (i.e. with the same error). It seem that despite the added lines, bazel still uses the installed version of clang. Log attached.
[fail.txt](https://github.com/tensorflow/tensorflow/files/9983390/fail.txt)",Compilation fails just the same (i.e. with the same error)
58445,cantonios,1310571626,2022-11-10 16:36:13,"Yes, we should remove the `(and between CPU and GPU)` claim.",(and between CPU and GPU)
56753,zcase,1310537778,2022-11-10 16:13:58,"Could this issue result from using cuda 11.6? Just asking because I switched back to a commit for the models repo that was back around the time of tensorflow 2.5.0 thinking this would make things work and was seeing about the same issue.
Only difference was that the model that was created and then exported to a .tflite file always seemed to run everything on the GPU except the last output layer where as in 2.10.0 it always seemed to split things between the gpu and cpu.","""Using cuda 11.6"""
51506,pooya-jannaty,1310536763,2022-11-10 16:13:10,Having a similar malformed trie issue both on an M1 mac and Intel-based.,Having a similar malformed trie issue both on an M1 mac and Intel-based.
58521,mohantym,1310422424,2022-11-10 15:00:07,Closing due to conflict.,Closing due to conflict.
58453,feranick,1310414318,2022-11-10 14:54:31,"The point of this issue, however, is not what works and what not based on a specific configuration. Rather that there is no way to download a viable compiler, as the `./configure` tool clearly intends to do. If I run compilation, I get this error (as stated above):
`ERROR: Config value 'download_clang' is not defined in any .rc file`
In essence, regardless of what works or not, even defining what `download_clang` should do is missing.",No way to download a viable compiler.
56913,ChipKerchner,1310373405,2022-11-10 14:30:38,We haven't seen a response to this MR in 3.5 months. Would it be possible for someone to look at this?,We haven't seen a response to this MR in 3.5 months.
58453,feranick,1310278760,2022-11-10 13:25:01,"Sorry, I don't see any attachment to the original email and in GitHub.",I don't see any attachment to the original email and in GitHub.
58379,mengran1234,1309890466,2022-11-10 7:40:55,"> > I use tflite model . IF use the TFlite2.10 to compile benchmark on PC, can also test performance. I’m not sure the tflite2.10 is or not exist this problem.
> > We always suggest to test the last available version for bug reports. Can you test 2.10 and 2.11rc2?
> > if still confirmed on the last releases, it would be nice if you can replicate it on the master branch.
Can you test this problem on AMD notebook?","I use tflite model . IF use the TFlite2.10 to compile benchmark on PC, can also test performance. I’m not sure the tflite2.10 is or not exist this problem. > > We always suggest to test the last available version for bug reports. Can you test 2.10 and 2.11rc2? > > if still confirmed on the last releases, it would be nice if you can replicate it on the master branch. Can you test this problem on AMD notebook?."
58140,rrajkumar1990,1309743917,2022-11-10 4:11:58,"Hi @SuryanarayanaY ,
I don't think so the memory consumption changes are due to the ""MLIR V1 optimization"". I remember trying that and still the pod memory consumption was more in 2.8.1 and 2.9.1. Could you try the steps I have mentioned and let me know","""MLIR V1 optimization"""
58398,yishuangP,1309742398,2022-11-10 4:09:29,"Hi, sorry I just tried switching branch to v2.10.0, it also failed with the same error, but it succeeds on master. We already drop support for 32 bits since Apple drops support for it since iOS 11. To build for 64bits devices, you can run the following command:
```
bazel build --ios_multi_cpus=arm64 -c opt --config=ios --cxxopt=-std=c++17 //tensorflow/lite/ios:TensorFlowLiteC_framework
```","""I just tried switching branch to v2.10.0, it also failed with the same error, but it succeeds on master."""
58445,wangpengmit,1309686512,2022-11-10 2:33:46,BTW I suspect the reason it's not reproduced in graph mode is because soft placement placed both ops on CPU.,BTW I suspect the reason it's not reproduced in graph mode is because soft placement placed both ops on CPU.
57873,sachinprasadhs,1309577496,2022-11-10 0:21:07,"I guess the raw_ops does not have python file, and the example mentioned here https://www.tensorflow.org/api_docs/python/tf/raw_ops/UniqueV2 in the document throws an exception.","I guess the raw_ops does not have python file, and the example mentioned here https://www.tensorflow.org/api_docs/python/tf/raw_ops/UniqueV2 in the document throws an exception."
58499,swapnilsayansaha,1309549638,2022-11-09 23:45:09,"Found a totally unacceptable solution. Not sure but the training step for the pruned model only works on CPU and not GPU:
```
with tf.device('/cpu:0'):
model_for_pruning.fit(train_images, train_labels,
batch_size=batch_size, epochs=epochs, validation_split=validation_split,
callbacks=callbacks)
```
So TensorFlow's sparse operators aren't GPU friendly even now?!","""So TensorFlow's sparse operators aren't GPU friendly even now!"""
57779,cantonios,1309437017,2022-11-09 22:03:31,"@jxy we know the cause, and we know the proper solution is to use `tf.convert_to_tensor` rather than `tf.cast` to convert numpy arrays to tensors. We're blocked from changing `tf.cast` to address this directly, since doing so changes the graph representation which breaks existing models and workflows. So I think our hands are tied. You should be using `tf.convert_to_tensor` to avoid precision loss.","""We're blocked from changing tf.cast to address this directly, since doing so changes the graph representation which breaks existing models and workflows."""
58481,tensorbuffer,1309302761,2022-11-09 20:03:24,"Tried this patch, modified the two files and still same result.","Tried this patch, modified the two files and still same result."
58144,DEKHTIARJonathan,1309189161,2022-11-09 18:26:04,@bixia1 no more comments. Can you re-approve,No more comments.
58140,SuryanarayanaY,1309109988,2022-11-09 17:40:11,"Hi @rrajkumar1990 ,
From the error log as we can see `MLIR V1 optimization pass is not enabled` as the API `tf.config.experimental.enable_mlir_graph_optimization` is under development. Could you please spare some time to refer the attached [TF Forum](https://discuss.tensorflow.org/t/none-of-the-mlir-optimization-passes-are-enabled/2247/18) link regarding similar issue and please let us know if they works for your case.
Thankyou!","""MLIR V1 optimization pass is not enabled"""
58290,bhack,1308917164,2022-11-09 15:15:48,"I've tested the gist again with the last nightly and the code path of this example it seems to be different from the one fixed with the PR https://github.com/tensorflow/tensorflow/pull/58328:
https://colab.research.google.com/gist/bhack/9dc16546d360291eb943bbeb36a01c57/untitled192.ipynb
But I don't have enough cloud resources to recompile TF again a prepare a new PR /cc @mihaimaruseac",I've tested the gist again with the last nightly and the code path of this example it seems to be different from the one fixed with the PR
58469,Kristoff-starling,1308740000,2022-11-09 13:17:05,It seems that out-of-bound cannot be reproduced with positive values even if the sequence is non-increasing. Actually we need a negative value small enough to trigger the out-of-bound error.,"""It seems that out-of-bound cannot be reproduced with positive values even if the sequence is non-increasing."""
58469,bhack,1308646945,2022-11-09 12:02:37,"> I also noticed the restriction mentioned in the high level API doc, which means that an increasing sequence with negative values is also invalid
@Kristoff-starling I don't know if it is really a cosntrain it was more an open question as if you try it is also not validated by the high level API op `tf.math.segment_max`.","I also noticed the restriction mentioned in the high level API doc, which means that an increasing sequence with negative values is also invalid"
58481,bhack,1308009281,2022-11-09 0:11:31,Yes the root cause is the same as the ticket but I think that the Cmake definition was introduced almost at the same time with https://github.com/tensorflow/tensorflow/commit/888dd68c774ba5d48dbb5181077ee454936822ed that it is forward to your 21b567d26ca4cfa5c02260acd77b8b5f2fd57d84,I think that the Cmake definition was introduced almost at the same time with https://github.com/tensorflow/tensorflow/commit/888dd68c774ba5d48dbb5181077ee454936822ed that it is forward to your 21b567d26ca4cfa5c02260acd77b8b5f2fd57d84.
58481,tensorbuffer,1307993293,2022-11-08 23:52:09,"No, that commit only changed BUILD file, which is for bazel build, while I am using cmake.
I fixed it by changing the define to 4 in eigen/Eigen/src/Core/arch/NEON/GeneralBlockPanelKernel.h:
#if EIGEN_ARCH_ARM64
#ifndef EIGEN_NEON_GEBP_NR
#define EIGEN_NEON_GEBP_NR 4
#endif
But I think a better fix would be a similar change in cmake files.","No, that commit only changed BUILD file, which is for bazel build, while I am using cmake."
56692,pranavladkat,1307844302,2022-11-08 21:24:04,"The issue with above suggestion is that it does not work with nested modules. If one of the layer is deep inside other layers which uses kwarg, this approach does not work.","The issue with above suggestion is that it does not work with nested modules. If one of the layer is deep inside other layers which uses kwarg, this approach does not work."
45256,gaetanbahl,1307538963,2022-11-08 17:03:52,This is still a problem in tensorflow 2.9.1.,This is still a problem in tensorflow 2.9.1.
58328,mihaimaruseac,1307410177,2022-11-08 15:34:39,I did a manual import.,I did a manual import.
57185,Genuifx,1307281393,2022-11-08 14:10:01,"Nowadays, we can follow apple's [docs](https://developer.apple.com/metal/tensorflow-plugin/) to set up TensorFlow with mac apple silicon. but these don't work with macOS docker build. I can't install `pip install tensorflow` cause the inter AVX doesn't work with arm64. Also `pip install tensorflow-macos` doesn't work in linux docker.","""I can't install pip install tensorflow cause the inter AVX doesn't work with arm64."""
58354,bhack,1307268410,2022-11-08 14:02:51,"> Unlikely to backport if it doesn't turn out to be a security issue
So as it is solved in 2.11rc I think it could go stale/closed.",Unlikely to backport if it doesn't turn out to be a security issue
56177,rivershah,1307119427,2022-11-08 12:16:57,Seeing same issue,Seeing same issue.
57550,tmlstrong,1306844053,2022-11-08 8:48:00,"@sayakpaul Because Conv2D that you using have dimension is 5, tflite support is 4. You can quick solve by reduce dimension from 5 to 4 by remove batch dimension","Because Conv2D that you using have dimension is 5, tflite support is 4"
58442,mraunak,1306697860,2022-11-08 6:25:19,"Hey @bhack and @MarkDaoust, I have attached the logs, but I didn't find much information in them. I re-run it but got the same output log. Below is the log when there is a TIMEOUT error
[error_log.docx](https://github.com/tensorflow/tensorflow/files/9958245/error_log.docx).","""I have attached the logs, but I didn't find much information in them."""
58354,mihaimaruseac,1306551552,2022-11-08 2:54:07,Unlikely to backport if it doesn't turn out to be a security issue,Unlikely to backport if it doesn't turn out to be a security issue.
58472,elfringham,1306042666,2022-11-07 18:51:12,The size of the elements needed seems to have recently increased causing this test to begin failing again.,The size of the elements needed seems to have recently increased causing this test to begin failing again.
58451,bhack,1306004049,2022-11-07 18:14:06,"Yes it seems we have a segmentation fault on nightly:
```gdb
#0 0x00007f623573d7d3 in mlir::quant::QuantizedType::getExpressedType() const () from /usr/local/lib/python3.9/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#1 0x00007f623573e1ac in mlir::quant::QuantizedType::castFromExpressedType(mlir::Type) ()
from /usr/local/lib/python3.9/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
```",gdb #0 0x00007f623573d7d3 in mlir::quant::QuantizedType::getExpressedType() const () from /usr/local/lib/python3.9/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so #1 0x00007f623573e1ac in mlir::quant::QuantizedType::cast
55495,infil00p,1306001108,2022-11-07 18:11:47,I'm still getting the issue on 2.10 when I use the precompiled libraries provided in the AAR on Gradle.,I'm still getting the issue on 2.10 when I use the precompiled libraries provided in the AAR on Gradle.
58451,nyadla-sys,1305969908,2022-11-07 17:46:25,"@bhack I changed first cell of colab to **!pip install tf-nightly**, but I still see the crash while converting to quantized int8 model","I changed first cell of colab to **!pip install tf-nightly**, but I still see the crash while converting to quantized int8 model."
58451,bhack,1305954625,2022-11-07 17:32:43,I have some memory limit running your Colab. Have you really tried to run it with TF nightly as it seems it was mispelled at the first cell of your colab.,I have some memory limit running your Colab. Have you really tried to run it with TF nightly as it seems it was mispelled at the first cell of your colab.
53708,paazca,1305522332,2022-11-07 12:16:28,"I've run into the same issue.
`tflite_flex_shared_library` ignores my model and outputs the full Select TF Ops library, instead of building only the needed operators.","tflite_flex_shared_library ignores my model and outputs the full Select TF Ops library, instead of building only the needed operators."
58122,fuzzyswan,1305345397,2022-11-07 9:43:07,"hi @tilakrayal I checked the issue you pointed but I think the problem I reported here is not related to gradient computation. Here, the problem is the lack of input validity checking in the forward pass. A simple data type check would help to reject invalid floating `depth_radius`, and no value validity checking is needed (which I assume would be costly).",I checked the issue you pointed but I think the problem I reported here is not related to gradient computation.
57826,njzjz,1305269963,2022-11-07 8:44:44,"Same error, so I roll back to 2.9.2.","Same error, so I roll back to 2.9.2."
58448,tilakrayal,1305176643,2022-11-07 7:09:03,"@dasmehdix,
Code shared is full of [indentation errors](https://colab.research.google.com/gist/tilakrayal/bdc11b8f1dee2af32af25afdf3e56ff0/untitled731.ipynb), Kindly share a colab gist with issue reported or indented code with all dependencies such that we can replicate the issue reported. Thank you!",Code shared is full of [indentation errors](https://colab.research.google.com/gist/tilakrayal/bdc11b8f1dee2af32af25afdf3e56ff0/untitled731.ipynb)
58457,mohantym,1305063090,2022-11-07 4:20:43,Closing due to Pylint conflict.,Closing due to Pylint conflict.
58458,Zonbi-san,1304901050,2022-11-06 21:42:25,I don't really want to use wsl2 though,I don't really want to use wsl2 though.
49725,ssuhas76,1304843636,2022-11-06 16:53:09,"> It seems like a bug with `EfficientNetB0`itself. I was able to run `mixed_precision` without any errors on the Resnet101 model.
> ![Screenshot 2021-05-27 at 5 14 47 AM](https://user-images.githubusercontent.com/45290954/119745070-048d8280-beab-11eb-871e-d1215ab3b577.png)
Why is pooling layer not sued for ResNet 101 code?",I was able to run mixed_precision without any errors on the Resnet101 model.
58458,Zonbi-san,1304715046,2022-11-06 4:55:39,i also i tried conda and it didnt work,i also i tried conda and it didnt work.
58444,prsatyal,1304446460,2022-11-05 8:53:54,"Thank you for your help! The model loaded perfectly but is not performing as well as it did when i didnt load the model. It also gave me this error and im getting mixed answers all over the internet. How do i resolve this error and what does it mean?
WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.","""Error in loading the saved optimizer state"""
58091,martin-ueding,1304440381,2022-11-05 8:29:04,I honestly don't want to set up a TensorFlow build environment to test the changes. I will just wait until Google switches over to C++20 and ignore these warnings until then.,I honestly don't want to set up a TensorFlow build environment to test the changes.
37729,RPGillespie6,1304395065,2022-11-05 3:57:21,"This issue happens all the time in GCP `c2-standard-8` with TF 2.10. The very first `import tensorflow` is very slow (20+ seconds). The funny thing is, it doesn't seem to matter _where_ the first `import tensorflow` happens - it could even happen inside a docker container. But after that very first slow import, any subsequent imports (inside or outside docker container) are fast (1 second or less).
Can we get this re-opened?","""The very first import tensorflow is very slow (20+ seconds"")"
58454,odieXin,1304376011,2022-11-05 2:06:57,"Yes, @bhack I tried [tf.keras.backend.clear_session](https://www.tensorflow.org/api_docs/python/tf/keras/backend/clear_session) and it doesn't work.
I found re-loading weights doesn't impact the inference performance (inference time 3). The problem is of creating new models with Keras and the models could not be deleted clearly with the clear_session function or the del variable function in python.
gc.collect() doesn't work too.
I updated the comment.",I tried [tf.keras.backend.clear_session](https://www.tensorflow.org/api_docs/python/tf/keras/backend/clear_session) and it doesn't work.
58375,bhack,1304367211,2022-11-05 1:18:26,"> These should be equivalent?
In this formulation I think that this currently will not work for https://github.com/bazelbuild/bazel/issues/14848. Right?",I think that this currently will not work for https://github.com/bazelbuild/bazel/issues/14848. Right?
547,I-Thompson,1304361455,2022-11-05 0:51:17,"I use tensorflow for quantum physics calculations that involve much matrix arithmetic.
And float64 makes a huge difference to accuracy. With float32, fitting data stops too often at false minima.
And now I discover that DTensorAllGather cannot do float64.
(Strangely?) the first missinig float64 support I have met in 2 years now of tf.
I really do want full float64 support",DTensorAllGather cannot do float64.
58453,feranick,1304299669,2022-11-04 22:11:09,"This is a similar issue, but it's now closed due to lack of activity. Since I can't reopen it, I submit it with updated info as this issue. https://github.com/tensorflow/tensorflow/issues/46976","""I can't reopen it, I submit it with updated info as this issue."""
58368,feranick,1303880179,2022-11-04 16:56:30,"Technically, bazel allows for downloading a ""fresh version"" of clang upon configuring TF previous to compilation. But that option is broken. See related issues: https://github.com/tensorflow/tensorflow/issues/46976
https://github.com/tensorflow/tensorflow/issues/58453","Technically, bazel allows for downloading a ""fresh version"" of clang upon configuring TF previous to compilation. But that option is broken."
46976,feranick,1303861946,2022-11-04 16:45:19,This is still an open issue with TF 2.10 or later. Same error message as the original poster.,This is still an open issue with TF 2.10 or later. Same error message as the original poster.
32122,mihaimaruseac,1303605714,2022-11-04 14:04:07,"My bad, I misread the `+` meaning.",I misread the + meaning.
58433,bhack,1303556355,2022-11-04 13:42:40,"> Unfortunately for your first code snippet, you mention it works for you. Indeed it works but if you read carefully my initial comment
Yes sorry is that it seems, without the output, that it was empty. But reading again the ""text"" it is clear. So the pointer to the stateless version is correct.
P.s. the autograph problem mentioned for the non stateless impl at https://github.com/tensorflow/tensorflow/issues/58433#issuecomment-1302627410 is still valid.","""Unfortunately for your first code snippet, you mention it works for you."""
58030,smit-hinsu,1303160977,2022-11-04 9:20:59,Comparison is undefined for complex types so CategoricalAccuracy doesn't make sense for it. Later TensorFlow versions will remove support for complex types to avoid this crash and have an explicit failure.,Comparison is undefined for complex types so CategoricalAccuracy doesn't make sense for it.
58427,bhack,1302866025,2022-11-04 2:02:03,Generally it could still happen a rollback after the merge (https://github.com/tensorflow/community/issues/425),Generally it could still happen a rollback after the merge.
58368,freedomtan,1302760738,2022-11-03 22:43:23,"FYR, mostly, this is a bug of Xcode 14.x (or more specifically ld? There were some new changes in ld. It failed to build gcc with some early Xcode 14 versions). I ran into this problem since early Xcode 14 betas.
I could build TensorFlow with Xcode 13.x on macOS Monterey (12.x) without problems. When I tried to build it with Xcode 14 on macOS 12.x, it failed.","""I ran into this problem since early Xcode 14 betas."""
58159,kaixih,1302643249,2022-11-03 20:44:45,"Noticed a couple of newly added unit tests of bf16 failed on CPU. Just skip these tests on CPU, since they are not focus of this PR. @reedwm","""A couple of newly added unit tests of bf16 failed on CPU"""
58433,bhack,1302627410,2022-11-03 20:27:18,"Basically It is like
```python
@tf.function
def f(X):
if X == (0,0):
return True
return False
print(f(1))
print(f(tf.constant(1))) # This will not work
```","python @tf.function def f(X): if X == (0,0): return True return False print(f(1)) print(f(tf.constant(1))) # This will not work "
58053,huan1372,1302585004,2022-11-03 19:42:00,"In the documentation, it said This function casts the input to dtype without applying any scaling.
I want to cast a complex type to float type.","""This function casts the input to dtype without applying any scaling."""
58053,huan1372,1302558922,2022-11-03 19:14:50,"The saturate_cast function will use minimum value which not implemented in complex type as you said, but in the documentation of saturate_cast, it did not said we cannot use complex dtype. Thats why I suggest change the documentation for saturate_cast.","The saturate_cast function will use minimum value which not implemented in complex type as you said, but in the documentation of saturate_cast, it did not said we cannot use complex dtype. Thats why I suggest change the documentation for saturate_cast."
58419,feranick,1302442044,2022-11-03 17:26:24,"TensorRT 7.x is no longer available in NVidia Repositories, so there is no way for me to install them. Probably useful: I was able to compile earller on TF 2.11.0-rc1 using a slightly older version of tensorRT 8 than the current one. Unfortunately, that old library is also not accessible from the repos. The issue, as in the past, seems that NVidia updates the libraries without preserving the fallback older ones.","""TensorRT 7.x is no longer available in NVidia Repositories, so there is no way for me to install them."""
32122,mihaimaruseac,1302417523,2022-11-03 17:07:22,I think `test_seek_v2` is actually correct. You are opening the file only for writing so of course you should not be able to read from it,I think test_seek_v2 is actually correct. You are opening the file only for writing so of course you should not be able to read from it.
58032,mihaimaruseac,1302414957,2022-11-03 17:05:39,The infra team needs to create jobs for the nightly jobs. CC @learning-to-play @angerson @nitins17,The infra team needs to create jobs for the nightly jobs.
58177,mihaimaruseac,1302414079,2022-11-03 17:05:01,"This is a real issue that used to be considered a vulnerability but due to large number of occurences versus limited impact it was decided to only consider it a bug. Please don't autoclose these types of issues.
However, please do check if these are happening in tf-nightly. If they don't, then the issue can be closed.","""due to large number of occurences versus limited impact it was decided to only consider it a bug"""
58180,mihaimaruseac,1302413001,2022-11-03 17:04:13,This is a real issue that used to be considered a vulnerability but due to large number of occurences versus limited impact it was decided to only consider it a bug. Please don't autoclose these types of issues.,"""It was decided to only consider it a bug."""
58086,cantonios,1302385343,2022-11-03 16:44:39,"Marking as closed then. For the slow transpose issue, if it's a blocker, please re-open #13017.",slow transpose issue
58055,cantonios,1302294753,2022-11-03 15:35:35,"Commented on the bug, but I think we should instead disable the op definition for non-integer types in [math_ops.cc](https://github.com/tensorflow/tensorflow/blob/c676c85dd4d48956729857fe98981e50bd80e0a5/tensorflow/core/ops/math_ops.cc#L520).
Instead of `.BINARY_MORE()`, that should probably read
```
.Input(""x: T"").Input(""y: T"").Output(""z: T"").Attr(
""T: {uint8, int8, uint16, int16, int32, uint32, uint64, int64}"")
```","""I think we should instead disable the op definition for non-integer types in [math_ops.cc](https://github.com/tensorflow/tensorflow/blob/c676c85dd4d48956729857fe98981e50bd80e0a5/tensorflow/core/ops/math_ops.cc#L520)"""
58395,ductranit,1302287707,2022-11-03 15:30:36,Just notice that I got the same error on nightly build `0.0.1-nightly.20221102`,I got the same error on nightly build 0.0.1-nightly.20221102.
48845,tanzhenyu,1302277365,2022-11-03 15:23:25,TF 2.10 + running locally = issue is still here,TF 2.10 + running locally = issue is still here.
58428,bhack,1302258199,2022-11-03 15:10:32,As you car read there if you manually access to that sysfs on the host and it is `< 0` it could be a firmware issue. You can use the suggested workaround and report it to your PC/laptop/server vendor as it is probably a firmware bug.,As you car read there if you manually access to that sysfs on the host and it is  0 it could be a firmware issue.
58392,bhack,1302241309,2022-11-03 14:58:37,"@chunky This is why I have asked you to try to reproduce the same with a controllable environment like docker so we are for sure exactly on the same page.
If you don't want to use a clean/under control environment please check that your custom env respect the tested build configuration:
https://www.tensorflow.org/install/source#tested_build_configurations
`identification is GNU 11.3.0` is not in the table.","""If you don't want to use a clean/under control environment please check that your custom env respect the tested build configuration: https://www.tensorflow.org/install/source#tested_build_configurations identification is GNU 11.3.0 is not in the table."""
58431,bhack,1302236619,2022-11-03 14:55:17,"It is hard to compare results between TF version with you notebook as if you see your code gives different results with the same TF version across multiple runs.
Other then seeds you need probably to remove other random sources for a reproducible experiment. When you will reproduce the same results across multiple run on the same TF version we could check if there is really a numerically relevant difference.","""Hard to compare results between TF version with you notebook as if you see your code gives different results with the same TF version across multiple runs"""
58431,bhack,1302109196,2022-11-03 13:22:06,"Your code is not accessible.
Please can you try also with the last stable version TF 2.10?","""Your code is not accessible"""
58395,ductranit,1301793707,2022-11-03 8:46:26,"hi @mohantym, yes, I did add force load in other linker flags
```
-force_load $(SRCROOT)/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.xcframework/ios-arm64/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps
```
Without this flag it has another build error (can't find TensorFlowLiteSelectTfOps binary)
And version `2.6.0` is working fine on my M1 mac. This issue only happens on `2.10`","""Can't find TensorFlowLiteSelectTfOps binary"""
58379,bhack,1301789598,2022-11-03 8:41:53,"> I use tflite model . IF use the TFlite2.10 to compile benchmark on PC, can also test performance. I’m not sure the tflite2.10 is or not exist this problem.
> We always suggest to test the last available version for bug reports. Can you test 2.10 and 2.11rc2?
if still confirmed on the last releases, it would be nice if you can replicate it on the master branch.","I use tflite model . IF use the TFlite2.10 to compile benchmark on PC, can also test performance. I’m not sure the tflite2.10 is or not exist this problem. > We always suggest to test the last available version for bug reports. Can you test 2.10 and 2.11rc2? if still confirmed on the last releases, it would be nice if you can replicate it on the master branch."
47554,timedcy,1301734613,2022-11-03 7:33:55,"i have the same problem in tf2.10, but change `Layer` to `Model`, the warnings will disappear, like
> \# class Transform(K.layers.Layer):
> class Transform(K.Model):",# class Transform(K.layers.Layer): > class Transform(K.Model):
58407,vinayburugu,1301679493,2022-11-03 5:59:01,"@bhack Since XLA is common to both Tensorflow and Pytorch, accessing XlaRuntimeError and able to throw / catch will enable me to rethrow relevant message that the exception root cause is within XLA and not the framework.
@mohantym I don't think [1](https://www.tensorflow.org/api_docs/python/tf/xla/experimental/jit_scope) and [2](https://www.tensorflow.org/xla/custom_call#signalling_an_error) would solve my use case.","""I don't think [1](https://www.tensorflow.org/api_docs/python/tf/xla/experimental/jit_scope) and [2](https://www.tensorflow.org/xla/custom_call#signalling_an_error) would solve my use case."""
58368,feranick,1301604263,2022-11-03 3:35:11,Bug persists with TF 2.11-rc2 and stable release of XCode 14.1.,Bug persists with TF 2.11-rc2 and stable release of XCode 14.1.
57390,dixr,1301481832,2022-11-02 23:16:54,"@wangpengmit Can you justify why this is really the best solution? Why not change the use of `.ndim` in `kron`?
I mean, none of all the other functions in `np_math_ops.py` require `.ndim`, they all call some alternative rank functions that don't produce this error.
If for some reason `kron` really has to use `.ndim` instead of an alternative, then you should make the error message more helpful.","""Why not change the use of .ndim in kron?"""
58027,sachinprasadhs,1301251871,2022-11-02 21:03:21,"Negative value in the `seq_axis` is handled in the code and it returns the InvalidArgument Error, below is the code and the output for that.
```
import tensorflow as tf
seq_lengths = [7, 2, 3, 5]
input = [[1, 2, 3, 4, 5, 0, 0, 0], [1, 2, 0, 0, 0, 0, 0, 0],
[1, 2, 3, 4, 0, 0, 0, 0], [1, 2, 3, 4, 5, 6, 7, 8]]
output = tf.reverse_sequence(input, seq_lengths, seq_axis=-1, batch_axis=0)
print(output)
```
`InvalidArgumentError: Invalid seq_dim -1 [Op:ReverseSequence]`",InvalidArgumentError: Invalid seq_dim -1 [Op:ReverseSequence]
57601,sachinprasadhs,1301072760,2022-11-02 18:47:49,"> Saving it using the newest version of Tensorflow 1? It only occurs when saving a model in Tensorflow 1 and I tried with Tensorflow 1.15 which should be the newest version.
Try saving the model in Tensorflow version 2.5 and above, for migrating your 1.x code to 2.x, you can follow https://www.tensorflow.org/guide/migrate",Saving it using the newest version of Tensorflow 1?
57674,kaixih,1300998223,2022-11-02 17:38:01,"> this breaks TF2 tests that previously ran BF16 ops and had pre-Ampere GPUs. Can you elaborate a little more about the TF2 tests? Are they some of your internal tests?
Or are you talking about the unit tests in this PR?
For the conv ops, we didn't have the bf16 kernels before, so how did you test it on pre-ampere gpus? I think I am still a bit confused about why this PR would break the ""TF2 tests"".
@reedwm",This breaks TF2 tests that previously ran BF16 ops and had pre-Ampere GPUs.
58334,bhack,1300837992,2022-11-02 16:29:24,"> TF just calls Eigen's matrix sqrt [here](https://github.com/tensorflow/tensorflow/blob/38a677b555140410598603ff15abbe408b252e40/tensorflow/core/kernels/linalg/matrix_square_root_op.cc#L45).
> > I don't think we have that in Eigen either.
So, probably still waiting on https://gitlab.com/libeigen/eigen/-/issues/840","""TF just calls Eigen's matrix sqrt"""
58334,cantonios,1300809722,2022-11-02 16:17:58,"TF just calls Eigen's matrix sqrt [here](https://github.com/tensorflow/tensorflow/blob/38a677b555140410598603ff15abbe408b252e40/tensorflow/core/kernels/linalg/matrix_square_root_op.cc#L45).
I don't think we have that in Eigen either.",TF just calls Eigen's matrix sqrt [here](https://github.com/tensorflow/tensorflow/blob/38a677b555140410598603ff15abbe408b252e40/tensorflow/core/kernels/linalg/matrix_square_root_op.cc#L45)
58295,roserg,1300397221,2022-11-02 13:20:24,"""Is there a particular reason why the support for this op on GPU is not included already? Is it bugged?""
The reason is very limited support of parameters, it will work correctly only in specific case. Looks like your case is not supported or smt else is wrong.","""Looks like your case is not supported or smt else is wrong."""
47554,YinuoWen,1300304337,2022-11-02 12:33:47,"> > I'm facing the issue too, can I assume my model will still works correctly ? I intend to convert it to TFLite format to run on ESP32 device ...
> > I have the same Warning when saving and can confirm the model still works as normal when doing inference
'tf' format saved model worked and no warning in tf version==2.7.0 before, but have same warning issue in 2.9.0 and colab. The saved model does can works normally.","I have the same Warning when saving and can confirm the model still works as normal when doing inference 'tf' format saved model worked and no warning in tf version==2.7.0 before, but have same warning issue in 2.9.0 and colab. The saved model does can works normally."
57601,muxamilian,1300207077,2022-11-02 11:54:10,Saving it using the newest version of Tensorflow 1? It only occurs when saving a model in Tensorflow 1 and I tried with Tensorflow 1.15 which should be the newest version.,Saving it using the newest version of Tensorflow 1?
58379,bhack,1299987507,2022-11-02 10:15:31,"Yes, It seems that some convolutional kernels are slower on the AMD GPU. Just taking the first 3 in the log.
AMD Ryzen 5 5600U
```
convolution_2d 0 linked : relu 1 - 1.043610 ms
convolution_2d 2 linked : relu 3 - 0.409920 ms
convolution_2d 4 linked : relu 5 - 0.205160 ms
```
intel(R) UHD Graphics 620 ```
convolution_2d 0 linked : relu 1 - 0.538500 ms
convolution_2d 2 linked : relu 3 - 0.096083 ms
convolution_2d 4 linked : relu 5 - 0.100500 ms
```
Is the AMD driver updated?
/cc @roserg",Is the AMD driver updated? /cc @roserg.
58297,wwdok,1299465723,2022-11-02 2:14:17,"I can build tensorflow lite on ubuntu with bazel, but I need to delete `.bazelversion` . And for the build with cmake, I still can did not get it work.","I can build tensorflow lite on ubuntu with bazel, but I need to delete .bazelversion . And for the build with cmake, I still can did not get it work."
57757,cantonios,1298941374,2022-11-01 18:30:27,"`std::pow(1, NaN) = 1` in c++ [[docs](https://en.cppreference.com/w/cpp/numeric/math/pow)]. Looks like XLA is violating this, returning `NaN`.","std::pow(1, NaN) = 1"
58369,fuzzyswan,1298711120,2022-11-01 15:32:37,@tilakrayal I think the output is not desired as the GPU outputs nan.,I think the output is not desired as the GPU outputs nan.
47554,AidanDugganCIT,1298650245,2022-11-01 15:05:21,"> I'm facing the issue too, can I assume my model will still works correctly ? I intend to convert it to TFLite format to run on ESP32 device ...
I have the same Warning when saving and can confirm the model still works as normal when doing inference","I'm facing the issue too, can I assume my model will still works correctly ?"
57794,cheshire,1298542571,2022-11-01 13:51:54,"Hm seems like `tensorflow/compiler/xla/service/gpu:cudnn_fused_conv_rewriter_test` is still failing =/
It fails comparing module equality in `CudnnFusedConvRewriterHloTest.DontFuseSideInputThroughRelu`",seems like tensorflow/compiler/xla/service/gpu:cudnn_fused_conv_rewriter_test is still failing =/
58379,mengran1234,1298424634,2022-11-01 12:13:12,"\tensorflow\tensorflow\lite\delegates\gpu\cl\testing\performance_profiling.cc
I use the file to test every kernel's time .Per kernel time is bigger than on intel notebook, Then add the time of every kernel is 28MS","Per kernel time is bigger than on intel notebook, Then add the time of every kernel is 28MS."
58379,mengran1234,1298391802,2022-11-01 11:38:01,"> Have you checked to not include in the benchmark timing the kernel compilation?
> > Check https://www.tensorflow.org/lite/performance/gpu?hl=en#delegate_serialization
while (1) {
uint64_t timestart = NowMicros();
TfLiteInterpreterInvoke(interpreter_);
cnt++;
time_total += (NowMicros() - timestart);
}
}
I use the time_total ,then time_total / cnt","""I use the time_total ,then time_total / cnt"""
56762,SandSnip3r,1297405409,2022-10-31 17:11:35,"Unfortunately, this item is taking a back seat to some other higher priority issues. Also, I am having trouble getting permissions to update our internal rocm testing version. I am trying to pass this off to someone who has permissions and time to help. Sorry about the delay.",I am having trouble getting permissions to update our internal rocm testing version.
58335,mihaimaruseac,1297352302,2022-10-31 16:28:19,"(I no longer work in TF, just doing drive-by reviews on stale items or on work that crosses boundary)","I no longer work in TF, just doing drive-by reviews on stale items or on work that crosses boundary."
58051,bhack,1297350330,2022-10-31 16:26:51,"> here has been no progress on Dynamic Reshape for XLA Literals as far as I'm aware.
Is it something that could be contributed? Do we have any pointer to contribute this?",No progress on Dynamic Reshape for XLA Literals.
58051,JoshVarty,1297347085,2022-10-31 16:24:29,There has been progress on support for dynamic shapes within XLA since 2020 but there has been no progress on Dynamic Reshape for XLA Literals as far as I'm aware.,No progress on Dynamic Reshape for XLA Literals.
58363,xiaodathereal,1296803064,2022-10-31 9:12:16,"By GSPMD, we mean this paper: https://arxiv.org/abs/2105.04663.
We use the 'xla_sharding.mesh_split' API, but it seems that GSPMD does not enabled at all.","By GSPMD, we mean this paper: https://arxiv.org/abs/2105.04663. We use the 'xla_sharding.mesh_split' API, but it seems that GSPMD does not enabled at all."
41732,dwyatte,1296293170,2022-10-30 16:12:13,BoostedTreesClassifier and other estimators are deprecated as of TensorFlow 2.9,BoostedTreesClassifier and other estimators are deprecated as of TensorFlow 2.9.
58212,mihaimaruseac,1296292692,2022-10-30 16:09:46,Merging manually since Copybara/GitHub integration seems to consider this to be a conflict,Merging manually since Copybara/GitHub integration seems to consider this to be a conflict.
47554,EParisot,1296292384,2022-10-30 16:08:34,"I'm facing the issue too, can I assume my model will still works correctly ? I intend to convert it to TFLite format to run on ESP32 device ...","I'm facing the issue too, can I assume my model will still works correctly ?"
58369,bhack,1296272479,2022-10-30 14:24:48,"We don't have explicit tests for `mod` with non int/unit input types:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/math_ops/cwise_ops_binary_test.py#L335","""We don't have explicit tests for mod with non int/unit input types"""
58298,delphianaus,1296155404,2022-10-30 7:53:45,"Is the windows build broken ? i want to build cuda debug and see if i can import the dll into C++ builder.
Bazel - 5.3.1
VC 2022
Branch 2.10 from git hub,
using latest copy of anaconda3 ( Python 3.10)",Is the windows build broken ?
58136,PCJimmmy,1296086289,2022-10-30 3:26:40,Getting tensorflow to work with cudnn and cudatoolkit is a huge roadblock towards using the latest tf versions. Any chance you folks can make cudnn and cudatoolkit a better priority when you generate new versions that are not compatible with combinations that worked on the previous tf version. It seems weeks often pass before it becomes clear what the new dependecy is for a new tf for these two.,Getting tensorflow to work with cudnn and cudatoolkit is a huge roadblock towards using the latest tf versions.
55743,stewartmiles,1295477439,2022-10-28 21:19:23,"@gbaned @JunyoungLim hello again, I'm still waiting for the AMD ROCm build to be fixed - it isn't due to this change - and this commit to be merged. As I've mentioned before I don't mind you taking this commit and manually patching in g3 then I can just close this out when it is auto merged to this repo.",I'm still waiting for the AMD ROCm build to be fixed - it isn't due to this change - and this commit to be merged. As I've mentioned before I don't mind you taking this commit and manually patching in g3 then I can just close this out when it is auto merged to this repo.
51506,dorothykiz1,1295361604,2022-10-28 19:13:20,Running this `bazel build --config=macos_arm64 tensorflow/tools/pip_package:build_pip_package` compiles for a very long time.,Running this bazel build --config=macos_arm64 tensorflow/tools/pip_package:build_pip_package compiles for a very long time.
58351,bhack,1295266247,2022-10-28 17:30:12,"It was already protected emitting `UnimplementedError` in our stable release 2.10.0:
```python UnimplementedError: {{function_node __wrapped__DepthwiseConv2dNative_device_/job:localhost/replica:0/task:0/device:CPU:0}} Current kernel implementation does not support dilations, received [1 2 3 1] [Op:DepthwiseConv2dNative]
```",UnimplementedError
56892,Craigacp,1295230384,2022-10-28 16:51:42,The Tensorflow Java API in this repo isn't maintained. The maintained version of TF-Java lives in https://github.com/tensorflow/java.,The Tensorflow Java API in this repo isn't maintained.
34962,anya-chan,1294884893,2022-10-28 11:31:36,"I have commented two lines 2163 & 2164, but problem not solved.","I have commented two lines 2163 & 2164, but problem not solved."
58344,dradenvandewind,1294770417,2022-10-28 9:37:30,"can you tell me how to load my model at *.pb format on the virtual GPU with restrict memory
i can t force it
with tf.compat.v1.device(f""/job:localhost/replica:0/task:0/device:GPU:1""):
graph = tf.Graph()
with graph.as_default():
tf.import_graph_def(graph_def, name=name)","""Can you tell me how to load my model at *.pb format on the virtual GPU with restrict memory"""
58283,reedwm,1294393406,2022-10-28 3:12:25,"@pgpetrak can you review? I think merging this will also require updating the cuDNN frontend library internally.
/CC @artem-b",I think merging this will also require updating the cuDNN frontend library internally.
58346,DEKHTIARJonathan,1293820487,2022-10-27 17:03:43,"@bixia1 for review.
Sorry for the large PR, I really don't have a way to split this in small incremental changes",I really don't have a way to split this in small incremental changes.
57983,cantonios,1293787191,2022-10-27 16:32:25,"> I did a search and believe there is no uses (other than the definition itself) within public tensorflow code base. However, I am not sure if there is any internal code inside google that is still using `ShapeFromFormat`, it will be a little harder to debug if internal tests are failing.
Let's try to remove it then. If there are any internal failures, I can update those on my end.","I am not sure if there is any internal code inside google that is still using ShapeFromFormat, it will be a little harder to debug if internal tests are failing."
57983,yongtang,1293778669,2022-10-27 16:25:12,"@cantonios I did a search and believe there is no uses (other than the definition itself) within public tensorflow code base. However, I am not sure if there is any internal code inside google that is still using `ShapeFromFormat`, it will be a little harder to debug if internal tests are failing.","I am not sure if there is any internal code inside google that is still using ShapeFromFormat, it will be a little harder to debug if internal tests are failing."
58323,henrysky,1293721559,2022-10-27 15:39:04,"@tilakrayal Yes I compile it from https://github.com/tensorflow/tensorflow/archive/refs/tags/v2.11.0-rc1.zip. Moreover, the fact https://pypi.org/project/tensorflow/2.11.0rc1/#files windows wheel contains nothing indicates something is wrong with tensorflow 2.11 compilation on windows even for official release!","Moreover, the fact https://pypi.org/project/tensorflow/2.11.0rc1/#files windows wheel contains nothing indicates something is wrong with tensorflow 2.11 compilation on windows even for official release!"
29840,chaithyagr,1293643324,2022-10-27 14:50:59,"@smatzek Yes indeed.. Thats the best way to work forward i guess, but then it gets reallly hard to move forward from there right... I am expecting a lot of changes happened over time in meantime in these files.. The thing is, I really want to use this, but most of my codes (even some codes in GOOGLE_CUDA), are based on tensorflow-2.9 ... So it would mean recompiling them all for tensorflow 2.2 with these patches","""I really want to use this, but most of my codes (even some codes in GOOGLE_CUDA), are based on tensorflow-2.9 ... So it would mean recompiling them all for tensorflow 2.2 with these patches."""
57663,donglinz,1293413771,2022-10-27 11:53:42,@rishikasinha-tf @mihaimaruseac Kindly ping... This issue is still present on the latest r2.10 branch. commit: 829f9ebd6bf7fa9ce4fb7e79c0142bda3cadc134,"""This issue is still present on the latest r2.10 branch."""
58315,jdanceze,1293363596,2022-10-27 11:08:39,"@tilakrayal according to your colab, the crash will only trigger when `dtype` argument in `tf.raw_ops.ResourceGather` is not match with dtype value of `indice`. Hence, if both dtype is `tf.int32`, it will not trigger crash.","""crash"""
58315,bhack,1293352092,2022-10-27 10:58:56,"@tilakrayal Please check internally with the teams how we need to handle assert issues on raw ops after we have declassified these as security bugs.
Cause in the last few days we had many tickets like this one.",Cause in the last few days we had many tickets like this one.
58316,bhack,1293344594,2022-10-27 10:51:28,"We had other issues with GCC 11 https://github.com/tensorflow/tensorflow/issues/50303
As the build is only tested with what you see in the table we could have still some compatibility issue (or with our dependencies versions) with other GCC releases.",We had other issues with GCC 11
57502,albertz,1293318104,2022-10-27 10:26:46,I assume this is not really fixed? We get the same in `MaxPoolingNoMaskOp`.,I assume this is not really fixed? We get the same in MaxPoolingNoMaskOp.
51803,albertz,1293159175,2022-10-27 8:16:00,"For reference, others report (https://github.com/matterport/Mask_RCNN/issues/521#issuecomment-780459992):
> I had accidentally created a Dense layer with zero ""units"" (layers in the output)
The error likely originates from sth like that.","""I had accidentally created a Dense layer with zero ""units"" (layers in the output) The error likely originates from sth like that."""
31731,kyamagu,1293043506,2022-10-27 6:14:43,"For anyone seeing this issue, the following approach can mitigate the `nan` issue though numerical stability is another issue.
```python
tf.cast(tf.linalg.sqrtm(tf.cast(matrix, tf.complex128)), tf.float64)
```",nan issue though numerical stability is another issue.
58291,971766904,1292861733,2022-10-27 1:54:19,"> Have you tried to change the name of the Conda environment in ""3. Create a conda environment"" so you can test the procedure on a fresh one?
Thanks, the problem has been solved by reinstall tensorflow with`conda install tensorflow`. But why `pip install tensorflow` can't work.","""Why pip install tensorflow can't work.?"""
58149,penpornk,1292857419,2022-10-27 1:45:05,"Also cc'ing @nSircombe and @milpuz01:
FYI, since this affects the aarch64 build as well.","cc'ing @nSircombe and @milpuz01: FYI, since this affects the aarch64 build as well."
57779,jxy,1292482771,2022-10-26 19:01:55,"It is an issue when we cast a variable the precision changes depending on whether the variable contains a python float or a TF tensor.
Can we at least leave the issue open until our ""long-standing desire"" gets satisfied?","""Can we at least leave the issue open until our ""long-standing desire"" gets satisfied?"""
58316,babarmehta,1292312700,2022-10-26 16:36:03,"Maybe this newer version of `abseil` is just not compatible with my environment, although I would like to understand why that is. Do you think I could just patch the source code to use the same `abseil` that `2.9` does and still get a working installation?","""Maybe this newer version of abseil is just not compatible with my environment, although I would like to understand why that is."""
58316,babarmehta,1292306205,2022-10-26 16:30:20,"I can compile inside a container with that image, and I believe it is definitely my environment. But what I am trying to understand is what in my environment is causing this, because I can build `2.9` GPU variant just fine. :(","I can compile inside a container with that image, and I believe it is definitely my environment. But what I am trying to understand is what in my environment is causing this, because I can build 2.9 GPU variant just fine."
57069,cantonios,1292236294,2022-10-26 15:36:43,"> Could you please check this PR when you have some time? Are there any other header issues which need to be addressed?
Yes, we're still seeing errors of the form:
```
tensorflow/core/framework/tensor.cc:63:10: fatal error: 'tensorflow/tsl/util/byte_swap_array.h' file not found
```
for mobile builds. Once all the tests pass, this should be submitted automatically.","""We're still seeing errors of the form:  tensorflow/core/framework/tensor.cc:63:10: fatal error: 'tensorflow/tsl/util/byte_swap_array.h' file not found  for mobile builds."""
57653,nouiz,1292219327,2022-10-26 15:24:09,"I suppose you meant kAfterOptimizationsDumpName instead of kBeforeOptimizationsDumpName.
I did it and created/moved those constant string to the right place.",I suppose you meant kAfterOptimizationsDumpName instead of kBeforeOptimizationsDumpName.
58316,babarmehta,1292149939,2022-10-26 14:35:36,"Yes, I tried removing `DNO_CONSTEXPR_FOR_YOU` and explicitly enabled C++17, but it is always erroring out at same place. Anywhere `GpuLaunchKernel` is called the build errors out. I don't know much about the code base but do the non-GPU builds not use `abseil`? Because I can build the CPU variant (with and without `mkl`) just fine, only the GPU variant fails. Is this a problem with `nvcc` maybe?","I can build the CPU variant (with and without mkl) just fine, only the GPU variant fails. Is this a problem with nvcc maybe?"
58293,tilakrayal,1292016262,2022-10-26 13:13:50,"@xushanthu-2014,
To expedite the trouble-shooting process, could you please provide a complete code you are using. Also as metioned, please don't import keras directly and try to import ```
from tensorflow import keras
from tensorflow.keras.models import load_model
```","""To expedite the trouble-shooting process, could you please provide a complete code you are using."""
58247,bhack,1291835270,2022-10-26 10:38:36,"> The Tensorflow build seems to choke on the python3-protobuf package for some reason. Installing protobuf via pip fixed the issue here:
Yes sorry I just looked at the first error on the 2nd log.
I think it is an instance of the old https://github.com/tensorflow/tensorflow/issues/6341
@berndporr Can you try to install protobuf from pip?",The Tensorflow build seems to choke on the python3-protobuf package for some reason.
57794,kaixih,1291524970,2022-10-26 5:29:14,"I can repro the failure on my V100 machine. Basically, we need to skip the `CudnnFusedConvRewriterHloTest.FuseElu` test on the pre-Ampere GPUs as well, because the test will expect the fused hlo instruction but the `RunHloPass` will check the GPU generation and skip the fusion when it is lower than Ampere. So, we should skip this test for the pre-Ampere platforms.
The fix has been submitted. PTAL. @cheshire","""Can repro the failure on my V100 machine."""
58291,971766904,1291471674,2022-10-26 4:12:27,"> Can you try to make a fresh install from the terminal using the official guide?
> > https://www.tensorflow.org/install/pip#linux
thanks, the official guide can't help. I got the same importerror.","""I got the same importerror.."""
58247,pwuertz,1291231197,2022-10-25 23:06:39,"The Tensorflow build seems to choke on the `python3-protobuf` package for some reason. Installing `protobuf` via pip fixed the issue here:
```
pip install protobuf==3.19
```",The Tensorflow build seems to choke on the python3-protobuf package for some reason.
56571,jishminor,1291215909,2022-10-25 22:40:47,"The issue is that if I build with the cmake flag TFLITE_ENABLE_XNNPACK = OFF, xnnpack is not included in the build, and application logic will not be able to apply the xnnpack delegate explicitly, which is the behavior i'm looking for. You can verify this yourself via running the cmake build for tflite and passing `TFLITE_ENABLE_XNNPACK = OFF` as one of the cmake args. libXNNPACK.a is not built.",TFLITE_ENABLE_XNNPACK = OFF
58305,mihaimaruseac,1290912214,2022-10-25 17:37:32,Need to immediately be followed by the revert of the cherrypick,Need to immediately be followed by the revert of the cherrypick.
58304,mihaimaruseac,1290905991,2022-10-25 17:34:23,This PR just complicates matters. The original PR needs to be reverted and the original fix cherrypicked properly.,This PR just complicates matters.
58275,jepikol,1290831248,2022-10-25 16:26:10,"I juste had to install wget on macos. Befor this I have been reading tens of web pages, nobody never said something about wget.","I have been reading tens of web pages, nobody never said something about wget."
56762,SandSnip3r,1290709957,2022-10-25 15:03:20,All tests are not passing. Still working on this. Sorry it's taking so long.,All tests are not passing.
57913,ra9hur,1290667282,2022-10-25 14:42:27,"@ abattery, @haozha111, @sachinprasad, @mohantym - It has been more than 3 weeks. Can someone please respond ?",It has been more than 3 weeks.
57931,ra9hur,1290665690,2022-10-25 14:41:15,"@miaout17, @sachinprasad, @mohantym - It has been more than 3 weeks. I have raised 3 issues in the last week of Sep and there has been no response for any of these. Can someone please respond ?",I have raised 3 issues in the last week of Sep and there has been no response for any of these.
57904,ra9hur,1290662280,2022-10-25 14:39:03,"@miaout17, @sachinprasad, @mohantym - It has been more than 3 weeks. Can someone please respond ?",It has been more than 3 weeks.
58296,bhack,1290617128,2022-10-25 14:06:22,"It is mainly related to:
https://github.com/keras-team/keras-cv/issues/291
https://github.com/tensorflow/tensorflow/issues/55639
Some warnings were suppressed https://github.com/tensorflow/tensorflow/commit/88a263ee53fdfa172c37090ef245ac0668b6200c but it just reduces messages as it was not going to solve the performance issues.","""It is mainly related to: https://github.com/keras-team/keras-cv/issues/291 https://github.com/tensorflow/tensorflow/issues/55639 Some warnings were suppressed https://github.com/tensorflow/tensorflow/commit/88a263ee53fdfa172c37090ef245ac0668b6200c but it just reduces messages as it was not going to solve the performance issues."""
48881,372046933,1290499891,2022-10-25 12:46:54,"CPU memory profiling does not work on https://colab.research.google.com/gist/rmothukuru/db189c30d7acd81df1b2b75423315238/tensorboard_profiling_keras.ipynb#scrollTo=dFWOMyaHkUX5
even with TF 2.9",CPU memory profiling does not work on https://colab.research.google.com/gist/rmothukuru/db189c30d7acd81df1b2b75423315238/tensorboard_profiling_keras.ipynb#scrollTo=dFWOMyaHkUX5 even with TF 2.9.
58277,bhack,1290470344,2022-10-25 12:32:07,"It is not the best warning message but the issue is that you have used `set_loop_options` twice so your second entry detect another instruction before and instead `""set_loop_options"" must be the first statement in the loop block`
Can you try with a single entry?:
`tf.autograph.experimental.set_loop_options(shape_invariants=[(y_n, tf.TensorShape([None])), (interior, tf.TensorShape([None]))])`","""set_loop_options"" must be the first statement in the loop block"
58293,bhack,1290442366,2022-10-25 12:08:22,"I cannot reproduce this but please don't import keras directly cause you are probably going to use an old Keras wheel.
The right import is: `from tensorflow.keras.models import load_model`",I cannot reproduce this but please don't import keras directly cause you are probably going to use an old Keras wheel.
47554,Psyhich,1289977736,2022-10-25 4:52:43,"Having the same issue, can save in h5 format, however tf still not supported
Tensorflow 2.10.0","Having the same issue, can save in h5 format, however tf still not supported Tensorflow 2.10.0."
55941,reedwm,1289775016,2022-10-24 23:26:50,"Unfortunately this was rolled back in 970c3b44ea8b0db78d92ada624f03aeacf2e4518 because it broke the TF serving build. @learning-to-play, can you debug this or triage?","""Unfortunately this was rolled back in 970c3b44ea8b0db78d92ada624f03aeacf2e4518 because it broke the TF serving build."""
58007,cantonios,1289587686,2022-10-24 20:38:52,"From the above MR, this is a GPU kernel issue. Please specify these details in the bug description - the test in question passes on CPU, but fails on GPU.","""The test in question passes on CPU, but fails on GPU."""
58131,cantonios,1289406316,2022-10-24 18:11:42,"`std::pow` always converts to float/double/long-double, so there we can use `inf`. In TF, we want to keep the output type as integer, which doesn't support `inf`. The overflow will result in undefined behavior, which is exactly what we are seeing. We do not need alignment (on CPU/GPU, or between any platform).","std::pow always converts to float/double/long-double, so there we can use inf. In TF, we want to keep the output type as integer, which doesn't support inf. The overflow will result in undefined behavior, which is exactly what we are seeing. We do not need alignment (on CPU/GPU, or between any platform).."
58145,bixia1,1289399529,2022-10-24 18:05:32,This line change is already the existing code. Should close this PR.,This line change is already the existing code. Should close this PR.
58145,bixia1,1289318961,2022-10-24 16:52:20,trying to manually merge it.,Trying to manually merge it.
58282,drivanov,1289318508,2022-10-24 16:51:57,@bixia1 : This is a replacement for [PR#](https://github.com/tensorflow/tensorflow/pull/58256) which was unexpectedly closed when I tried to resolve the merge conflicts.,"""unexpectedly closed"""
58217,mihaimaruseac,1289310712,2022-10-24 16:45:38,Can you squash these commits please? It doesn't make sense to have 5 commits for a line change and one extra empty line,Can you squash these commits please? It doesn't make sense to have 5 commits for a line change and one extra empty line.
58131,bhack,1289086781,2022-10-24 14:03:33,Probably we could close this. I don't expect that MLIR kernel codegen could/would align on the mul overflow type of the Eigen implementation.,I don't expect that MLIR kernel codegen could/would align on the mul overflow type of the Eigen implementation.
58131,bhack,1289071510,2022-10-24 13:53:13,"@creakseek Also please note that more in general you cannot rely on unspecified behavior for an expected output so please consider the right input/tensor type for the values you feed in a specific operation:
https://en.cppreference.com/w/c/language/conversion
> Although signed integer overflow in any arithmetic operator is undefined behavior, overflowing a signed integer type in an integer conversion is merely unspecified behavior.","""Cannot rely on unspecified behavior"""
57663,donglinz,1288924252,2022-10-24 11:56:39,Has the cherrypick PR merged in r2.10? This error still blocks building from the source with the r2.10 branch.,This error still blocks building from the source with the r2.10 branch.
58264,johnthagen,1288906031,2022-10-24 11:38:06,"This is essentially a duplicate of:
- https://github.com/tensorflow/tensorflow/issues/56137#issuecomment-1156723441
@terryheo mentioned he could build & upload 3.10 wheels for `tflite-runtime`:
- https://github.com/tensorflow/tensorflow/issues/56137#issuecomment-1156729053
But unfortunately it has been several months and the [latest `tflite-runtime-nightly`](https://pypi.org/project/tflite-runtime-nightly/2.12.0.dev20221023/#files) still only has Python 3.7-3.9 wheels.","""It has been several months and the [latest tflite-runtime-nightly](https://pypi.org/project/tflite-runtime-nightly/2.12.0.dev20221023/#files) still only has Python 3.7-3.9 wheels."""
58276,penpornk,1288706461,2022-10-24 9:15:05,"Closing this PR as I forgot that I made additional changes to the [original PR](https://github.com/tensorflow/tensorflow/pull/57998) during merge, so we can't just use the original commit. Creating a new one soon.","I made additional changes to the [original PR](https://github.com/tensorflow/tensorflow/pull/57998) during merge, so we can't just use the original commit."
46475,leleogere,1288677907,2022-10-24 8:55:21,"Same issue here with tensorflow 2.9.1 and cudatoolkit 11.7.0. Fixed by changing activations from `'relu'` to `lambda x: tf.math.maximum(x, 0.0)`.
I changed the following code:
```py
model = Sequential([
Conv2D(3, 3, activation='relu'),
Flatten(),
Dense(8, activation='relu')
])
```
to this one:
```py
relu = lambda x: tf.math.maximum(x, 0.0)
model = Sequential([
Conv2D(3, 3, activation=relu),
Flatten(),
Dense(8, activation=relu)
])
```","""Same issue here with tensorflow 2.9.1 and cudatoolkit 11.7.0"""
58269,penpornk,1288631704,2022-10-24 8:28:44,"[ARM CI](https://github.com/tensorflow/tensorflow/actions/runs/3309074945/jobs/5461899463) failed, but it seems to be an existing failure since [another run](https://github.com/tensorflow/tensorflow/actions/runs/3308078945) before it (from another PR) also got the same failure.","""It seems to be an existing failure since [another run](https://github.com/tensorflow/tensorflow/actions/runs/3308078945) before it (from another PR) also got the same failure."""
2625,bhack,1288086930,2022-10-23 11:02:59,"@tensorflow/dev-support Can you remove the contribution welcome label and retriage this?
I think we don't have a clear enough contribution path to label this as contribution welcome so we need to re-triage it. This ticket is very old but recently we had a related bug at https://github.com/tensorflow/tensorflow/issues/58133","""I think we don't have a clear enough contribution path to label this as contribution welcome so we need to re-triage it."""
58141,hebrotem,1287953958,2022-10-23 0:11:41,"> @hebrotem, Code shared is full of [indentation errors](https://colab.research.google.com/gist/tilakrayal/64cbc51b2cb4a7df11cd6a39f76a3f95/untitled708.ipynb), please share a colab gist with issue reported or simple stand alone indented code with all dependencies such that we can replicate the issue reported. Thank you!
please check this link for the colab gist https://gist.github.com/hebrotem/2ad4fd6df14261a90128db8e45b4052e","Code shared is full of [indentation errors](https://colab.research.google.com/gist/tilakrayal/64cbc51b2cb4a7df11cd6a39f76a3f95/untitled708.ipynb), please share a colab gist with issue reported or simple stand alone indented code with all dependencies such that we can replicate the issue reported. Thank you! please check this link for the colab gist https://gist.github"
37026,marlon-shiftone,1287849624,2022-10-22 16:35:34,"I have tried the solution of turning the `should_recreate_iterator` value to `False` but it hasn't had any effect on me. Up to now, the problem seems unsolved to me, which is a scandal since the data generator is supposed to address this problem precisely.",I have tried the solution of turning the should_recreate_iterator value to False but it hasn't had any effect on me.
58263,bhack,1287831818,2022-10-22 15:46:55,"I am also tracking the same at https://github.com/tensorflow/profiler/issues/503
But the ownership of this issue is still not clear.
/cc @yatbear",I am also tracking the same at https://github.com/tensorflow/profiler/issues/503 But the ownership of this issue is still not clear.
56761,SandSnip3r,1287492549,2022-10-21 22:23:23,FYI. Still working on resolving internal build issues.,Still working on resolving internal build issues.
35677,MarkDaoust,1286957414,2022-10-21 13:24:00,"There'd be no harm in merging the PR. But there are a bunch of merge conflicts. Since Random seeds are such a common topic in software I'm not sure we need to be explaining it here. So I'm just closing, unless someone wants to fix the conflicts. Sorry!","""But there are a bunch of merge conflicts."""
48531,code2k13,1286553359,2022-10-21 7:12:38,"This issue still persists. In my example, I have a custom loss function that uses mae over intermediate layers of a neural network. The `model.get_layer(""xxx"").output` API returns a 'KerasTensor' which I believe causes this issue.",This issue still persists.
58160,suggfudge,1286487470,2022-10-21 5:43:32,"looking at pintoo's instructions, it seems there are only dockerfiles for 2.8.0 that can be curled.
edit: nvermind, theres patches for that, but all the curls have failed to fetch the files. I had to copy the dockerfile contents from my browser","looking at pintoo's instructions, it seems there are only dockerfiles for 2.8.0 that can be curled."
57990,sachinprasadhs,1286279938,2022-10-20 23:43:33,"Right now, we don't have option to support the formats automatically as explained above.
For any specific OP to support any type, it has to be registered in kernels to perform any operations.","""For any specific OP to support any type, it has to be registered in kernels to perform any operations."""
57762,ekuznetsov139,1286044709,2022-10-20 19:36:49,It is not easy to split it up. I will attempt to cut it into two pieces.,I will attempt to cut it into two pieces.
58171,rsanthanam-amd,1285526444,2022-10-20 13:18:41,Closing this because it does not work on CUDA. I will open another PR with a better fix.,"""I will open another PR with a better fix."""
58181,shijy16,1285429657,2022-10-20 12:11:24,"@tiruk007 Hi,
I've checked the links in your response. I got that the correct device name has to be specified when calling `tf.device` in this issue. But I am still confused why something like `tf.device(""CPU"")` works in other situations, but not this one. And also, if I delete the line `tf.device` and let TF choose device automatically, the bug can sill be replicated.","I am still confused why something like tf.device(""CPU"") works in other situations, but not this one."
58181,maifeeulasad,1285409997,2022-10-20 12:03:20,"@shijy16 , my bad. Not sure why this is not working for you. Ref: https://www.kaggle.com/code/maifeeulasad/paddy-doctor-paddy-disease-classification?scriptVersionId=95678277&cellId=11
Env: `TF-2.6.4`",Not sure why this is not working for you.
58078,jpienaar,1284779382,2022-10-20 1:29:42,"Yes, we created an internal copy, modified to pass and then it triggered a test not on presubmit that resulted in it reverting. @not-jenni would be able to add some more details.","""We created an internal copy, modified to pass and then it triggered a test not on presubmit that resulted in it reverting."""
57934,dmitryfisko,1284543287,2022-10-19 20:35:46,"@sachinprasadhs I'm guess, that the right solution is in adding transitive dependency from tensorflow-lite-gpu to tensorflow-lite-gpu-api. This is very unobvious, that the consumer of TensorFlow SDK must add an additional dependency manually. Fix will be available in the future version of TF SDK?","This is very unobvious, that the consumer of TensorFlow SDK must add an additional dependency manually."
58131,bhack,1284194669,2022-10-19 15:27:38,"> I find that this issue only exists with dtype=np.int64
I think that case is the same without any explicit dtype",I find that this issue only exists with dtype=np.int64 I think that case is the same without any explicit dtype.
58133,bhack,1284181763,2022-10-19 15:18:23,"@fuzzyswan Python floats are represented as 64-bit double-precision values:
Can you try to reproduce the same error using `dtype=tf.float64` in both `tf.range` calls?","""Can you try to reproduce the same error using dtype=tf.float64 in both tf.range() calls?"""
58131,bhack,1284171644,2022-10-19 15:11:47,@mohantym I suppose that you cannot reproduce this with `dtype=np.float32` or `dtype=np.float64` used in both the CPU/GPU inputs right?,I suppose that you cannot reproduce this with dtype=np.float32 or dtype=np.float64 used in both the CPU/GPU inputs right?
58022,cheshire,1283785633,2022-10-19 10:31:37,"> only supports the default allocator (backend().memory_allocator()). Is it possible to run these tests with GPUBFCAllocator
Is it possible to pipe through a method which supplies a custom allocator? TF runtime does pipe it through, so the API must be there?","""only supports the default allocator"""
58143,catqaq,1283642851,2022-10-19 8:41:26,"@mohantym Yes, it is resolved. But i did not use quantization when converting h5 model to tflite model.","""But i did not use quantization when converting h5 model to tflite model."""
57948,rsanthanam-amd,1283147621,2022-10-18 23:49:39,@cheshire I had to fix this up because what I had previously was failing on CUDA.,I had to fix this up because what I had previously was failing on CUDA.
56497,bixia1,1283041184,2022-10-18 21:43:45,emailed you the error log again.,emailed you the error log again.
58122,fuzzyswan,1283028206,2022-10-18 21:31:02,"Hi @tilakrayal , indeed `depth_radius` should be int according to documentation, however, input validity checking is missing on CPU and causing inconsistent behaviour on CPU and on GPU. Specifically, CPU accepts `0.1` which is not an integer, but GPU will not accept it. I think if the argument `depth_radius` has to be an integer, then on CPU the API should do proper checking to avoid API misuse.","depth_radius should be int according to documentation, however, input validity checking is missing on CPU and causing inconsistent behaviour on CPU and on GPU. Specifically, CPU accepts 0.1 which is not an integer, but GPU will not accept it."
57212,cantonios,1283000310,2022-10-18 21:02:49,This is a tf-addons issue. `tfa.image.mean_filter2d` currently needs a constant filter size.,tfa.image.mean_filter2d currently needs a constant filter size.
58102,akashAD98,1282283554,2022-10-18 12:09:27,"@mohantym i m using the script provided by TF. I just want to do inference of my trained model, so it s fine if it's not tf.lite weight even .hdf5 \ hf5 inferencing is fine, but that part is not provided here.
my goal is to do inference of this model .
here is official colab for same https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb","""I just want to do inference of my trained model, so it s fine if it's not tf.lite weight even .hdf5  hf5 inferencing is fine, but that part is not provided here."""
58135,JustASquid,1281888672,2022-10-18 6:45:05,"@mohantym I want to XLA compile the entire model, not just the custom layer. setting `jit_compile=True` on the `@tf.function` while still using `jit_compile=True` on the model compilation results in the same error.",setting jit_compile=True on the @tf.function while still using jit_compile=True on the model compilation results in the same error.
58102,akashAD98,1281828521,2022-10-18 5:19:26,"@mohantym its not working, im getting a tracker warning, & no output ```python pose_estimation.py --label_file labels.txt --model movenet_thunder --classifier pose_classifierfloat32```
#size of converted tflite model is 27kb is this fine? ```
warning I'm getting
'''WARNING:root:No tracker will be used as tracker can only be enabled for MoveNet MultiPose model.'''
& I'm not using any tracker
default weight which provided by your repo is working fine, but my trained model is not working here","""I'm not using any tracker"""
53365,enfild,1281824378,2022-10-18 5:12:29,"@catqaq in my case this fact ( several objects which has this interpreter) was reason of falling.
I cannot give you advice for Java, but in c++ you can fix this with using `shared_ptr` for interpreter and `static mutex` for all owners",several objects which has this interpreter
57977,DLumi,1281777366,2022-10-18 3:49:14,"Mind you, the collab results are not as bad (I'd say they are at least tolerable) as they get when you run the same notebook on a windows machine.","""Mind you, the collab results are not as bad (I'd say they are at least tolerable) as they get when you run the same notebook on a windows machine."""
57943,bhpatray,1281744102,2022-10-18 2:49:01,"I followed the same steps and found incorrect results.
I got correct results in LINUX means I have done pre processing correct. Just check output using TFLite C++ windows library and compare with python and TFLite C++ linux library",I followed the same steps and found incorrect results.
57975,JustASquid,1281669758,2022-10-18 0:44:44,"@tilakrayal - I have run this in a fresh virtual environment following the recommended GPU installation on Windows Native:
https://www.tensorflow.org/install/pip#windows-native_1
And the issue still comes up. It seems like this is a Windows issue, so it makes sense that the colab gist works correctly.",I have run this in a fresh virtual environment following the recommended GPU installation on Windows Native: https://www.tensorflow.org/install/pip#windows-native_1 And the issue still comes up.
57956,cantonios,1281316057,2022-10-17 18:41:42,"> The “compressed” keyword could be renamed. Introducing an entirely new datatype might be a substantial effort.
The issue is it's not _really_ a generic sparse tensor anymore - with ""compressed"" on, it's a severely restricted kind of sparse tensor - one with a very specific structure that really only comes up with embeddings. You're really trying to squeeze in a different type into the existing `SparseTensor` class. This should be an embedding-specific type.",The “compressed” keyword could be renamed.
57956,cantonios,1281222402,2022-10-17 17:36:14,"I don't think ""compressed"" is the right term here. A ""compressed"" sparse tensor signals to me something like compressed sparse row/column storage. What we're talking about here is a very special kind of sparse tensor that seems specific to embeddings: (as Rohan mentioned) one which acts more like a ragged tensor.
My suggestion is to either try to use RaggedTensor directly, or create a completely new type for this rather than modify `SparseTensor`.","""compressed"" sparse tensor signals to me something like compressed sparse row/column storage. What we're talking about here is a very special kind of sparse tensor that seems specific to embeddings: (as Rohan mentioned)  one which acts more like a ragged tensor. My suggestion is to either try to use RaggedTensor directly, or create a completely new type for this rather than modify SparseTensor."
57069,cantonios,1281179202,2022-10-17 17:03:39,"@kun-lu20 sorry, I've been away for a week.
We still have header issues. Now it's complaining about
```
./tensorflow/core/framework/tensor_util.h:24:10: error: module //tensorflow/core/framework:tensor does not depend on a module exporting 'tensorflow/core/framework/tensor_shape.proto.h'
```
I'm not sure why we're getting this now, since AFAIK we always included that header. Maybe we actually rely on it now though with your additions.","""I'm not sure why we're getting this now, since AFAIK we always included that header."""
57805,mihaimaruseac,1281171059,2022-10-17 16:56:42,"Closing as duplicate of #58032 (later issue, but with more content and fewer bots)","Closing as duplicate of #58032 (later issue, but with more content and fewer bots)"
57778,jpienaar,1280876459,2022-10-17 13:38:28,"That's an assert failure and I'm not sure what config=v2 sets, but setting ""--copt=-UNDEBUG"" explicitly should enable the asserts.","That's an assert failure and I'm not sure what config=v2 sets, but setting ""--copt=-UNDEBUG"" explicitly should enable the asserts."
57959,jiannanWang,1280848870,2022-10-17 13:19:26,"Hi @tilakrayal . The inconsistency was in tensorflow-cpu. After installing tensorflow-cpu 2.10.0 and executing the rest code, the results will be ```
2.10.0
tf.Tensor(0, shape=(), dtype=int32)
tf.Tensor(4, shape=(), dtype=int32)
```
. Comparing to the results from tensorflow-gpu, there might be a bug in tensorflow-cpu's graph mode.","""The inconsistency was in tensorflow-cpu"""
44711,herossa,1280844213,2022-10-17 13:16:04,"My attempts:
2.4.1: leak
2.7.1: leak
How the problem occurs to me?
```
def predict(self, img: np.ndarray) -> np.ndarray:
return self._model.predict(np.expand_dims(img, axis=0))
```
How did I solve it?
```
def predict(self, img: np.ndarray) -> np.ndarray:
return self._model(convert_to_tensor(np.expand_dims(img, axis=0)), training=False).numpy()
```","""How the problem occurs to me"""
58107,tilakrayal,1280549786,2022-10-17 9:21:39,"@sachinprasadhs,
I was able to reproduce the issue on tensorflow-gpu v2.9 and nightly whereas on [cpu](https://colab.research.google.com/gist/tilakrayal/3f5ad2b8c85bc3b85d5ff26e808f0419/cpu.ipynb), the code was able to execute without any issues/error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/56a3bdd6a9e646999d77e49eb8ac4adf/gpu.ipynb).","I was able to reproduce the issue on tensorflow-gpu v2.9 and nightly whereas on [cpu](https://colab.research.google.com/gist/tilakrayal/3f5ad2b8c85bc3b85d5ff26e808f0419/cpu.ipynb), the code was able to execute without any issues/error."
58085,divyajaincs,1280532739,2022-10-17 9:08:51,"@mohantym I'm testing with original image size of (760,1000), with gpu it's working fine but with cpu it's crashing.","I'm testing with original image size of (760,1000), with gpu it's working fine but with cpu it's crashing."
58085,divyajaincs,1280477838,2022-10-17 8:25:55,"@mohantym when i'm testing with your updated code, colab session is crashing with cpu. any idea why this is happening?","""colab session is crashing with cpu"""
58105,shijy16,1280424290,2022-10-17 7:40:25,"Yeah, but `window_size` should be checked before calculating to avoid crashes such as this for potential security issues.
Crashes such as segment fault should never be triggered in TensorFlow.",window_size should be checked before calculating to avoid crashes such as this for potential security issues.
58072,iceteahh,1280288448,2022-10-17 4:59:07,"@sushreebarsa I tried it with tensorflow 2.10.0, this error still remains","I tried it with tensorflow 2.10.0, this error still remains."
58074,WangFengtu1996,1280193188,2022-10-17 2:33:38,"HI,all
@mohantym I can compile a hello world demo using cpp 17 and 20. I can not build tensorflow minimal demo. So, please reopen the issue!","I can compile a hello world demo using cpp 17 and 20. I can not build tensorflow minimal demo. So, please reopen the issue!"
57943,BhaskarsarmaP,1279989704,2022-10-16 15:16:14,"Hello, I tried with resized input only.
It seems windows library is producing incorrect results even for basic quant models.
For other OS , It is matching and proper.
This seems a critical bug.
Could you check and resolve ? @tilakrayal @tiruk007 @mohantym","""I tried with resized input only"""
58098,leland-hepworth,1279529529,2022-10-14 22:19:42,"Also the ""Current Behaviour"" section in the issue template is being formatted as code. I had to edit it to remove the code formatting.",I had to edit it to remove the code formatting.
58091,mihaimaruseac,1279443821,2022-10-14 20:40:57,"Please send PRs to fix these. Google's internal build is C++17, so we won't notice this.","""Google's internal build is C++17, so we won't notice this."""
56869,rahulbatra85,1279239908,2022-10-14 16:47:00,"@SandSnip3r I noticed feedback/copybara CI failed.
Is this still failing because of this
[tensorflow/tsl/platform/default/rocm_rocdl_path.cc:23] fatal error: rocm/rocm_config.h: No such file or directory
23 | #include ""rocm/rocm_config.h""
| ^~~~~~~~~~~~~~~~~~~~
?","""I noticed feedback/copybara CI failed. Is this still failing because of this [tensorflow/tsl/platform/default/rocm_rocdl_path.cc:23] fatal error: rocm/rocm_config.h: No such file or directory 23 | #include ""rocm/rocm_config.h"" |  ?"""
57732,dmvieira,1279137106,2022-10-14 15:11:25,I didn't change files that are broken. I think these tests are already broken,I didn't change files that are broken. I think these tests are already broken.
57778,jpienaar,1279101181,2022-10-14 14:40:18,tensorflow/compiler/mlir/tosa/tests:tf-to-tosa-pipeline.mlir.test fails with this test: out of range smallvector access in mlir::tosa::convertMirrorPadCommon().,out of range smallvector access in mlir::tosa::convertMirrorPadCommon().
58092,elfringham,1278986614,2022-10-14 13:07:07,Those 10 failures were introduced by this commit https://github.com/tensorflow/tensorflow/commit/720df16242fb5cace4fae147a45688750b660ee4,Those 10 failures were introduced by this commit.
57816,kkzheng,1278719259,2022-10-14 9:11:29,"> Closing as stale. Please reopen if you'd like to work on this further.
> Closing as stale. Please reopen if you'd like to work on this further.
how to re-open this issue",Closing as stale. Please reopen if you'd like to work on this further.
41810,federhub,1278614186,2022-10-14 7:41:52,"Hello everyone, I am facing issues in working on an RNN with variable length in both the input and output sequences, and I found this thread. I went through the colab posted by @foxik (https://colab.research.google.com/drive/18P6gZQUlP6qxBq70UCRfI42RSilRJ2Mx?usp=sharing) but it seems it is not working (or no longer working with tf 2.11.0-dev20221013). Any idea of why?","I am facing issues in working on an RNN with variable length in both the input and output sequences, and I found this thread. I went through the colab posted by @foxik (https://colab.research.google.com/drive/18P6gZQUlP6qxBq70UCRfI42RSilRJ2Mx?usp=sharing) but it seems it is not working (or no longer working with tf 2.11.0-dev20221013). Any idea of why?"
58088,ebnertom,1278536287,2022-10-14 6:17:50,@sushreebarsa I can confirm that I get the same error.,I can confirm that I get the same error.
56762,SandSnip3r,1278187704,2022-10-13 21:16:24,"I am seeing a ton of failures in our internal CI build of Tensorflow as OSS.
They are all errors that rocm headers can't be found. The offending headers are:
rocm/include/hipfft/hipfft.h
rocm/include/hipsparse/hipsparse.h
rocm/include/rccl/rccl.h
rocm/include/rocsolver/rocsolver.h
Do you have any intuition as to what could be wrong?",I am seeing a ton of failures in our internal CI build of Tensorflow as OSS. They are all errors that rocm headers can't be found.
52920,kouckma,1278141492,2022-10-13 20:25:02,I also have this issue with Bazel 3.1.0 and tf 2.4.0,I also have this issue with Bazel 3.1.0 and tf 2.4.0.
58090,rsanthanam-amd,1278095997,2022-10-13 19:37:26,"> The mentioned commit was actually meant to be fixing multi-manager instantiation for NCCL, not making it worse, but I guess it went wrong with AMD.
> > It's weird that `add_device_filters` is not working for ROCM, do you know why?
I'm not sure, let me investigate.
Is there a way to prevent this PR from landing until I can investigate this?","""I guess it went wrong with AMD"""
58003,cheshire,1278050364,2022-10-13 18:59:24,"@ezhulenev > xla_gpu_bef_executable to true in amoschenyq-debug and xla_gpu_enable_xla_runtime_executable
I think those are not supported yet, so it's not unexpected it does not work for you right now.","""I think those are not supported yet, so it's not unexpected it does not work for you right now."""
56804,Raj-Gohil,1278016136,2022-10-13 18:27:12,@LukeBoyer please refer to #57988 and the https://github.com/tensorflow/tensorflow/files/9724024/test_new1.zip for the files to reproduce the error.,"""Please refer to #57988 and the https://github.com/tensorflow/tensorflow/files/9724024/test_new1.zip for the files to reproduce the error."""
58003,AmosChenYQ,1277845979,2022-10-13 16:03:58,"I tried to add lmhlo_gpu into it, and this error disapper but runs into a new kind of error saying about runtime custom call error....","I tried to add lmhlo_gpu into it, and this error disapper but runs into a new kind of error saying about runtime custom call error."
58003,AmosChenYQ,1277843825,2022-10-13 16:02:16,And I think this problem is because when instantiating a new JitExecutable from the MLIR source with options defined [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc#L1534) this register dialects [config](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/mlir/transforms/runtime/compilation_pipeline_gpu.cc#L35) doesn't contain lmhlo_gpu,And I think this problem is because when instantiating a new JitExecutable from the MLIR source with options defined [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc#L1534) this register dialects [config](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/
39467,mihaimaruseac,1277758549,2022-10-13 15:02:25,CC @learning-to-play : this is another issue blocked on protobuf/grpc bump,CC @learning-to-play : this is another issue blocked on protobuf/grpc bump.
58077,Chizkiyahu,1277410858,2022-10-13 10:44:33,"I open [issue in Keras repo](https://github.com/keras-team/keras/issues/17148)
but in my debug the bug is probably in this repo",I open [issue in Keras repo](https://github.com/keras-team/keras/issues/17148) but in my debug the bug is probably in this repo.
58074,WangFengtu1996,1277090290,2022-10-13 6:25:21,"* if I use original `CMakelists.txt`, I got the error.
![image](https://user-images.githubusercontent.com/21277368/195518211-100a9cae-2d88-433b-92b0-af4852d0da0f.png)","if I use original CMakelists.txt, I got the error."
57915,sachinprasadhs,1276850253,2022-10-12 23:43:24,"Hi, In the above comment I did not see the inclusion of Select tf ops.
You can try the below snippet in your code and let us know the outcome. Thanks.
```
converter.target_spec.supported_ops = [
tf.lite.OpsSet.TFLITE_BUILTINS_INT8, # enable TensorFlow Lite ops.
tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
tflite_model = converter.convert()
```
Note that all ops are right now are not available in `INT8`.",I did not see the inclusion of Select tf ops.
57954,bixia1,1276825664,2022-10-12 23:14:12,"Build failed:
third_party/tensorflow/compiler/tf2tensorrt/convert/weights.cc:61:11: error: enumeration value 'kBOOL' not handled in switch [-Werror,-Wswitch]
switch (type_) {
^~~~~
1 error generated.
third_party/tensorflow/compiler/tf2tensorrt/convert/weights.cc:61:11: error: enumeration value 'kBOOL' not handled in switch [-Werror,-Wswitch]
switch (type_) {
^~~~~
1 error generated.","Build failed: third_party/tensorflow/compiler/tf2tensorrt/convert/weights.cc:61:11: error: enumeration value 'kBOOL' not handled in switch [-Werror,-Wswitch] switch (type_)   1 error generated."
56333,sachinprasadhs,1276789118,2022-10-12 22:15:47,"As you have mentioned, it throws `InvalidArgumentError` error for very small output value, but it does not have any standard restriction on the range of values to document it.","""It throws InvalidArgumentError error for very small output value, but it does not have any standard restriction on the range of values to document it."""
57844,cota,1276729858,2022-10-12 20:57:16,"The fix is not trivial, so here's a possible workaround.
Assuming you don't need XLA, can you please try to build with `--define=with_xla_support=false`?","The fix is not trivial, so here's a possible workaround. Assuming you don't need XLA, can you please try to build with --define=with_xla_support=false?"
57887,mihaimaruseac,1276729694,2022-10-12 20:57:04,"Also, please don't use ""Update <file>"". Use better commit messages and PR titles: https://cbea.ms/git-commit/","""Update file>"""
53529,elvinagam,1276590570,2022-10-12 18:44:36,This one seems to be still causing headaches...,This one seems to be still causing headaches...
52030,bhack,1276554142,2022-10-12 18:10:02,"We had a similar error in Keras CV enabling the MLIR bridge for https://github.com/keras-team/keras-cv/issues/895:
`tf.TensorListReserve' op unknown tensor list element shape`
`note: see current operation: %38 = ""tf.TensorListReserve""(%26, %25) {device = """"} : (tensor<i32>, tensor<i32>) -> tensor<!tf_type.variant<tensor<*xi32>>>`",tf.TensorListReserve' op unknown tensor list element shape
57663,mihaimaruseac,1276536231,2022-10-12 17:52:55,"> @rishikasinha-tf, would it be possible to include the PR in a TF 2.10.1 release? The log message is confusing as it (incorrectly) implies that an error has occurred that would prevent TF from running correctly.
Please send a cherrypick PR for `r2.10` too to fix the error message. See [go/tf-release/cherrypick](https://goto.google.com/tf-release/cherrypick) (internal link)",The log message is confusing as it (incorrectly) implies that an error has occurred that would prevent TF from running correctly.
57926,jpienaar,1276530379,2022-10-12 17:47:00,The markdown is automatically generated from the .td file. If it is mangled that means someone added something unexpected. In this case it looks like TensorArrayConcatV3's doc.,The markdown is automatically generated from the .td file. If it is mangled that means someone added something unexpected. In this case it looks like TensorArrayConcatV3's doc..
53822,MakGulati,1276435323,2022-10-12 16:21:05,this issue still exists and I also observing similar pattern as mentioned by @fitoule,This issue still exists and I also observing similar pattern as mentioned by @fitoule.
58052,vn218,1276360887,2022-10-12 15:24:48,"@mohantym Thanks!
I tried it, but its throwing this error
```
v@v-VirtualBox:~/Desktop/tensorflow_src$ tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \
tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh armhf
Invalid Dockerfile path: ""/home/v/Desktop/tensorflow_src/tensorflow/tools/ci_build/Dockerfile.pi-python37""
```","""Invalid Dockerfile path: ""/home/v/Desktop/tensorflow_src/tensorflow/tools/ci_build/Dockerfile.pi-python37"" "
57967,MayStepanyan,1276146317,2022-10-12 13:06:28,"@gadagashwini of course I can, but It doesn't solve the problem of not being able to save in tf format",not being able to save in tf format
57829,nouiz,1275378710,2022-10-11 22:59:23,"> @nouiz Could you please name the fusions from your HLO which suffer significantly from merging into broadcasts?
I updated the description to have a unit test HLO. I just checkout upstream TF and it was fusion 15 that has the issue.",I updated the description to have a unit test HLO. I just checkout upstream TF and it was fusion 15 that has the issue.
56761,philipphack,1275288633,2022-10-11 21:21:36,"> EDIT: I do not see the issue on my local 3090.
The *F16Padded tests verify the padding and slicing that is applied to run GEMMs based on operands with sizes that aren't multiples of 8 on Tensor Cores. The Pascal architecture didn't have Tensor Cores, and therefore these steps are skipped which leads to the HLO mismatch. A possible solution could be to deactivate these tests on older GPUs.",I do not see the issue on my local 3090.
56761,SandSnip3r,1275270270,2022-10-11 20:59:47,"I'm seeing it on a P100. I'm seeing some unrelated crash on V100. I haven't yet checked to see what's going on there. Something about ""blas_lt != nullptr"". I suspect that's not related to this PR.
EDIT: I do not see the issue on my local 3090.","I'm seeing it on a P100. I'm seeing some unrelated crash on V100. I haven't yet checked to see what's going on there. Something about ""blas_lt != nullptr"". I suspect that's not related to this PR."
58057,vinila21,1275184500,2022-10-11 19:39:17,Not required aymore,Not required aymore.
43696,ml-0,1274916477,2022-10-11 15:50:52,"The description requests:
Once, #41860 is merged all lines like
INCLUDES += isystem$(MAKEFILE_DIR)/downloads/cmsis/CMSIS/Core/Include/
should be removed from all Makefiles. This is still required if not already done in another context.
It refers to the Makefiles from tflite-micro which are in a separate repo (https://github.com/tensorflow/tflite-micro), meanwhile.","The description requests: Once, #41860 is merged all lines like INCLUDES += isystem$(MAKEFILE_DIR)/downloads/cmsis/CMSIS/Core/Include/ should be removed from all Makefiles. This is still required if not already done in another context. It refers to the Makefiles from tflite-micro which are in a separate repo (https://github.com/tensorflow/tflite-micro), meanwhile."
56661,otavio-silva,1274757672,2022-10-11 14:13:08,"Just tested with TensorFlow 2.9.1, this exact behavior persists. The workaround proposed by @hahouari seems to work nicely though. The problem is, shouldn't TensorFlow dead with this automatically?","Just tested with TensorFlow 2.9.1, this exact behavior persists."
58005,cheshire,1274442125,2022-10-11 10:00:55,> but PlatformUtil doesn't have a static function can provide the running platform name in the runtime. Do you think you could add one?,"""but PlatformUtil doesn't have a static function can provide the running platform name in the runtime."""
57947,febrifahmi,1274095447,2022-10-11 5:09:29,"Still has no luck with the building process of tflite-support using Bazel or Bazel on Arm, and I think I will skip the CMake method since I don't want to download all Tensorflow src to my pi while I only need selected features of tflite-support/tflite-runtime. Also, I cannot install tflite-runtime using virtualenv with both Python 3.7 and Python 3.9, and unfortunately, I don't use microcontrollers in this research project.","""still has no luck with the building process of tflite-support using Bazel or Bazel on Arm, and I think I will skip the CMake method since I don't want to download all Tensorflow src to my pi while I only need selected features of tflite-support/tflite-runtime."""
55941,learning-to-play,1274022028,2022-10-11 2:53:49,"Hi @trevor-m, It seems the macOS build is still failing.",It seems the macOS build is still failing.
58037,arav-agarwal2,1273745450,2022-10-10 19:52:22,"This might not be the correct place to put it, but isn't the results of AlphaTensor more of a theoretical exercise than a practical one, as they're looking more at theoretical complexity than true run-time performance? Algorithms like Strassen are sometimes not used for this purpose, as they generally do not lead to better performance as far as I know / have read.","""Algorithms like Strassen are sometimes not used for this purpose, as they generally do not lead to better performance as far as I know / have read."""
58027,MThalberg,1273706177,2022-10-10 19:06:45,"Hi @tilakrayal, the `seq_axis` is `-1` in the code example. Here, the negative integer value cause crash. This could potentially be used for DoS attack.","""The negative integer value cause crash. This could potentially be used for DoS attack."""
58005,i-chaochen,1273553466,2022-10-10 16:22:07,"> Could we minimize duplication and ideally make this a runtime switch?
Appricate for the feedback! Now I use a more genric PLATFORM to avoid the duplication.
Yes, you're correct that will be great if it can determine it at runtime instead of preprocessing, but PlatformUtil doesn't have a static function can provide the running platform name in the runtime. IIUIC, it's the StreamExecutor to take charge of the running platform name in the runtime?","""IIUIC, it's the StreamExecutor to take charge of the running platform name in the runtime."""
58039,Ingkarat,1273256981,2022-10-10 12:42:02,"For the CLA issue, it shows that there are 2 contributors possibly because the email set for git commits (non-gmail) and the email for CLA were different when creating this PR. I have changed the git email to match the CLA one but the issue still remains. I am not sure how to resolve this. Do I need to re-open the PR?","""I am not sure how to resolve this."""
58038,dverheijden,1273206810,2022-10-10 11:58:43,"I did some additional digging: When using the same pylint version as is used in the TensorFlow CI: [`2.7.4`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/install/install_pip_packages.sh#L82), the snippet that is now working (the one listed in the related issue), actually fails again. I'm not sure when this broke again.","I did some additional digging: When using the same pylint version as is used in the TensorFlow CI: [2.7.4](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/install/install_pip_packages.sh#L82), the snippet that is now working (the one listed in the related issue), actually fails again."
57959,tilakrayal,1273126016,2022-10-10 10:46:51,"@jiannanWang,
Sorry for the delay. I tried to execute the code on tensorflow-gpu and the results are the same in both cases. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/76627395e7ec322a3c5b34bbe586c7fb/untitled660.ipynb).",I tried to execute the code on tensorflow-gpu and the results are the same in both cases.
38800,xuyao91,1272732566,2022-10-10 3:07:05,"Hi
same issue
python 3.10.7
tensorflow 2.9.2
Mac pro 11.6
error:
`Traceback (most recent call last):
File ""/Users/xuyao/Workspaces/ai/tensorflow/models/research/object_detection/builders/model_builder_tf2_test.py"", line 21, in <module>
import tensorflow.compat.v1 as tf
ModuleNotFoundError: No module named 'tensorflow.compat'
`","Traceback (most recent call last): File ""/Users/xuyao/Workspaces/ai/tensorflow/models/research/object_detection/builders/model_builder_tf2_test.py"", line 21, in module> import tensorflow.compat.v1 as tf ModuleNotFoundError: No module named 'tensorflow.compat' ."
57902,VictoriaGriffith,1272614302,2022-10-09 19:41:03,"@tiruk007 sorry that I forgot to mention this bug is on GPU only.
I turn on GPU in Colab and can reproduce the issue. Could you please confirm the same?",I turn on GPU in Colab and can reproduce the issue. Could you please confirm the same?
56112,iceteahh,1272509010,2022-10-09 10:22:27,"This error still happens to me. I used tensorflow 2.8.0 [screenshot](https://drive.google.com/file/d/1MxlfuCEFY_CWXKwCVNgNWUIfb9nCymdS/view?usp=sharing)
@muja555 @sachinprasadhs Do you have any ideas how to fix it?",I used tensorflow 2.8.0 [screenshot](https://drive.google.com/file/d/1MxlfuCEFY_CWXKwCVNgNWUIfb9nCymdS/view?usp=sharing)
56420,guweixin,1272438553,2022-10-09 2:40:16,"@gadagashwini Sorry, I follow step1 and step2, but it doesn't work, I meet the same error again.","""I follow step1 and step2, but it doesn't work, I meet the same error again."""
57934,dmitryfisko,1272358833,2022-10-08 17:01:45,Developers of library forgot to include a transitive dependency to **tensorflow-lite-gpu-api** from **tensorflow-lite-gpu**,Developers of library forgot to include a transitive dependency to **tensorflow-lite-gpu-api** from **tensorflow-lite-gpu**.
57947,febrifahmi,1272164536,2022-10-07 23:46:49,Thanks.. I'll try it. But it seems the binary are only for armv7l while my pi is the old armv6l.,"""But it seems the binary are only for armv7l while my pi is the old armv6l."""
56762,rahulbatra85,1272102067,2022-10-07 21:30:39,"Yeah, we already split it into other PRs. It was much larger. Also, I understand larger commits are not a good idea, but in this case changes really need to go together
Also note, most of the code changes are ROCm specific. About half(may be more) the files touched are only for ROCm. In common files, almost all the changes are again ROCm specific. They are #defines like TENSORFLOW_ROCM","""I understand larger commits are not a good idea, but in this case changes really need to go together"""
56762,SandSnip3r,1272099176,2022-10-07 21:25:48,"@rahulbatra85 The description says that this will be split. @cheshire asked if it could be split. I suspect the reason this is taking so long is because its so large. Smaller PRs are substantially easy to get in.
Let me check what internal CI looks like.",I suspect the reason this is taking so long is because its so large.
58007,wenscarl,1272009805,2022-10-07 19:17:13,"@NeilGirdhar I don't think it's that PR to blame. I will remove your name. There is a non-defined path for rank-1 complex tensor with conjugate required. Should be easy to fix but I am not sure what should be a reasonable behavior, doing a conjugate or treated as no-op?","I don't think it's that PR to blame. I will remove your name. There is a non-defined path for rank-1 complex tensor with conjugate required. Should be easy to fix but I am not sure what should be a reasonable behavior, doing a conjugate or treated as no-op?."
52845,harraz,1271699763,2022-10-07 14:51:34,"> I've tried `Tensorflow 2.3.1 ` and I still get `F tensorflow/core/lib/monitoring/sampler.cc:42] Check failed: bucket_limits_[i] > bucket_limits_[i - 1] (0 vs. 10) qemu: uncaught target signal 6 (Aborted) - core dumped` Any suggestions would be great - thanks.
Any luck with this issue. I get this when i try to import tensorflow in python",I've tried Tensorflow 2.3.1  and I still get F tensorflow/core/lib/monitoring/sampler.cc:42] Check failed: bucket_limits_[i] > bucket_limits_[i - 1] (0 vs. 10) qemu: uncaught target signal 6 (Aborted) - core dumped
42387,harraz,1271698537,2022-10-07 14:50:30,"> <img alt=""image"" width=""1164"" src=""https://user-images.githubusercontent.com/32613582/106357008-18c6d600-6336-11eb-87fb-4949bc0d543d.png"">
> > I'm stucking from this monitoring. Could anyone help?
Did you find a way to solve this problem?",I'm stucking from this monitoring. Could anyone help? Did you find a way to solve this problem?
42387,harraz,1271697821,2022-10-07 14:49:52,"Still getting this error message every time I try to import tensorflow
F tensorflow/core/lib/monitoring/sampler.cc:42] Check failed: bucket_limits_[i] > bucket_limits_[i - 1] (0 vs. 10)
qemu: uncaught target signal 6 (Aborted) - core dumped
Any ideas or clear steps to fix this?
I'm running a docker container on a Apple M1 Pro chip","""Check failed: bucket_limits_[i] > bucket_limits_[i - 1] (0 vs. 10) qemu: uncaught target signal 6 (Aborted) - core dumped"""
57979,i-chaochen,1271561119,2022-10-07 13:00:21,"@cheshire it seems my last changes will fail on cuda side, so I have added the same test for cuda as well.","""I have added the same test for cuda as well."""
56088,elfringham,1271505445,2022-10-07 12:05:43,My PR https://github.com/tensorflow/tensorflow/pull/57969 was approved 3 days ago but has not yet been merged. Can anyone please see if there is anything that needs to be addressed?,"""My PR https://github.com/tensorflow/tensorflow/pull/57969 was approved 3 days ago but has not yet been merged."""
57744,d0k,1271486086,2022-10-07 11:45:06,"Algebraic Simplifier rewrites `ln(pow(A,B)) => B*ln(abs(A))`. That doesn't look right for negative `B`.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/algebraic_simplifier.cc#L3373","Algebraic Simplifier rewrites ln(pow(A,B)) => B*ln(abs(A)). That doesn't look right for negative B."
46178,ffaisal93,1270696467,2022-10-06 21:04:09,"facing similar issue in October for other staffs ```
Python 3.9.12 (main, Apr 5 2022, 01:53:17) [Clang 12.0.0 ] :: Anaconda, Inc. on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from transformers import BertTokenizer, BertModel
>>> tokenizer = BertTokenizer.from_pretrained(""bert-base-uncased"")
>>> tokenizer.decode([101, 102])
zsh: illegal hardware instruction python
```"," ""zsh: illegal hardware instruction python"" "
58002,RickSanchezStoic,1270517323,2022-10-06 18:34:03,"Yes, it has the condition `Length must be the same as the number of dimensions in input` which is not the case in `numpy` or `torch`. You can confirm this by running the standalone code provided.","Yes, it has the condition Length must be the same as the number of dimensions in input which is not the case in numpy or torch."
57099,sandeep5193,1269906600,2022-10-06 12:01:12,"Well, we already mentioned this issue for TFLite module as well, [here](https://github.com/tensorflow/tensorflow/issues/57099#issuecomment-1256375155). But after that we stopped receiving it, and we [thought](https://github.com/tensorflow/tensorflow/issues/57099#issuecomment-1257872633) dynamically loading modules through Play Services might have solved it.","""We already mentioned this issue for TFLite module as well, [here](https://github.com/tensorflow/tensorflow/issues/57099#issuecomment-1256375155). But after that we stopped receiving it, and we [thought](https://github.com/tensorflow/tensorflow/issues/57099#issuecomment-1257872633) dynamically loading modules through Play Services might have solved it."""
57099,acq,1269889585,2022-10-06 11:50:42,"Ah, that's frustrating...
Thanks for letting us know.
I was hoping the issue was only impacting the GPU delegate optional module, but your error points to the problem also impacting the main TFLite optional module.
So we'll have to publish SDK updates of play-services-tflite-java, play-services-tflite-support, ...
I'll let you know when those are available","""That's frustrating"""
57945,Urkchar,1269344748,2022-10-06 5:36:03,It seems like my installation of openssl was faulty or something. Installing it again seemed to fix it.,Installing it again seemed to fix it.
57955,rsanthanam-amd,1269266210,2022-10-06 3:38:50,"i'm not quite sure how to craft a specific unit test to check this.
one thing i will mention is that this fix was borne out of several existing unit test failures on gfx90a:
- //tensorflow/compiler/xla/tests:conv_depthwise_test_gpu - //tensorflow/compiler/xla/tests:convolution_test_gpu
- //tensorflow/compiler/xla/tests:convolution_test_gpu_alternative_layout_gpu
because there was no memory fence, we were seeing intermittent mismatches between expected and actual results",i'm not quite sure how to craft a specific unit test to check this.
49896,gowthamkpr,1269084320,2022-10-05 23:04:11,"@rakeshmothukuru1 As mentioned [here](https://stackoverflow.com/questions/67754649/mean-of-tensorflow-kerass-glorot-normal-initializer-is-not-zero), The initializer's mean is zero, you are random sampling from it. That's normal that sample's mean will not be exactly 0.","""The initializer's mean is zero, you are random sampling from it. That's normal that sample's mean will not be exactly 0."""
57069,cantonios,1268683284,2022-10-05 16:49:30,"We're still seeing internal build failures related to visibility of `tensor_util.h`, specifically for android builds and tf-lite. I'll need to investigate later today.","""still seeing internal build failures related to visibility of tensor_util.h, specifically for android builds and tf-lite"""
57934,DorianRudolph,1268681853,2022-10-05 16:47:54,"@mohantym importing DelegateFactory does not help. It really does seem like the class is just missing from the AAR, as @Erdemtsynduev points out.",importing DelegateFactory does not help.
57934,Erdemtsynduev,1268607760,2022-10-05 15:39:29,"It seems that an error occurred during the assembly of the build. I also checked, but even aar does not have this file","I also checked, but even aar does not have this file."
57934,Erdemtsynduev,1268605803,2022-10-05 15:37:21,"<img width=""1028"" alt=""image"" src=""https://user-images.githubusercontent.com/12883992/194101912-af5d73bb-529d-4b28-94c5-b36854470f81.png"">
@mohantym Maybe that will be obvious. But the file just doesn't exist.","""But the file just doesn't exist."""
57954,bixia1,1268566907,2022-10-05 15:03:16,"@DEKHTIARJonathan something is wrong with this PR, it modifies > 350 files.","""something is wrong with this PR, it modifies > 350 files."""
57052,elina-israyelyan,1267990334,2022-10-05 6:15:48,Cannot reopen from my side will open a new issue,Cannot reopen from my side will open a new issue.
57963,Urkchar,1267767682,2022-10-05 0:36:45,The wrong Python interpreter was selected so TensorFlow 2.4.0 was running instead of 2.10.0. This issue may be closed.,The wrong Python interpreter was selected so TensorFlow 2.4.0 was running instead of 2.10.0.
57887,bhack,1267683680,2022-10-04 22:44:52,"> We don't support anaconda installation and there is a concern that listing that in the README might result in unmet expectations from developers.
Can we use something like:
https://github.com/tensorflow/build/#community-supported-tensorflow-builds ?","""We don't support anaconda installation and there is a concern that listing that in the README might result in unmet expectations from developers."""
56557,ashiqimranintel,1267348428,2022-10-04 17:47:01,"@penpornk , can you re-run the test?",Can you re-run the test?
57960,jiannanWang,1267227981,2022-10-04 16:00:54,"Hi @tilakrayal.
At the end of your gist, it prints false which indicates the outputs from calling tf.signal.stft directly and from calling tf.signal.stft with @tf.function are different. The inconsistencies here might be caused by some potential bugs.
I also provided my [gist](https://colab.research.google.com/drive/1WleKXby71iZXOL12r8nIN8B_jd2wJQks?usp=sharing) in the reproduce code in case you miss it. I was using tf 2.10 on cpu.","""The inconsistencies here might be caused by some potential bugs."""
57935,fuzzyswan,1267063207,2022-10-04 14:06:33,"Hi @tilakrayal , I totally agree that an exception is the intended behavior for 0/0. However, the tf divide ops are not giving consistent results: `tf.experimental.numpy.floor_divide` gives **-1** for `0/0` on CUDA (**gives a wrong output without throwing any exceptions**), and `tf.experimental.numpy.remainder` gives `0`.","""The tf divide ops are not giving consistent results: tf.experimental.numpy.floor_divide gives **-1** for 0/0 on CUDA (**gives a wrong output without throwing any exceptions**), and tf.experimental.numpy.remainder gives 0.."""
57794,cheshire,1266620067,2022-10-04 8:54:28,"Seems to break `//tensorflow/compiler/xla/service/gpu:cudnn_fused_conv_rewriter_test` when running with `XLA_FLAGS=--xla_gpu_enable_xla_runtime_executable`, judging by the error message the error is with the test. @ezhulenev","""Seems to break"""
57934,OleksandrGrument,1266519640,2022-10-04 7:26:22,Hi @mohantym . Looks like an issue in 2.10. I'm getting runtime exception with GpuDelegateFactory. Looks like it missing ![image](https://user-images.githubusercontent.com/20426405/193759568-68eab6fe-3931-4e34-a6dd-2c65867b4ed2.png),I'm getting runtime exception with GpuDelegateFactory. Looks like it missing ![image](https://user-images.githubusercontent.com/20426405/193759568-68eab6fe-3931-4e34-a6dd-2c65867b4ed2.png)
55639,bhack,1266234528,2022-10-04 0:17:03,"> Will look into a solution for invariant issue for pfor in this quarter and update. Thanks!
With the within the batch augmentation policy we have (imho wrongly) tried to use `vectorized_map` to workaround the limits of the image ops that are working with an images (batch) but with a fixed transformation (whole batch) arg.
If we are still talking about covering the missing converter with invariant I don't think that we will solve this cases.","""If we are still talking about covering the missing converter with invariant I don't think that we will solve this cases."""
57751,cantonios,1266147083,2022-10-03 22:40:33,"The CPU behavior is actually intentional to match numpy. Using `==` between tensors returns False if there is a shape mismatch [doc](https://www.tensorflow.org/api_docs/python/tf/Tensor#__eq__). If you use `tf.math.equal`, it will assert on incompatible dimensions.
Looks like the GPU behavior is the buggy one.","""Looks like the GPU behavior is the buggy one."""
39103,justin-nevermore,1265788626,2022-10-03 17:23:55,"> I've got the same WARNING, the reason is the tf versions for training and saving are different.
How do I check training version and saving version?","I've got the same WARNING, the reason is the tf versions for training and saving are different. How do I check training version and saving version?"
56088,elfringham,1265752571,2022-10-03 16:58:02,@cantonios your recent commit introducing FP8 is not building on AARCH64. https://github.com/tensorflow/tensorflow/issues/57918,"""Your recent commit introducing FP8 is not building on AARCH64."""
57890,gadagashwini,1265345548,2022-10-03 12:07:49,"Hi @surajitkundu-dazn, Still I am unable to access the given link. <img width=""1662"" alt=""Screen Shot 2022-10-03 at 5 36 17 PM"" src=""https://user-images.githubusercontent.com/99852755/193573001-26f0dcf7-bf2a-4fdb-b1b8-5b7bbe38c99e.png"">",Still I am unable to access the given link.
57844,namrata-ibm,1265200920,2022-10-03 9:52:15,"@angerson Could you please check?
This issue will be seen on all big endian machines. Also as per [this](https://github.com/tensorflow/tensorflow/blob/9e12018a40b5024f51bbaa4d27dcad047bd8449f/.bazelrc#L686) TFRT integration should be optional as of now. Is there a way to disable it?","""TFRT integration should be optional as of now. Is there a way to disable it?"""
57936,BenjaminDEMAILLE,1264603158,2022-10-02 10:10:20,I can't install Tensorflow and I need it for my job,I can't install Tensorflow and I need it for my job.
57052,foxik,1264588517,2022-10-02 8:55:58,"The Keras guys wrote in https://github.com/keras-team/keras/issues/16978 that it looks like a TF issue and that we should report it here.
@elina-israyelyan Could you please reopen the issue?",The Keras guys wrote in https://github.com/keras-team/keras/issues/16978 that it looks like a TF issue and that we should report it here.
57914,sun1638650145,1264529210,2022-10-02 2:16:00,"I've looked at this issue, but this issue doesn't provide any installation help.","I've looked at this issue, but this issue doesn't provide any installation help."
57879,joker-eph,1263942410,2022-09-30 19:32:40,"`--dynamic=off` was missing I think. It is not longer needed at HEAD, I rebased the change",--dynamic=off was missing
57794,cheshire,1263586072,2022-09-30 13:36:14,"We can try to run our benchmarks first, then it's probably simplest to enable it by default in this PR. Having it disabled by default is not great, as we'll essentially get a lot of dead untested code.","Having it disabled by default is not great, as we'll essentially get a lot of dead untested code."
57845,rjodinchr,1263394882,2022-09-30 10:25:56,"Hi @sirakiin ,
Is there something I can do to help you review this PR?
About the failing tests:
- ARM CI / build: it has the same failure as the master branch at branch point
- AMD ROCm: The issue seems to be jenkins infra related","""It has the same failure as the master branch at branch point"""
57905,Shubham-23Joshi,1263269533,2022-09-30 8:25:13,"Hallo @mohantym thanks for the tip. I have put in the line and converted the model. However, the error still prevails at the compilation time.","I have put in the line and converted the model. However, the error still prevails at the compilation time."
45241,HTGorji,1262843192,2022-09-29 21:37:11,Me too. I am getting this error after 20 epochs.,I am getting this error after 20 epochs.
57732,dmvieira,1262733828,2022-09-29 19:42:17,"It's done, but I think AMD CI is broken...",I think AMD CI is broken.
57861,impjdi,1262662236,2022-09-29 18:28:40,"You can't use the OpenGL delegate to run it on Vulkan. We do have a Vulkan implementation internally, but we haven't open sourced it yet due to binary size reason. The Vulkan impl would have to be shipped with shaderc which is another tens of megabytes; we're waiting for Android to ship shaderc as part of the OS so that devs don't have to eat the app size cost, but it's seeing very little progress on that end. So ... no Vulkan impl in the short term.","""We do have a Vulkan implementation internally, but we haven't open sourced it yet due to binary size reason."""
57069,cantonios,1262488005,2022-09-29 16:00:39,@kun-lu20 this PR now has a cyclic dependency and fails to build. Can you rebase and address it?,This PR now has a cyclic dependency and fails to build.
51594,Onkar20018,1262144926,2022-09-29 11:28:07,"AttributeError: module 'keras.engine' has no attribute 'Layer'
I am getting this error , even after using the keras version2.0.8 and tensor flow version 1.14.0","I am getting this error , even after using the keras version2.0.8 and tensor flow version 1.14.0."
43193,brechtdecock,1262143125,2022-09-29 11:26:16,"Hi, i have a similar issue. Made a post about it <https://stackoverflow.com/questions/73886668/tensorflow-gpu-could-not-load-dynamic-library-cudart64-110-dll>, tried some suggentions from here but things are still not working","""Tried some suggentions from here but things are still not working."""
57906,mohantym,1262110637,2022-09-29 10:53:50,"Hi @Saar-Ken-Ji !
In Colab , Session was crashing in Colab with 2.9 . rsqrt in select ops list. So could you try with below syntax in lite conversion.
```
converter.target_spec.supported_ops = [
tf.lite.OpsSet.TFLITE_BUILTINS_INT8, # enable TensorFlow Lite ops.
tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
converter.experimental_enable_quantizer = True
```",crashing in Colab with 2.9 . rsqrt in select ops list. So could you try with below syntax in lite conversion.
57528,sky712345678,1262023764,2022-09-29 9:34:38,I found that this issue still persists on Tensorflow 2.10 on Linux Ubuntu 18.04.,I found that this issue still persists on Tensorflow 2.10 on Linux Ubuntu 18.04.
53101,JinXiaozhao,1261852789,2022-09-29 7:01:51,"@jianyuzzz I think tflite_ops can't support dynamic batch; so when the model is converted, TF (Tensorflow>2.7.0) uses tf_ops and tflite_ops. Before, TF(Tensorflw<2.7.0) was default to set dynamic batch to 1.","""I think tflite_ops can't support dynamic batch; so when the model is converted, TF (Tensorflow>2.7.0) uses tf_ops and tflite_ops."""
56762,SandSnip3r,1261361775,2022-09-28 19:18:10,"I see that the Internal CI build (Py+CPP Test Suite - Ubuntu GPU, Python 3.9) fails. I think you can see the results. The failures are in the same files that you have changed. Have you tried resolving this issue?","I see that the Internal CI build (Py+CPP Test Suite - Ubuntu GPU, Python 3.9) fails."
57666,willi3by,1261024200,2022-09-28 14:45:53,@tilakrayal I apologize for the delay. I updated TF and am still getting the same error. My current version of TF is 2.11.0-dev20220928,I apologize for the delay. I updated TF and am still getting the same error.
57087,hawkinsp,1260976244,2022-09-28 14:11:23,"For anyone else reading this, ""Bus error"" was from running in a docker container without enough SHM space for NCCL.","""Bus error"""
57883,dengyinlin,1260853586,2022-09-28 12:46:27,"I think according to [tf.cast docs](https://www.tensorflow.org/api_docs/python/tf/cast), ""casting nan and inf values to integral types has undefined behavior"", that's probably why the result is unpredictable.","I think according to [tf.cast docs](https://www.tensorflow.org/api_docs/python/tf/cast), ""casting nan and inf values to integral types has undefined behavior"", that's probably why the result is unpredictable."
57801,cheshire,1260621202,2022-09-28 9:14:59,"CHECK really isn't great as it crashes the entire process which potentially runs other things as well.
However I agree that reaching parity with CUDA would be sufficient.
How about: use CHECK if CUDA does so, and propagate bad Status/do logging if CUDA does so?",CHECK really isn't great as it crashes the entire process which potentially runs other things as well.
44670,adriangb,1260280162,2022-09-28 1:37:04,They’re kinda different issues and this has been open for 2 years now so I’d rather leave this open until it gets fixed,They’re kinda different issues and this has been open for 2 years now
53101,blaine-fs,1260245347,2022-09-28 0:41:35,"On 2.11, I was able to work around the issue by setting `batch_size=1` on all `Keras.Input` layers. It seems this error only occurs for LSTMs with variable batch sizes.","On 2.11, I was able to work around the issue by setting batch_size=1 on all Keras.Input layers. It seems this error only occurs for LSTMs with variable batch sizes."
57855,reedwm,1260192424,2022-09-27 23:55:35,"I confirmed we don't have native Windows GPU builds anymore, and no CI will run for Windows GPUs. So this can be marked as a non-draft PR.","I confirmed we don't have native Windows GPU builds anymore, and no CI will run for Windows GPUs. So this can be marked as a non-draft PR."
47989,dmvieira,1260085676,2022-09-27 21:43:06,"But my issue here @bhack is not just running lint, but unit tests too...","""Not just running lint, but unit tests too."""
57656,nouiz,1259959125,2022-09-27 19:32:49,"> That would be untested code though. Sorry for us lagging behind with Ampere CI. Could you predicate on compute capability at runtime? We do have a number of tests doing that.
I rebased and added a test. I already modified one existing tests, but an older rebase removed it.",untested code though
56761,SandSnip3r,1259956719,2022-09-27 19:30:25,"We were waiting on #57648 to be merged before continuing this, right?","Waiting on #57648 to be merged before continuing this, right?"
57872,ganler,1259932751,2022-09-27 19:06:23,"This seems to be related to:
- https://github.com/tensorflow/tensorflow/issues/55563
- https://github.com/bazelbuild/bazel/issues/15359
I tried all of the mentioned fixes but none of them work...",I tried all of the mentioned fixes but none of them work.
47989,dmvieira,1259865757,2022-09-27 18:01:34,"Every path and file in this PR doesn't exists. I think they moved these files to some other place, but CONTRIBUTING documentation is still not working, so this BUG still open","""Every path and file in this PR doesn't exists"""
57843,flixxox,1259479243,2022-09-27 13:06:03,Sorry I cannot see where the linked issues relate to my issue. Where do I return an array instead of a tuple? I did not append the full error message. Sorry for that. I updated the issue with the full traceback. Does that change something?,I cannot see where the linked issues relate to my issue.
40195,harahu,1259375847,2022-09-27 11:39:37,"@mohantym I agree that this issue probably belongs in the keras repo. However, it was created before the split was made. I would have expected the organization to have made an effort in porting over any keras-related issues, including this one, if you feel community-created issues are valuable.
I take it this was not done. Would you like this issue to be ported over now? How do you suggest we go about that?",I take it this was not done.
14798,bhack,1259347323,2022-09-27 11:12:18,"@ganler As you see I cannot make progress on my PR at https://github.com/tensorflow/tensorflow/pull/56510
/cc @cheshire @theadactyl",I cannot make progress on my PR at https://github.com/tensorflow/tensorflow/pull/56510 /cc @cheshire @theadactyl.
57840,farmaker47,1259147327,2022-09-27 8:13:11,"Hi @mohantym Unfortunately the 2 generated files do not work inside android. I am getting
""E/om.vicom.teeja: No implementation found for void org.tensorflow.lite.TensorFlowLite.nativeDoNothing() (tried Java_org_tensorflow_lite_TensorFlowLite_nativeDoNothing and Java_org_tensorflow_lite_TensorFlowLite_nativeDoNothing__)
""
Any hints?","""I am getting ""E/om.vicom.teeja: No implementation found for void org.tensorflow.lite.TensorFlowLite.nativeDoNothing() (tried Java_org_tensorflow_lite_TensorFlowLite_nativeDoNothing and Java_org_tensorflow_lite_TensorFlowLite_nativeDoNothing__)"""
47989,dmvieira,1258778793,2022-09-26 23:48:28,Same issue here... Still crashing with the same error message on main branch,Still crashing with the same error message on main branch.
32085,MarkDaoust,1258750396,2022-09-26 23:10:58,"> since 2 years ago we no longer use ./configure / configure.py for the standard builds
Oh! So I guess we need an update to the build from source instructions. To say... ""only `configure` the build if you need to""?","""since 2 years ago we no longer use ./configure / configure.py for the standard builds"""
56493,advaitjain,1258712066,2022-09-26 22:30:50,"@Tombana, please note that the fix had to be reverted with https://github.com/tensorflow/tensorflow/commit/db1dd64675e4cb471390ba2aacdedf27b842399a due to some internal regressions. We're going to see how we can reconcile the different moving pieces but its going to take some time.","""The fix had to be reverted with https://github.com/tensorflow/tensorflow/commit/db1dd64675e4cb471390ba2aacdedf27b842399a due to some internal regressions"""
57536,MarkDaoust,1258557854,2022-09-26 20:07:06,"> #PROBLEM!!! NEEDS INDENTATION!
It doesn't actually. Why do you think it does?
Closing the tape is ""stop writing new gradient ops to the tape"", not ""throw out the tape"".","""It doesn't actually. Why do you think it does?"""
57828,petered,1258506003,2022-09-26 19:20:12,"See also https://stackoverflow.com/questions/72733907/efficient-image-dilation-in-tensorflow - it's about morphological dilation being slow, but since one way to implement morphological dilation is with `max_pool_2d` (which itself is slow), it relates. So whatever the fix is to this, could also possibly fix the ""slow dilation"" problem.","See also https://stackoverflow.com/questions/72733907/efficient-image-dilation-in-tensorflow - it's about morphological dilation being slow, but since one way to implement morphological dilation is with max_pool_2d (which itself is slow), it relates. So whatever the fix is to this, could also possibly fix the ""slow dilation"" problem."
57747,ganler,1258339183,2022-09-26 16:51:52,"Notably, the result is only wrong on XLA-GPU. It is correct on Eager-CPU/GPU and XLA-CPU, as well as native Python, PyTorch and NumPy. We suspect it might be due to some miss-casting. This bug is also confirmed by OSS VRP but was not regarded as a security vulnerability as the exploit can be non-trivial.","Notably, the result is only wrong on XLA-GPU. It is correct on Eager-CPU/GPU and XLA-CPU, as well as native Python, PyTorch and NumPy. We suspect it might be due to some miss-casting. This bug is also confirmed by OSS VRP but was not regarded as a security vulnerability as the exploit can be non-trivial."
40195,harahu,1258099170,2022-09-26 14:09:19,@mohantym There's nothing in the documentation you linked that indicates this issue having been addressed.,Nothing in the documentation you linked that indicates this issue having been addressed.
56133,rovinellimarco,1258037165,2022-09-26 13:23:36,"As far as I know, that line is needed to have fully integer models so it is not an option to remove it.",That line is needed to have fully integer models so it is not an option to remove it.
57099,sandeep5193,1257882267,2022-09-26 11:23:01,"We have not distributed this anywhere other than Play Store. But we can see it listed in various unofficial stores when we search by name on Google, and many of them do not redirect it to Play Store.
So yes, it's possible to install it on devices where Play Services is not supported.","""But we can see it listed in various unofficial stores when we search by name on Google, and many of them do not redirect it to Play Store."""
33389,das-ankur,1257878279,2022-09-26 11:19:31,"I use the same suggestion of @lc0 but in my case I am having trouble to save the model. I am getting error like this: raise TypeError(""Expected any non-tensor type, got a tensor instead."")
TypeError: Expected any non-tensor type, got a tensor instead.",I am having trouble to save the model.
57840,farmaker47,1257516245,2022-09-26 5:54:23,Build of tensorflow-lite.aar is successful but fails when it tries to build tensorflow-lite-select-tf-ops.aar,Build of tensorflow-lite.aar is successful but fails when it tries to build tensorflow-lite-select-tf-ops.aar.
55941,learning-to-play,1257440580,2022-09-26 3:57:52,"@angerson @nitins17 MacOS presubmits failed, but the ""Details"" link to see the logs is missing. Can you help fix this issue?","""Can you help fix this issue?"""
54276,andreped,1257047957,2022-09-24 19:14:03,"> Hi @benbarsdell, Can we move this issue to closed state, If it was resolved. Thank you!
@gadagashwini I don't think it was resolved. I am experiencing the same issue on Windows with TensorFlow 2.10.0. Works fine on CPU though, but same issue on GPU as described above. Any simple fix for this?",I am experiencing the same issue on Windows with TensorFlow 2.10.0.
57762,cheshire,1256934825,2022-09-24 10:33:03,"@sergeykozub @SandSnip3r @ekuznetsov139 due to merge conflict, landing this all at once might be very hard. It would be much easier to split it up.","""due to merge conflict, landing this all at once might be very hard."""
22710,Keno,1256910605,2022-09-24 8:35:38,This code path is obsolete - I don't think anyone is using it.,This code path is obsolete - I don't think anyone is using it.
57759,ganler,1256837137,2022-09-24 2:25:04,"@mohantym @sachinprasadhs It seems this bug only occurs when one of the arguments in max is `tf.zeros`. When setting it to `tf.ones`, the inferred shape will be correct.","""It seems this bug only occurs when one of the arguments in max is tf.zeros."""
57779,sachinprasadhs,1256620856,2022-09-23 20:00:03,"Update: With the changes, tests are failing again in multiple files.",tests are failing again in multiple files.
57779,sachinprasadhs,1256618229,2022-09-23 19:56:31,"The above PR was automatically rolled back, since the tests were failing.
I have submitted the changes again. I will update once the test results are completed.","""The tests were failing"""
57645,sachinprasadhs,1256547299,2022-09-23 18:40:13,"Apologies for the delayed response,
`tf.compat.v1.disable_eager_execution()` would force the entire code to run in graph mode and results in faster execution as compared to Tensorflow eager mode where only model logic part is wrapped in `tf.function` and runs in graph mode when `run_eagerly` is set to `False`.",delayed response
56928,Whrothus,1256505682,2022-09-23 17:48:34,Think i f up att converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8],I f up att converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
57099,sandeep5193,1256123035,2022-09-23 12:01:34,"Don't have exact figures, but definitely below 5 per cent, and real count can be way less than it. Currently we have disabled this feature through Firebase Remote Config because of this issue, will share app once we enable it again.","Currently we have disabled this feature through Firebase Remote Config because of this issue, will share app once we enable it again."
57798,Co1lin,1255831675,2022-09-23 6:09:56,"Hi! It seems that the doc says out-of-bound indices for argument `segment_ids` result in safe but unspecified behavior, but the bug I provide above is about giving a very large number to the argument `num_segments`, which results in process crash as represented in the CoLab link. So my suggestion is taking care of `num_segments` to avoid crash for some inputs.","""It seems that the doc says out-of-bound indices for argument segment_ids result in safe but unspecified behavior, but the bug I provide above is about giving a very large number to the argument num_segments, which results in process crash as represented in the CoLab link."""
57099,sandeep5193,1255826096,2022-09-23 5:56:52,"@acq Any hint on how we can catch it for the time being?
We already tried to catch it for TfLiteGpu.isGpuDelegateAvailable(Context) method, still it's throwing exceptions in prod.
TfLite.initialize(Context, TfLiteInitializationOptions) is next suspect.","""We already tried to catch it for TfLiteGpu.isGpuDelegateAvailable(Context) method, still it's throwing exceptions in prod."""
16933,austinzh,1255592724,2022-09-22 21:52:15,"@frankchn any update on the save and load?
We facing same issue when try to convert npz into tfrecord. writing it 1 by 1 in python is really slow.",We facing same issue when try to convert npz into tfrecord. writing it 1 by 1 in python is really slow.
47554,aaarrti,1255567081,2022-09-22 21:29:25,"Hi, I still have the same issue with `tf version: 2.10.0`. Namely, `Found untraced functions such as embedding_10_layer_call_fn`. I did execute `model.predict` on a batch of dummy data.","I still have the same issue with tf version: 2.10.0. Namely, Found untraced functions such as embedding_10_layer_call_fn. I did execute model.predict on a batch of dummy data.."
57808,mihaimaruseac,1255537891,2022-09-22 20:53:22,"We no longer patch 2.7, so only 2.8 to 2.10 need to be cherrypicked","We no longer patch 2.7, so only 2.8 to 2.10 need to be cherrypicked."
57781,covercash2,1255241889,2022-09-22 16:05:53,"if i change the code to ```
interpreter.runForMultipleInputsOutputs(arrayOf(buffer.buffer), mapOf(0 to outputBuffer))
```
i get a new error:
```
Cannot convert between a TensorFlowLite tensor with type UINT8 and a Java object of type [[I (which is compatible with the TensorFlowLite type INT32).
```
which means, according to the `TensorImpl.dataTypeOf`, the buffer is being determined as an `IntBuffer` instead of a `ByteBuffer` for some reason.","if i change the code to  interpreter.runForMultipleInputsOutputs(arrayOf(buffer.buffer), mapOf(0 to outputBuffer))  i get a new error:  Cannot convert between a TensorFlowLite tensor with type UINT8 and a Java object of type [I (which is compatible with the TensorFlowLite type INT32).  which means, according to the TensorImpl.dataTypeOf"
57800,i-chaochen,1255045918,2022-09-22 13:43:24,I closed it and opened another one as this is from the wrong branch.,I closed it and opened another one as this is from the wrong branch.
55815,michaeldietz,1254694629,2022-09-22 8:26:39,"Oh I thought the comment was meant for someone from the tensorflow team since the status was still on ""awaiting tensorflower"". I don't know how to profile the malloc performance. However, I did observe that the problem always occurred at the same time, regardless of the memory of the system (I tested it with 128GB and 256GB RAM).","I thought the comment was meant for someone from the tensorflow team since the status was still on ""awaiting tensorflower""."
57735,mihaimaruseac,1254322912,2022-09-21 23:08:22,"TF does not generate VEX statements, so the only option would be to read the entire codebase / send PR to update and then use `tf-nightly` / patched version when it would be released","TF does not generate VEX statements, so the only option would be to read the entire codebase / send PR to update and then use tf-nightly / patched version when it would be released."
57105,sconde,1254322278,2022-09-21 23:07:15,@sachinprasadhs @terryheo Is there any help you can offer here? I'm still not able to build from the source using CMake.,I'm still not able to build from the source using CMake.
57672,cantonios,1253904532,2022-09-21 15:54:22,"Those are two separate things. Adding an option to `tf.pad` is relatively straightforward. Adding to all convolutions is a whole other beast which requires changing our APIs - our current convolution API doesn't have separate `padding` and `padding_mode` options, it assumes we always pad with zeros and only controls the size.","Those are two separate things. Adding an option to tf.pad is relatively straightforward. Adding to all convolutions is a whole other beast which requires changing our APIs - our current convolution API doesn't have separate padding and padding_mode options, it assumes we always pad with zeros and only controls the size."
55178,surajitkundu-dazn,1253729259,2022-09-21 13:41:03,"I have a similar problem , I am trying to distribute the load accross multiple nodes , if I initiate the Strategy first then it's not using the TF_CONFIG so as a result it's using only one GPU, but if I initiate the config first then getting error ""Collective ops must be configured at program startup""","I am trying to distribute the load accross multiple nodes , if I initiate the Strategy first then it's not using the TF_CONFIG so as a result it's using only one GPU, but if I initiate the config first then getting error ""Collective ops must be configured at program startup""."
57682,forics,1253535050,2022-09-21 10:54:30,"@tilakrayal ,
The error is out of the scope of the issue, you can find the issue also exists in TF v2.8.
After a simple search on google, your error may caused by the TF version confliction of host and device. Ref to #39550 and [this](https://stackoverflow.com/questions/55181024/tensorflow-tpu-training-notfounderror-parallelinterleavedataset). Good luck!","The error is out of the scope of the issue, you can find the issue also exists in TF v2.8."
57741,cnglen,1253395437,2022-09-21 8:46:20,"You can change 'vsize = [1000, 10000, 100000, 1000000, 25000000]' to 'vsize = [1000, 10000, 100000, 1000000]'
It takes too much time to save the huge preprocessor with 2500e4 vocab size.",It takes too much time to save the huge preprocessor with 2500e4 vocab size.
35477,ssarkar2,1253195839,2022-09-21 4:39:58,@AakashKumarNain @vkumar1997 SequenceEnqueuer seems to be an abstract class with non implemented functions. you can try [OrderedEnqueuer ](https://www.tensorflow.org/api_docs/python/tf/keras/utils/OrderedEnqueuer),"""SequenceEnqueuer seems to be an abstract class with non implemented functions."""
56207,aliencaocao,1253145650,2022-09-21 2:59:46,"Yes mine triggers 100%, until i remove a specific preprocessing function. Its not even the model itself that is causing the issue, it is the specific preprocessing function of this specific model (all other preprocessor have no issue)","""specific preprocessing function of this specific model"""
50945,tarik0,1252895623,2022-09-20 20:48:36,"> Hi! Did you solve this issue? Because I am having the same problem, when I try to save my CuDNNLSTM model.
Nope, I gave up.","I am having the same problem, when I try to save my CuDNNLSTM model."
57663,mihaimaruseac,1252866174,2022-09-20 20:17:25,"Unless there is a cherrypick for 2.10 (and a 2.10.1 patch release), #56691 will only be included in 2.11 and later","Unless there is a cherrypick for 2.10 (and a 2.10.1 patch release), #56691 will only be included in 2.11 and later."
57663,mufaddal-rohawala,1252858280,2022-09-20 20:09:15,"The patch https://github.com/tensorflow/tensorflow/pull/56691 is not released yet for tensorflow 2.10.0, @trevor-m is there a plan for release?","The patch https://github.com/tensorflow/tensorflow/pull/56691 is not released yet for tensorflow 2.10.0, @trevor-m is there a plan for release?"
57735,mihaimaruseac,1252773332,2022-09-20 18:49:32,"Unfortunately there is no SBOM generated by TF during build, so you have to check the source code
Main branch has https://github.com/tensorflow/tensorflow/blob/92d57e61d41f8e6fca50d582ac7a56046a391591/tensorflow/workspace2.bzl#L568-L576
So using vulnerable zlib.","""Using vulnerable zlib.."""
57763,edwardyehuang,1252603588,2022-09-20 16:29:19,"> Thanks for reporting this issue. As a short cut for now (to have a determined initializer for your test), you can use `tf.keras.utils.set_random_seed()` to set the seed for python/numpy/tf/keras.
Hi. I think you misunderstand it. Even set random seed using `tf.keras.utils.set_random_seed()`, the generated weights is different between 2.10 and 2.9/2.8 (Stateless vs Stateful rng)","""Even set random seed using tf.keras.utils.set_random_seed(), the generated weights is different between 2.10 and 2.9/2.8 (Stateless vs Stateful rng)"""
56050,Nyrio,1252117312,2022-09-20 9:55:33,"@bixia1 I haven't received the test logs, can you please try to send them again?","I haven't received the test logs, can you please try to send them again?"
57718,zhihuiloke,1252103188,2022-09-20 9:43:04,"Sorry @mohantym to have to reopen this. I found out that my problem was slightly different from #56287. From my own investigation it seems that the leak is coming from ModifyGraphWithDelegate, instead of Invoke.
Attached is a screenshot of instruments, and a 5MB growth per call.
![image](https://user-images.githubusercontent.com/48172598/191225054-cfc30bd2-cc8d-4b42-97c5-131ffdbde9d3.png)",I found out that my problem was slightly different from #56287.
56287,zhihuiloke,1251838057,2022-09-20 4:50:11,Any updates? Modifying source is hard to maintain in our situation.,Modifying source is hard to maintain in our situation.
56918,sirakiin,1251623617,2022-09-19 22:12:56,"The PR file changes LGTM and already approved / reviewed, please see the checks for the reason merging is blocked. Some of the CI failures seems to be transient issues and shall resolve after re-triggering those. For CLA please see [this](https://github.com/tensorflow/tensorflow/pull/56918/checks?check_run_id=7534827657) on how to resolve the check.","""The PR file changes LGTM and already approved / reviewed, please see the checks for the reason merging is blocked."""
57725,blaine-fs,1251599606,2022-09-19 21:50:55,"Actually, looking at the MLIR dialect [spec](https://www.tensorflow.org/mlir/tfl_ops), it seems like some of these ops (for example `tfl.log`) do not support fixed point at all. When I try to compile these in 8x8 mode, I seem to silently get floating points ops instead. Do we always need to allow floating point if we want to use these operators? Or put another way, is full integer quantization supported for only a subset of TFLite?",tfl.log
57087,nouiz,1251408805,2022-09-19 18:45:07,"> @nouiz We could provide HLO dump, but I expect this situation to be quite frequent in the future, as JAX tends to have better test coverage than we do.
> > I think it would be worthwhile to figure out how to run JAX tests with a given revision, do you think this is a reasonable process?
It makes sense. But we need to find a way that doesn't slow down the workflow.","""I expect this situation to be quite frequent in the future, as JAX tends to have better test coverage than we do."""
47559,Regenhardt,1251380185,2022-09-19 18:19:43,"How to find out what normalization values I need? I just used 127.5/127.5 because it's the default and I just...threw images into training with a mostly default pipeline.config, yet I'm getting this error.","""How to find out what normalization values I need?"""
57734,mihaimaruseac,1251303918,2022-09-19 17:08:37,TF 2.6 is no longer in the window of supported versions,TF 2.6 is no longer in the window of supported versions.
57735,mihaimaruseac,1251303491,2022-09-19 17:08:15,TF 2.6 is no longer in the windows of supported versions.,TF 2.6 is no longer in the windows of supported versions.
57661,samrajkodai,1251006150,2022-09-19 13:15:40,https://github.com/keras-team/keras/issues/4875 i have tried this also but my problem is not solved.,"""I have tried this also but my problem is not solved."""
57738,aliencaocao,1251000115,2022-09-19 13:10:42,"We probably have a different issue here because I am using cuda 11.7 with cudnn 8.5, while you are using 8.1. If you have cuda 11.7 installed but cudnn 8.1, it will not work and will give the PTXAS error.","I am using cuda 11.7 with cudnn 8.5, while you are using 8.1."
57741,cnglen,1250984750,2022-09-19 12:57:13,"``` python
text_vectorizer._self_tracked_trackables.pop()
text_vectorizer._lookup_layer._self_tracked_trackables.pop()
```
It's caused by the large list member of:
- text_vectorizer._self_tracked_trackables
- text_vectorizer._lookup_layer._self_tracked_trackables
It's seems it's not necessary to add the huge list to ._self_tracked_tradckables(which is recursive traversal). Maybe The preprocess layers should disable _self_tracked_trackables.",It's seems it's not necessary to add the huge list to ._self_tracked_tradckables(which is recursive traversal)
57661,samrajkodai,1250736354,2022-09-19 8:45:56,"hi,
hereby i have attached the code, actually after training and saving this model works fine but when i open new python terminal and load the model for testing it gives 0% accuracy.
my tensorflow version is 2.11.0-dev20220812
[issue.txt](https://github.com/tensorflow/tensorflow/files/9597363/issue.txt)","""actually after training and saving this model works fine but when i open new python terminal and load the model for testing it gives 0% accuracy."""
54390,abattery,1250582711,2022-09-19 5:22:33,I don't have permissions to approve the TOSA related changes. I will manually merge this change without the TOSA comment updates. Could you send a separate change for the TOSA related one and get approvals from the right owner instead?,I don't have permissions to approve the TOSA related changes.
57702,MThalberg,1250541364,2022-09-19 4:01:50,"Hi @tiruk007 , thanks for reproducing, it seems that this issue has been fixed on TF2.10. However, The session crash happens in TF2.9.1.","""The session crash happens in TF2.9.1"""
57718,zhihuiloke,1250462586,2022-09-19 1:50:19,"Yes, this is definitely related to #56287. Sorry for the duplicate. However, I don't see the workaround in that thread, it seems unresolved.","I don't see the workaround in that thread, it seems unresolved."
56207,aliencaocao,1250187863,2022-09-18 4:22:07,"Just an update, I have been busy with work recently so only got time to try again now. My display was already at 100%. I however cannot reproduce the issue now due to another issue that made it impossible to train any model: https://github.com/tensorflow/tensorflow/issues/57738
Therefore, I can only continue to test on this bug after I manage to resolve that.
Sorry for the stall.","""I have been busy with work recently so only got time to try again now."""
56850,lu-wang-g,1250157806,2022-09-17 23:58:46,"Seems like I can't approve this CL. Khanh, can you?","I can't approve this CL. Khanh, can you?"
57693,gideonKogan,1250129779,2022-09-17 19:39:28,"@sachinprasadhs, several solutions have been suggested but non of them have been demonstrated on a minimal example. Since I have tried implementing some of those solutions on my example without any success, I wanted to encourage the responders in the attached posts to demonstrate their solutions on this example. Do you think that we can integrate all these posts into one with a minimal example?","""Several solutions have been suggested but non of them have been demonstrated on a minimal example."""
57674,reedwm,1249897478,2022-09-16 22:13:59,"Do you plan on supporting BF16 in all or most kernels so that it is usable outside XLA? I am concerned about the binary size increase that would be caused by instantiating many kernels with BF16, as well as the kernel maintenance cost.","I am concerned about the binary size increase that would be caused by instantiating many kernels with BF16, as well as the kernel maintenance cost."
43546,fernandocfbf,1249798874,2022-09-16 20:49:39,"I'm facing the same problem. To solve this you can try changing your image shape, use lower width and height.",I'm facing the same problem.
57711,sachinprasadhs,1249679156,2022-09-16 18:29:56,"This is due to the very large input which is causing OOM/ memory overflow with large input, when you try large value like tf.int32.max, you will get the error output.
Below is the error output.
```
import tensorflow as tf
tf.eye(2147483647)
ResourceExhaustedError: OOM when allocating tensor with shape[2147483647,2147483647] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:MatrixDiagV3] name: diag ```",OOM/ memory overflow with large input
56088,elfringham,1249460532,2022-09-16 14:45:44,"> CD on master branch failing on grappler remapper tests after this commit [9d4e950](https://github.com/tensorflow/tensorflow/commit/9d4e950687b25f78fe30a74a8b40ba16f12c9fd1) , is this a known issue and is the fix WIP? @nitins17
tensorflow.python.framework.errors_impl.UnimplementedError: Fusion is not implemented: [BiasAdd,GeluExact]","""UnimplementedError: Fusion is not implemented: [BiasAdd,GeluExact]"""
56155,anotheruserofgithub,1249323510,2022-09-16 12:50:53,"@nutsiepully @gbaned Could you merge this now, please? It has been a very long time since this PR has been opened.
If you really need more explanations about this PR, I can certainly provide you with a detailed manuscript.","""It has been a very long time since this PR has been opened."""
31312,cmantas,1249079847,2022-09-16 8:31:55,"I'm seeming a similar pattern unfortunately, IDK why this issue is closed.
* Mac M1 pro
* tensorflow-macos 2.9.2
* tensorflow-metal 0.5.1
I tried
* running `gc.collect()`, `k.clear_session()` on my Sequence generator class (helped, but did not solve it)
* using a separate `Activation` layer (didn't work)
* `model.compile(..` with `run_eagerly=True` (I'm not sure if it helped, but it did not solve it)
I haven't figured out how to try`tcmalloc` on my mac.","I'm seeming a similar pattern unfortunately, IDK why this issue is closed."
57586,elfringham,1249067913,2022-09-16 8:21:27,Made moot by https://github.com/tensorflow/tensorflow/commit/6343eb47624d27fb04c219093db9d6e8a8d15653,"""Made moot by"""
56946,tgeffroyModuleus,1249046711,2022-09-16 8:01:09,"The very basic sample seems to work while using np.expand_dims.
However, I have others operator in my model, and I cannot test the solution right now. I have tried to install my packages with this new version of tensorflow and packages tensorflow-base, keras-base, and keras-gpu cannot be installed with this latest version.
Without them I cannot launch my model that uses other operators:
- Conv3D
- MaxPooling3D
- Dropout
- Dense
- GlobalAveragePooling3D","I have tried to install my packages with this new version of tensorflow and packages tensorflow-base, keras-base, and keras-gpu cannot be installed with this latest version. Without them I cannot launch my model that uses other operators: - Conv3D - MaxPooling3D - Dropout - Dense - GlobalAveragePooling3D."
57717,IBSApple,1248937394,2022-09-16 5:35:50,"Also I stop running, in viewWillDisappear override func viewWillDisappear(_ animated: Bool) {
super.viewWillDisappear(animated)
cameraFeedManager?.stopRunning()
}
Please give be suggestions to sort out this issue.
in PoseNet demo its memory will be clear automatically.","""I stop running, in viewWillDisappear override func viewWillDisappear(_ animated: Bool)  super.viewWillDisappear(animated) cameraFeedManager?.stopRunning() """
57704,dh-Kang,1248793262,2022-09-16 0:46:01,"> 2. I tried with python3.8.5 + -D_GLIBCXX_USE_CXX11_ABI=1 => failed
Clean build also failed",I tried with python3.8.5 + -D_GLIBCXX_USE_CXX11_ABI=1 => failed Clean build also failed.
57704,dh-Kang,1248422297,2022-09-15 17:54:15,"And, Build takes over 4 hours.
Is there any faster way?",Build takes over 4 hours. Is there any faster way?
57666,markemus,1248064796,2022-09-15 12:56:18,@tilakrayal There is a minimal code example here: https://github.com/tensorflow/tensorflow/issues/54463 for recreating the bug. I tried it on my setup and got the exception.,I tried it on my setup and got the exception.
57087,cheshire,1247921088,2022-09-15 10:41:30,"> I confirmed that this test fail here with upstream/master
Could be both? We only test internally on Volta/Pascal now =/","""We only test internally on Volta/Pascal now =/"""
57688,arnavmehta7,1247903983,2022-09-15 10:25:53,"@wangchenghaonl I think you are confusing SparseTensor with SparseMatrixTensor
```
from tensorflow.compat.v1.raw_ops import SparseMatrixZeros
from tensorflow.sparse import SparseTensor
import tensorflow as tf
print(SparseTensor(indices=[[0, 3], [2, 4]],
values=[10, 20],
dense_shape=[3, 10]))
v1 = SparseMatrixZeros(dense_shape=(5,3,2),type=tf.float64)
print(v1)
```
Check this code, it might help you.",I think you are confusing SparseTensor with SparseMatrixTensor
52331,getsanjeevdubey,1247887025,2022-09-15 10:10:14,"@somum Do we have any solution which works with Flask or any serving app?
Reloading/Restarting cannot be accepted as answers in many cases as it increases the latency.
I have searched enough; tried a lot of things; nothing seems to be working. Please help.",I have searched enough; tried a lot of things; nothing seems to be working.
57561,sachinprasadhs,1247656614,2022-09-15 6:49:43,"I do not think we have builtin ops listed anywhere apart from the sources which is already been linked in the issue above.
But, if you want to check if certain op is supported or not during conversion, you can include only `tf.lite.OpsSet.TFLITE_BUILTINS` in your converter, if that op is not available in TFLITE builtin, it will throw error.
```
converter.target_spec.supported_ops = [
tf.lite.OpsSet.TFLITE_BUILTINS]
```","""I do not think we have builtin ops listed anywhere apart from the sources which is already been linked in the issue above."""
57666,markemus,1247645520,2022-09-15 6:36:31,"I'm having this bug as well in tf2.10.0, cudatoolkit 11.2. Also failing on a giant matrix.
```
Blas xGEMV launch failed : a.shape=[1,13238742,6], b.shape=[1,6,1], m=13238742, n=1, k=6
[[{{node model/attention/dense_2/MatMul}}]] [Op:__inference_train_function_1902]
```","blas xGEMV launch failed : a.shape=[1,13238742,6], b.shape=[1,6,1], m=13238742, n=1, k=6"
57385,wukong1992,1247633835,2022-09-15 6:22:00,Figure out! Mig does not support one process multi mig devices: [https://forums.developer.nvidia.com/t/cudagetdevicecount-always-return-1-on-multiple-mig-cis-env/208560](here) .,"""Mig does not support one process multi mig devices: [https://forums.developer.nvidia.com/t/cudagetdevicecount-always-return-1-on-multiple-mig-cis-env/208560](here) .."
57514,sachinprasadhs,1247626943,2022-09-15 6:12:16,"This has. to be edge case scenario.
`tf.int32.min` and `tf.int32.max` are not the same values, which are `-2147483648` and `2147483647` respectively.
Since, `tf.abs(tf.int32.min)` value which is `tf.abs(-2147483648)` exceeds the `tf.int32.max` value which is `2147483647`. it throws exception in that case.
When `tf.int32.min` is added with 1, it matches the `tf.int32.max` value and works in that case.",This has. to be edge case scenario.
57550,sayakpaul,1247470591,2022-09-15 1:40:36,I think you're still missing the point. I keep asking why Conv2D would be selected as the SELECT OP. Your answer actually does not tell me that.,I keep asking why Conv2D would be selected as the SELECT OP. Your answer actually does not tell me that.
57702,MThalberg,1247432923,2022-09-15 0:29:45,"If we don't use JIT, then it will not crash, but throw an exception instead (which is expected). Below is the code (removing the JIT), it throws `InvalidArgumentError`.
```
import tensorflow as tf
input_tensor = tf.zeros([1, 1], dtype=tf.int32)
def f(input):
shape = [-1, 1]
return tf.broadcast_to(input, shape, ) f(input_tensor) # tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimension -1 must be >= 0 [Op:BroadcastTo]
```","""InvalidArgumentError: Dimension -1 must be >= 0"""
57480,VictoriaGriffith,1247420255,2022-09-15 0:08:58,"Hi @sachinprasadhs this behavior only happens with complex, for example, `float32` will pass:
```
import tensorflow as tf print(tf.__version__)
def fn(x): return tf.atan(x)
theoretical, numerical = tf.test.compute_gradient(fn, [0.18])
print(theoretical)
print(numerical)
assert tf.experimental.numpy.allclose(theoretical, numerical) # Pass
```
Oututs:
```
(array([[0.9686168]], dtype=float32),)
(array([[0.9686203]], dtype=float32),)
```","""this behavior only happens with complex, for example, float32"""
44609,yishuangP,1247368986,2022-09-14 22:34:30,"Hi Dan, really sorry that you have to download the `tar.gz` file and unpack it to use it in Swift package. Hopefully we'll release it as Swift package soon. For `TensorFlowLiteSwift`, we just publish the sources and it depends on other XCFrameworks (like `TensorFlowLiteC`). Eventually we'll release it as Swift package.",really sorry that you have to download the tar.gz file and unpack it to use it in Swift package.
57526,cantonios,1247051141,2022-09-14 16:56:30,"Why? I don't think we want it officially part of the API that the axis is ignored when inputing a single tensor. It is still a user error. It just is not worth us checking for it since doing so only adds overhead, and we return the correct result if the specified axis was _any_ valid value.","""I don't think we want it officially part of the API that the axis is ignored when inputing a single tensor."""
57671,fehrin,1246985235,2022-09-14 16:03:02,"@mihaimaruseac You mean Tensorflow 2.10 uses TensorRT 7.x because it is targeted for CUDA 11.2 or older? This means support for Ubuntu up to 18.04 and not for 20.04/22.04, correct? And why is Tensorflow 2.10 now built with TensorRT only for Linux, for Windows it is not? I personally would go for current CUDA versions (which work perfect with Tensorflow btw, i use CUDA 11.7). And have both Windows/Linux packages use TensorRT or not (now: Linux uses TensorRT, Windows does not).","""And why is Tensorflow 2.10 now built with TensorRT only for Linux, for Windows it is not?"""
57671,mihaimaruseac,1246892819,2022-09-14 14:54:44,"Because of the way TF is built with optimizations, we can only target one version of GPU libraries.","Because of the way TF is built with optimizations, we can only target one version of GPU libraries."
57478,mdanatg,1246808820,2022-09-14 14:00:09,"Yes. Slightly modified, to allow `x` to be compatible with while_loop:
```
@tf.function
def test():
x = tf.constant(0)
for _ in tf.range(3):
foo = SomeRandomObject()
x = tf.reduce_sum(tf.map_fn(lambda n: foo.bar(n), tf.constant([1, 2, 3])))
return x
print(test())
```","""Slightly modified, to allow x to be compatible with while_loop:  @tf.function def test(): x = tf.constant(0) for _ in tf.range(3): foo = SomeRandomObject() x = tf.reduce_sum(tf.map_fn(lambda n: foo.bar(n), tf.constant([1, 2, 3]))) return x print(test())"
57478,mdanatg,1246766429,2022-09-14 13:28:11,"Yes, the former example (simpler is better). Right now that code doesn't error out because `foo` is loop-local. We have to retain backward compatibility with that somehow.
We can't change the behavior of the `_verify_loop_init_vars` without first changing how `tf.while_loop` first. `_verify_loop_init_vars` simply makes sure while_loop won't raise an error.",Right now that code doesn't error out because foo is loop-local. We have to retain backward compatibility with that somehow. We can't change the behavior of the _verify_loop_init_vars without first changing how tf.while_loop first. _verify_loop_init_vars simply makes sure while_loop won't raise an error.
57679,arnavmehta7,1246764373,2022-09-14 13:26:32,Make sure the versions are correct and check this link https://stackoverflow.com/questions/60368298/could-not-load-dynamic-library-libnvinfer-so-6,"""could not load dynamic-library-libnvinfer-so-6"""
51803,H0lzm1ch3l,1246719778,2022-09-14 12:51:16,"I personally experienced this problem due to model.call being called outside the train_step with raw data from a custom data generator. When a model is not yet built you would expect it to be built upon first calling it within the overwritten train_step, however this already happens before that when the data adapter is initialized. This situation can be avoided by either doing preprocessing in the data generator or in the model.call itself.","""This situation can be avoided by either doing preprocessing in the data generator or in the model.call itself."""
56604,namrata-ibm,1246712172,2022-09-14 12:45:22,"@rthadur @gbaned I am not able to view the failures, could you please check?","I am not able to view the failures, could you please check?"
57663,Mollylulu,1246561203,2022-09-14 10:26:36,"> ```shell
> tensorflow-2.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
> ```
I did not build from the repo, instead, I download the `tensorflow-2.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl` from [Pypi](https://pypi.org/project/tensorflow/#files) and then use command `pip install xxx.whl` to install tf2.10.0.","I did not build from the repo, instead, I download the tensorflow-2.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl from [Pypi](https://pypi.org/project/tensorflow/#files) and then use command pip install xxx.whl to install tf2.10.0."
57497,shijy16,1246340666,2022-09-14 7:10:44,I agree that some validations can be ignored for performance in GPU. But it would be better if such ABORTs are checked in advance with precise EXCEPTIONs.,I agree that some validations can be ignored for performance in GPU. But it would be better if such ABORTs are checked in advance with precise EXCEPTIONS.
57656,cheshire,1245903407,2022-09-13 20:10:12,That would be untested code though. Sorry for us lagging behind with Ampere CI. Could you predicate on compute capability at runtime? We do have a number of tests doing that.,That would be untested code though.
57669,mihaimaruseac,1245770991,2022-09-13 18:02:39,TF 2.4 is no longer supported. Please switch to TF 2.10,TF 2.4 is no longer supported. Please switch to TF 2.10
57481,cantonios,1245679426,2022-09-13 16:48:42,"Again, looks simply to be a numerical issue with estimating the gradients. Playing around with `delta` for the numerical causes significant swings in the maximum error. The theoretical gradient of `x * sigmoid(x)` is correct from #57357.","Again, looks simply to be a numerical issue with estimating the gradients."
57681,mstebelev,1245589795,2022-09-13 15:36:26,"@sushreebarsa Yes, I see that in 2.10 something is better, but the problem with copy in generated XLA code is the same and you can see it in new version https://colab.research.google.com/drive/1ESGaYLMQttOqqc5GbJZxeC0NDOfipGF7 where we have a lot of reshapes of the same tensor in one operation","""The problem with copy in generated XLA code is the same and you can see it in new version"""
57087,nouiz,1245539444,2022-09-13 14:58:14,"Any update? It was approved 4 days ago, but isn't merged yet.
One CI failed, but the test failure doesn't seem related to my PR. It is a grappler error:
//tensorflow/python/grappler:remapper_test_gpu","""One CI failed, but the test failure doesn't seem related to my PR."""
56796,cantonios,1245403322,2022-09-13 13:18:59,"The function `x/norm(x)` is not differentiable at 0, so what do you expect it to be? Without `epsilon`, a straight-forward implementation would produce `NaN`. With `epsilon`, the piecewise definition of the function is
```
l2_normalize(x) = x/sqrt(epsilon), if x < sqrt(epsilon)
x/norm(x), otherwise
```
so for small x, the gradient of this is `1/sqrt(epsilon)`, which is exactly what is returned.","The function x/norm(x) is not differentiable at 0, so what do you expect it to be? Without epsilon, a straight-forward implementation would produce NaN. With epsilon, the piecewise definition of the function is  l2_normalize(x) = x/sqrt(epsilon), if x  sqrt(epsilon)  x/norm(x), otherwise "
53767,lgeiger,1245374520,2022-09-13 12:55:50,"I retested the above example with `2.10.0` and the segfault seems to be fixed now, however conversion still fails with:
```
'tfl.transpose' op has mismatched quantized axes of input and output
```
See [here](https://colab.research.google.com/drive/1IXri5HeDc9qTAtDOp-LqZyQTL8CcemGq?usp=sharing).","I retested the above example with 2.10.0 and the segfault seems to be fixed now, however conversion still fails with:  'tfl.transpose' op has mismatched quantized axes of input and output "
54458,albertz,1245218462,2022-09-13 10:32:43,"But as you can read above: ""Be aware that the flag may increase the memory usage.""
So this is not really a solution.","""Be aware that the flag may increase the memory usage."""
57671,fehrin,1245217765,2022-09-13 10:32:00,"@tilakrayal I do not build tensorflow, i use the prebuilt package from pypi.org. And this package (the Linux one) is built with/against TensorRT, which is new (the Tensorflow versions 2.9.x are not built with TensorRT), is this intentional? The real problem is that the pypi Linux package of Tensorflow (with Python 3.10) is built against TensorRT version 7, this version is outdated, if at all TensorRT version 8 should be used.","""And this package (the Linux one) is built with/against TensorRT, which is new (the Tensorflow versions 2.9.x are not built with TensorRT), is this intentional?"""
57645,tilakrayal,1245174394,2022-09-13 9:55:27,"@sachinprasadhs,
I was able to reproduce the issue on tensorflow v2.8, v2.9 and [nightly](https://colab.research.google.com/gist/tilakrayal/6ec9204828bcc1a63fc2a70bf16f5747/untitled594.ipynb) with and without the line `tf.compat.v1.disable_eager_execution().` Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/0c33fe9588906eafd35f134f9815e234/untitled581.ipynb).","I was able to reproduce the issue on tensorflow v2.8, v2.9 and [nightly](https://colab.research.google.com/gist/tilakrayal/6ec9204828bcc1a63fc2a70bf16f5747/untitled594.ipynb) with and without the line tf.compat.v1.disable_eager_execution()."
57645,ducvinh-nguyen,1245147157,2022-09-13 9:32:00,"@tilakrayal,
With all due respect, you do not understand my problem yet and still ignore the issue reported. I believe my word is clear enough.
Yes, I do not want to disable eager execution either. The problem is that Keras training is very slow in TF2 with eager execution by default, despite the fact that keras told to run in graph mode (tf.function is not needed).
Is there anyone else who can actually look into the problem?","""You do not understand my problem yet and still ignore the issue reported."""
57675,ProfDoof,1244911119,2022-09-13 5:19:16,"I had tried that already using the keras callbacks. It also failed. I may be misunderstanding what you are saying though. Also, if that's the case then y'all should probably add better exceptions to wherever this is coming from originally (which I couldn't find even though I did look) so that others aren't confused by this.","I had tried that already using the keras callbacks. It also failed. I may be misunderstanding what you are saying though. Also, if that's the case then y'all should probably add better exceptions to wherever this is coming from originally (which I couldn't find even though I did look) so that others aren't confused by this."
51083,jung-hunsoo,1244763191,2022-09-13 1:04:48,"Had the same problem.
In addition, even `content = open(fpath, 'rb').read()` doesn't occur mem leakage, following tf codes are making the mem leakage eventually. For me, `tf.audio.decode_wav(content, ...)` after opning the file occurs mem leakage.
Only a file of 16MB wav consumes over 9GB of GPU mem.","Had the same problem. In addition, even content = open(fpath, 'rb').read() doesn't occur mem leakage, following tf codes are making the mem leakage eventually. For me, tf.audio.decode_wav(content, ...) after opning the file occurs mem leakage. Only a file of 16MB wav consumes over 9GB of GPU mem.."
26182,plopresti,1244480993,2022-09-12 21:11:12,"In case anyone finds this during a search...
I just had this problem with Tensorflow build under CUDA 11.7. The error was coming from the call to cudnnCreate() in stream_executor/cuda/cuda_dnn.c.
The problem was a mismatch between the versions of CUDA (specifically libcublas) and libcudnn8. In my case, I accidentally installed the cuda10.2 variant of the libcudnn8 RPM. Installing the correct cuda11.7 version resolved it.","""The error was coming from the call to cudnnCreate() in stream_executor/cuda/cuda_dnn.c"""
57653,nouiz,1243822454,2022-09-12 14:25:13,"> All I see is:
> > > [XLA] Add the XLA_FLAGS xla_dump_show_fusion_subcomputations.
> > It allows to not dump the fusion internal. This allows to display bigger graph.
Today it tooks longer for the web site to be updated. Can you refresh the page? Currently I see this text:
`Dumps [or not] the fusion internal in the dot and html output file.`.",Currently I see this text: Dumps [or not] the fusion internal in the dot and html output file.
57652,ducvinh-nguyen,1243713064,2022-09-12 13:06:10,"Thank you @sandy088, the line do the job disabling the eager mode. But at last, my trained keras model is still corrupted after reload from cache in Streamlit. It seems like there is no problem with _""tf.compat.v1.disable_eager_execution()""_ but something really changes inside Keras whether the eager mode is activated or not, which makes keras model not cacheable.","""It seems like there is no problem with _""tf.compat.v1.disable_eager_execution()""_ but something really changes inside Keras whether the eager mode is activated or not, which makes keras model not cacheable."""
57553,thomasjv799,1243684272,2022-09-12 12:41:48,"I tried like this
```python
import tensorflow as tf
tf_logs = tf.get_logger()
fh = logging.FileHandler(""tf.log"")
tf_logs.addHandler(fh)
```
Though the file is getting created its empty and all the warnings are shown in the console itself.",though the file is getting created its empty and all the warnings are shown in the console itself..
53260,mohantym,1243581649,2022-09-12 11:10:30,"@youchangkim ! @penpornk !
I could replicate this in my mac M1 machine . It is not replicating in [Colab ](https://colab.sandbox.google.com/drive/1c5OMqus-QBVvaETWzMelBi2LFz90GNLy?resourcekey=0-ph0Yv_zdI-SHm9i2Mwcwyg#scrollTo=eu6lvroGVYAu)though.",I could replicate this in my mac M1 machine . It is not replicating in [Colab](https://colab.sandbox.google.com/drive/1c5OMqus-QBVvaETWzMelBi2LFz90GNLy?resourcekey=0-ph0Yv_zdI-SHm9i2Mwcwyg#scrollTo=eu6lvroGVYAu)though..
52914,alexymumo,1243480236,2022-09-12 9:42:22,Still get the same error when using tensorfow and kotlin,Still get the same error when using tensorfow and kotlin.
57454,AndreyOrb,1243227516,2022-09-12 5:08:53,"I had the same exception with TF 2.8
ValueError: Node '/BatchNorm/cond/FusedBatchNorm_1_grad/FusedBatchNormGrad' has an _output_shapes attribute inconsistent with the GraphDef for output #3: Dimension 0 in both shapes must be equal, but are 0 and 512. Shapes are [0] and [512].
Cuda 11.2
CuDNN 8.1
Windows 10
Python 3.10
Changing to the latest TF 2.10 didn't help to resolve the problem.",I had the same exception with TF 2.8
57475,Bahar-BM,1242821755,2022-09-10 23:54:48,No. The crashes are specific to 888.,No. The crashes are specific to 888.
57494,EnricoMi,1242785178,2022-09-10 18:40:14,"As of `20220910`, this is still not fixed.","As of 20220910, this is still not fixed."
56606,Flamefire,1242719060,2022-09-10 12:33:21,"Unrelated CI failures, e.g.:
> ERROR: The project you're trying to build requires Bazel 5.3.0 (specified in /workspace/.bazelversion), but it wasn't found in /usr/local/lib/bazel/bin.","Unrelated CI failures, e.g.: > ERROR: The project you're trying to build requires Bazel 5.3.0 (specified in /workspace/.bazelversion), but it wasn't found in /usr/local/lib/bazel/bin.."
57649,hanbinyoon,1242171746,2022-09-09 16:10:00,"Looping in @anlunx --tf_xla_persistent_cache_directory should only work with the GpuCompiler when using XLA_FLAGS=--xla_gpu_jitrt_executable
@ezhulenev for any plans with the CpuCompiler.",Looping in @anlunx --tf_xla_persistent_cache_directory should only work with the GpuCompiler when using XLA_FLAGS=--xla_gpu_jitrt_executable @ezhulenev for any plans with the CpuCompiler.
21348,ebrevdo,1242163330,2022-09-09 16:02:39,"I expect this bug to be gone in TF2, and TF1 is no longer officially supported by TensorFlow team. Can you provide a TF2-based repro, one that does not use Sessions?","""I expect this bug to be gone in TF2 and TF1 is no longer officially supported by TensorFlow team."""
56737,cantonios,1242160005,2022-09-09 15:59:54,You're running into floating-point precision issues because the frame length is too large (representable numbers go out of range in forward accumulation of gradients). Changing to float64 fixes the issue.,running into floating-point precision issues because the frame length is too large
56819,madsjk816,1241132526,2022-09-08 19:24:06,"We in our team are also struggling this issue. We are trying to find a solution where we perhaps can overwrite some of the signatures within the model, but we have only managed to do so on the output for now. It's quite error-prone to rely on the args_0..args_x naming using tensorflow serving","""We are trying to find a solution where we perhaps can overwrite some of the signatures within the model, but we have only managed to do so on the output for now."""
57634,elfringham,1240798444,2022-09-08 14:30:10,Found some unexpected unit test failures so will convert to draft for the moment.,Found some unexpected unit test failures so will convert to draft for the moment.
57622,faysalhossain2007,1240055498,2022-09-08 0:31:21,"I tried with the Tensorflow master branch. Still the same problem. Now, I am trying with clang 13.0.0.","I tried with the Tensorflow master branch. Still the same problem. Now, I am trying with clang 13.0.0.."
57494,joker-eph,1239823690,2022-09-07 20:11:37,"> Could you include headers from the new location?
I don't think that is a good idea: this isn't the final location.",Could you include headers from the new location? I don't think that is a good idea: this isn't the final location.
57564,bhack,1239763565,2022-09-07 18:57:25,Too late for my comments... `copybara-service` has arrived to merge this.,Too late for my comments... copybara-service has arrived to merge this.
57494,rdzhabarov,1239667997,2022-09-07 17:17:11,"We are about to remove `tensorflow/stream_executor` completely.
Everything has been moved to the `tensorflow/compiler/xla/stream_executor`.
`tensorflow/stream_executor/...` pretty much has only header redirections to the `tensorflow/compiler/xla/stream_executor/...`
Could you include headers from the new location?",tensorflow/stream_executor/...
57378,bixia1,1239610540,2022-09-07 16:20:12,"I see there are 8 commits, can you squash them into 1 commit?","I see there are 8 commits, can you squash them into 1 commit?"
57630,bhack,1239472638,2022-09-07 14:32:41,"> Yeah, TF does not compile on this type of VM in less than 6hrs, even if using a cache.
I really want to be sure about this and it is why I am asking here to collaborate on the same ""public page"". As this GitHub action is extremely simple to be runned also [locally as is](https://github.com/nektos/act) I would like to clearly collaborate to away all doubts concerning a possible reproducibility of the cache/cache misses with this Action before going to investigate other longer term programs.","TF does not compile on this type of VM in less than 6hrs, even if using a cache."
57630,mihaimaruseac,1239459518,2022-09-07 14:22:08,"Yeah, TF does not compile on this type of VM in less than 6hrs, even if using a cache. On contrast, the [Linux kernel compiles on GH Actions in **slightly less over 2 hours without any cache**](https://github.com/whokilleddb/build-a-kernel-using-github-actions).
The build system of TF needs a lot of improvements, it should not be at least 3 times as complicated as the Linux kernel.
CC @learning-to-play","""TF does not compile on this type of VM in less than 6hrs, even if using a cache"""
53271,mihaimaruseac,1239400795,2022-09-07 13:36:13,"TF's APIs should be used via `tf.`. So `tf.keras.*` is legitimate, `keras.*` not so much.","TF's APIs should be used via tf.. So tf.keras.* is legitimate, keras.* not so much."
56231,hilanzy,1239196139,2022-09-07 10:18:39,"@qlzh727 This issue is still happened on v2.10.0
https://github.com/tensorflow/tensorflow/issues/53271#issuecomment-1239103757",This issue is still happened on v2.10.0
56927,edwardyehuang,1239195213,2022-09-07 10:17:36,"@mohantym Can you please reopen this issue? I find I have to address it after upgrading to TensorFlow 2.10
Also, this issue should be with a ""bug"" label.","I find I have to address it after upgrading to TensorFlow 2.10 Also, this issue should be with a ""bug"" label."
57499,shijy16,1239166040,2022-09-07 9:50:14,"@tiruk007 Hi, I think the execute result is different from what I reported. I've confirmed that this issue can only be reproduced on CPU with **OneDNN** enabled, but colab doesnot support OneDNN. Please take another look at this issue. Thank you!","I've confirmed that this issue can only be reproduced on CPU with **OneDNN** enabled, but colab doesnot support OneDNN."
57500,shijy16,1239163185,2022-09-07 9:47:42,"@tiruk007 Thanks for response, it seems to be an issue which can only be reproduced when execute on CPU with **OneDNN** enabled. But colab doesnot support **OneDNN**.","""But colab doesnot support **OneDNN**."""
57193,jakeh-gc,1239113693,2022-09-07 9:03:28,"Any idea what I can do to resolve this?
> feedback/copybara — Google internal checks FAILED for runs with create time 2022-09-06T16:34:12.870440305Z.
I also need to resolve a merge conflict.",I also need to resolve a merge conflict.
53271,hilanzy,1239103757,2022-09-07 8:54:39,@alkatar21 The issue still happened on v2.10.0 ![image](https://user-images.githubusercontent.com/106568143/188835585-81f4c7b1-398f-4217-827c-4abce7dc0f82.png),The issue still happened on v2.10.0.
57623,Dapid,1238999815,2022-09-07 7:13:54,"At least in the instance of Colab I am getting it is not possible to reproduce because the GPU has more memory than RAM. `nvidia-smi` reports `15109MiB`, while `free -h` reports `10G` free.
If you have an instance with more RAM, you can run `main(True, False)` or `main(True, True)`, and skip the argparse, which I only added because it is convenient for the CLI.
By the way, I tested 2.10, and the problem persists.","""GPU has more memory than RAM"""
57626,MadiTechapps,1238980506,2022-09-07 6:52:14,"I have tried 2.10.0 version too, but getting Cannot find symbol error (see the below screenshots)
Here the error details for 2.7.0 & 2.8.0 &2.9.0 versions
[Screenshot](https://i.stack.imgur.com/Jj9JY.png)
for 2.10.0 version error details
[Screenshot](https://i.stack.imgur.com/I6Ddj.png)","I have tried 2.10.0 version too, but getting Cannot find symbol error (see the below screenshots)"
57625,fregire,1238964013,2022-09-07 6:30:52,"Problem was in Dockerfile:
`
find /usr/local -depth \
\( \
\( -type d -a \( -name test -o -name tests -o -name idle_test \) \) \
-o \( -type f -a \( -name '*.pyc' -o -name '*.pyo' -o -name '*.exe' \) \) \
\) -exec rm -rf '{}' +
`
This command removed folder 'test' which was used in tensorflow",(  ( -type d -a ( -name test -o -name tests -o -name idle_test ) )  -o ( -type f -a ( -name '*.pyc' -o -name '*.pyo' -o -name '*.exe' ) )  ) -exec rm -rf ''
57422,creativesh,1238958524,2022-09-07 6:23:13,"@sachinprasadhs unfortunately , I do not have the train code, only the model checkpointy is available to me.","unfortunately , I do not have the train code, only the model checkpointy is available to me."
57405,Jerry-Ge,1238774333,2022-09-07 0:23:30,The ARM CI test failed. Seems due to flaky.,The ARM CI test failed. Seems due to flaky.
39833,cantonios,1238667583,2022-09-06 21:27:04,This likely belongs outside of core TF. We are trying to minimize the API surface.,This likely belongs outside of core TF. We are trying to minimize the API surface.
57494,joker-eph,1238619021,2022-09-06 20:30:06,Still not right apparently...,Still not right apparently...
56761,jlebar,1237428889,2022-09-05 20:07:14,"To over-communicate, I'm waiting for responses to my review comments above.",I'm waiting for responses to my review comments above.
57597,aliericcantona,1237122354,2022-09-05 14:27:15,I need through C-Api way not a python way. There is no description on that.,I need through C-Api way not a python way. There is no description on that.
56088,elfringham,1236756526,2022-09-05 9:26:49,"There was a new unit test a few days ago on master, //tensorflow/compiler/xla/runtime:custom_call_test that fails on Linaro CI. My testing also shows it failing on x86 so I am not sure why this is not picked up by the main CI testing. I could not find the run that made it pass, it went straight from not existing in the log to having a cached pass.
I have a PR to fix it that also needs an AARCH64 specific part https://github.com/tensorflow/tensorflow/pull/57586","I could not find the run that made it pass, it went straight from not existing in the log to having a cached pass."
46477,LiewenHuke,1236714342,2022-09-05 8:48:34,"I used ""bazel coverage --combined_report=lcov ..."" in my testing root-dir. Same issue in lcov_files.tmp, every record point at an empty file.","""bazel coverage --combined_report=lcov ..."""
45753,danielmimimi,1236647742,2022-09-05 7:40:41,I m also having this problem,I m also having this problem.
56088,mseth10,1236594396,2022-09-05 6:41:06,"CD on master branch failing on grappler remapper tests after this commit https://github.com/tensorflow/tensorflow/commit/9d4e950687b25f78fe30a74a8b40ba16f12c9fd1 , is this a known issue and is the fix WIP? @nitins17","""CD on master branch failing on grappler remapper tests after this commit"""
57498,shijy16,1236544221,2022-09-05 5:06:36,"@tilakrayal Hi, issuse #57497 is a `check-fail`, but this one is a `Floating-Point-Exception`, the input are different too. So I donot think they are the same issue.","""I donot think they are the same issue."""
47554,Mushoz,1236412595,2022-09-04 20:46:43,"Exact same issue for me, with a model using LSTM layers as well. Super frustrating that basic functionality like this does not work.","Exact same issue for me, with a model using LSTM layers as well. Super frustrating that basic functionality like this does not work."
57494,EnricoMi,1236080677,2022-09-03 9:11:24,"It looks like your change in `tensorflow/BUILD` (702e7a4f)
```
""//tensorflow/stream_executor:stream_executor_headers"",
```
should read
```
""//tensorflow/stream_executor:stream"",
```
or
```
""//tensorflow/stream_executor:headers"",
```"," ""//tensorflow/stream_executor:stream_executor_headers"",  should read  ""//tensorflow/stream_executor:stream"",  or  ""//tensorflow/stream_executor:headers"", ."
43645,sina-ss,1236053028,2022-09-03 5:32:11,"I have same issue too:
WARNING:tensorflow:Early stopping conditioned on metric `auc` which is not available. Available metrics are: loss,auc_1
WARNING:tensorflow:Early stopping conditioned on metric `auc` which is not available. Available metrics are: loss,auc_2
WARNING:tensorflow:Early stopping conditioned on metric `auc` which is not available. Available metrics are: loss,auc_3
and ...","""I have same issue too: WARNING:tensorflow:Early stopping conditioned on metric auc which is not available."""
57574,froody,1236015661,2022-09-03 1:13:23,"@sushreebarsa No the issue is not resolved, I still can't load a model trained with the original method (subclassing Layer/Model)","""I still can't load a model trained with the original method (subclassing Layer/Model)"""
53149,clarkzinzow,1235950332,2022-09-02 22:41:33,"Bump on this, this is still broken as of 2.9.1.","""This is still broken as of 2.9.1.."""
56769,bhack,1235932831,2022-09-02 22:14:12,@cheshire Is there a margin to do something more here? If not can we at least improve the error message in the code? As I think that the PR to `known_issues.md` is a little bit too marginal for the avg developer with the current error string in the code.,Is there a margin to do something more here? If not can we at least improve the error message in the code? As I think that the PR to known_issues.md is a little bit too marginal for the avg developer with the current error string in the code.
43693,pmeckoni,1235609446,2022-09-02 15:04:02,I am no longer trying to use Tensorflow on Ubuntu 20.04. I wanted to use it 2 years back. The owner(s) of this repo may decide to close this issue if they want to.,I am no longer trying to use Tensorflow on Ubuntu 20.04.
53844,ttdd11,1235587839,2022-09-02 14:42:35,"@jszaday thanks for the reply, sorry it took a little while to test this.
In practice, though it seems like it's loading (no crash, no exceptions), when I load the optimizer state with the code provided, the model loss get far worse quickly, than if the optimizer is not loaded at all. I'm not sure what is causing this, however this solution unfortunately does not seem to accomplish the goal.","I'm not sure what is causing this, however this solution unfortunately does not seem to accomplish the goal."
57580,IBSApple,1235289590,2022-09-02 9:39:50,"If I use only pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly', :subspecs => ['CoreML']
then many error occure in code",> 0.0.1-nightly/>
57087,nouiz,1234634890,2022-09-01 18:26:39,"This test shouldn't be executed on CPU. It execute, so it mean the build on CPU worked.
The BUILD file contains this:
```
tf_cc_test(
name = ""horizontal_loop_fusion_test"",
srcs = [""horizontal_loop_fusion_test.cc""],
tags = tf_cuda_tests_tags(),
```
`tf_cuda_tests_tags()` include the tag `require-gpu`.
So this should be an issue with the CPU builder that include this tests while it shouldn't.
Who can investigate that?","""So this should be an issue with the CPU builder that include this tests while it shouldn't."""
45200,matifali,1234176208,2022-09-01 11:56:11,or simply `sudo apt install libcudnn8` if you use the Nvidia Cuda docker image. I don't know why it has `libcudnn8.so` missing.,I don't know why it has libcudnn8.so missing.
57207,Wyverald,1234142147,2022-09-01 11:23:31,"My suspicion is that you need to upgrade to the latest version of rules_go, due to this line in the error logs:
```
ERROR: /var/lib/buildkite-agent/builds/bk-docker-9gkq/bazel-downstream-projects/tensorflow/tensorflow/tools/build_info/BUILD:9:10: While resolving toolchains for target //tensorflow/tools/build_info:gen_build_info: invalid registered toolchain '@go_sdk//:go_darwin_arm':
```","""Invalid registered toolchain"""
52944,rivershah,1234126146,2022-09-01 11:10:45,@sachinprasadhs any updates on this please? `tf.print` does not work with xla ```Detected unsupported operations when trying to compile graph __inference_run_step_2702[] on XLA_CPU_JIT: PrintV2 (No registered 'PrintV2' OpKernel for XLA_CPU_JIT devices compatible with node```,tf.print does not work with xla
52148,bhack,1234105682,2022-09-01 10:54:31,"> Hi, Could you please refer the comment here [#55639 (comment)](https://github.com/tensorflow/tensorflow/issues/55639#issuecomment-1233238644) which explains about the possible reasons for vectorized_map issues with different OPS.
I don't think it is related. It is the impl of the converter:
https://github.com/tensorflow/tensorflow/blob/d8ce9f9c301d021a69953134185ab728c1c248d3/tensorflow/python/ops/parallel_for/pfor.py#L2695-L2746
/cc @ishark",I don't think it is related. It is the impl of the converter.
57056,olipinski,1233971416,2022-09-01 8:57:44,"Seems like almost all tests succeeded except for one, but I am not sure what to make of this. I don't know if this PR is what breaks this?
```
tensorflow/c/eager/c_api_test.cc:681: Failure
Expected equality of these values:
orig_ptr
Which is: 0x55ab336cfc00
TF_TensorData(t)
Which is: 0x7f5d90089f40
```","seems like almost all tests succeeded except for one, but I am not sure what to make of this. I don't know if this PR is what breaks this"
57052,sky712345678,1233828952,2022-09-01 6:57:25,"@elina-israyelyan Sorry that I didn't confirmed it with limited resources on colab.
I've modified the code. Please use [this notebook](https://drive.google.com/file/d/1k78lpGVthB7nthEkYgUs3JNJTuR79r5E/view?usp=sharing). The issue can be replicated with TF 2.9 running on CPUs.
You can see the error message in runtime logs.
![image](https://user-images.githubusercontent.com/37875281/187851354-783b205c-faa8-4661-b828-7645f1db2dea.png)","""I've modified the code"""
56305,cantonios,1233735596,2022-09-01 4:47:00,"Hmm... seems to be because you are overwriting `a`. Creating an intermediate `b` seems to work around the issue:
```
def test():
a = tf.random.normal([3, 5])
with tf.GradientTape(persistent=True) as tape:
tape.watch(a)
b = a * tf.constant([[1],[0],[0]], dtype=tf.float32)
cos_score = tf.keras.losses.cosine_similarity(b[:,None], b[None,:])
loss = tf.reduce_sum(cos_score)
grad = tape.gradient(loss, a)
print(grad)
```",hmm... seems to be because you are overwriting a. Creating an intermediate b seems to work around the issue
56255,yishuangP,1233674970,2022-09-01 2:56:21,"Hi, I just verified that this build issue is resolved in build `0.0.1-nightly.20220831`. Unfortunately the fix won't be in 2.9.1, if you want to use 2.9.1, you can try building the framework using tensorflow/lite/ios/build_frameworks.sh. Besides, make sure you add this build flag `--define=tflite_with_xnnpack=false` when building the flex delegate framework.","""Unfortunately the fix won't be in 2.9.1"""
52149,nalzok,1233508618,2022-08-31 22:48:58,"Hi there, this directory is not present on my TPU VM created through the following command, probably because I specified the runtime version to be `tpu-vm-base` instead of `tpu-vm-tf-2.9.1`.
```
gcloud alpha compute tpus tpu-vm create trc-01 \
--zone us-central1-a \
--accelerator-type v3-8 \
--version tpu-vm-base
```
There seems to be a workaround though: the wheels can be downloaded from https://cloud.google.com/tpu/docs/supported-tpu-versions#tpu_software_versions","""I specified the runtime version to be tpu-vm-base instead of tpu-vm-tf-2.9.1."""
56826,kaixih,1233346577,2022-08-31 19:39:53,I see. I should have done less of the force-pushing. I did that mainly because I wanted to make the other PRs that depend on this one more easily to be rebased.,I should have done less of the force-pushing.
56826,kaixih,1233336203,2022-08-31 19:27:38,"Sorry, I reopened it because it showed ""closed"" instead of ""merged"". And yes, I just noticed it was actually merged. @reedwm I got a bit confused. Closing this one for now.",I got a bit confused.
41325,gowthamkpr,1233237703,2022-08-31 17:48:03,"@farotem The issue here is that, if you call the layer with inputs of different sizes, each time you call a layer your Function retraces a new graph for every call, you'll find that your code executes more slowly than if you didn't use tf.function. I would recoomend you to use unknown dimensions for flexibility as mentioned [here](https://www.tensorflow.org/guide/function#use_unknown_dimensions_for_flexibility) to control retracing","""I would recoomend you to use unknown dimensions for flexibility as mentioned [here](https://www.tensorflow.org/guide/function#use_unknown_dimensions_for_flexibility) to control retracing"""
57353,sachinprasadhs,1233214872,2022-08-31 17:23:00,"@Apprisco, Seems like it is the duplicate of the above linked issue. Could you please close this issue and track the progress in the above linked issue.","""Seems like it is the duplicate of the above linked issue."""
57207,Wyverald,1233188224,2022-08-31 16:56:07,"https://buildkite.com/bazel/bazel-at-head-plus-downstream/builds/2606#0182f214-c75f-48b4-aefb-b1cdfc945293
TF build remains broken, likely due to an outdated rules_go.","TF build remains broken, likely due to an outdated rules_go."
57388,yishuangP,1233184121,2022-08-31 16:51:49,Hi this seems like a duplicate of https://github.com/tensorflow/tensorflow/issues/56255 and it should be fixed in the latest nightly build (version 0.0.1-nightly.20220831),"""It should be fixed in the latest nightly build (version 0.0.1-nightly.20220831)."""
57027,mshawcroft,1233168098,2022-08-31 16:34:58,"Hi, Sorry, I have no code I can share, but the explanation of where the problem is should be pretty clear?","I have no code I can share, but the explanation of where the problem is should be pretty clear?"
57378,bixia1,1233114185,2022-08-31 15:46:40,Could you please squash the commits?,Could you please squash the commits?
57454,wwdok,1232816697,2022-08-31 11:32:46,"@sushreebarsa You should first git clone repo, then change the `MODEL_DIR` : ![image](https://user-images.githubusercontent.com/43233772/187669030-672519b2-38e8-43b1-bd55-cb0d1aab421f.png)
I try it myself, got the same error","I try it myself, got the same error."
57087,reedwm,1232296902,2022-08-31 0:08:46,Ah I missed the fact that this PR relies on #57020. That is probably way this PR is failing.,I missed the fact that this PR relies on #57020.
53816,cantonios,1232268119,2022-08-30 23:20:26,"@bersbersbers yes, it was reverted.
The issue is that the change to always use the `EuclideanNorm` op from within `tf.norm` breaks many workflows that rely on the existing `norm` function, since it introduces a new op that isn't supported on all devices.
The only workable solution is that if you *need* the higher accuracy, then use `tf.math.reduce_euclidean_norm` directly. Internally, it uses a more stable way of computing the norm that less prone to overflow and precision issues.","""The issue is that the change to always use the EuclideanNorm op from within tf.norm breaks many workflows that rely on the existing norm function, since it introduces a new op that isn't supported on all devices."""
29535,microprediction,1232246374,2022-08-30 22:45:14,"I also think the role of model.trainable could be clarified, and the overriding of trainability when the model is created. See this [gist](https://gist.github.com/microprediction/7aa395cec8a0ea2586936084a635cfba) showing that layer trainability is ignored. The behavior is a little counter-intuitive.","I also think the role of model.trainable could be clarified, and the overriding of trainability when the model is created."
57370,rsanthanam-amd,1232243743,2022-08-30 22:41:05,"i was trying to figure out a runtime switch, but the PR merged before i could change it.
rint can sometimes raise the inexact flag but nearbyint does not.
for AMDGPU/rocm, they are equivalent but i am not sure about cuda so i thought it prudent to preserve the original behavior.","i was trying to figure out a runtime switch, but the PR merged before i could change it."
57353,Apprisco,1232202277,2022-08-30 21:46:12,"Yes, it seems that default Tensorflow implementation of SSIM as well as MS-SSIM both introduce large issues regarding nan gradients in this manner. Our implementation does include a minor clamping, but consistently reports lower SSIM values than Tensorflow.","""It seems that default Tensorflow implementation of SSIM as well as MS-SSIM both introduce large issues regarding nan gradients in this manner."""
47554,rivershah,1231849022,2022-08-30 15:47:15,This issue still persists in `tf version: 2.11.0-dev20220830` Going to echo @vaskozl: this is a critical feature and should be working. @Saduf2019 this bug been around a long time. Can you please take a look or change assignee who can please help move this along.,This issue still persists in tf version: 2.11.0-dev20220830
55786,hoba87,1231577719,2022-08-30 12:08:58,"> Hi @hoba87, Have you tried `--config=nccl ` command during Tensorflow build run. Thank you!
That gives only: ERROR: Config value 'nccl' is not defined in any .rc file
At the end my workaround was to use a ubuntu docker image with fitting libc version and using that then.","""Error: Config value 'nccl' is not defined in any .rc file"""
57463,mstebelev,1231425770,2022-08-30 9:42:22,"@sushreebarsa Yes, the same problem in 2.9. removing jit_compile=True from tf.function speeds up loop duration from ~300ms to ~100ms",removing jit_compile=True from tf.function speeds up loop duration from 300ms to 100ms.
52045,szutenberg,1231398951,2022-08-30 9:20:24,"Hi @mohantym ,
Sure. Epsilon 1e-07 works but my point is that grappler's constant folding doesn't work correctly.","""My point is that grappler's constant folding doesn't work correctly."""
54458,albertz,1231298007,2022-08-30 7:58:57,"@gadagashwini I did.
But @rainwoodman just made a comment on the `output_all_intermediates` usage. However, as long as this issue here is not fixed, I don't see the alternative? Even the error message suggests that:
> Try using tf.compat.v1.experimental.output_all_intermediates(True).
The other suggested workaround to rearrange the code is not possible in my case.
So, as you see, it is important for me that you fix this.","""I did. But @rainwoodman just made a comment on the output_all_intermediates usage. However, as long as this issue here is not fixed, I don't see the alternative?"""
57466,maifeeulasad,1231181686,2022-08-30 6:00:44,"@tilakrayal, I understand it. But I couldn't fit it any of these:
- Tensorflow Issue Template
- TensorFlow Lite Converter Issue
- TensorFlow Lite in Play Services issue
- TensorFlow Lite Op Request
- Report a security vulnerability
That's why I created an empty issue.",I couldn't fit it any of these: - TensorFlow Issue Template - TensorFlow Lite Converter Issue - TensorFlow Lite in Play Services issue - TensorFlow Lite Op Request - Report a security vulnerability
57495,shijy16,1231144212,2022-08-30 5:03:03,"@tilakrayal Sorry, I forgot to mention that GPU must be enabled to reproduce the bug.
Please make sure the code is executed with GPU backend enabled.","""I forgot to mention that GPU must be enabled to reproduce the bug."""
57481,VictoriaGriffith,1231078918,2022-08-30 2:49:13,"Thanks @sushreebarsa for replicating the bug.
I have reported this issue in Keras repo https://github.com/keras-team/keras/issues/16962. However, I would still like to keep it here, because according to https://github.com/tensorflow/tensorflow/issues/57357, this bug seems to come from the same gradient problem as `tf.nn.silu` and `tf.nn.sigmoid` (because Swish computes `x*sigmoid(x)`), and could be related to tensorflow implementation for this sigmoid gradient computation.","""I have reported this issue in Keras repo https://github.com/keras-team/keras/issues/16962"""
57056,cantonios,1230827755,2022-08-29 20:29:27,"@olipinski sorry for the delay here.
Looks like you're missing a registration of `SplitVOpGPULaunch`. Register it in `split_lib_gpu.cu.cc`, and add a corresponding `extern template struct` declaration in `split_lib_gpu.h`.","""Looks like you're missing a registration of SplitVOpGPULaunch"""
57498,dennyX7,1230696272,2022-08-29 18:25:47,Facing the same issue.,Facing the same issue.
51439,bamurtaugh,1230580310,2022-08-29 16:59:44,"Apologies for a potential lack of clarity in my original comment. I meant to explain that our team's primary work has been on the dev container spec and the items I highlighted. https://github.com/microsoft/vscode-remote-release/issues/6746 is on the backlog, but spec work has been at the forefront, so there are no new updates on https://github.com/microsoft/vscode-remote-release/issues/6746 at this time.","""I meant to explain that our team's primary work has been on the dev container spec and the items I highlighted. https://github.com/microsoft/vscode-remote-release/issues/6746 is on the backlog, but spec work has been at the forefront, so there are no new updates on https://github.com/microsoft/vscode-remote-release/issues/6746 at this time."""
57468,mihaimaruseac,1230453635,2022-08-29 15:11:12,"Has to be on your side, or the Pr imported manually.","""Has to be on your side, or the Pr imported manually."""
57492,hosford42,1229672676,2022-08-29 2:22:09,"At this point, I see no other options for working around this besides modifying the tensor to be non-empty, and then cutting it back down to size again afterwards.","I see no other options for working around this besides modifying the tensor to be non-empty, and then cutting it back down to size again afterwards."
57486,VictoriaGriffith,1229544808,2022-08-28 20:12:45,"I think the bug comes from this low-level api `tf.raw_ops.MatrixSquareRoot`, which also crashes on the same input:
```
import tensorflow as tf
x = tf.random.uniform([3, 0, 0, 0])
y = tf.raw_ops.MatrixSquareRoot(input=x)
print(y)
with tf.GradientTape() as g:
g.watch(x)
y_2 = tf.raw_ops.MatrixSquareRoot(input=x)
print(""y_2: "", y_2)
# y_2: tf.Tensor([], shape=(3, 0, 0, 0), dtype=float32)
grad = g.jacobian(y_2, [x]) # floating point exception (core dumped)
```","""floating point exception (core dumped)"""
56904,ma7555,1229448633,2022-08-28 12:37:56,I am facing the same issue with ` tf.image.crop_to_bounding_box` with tf 2.8.2 on Win11,I am facing the same issue with  tf.image.crop_to_bounding_box with tf 2.8.2 on Win11.
47554,margeonie,1229416317,2022-08-28 9:28:11,"I am also experiencing the same error using tf version 2.9.1 - until this problem is solved, how can I hide the warning message?","I am also experiencing the same error using tf version 2.9.1 - until this problem is solved, how can I hide the warning message?"
56886,eeDigitalSeal,1229357951,2022-08-28 2:13:54,"@sushreebarsa Hi, as you replicated, the `ValueError` is thrown by forward-mode auto differentiation at line 17. But the input shape `[2]` is actually invalid so it should throw the same error in reverse-mode at line 13, that is
```python
res_backward = tf.reduce_max(input,axis=axis,keepdims=False)
```
I think it was caused by some missing input validation in reverse-mode autograd.","""ValueError"" is thrown by forward-mode auto differentiation at line 17 but the input shape [2] is actually invalid so it should throw the same error in reverse-mode at line 13, that is python res_backward = tf.reduce_max(input,axis=axis,keepdims=False)  I think it was caused by some missing input validation in reverse-mode autograd.."
57012,mihaimaruseac,1229331405,2022-08-27 23:35:53,Please don't make this type of PRs. They block automation. See https://github.com/tensorflow/community/issues/425 https://github.com/tensorflow/tensorflow/pull/56119 and https://github.com/tensorflow/tensorflow/pull/57478 for such a case.,"""They block automation"""
57450,VictoriaGriffith,1228956273,2022-08-26 21:17:08,"Even if TensorFlow decides not to support this feature, proper error message should be thrown during the forward pass, currently `res = layer(x)` succeeds and output a `(1, 5, 6, 2)`-shape tensor, and then back prop fails.","(1, 5, 6, 2)"
32532,gowthamkpr,1228827057,2022-08-26 19:09:42,@daviddiazvico It seems that the KerasRegressor and Kerasclassifier is deprecated and we are no longer maintaining/bug fixing it. Please forward this issue to https://github.com/adriangb/scikeras.,"""It seems that the KerasRegressor and Kerasclassifier is deprecated and we are no longer maintaining/bug fixing it."""
38866,gowthamkpr,1228766580,2022-08-26 17:53:15,Keras scikit-learn is deprecated and we are no longer maintaining/bug fixing it. Please forward this issue to https://github.com/adriangb/scikeras.,Keras scikit-learn is deprecated and we are no longer maintaining/bug fixing it.
57449,JVD9kh96,1228230648,2022-08-26 8:45:46,"> Thanks for the response. Yeah I know the previous versions and also the newer versions of tensorflow does not have this bug. But, I'm attending a Kaggle competition which requires the Internet to be disabled in the notebook. So, I'm not sure whether I can downgrade (or upgrade) the tensorflow version while the Internet is disabled in the notebook. Because of that, I was looking for a solution for making versin 2.6.4 work.","""I'm attending a Kaggle competition which requires the Internet to be disabled in the notebook."""
57461,talyz,1228211433,2022-08-26 8:28:52,"The cause was indeed that MSVC doesn't support `__attribute__((weak))` or similar hints. Never found out why it's only a problem when building a shared library, though.",MSVC doesn't support __attribute__((weak)) or similar hints.
57387,gadagashwini,1228082607,2022-08-26 5:52:22,"Hi @trungnhat-incoder, `ERROR: D:/nghich/tensorflow/tensorflow/BUILD:1035:21: //tensorflow:libtensorflow_framework.so.2.11.0: no such attribute 'shared_lib_name' in 'cc_shared_library' rule`
Tensorflow master branch has `cc_shared_library rule`. Please take a look at this reference [link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl#L2971). Could you build Tf master branch and check. Thank you!","""Error: D:/nghich/tensorflow/tensorflow/BUILD:1035:21: //tensorflow:libtensorflow_framework.so.2.11.0: no such attribute 'shared_lib_name' in 'cc_shared_library' rule"""
57174,cheyennee,1227890402,2022-08-26 0:41:34,"@sachinprasadhs But I think it's important for gpu to check the validity of input. If the input is invalid, then the calculation has no meaning.","""I think it's important for gpu to check the validity of input."""
56869,rthadur,1227856932,2022-08-25 23:35:00,@rahulbatra85 could you please resolve conflicts!,"""could you please resolve conflicts!"""
57116,bixia1,1227680164,2022-08-25 19:33:24,Can you fix the PR description and make sure that you rebase? I am still seeing the change that are done via https://github.com/tensorflow/tensorflow/pull/57139,I am still seeing the change that are done via https://github.com/tensorflow/tensorflow/pull/57139.
56604,namrata-ibm,1227335554,2022-08-25 14:26:13,"@haozha111 @gbaned I couldn't see relevant failures in above check, could you please let me know if something is blocking the merge?","I couldn't see relevant failures in above check, could you please let me know if something is blocking the merge?"
33303,misterBart,1227233017,2022-08-25 13:05:50,"Several threads exist about this issue. The issue persists now for more than a year. The proposed solution of using signature defs comes again with its own problems. My two work-arounds:
(1) Use tf.compat.v1.lite.TFLiteConverter
(2) Sort output indices
For details see at https://github.com/tensorflow/tensorflow/issues/57043",Several threads exist about this issue. The issue persists now for more than a year. The proposed solution of using signature defs comes again with its own problems. My two work-arounds: (1) Use tf.compat.v1.lite.TFLiteConverter (2) Sort output indices For details see at https://github.com/tensorflow/tensorflow/issues/57043.
57196,drubinstein,1227214822,2022-08-25 12:50:13,@sachinprasadhs I haven't checked it and didn't think it was worth the effort since all iPhones since 2013 have been 64-bit/arm64. I tried to get an armv7 build running on my M1 laptop but couldn't figure out a way to do it nor do I have a iPhone5 available to me unfortunately.,I haven't checked it and didn't think it was worth the effort since all iPhones since 2013 have been 64-bit/arm64.
57422,creativesh,1227121364,2022-08-25 11:16:01,"@mohantym
I have tested my model with several .jpeg images and also , I tested them with a python script to make sure whether they are real .jpeg files or not. my model is 42m and here upload limit is 24m. How can I share it ?","""my model is 42m and here upload limit is 24m"""
57052,sushreebarsa,1226616955,2022-08-25 0:06:33,"@elina-israyelyan I tried to replicate the issue using latest TF versions but colab is crashing [here](https://colab.research.google.com/gist/sushreebarsa/e2caffd985951f415afb29b2a8ae3c66/3d_unet_train.ipynb). Could you please find the attached gist and confirm the same?
Thank you!",I tried to replicate the issue using latest TF versions but colab is crashing
47554,TheMoMatthias,1226615506,2022-08-25 0:04:26,I am experiencing the same error in tensorflow version 2.9.1. I just now kept receiving the error message after removing the arguments: activation='relu' and dropout=0.2. Does this affect the model somehow or can the error message be ignored?,I am experiencing the same error in tensorflow version 2.9.1.
56939,mihaimaruseac,1226302073,2022-08-24 20:44:37,There is no separate branch cut for RCs,No separate branch cut for RCs.
56653,drivanov,1225991772,2022-08-24 16:58:47,"@bixia1: Please, don't merge that one into master. I have resolved the formal merge conflicts appeared after the merge of [PR#56942](https://github.com/tensorflow/tensorflow/pull/56942), but I need to fix something.","""I have resolved the formal merge conflicts appeared after the merge of [PR#56942](https://github.com/tensorflow/tensorflow/pull/56942), but I need to fix something."""
9090,nicoloceneda,1225395193,2022-08-24 8:47:17,"Dropbox works perfectly fine on Mac, OneDrive not working on Windows","Dropbox works perfectly fine on Mac, OneDrive not working on Windows."
57385,wukong1992,1225277375,2022-08-24 6:54:44,> I test with version 2.8 and the error still exists.,I test with version 2.8 and the error still exists.
57422,creativesh,1225272422,2022-08-24 6:48:19,"@mohantym Hi, thanks for your answer, but I have tested the input image with code and this is really a jpeg image. the problem is the decoding part of my network. please pay attention to this issue : https://github.com/tensorflow/tflite-support/issues/870",I have tested the input image with code and this is really a jpeg image. the problem is the decoding part of my network. please pay attention to this issue : https://github.com/tensorflow/tflite-support/issues/870.
57359,jhuus,1224834264,2022-08-23 20:31:03,"I'm getting the same error, on 2.9.0 but I reproduced it in 2.8.0 and 2.9.1 too. I'll see if I can create a small enough example to post.
2022-08-23 16:26:25.102620: E tensorflow/stream_executor/cuda/cuda_blas.cc:197] failed to set new cublas math mode: CUBLAS_STATUS_INVALID_VALUE
2022-08-23 16:26:25.102641: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at matmul_op_impl.h:438 : INTERNAL: Failed initializing math mode","""I'm getting the same error, on 2.9.0 but I reproduced it in 2.8.0 and 2.9.1 too."""
51502,cantonios,1224322132,2022-08-23 16:37:57,This error message is consistent for all ops across TF when you try to run something on a dtype that isn't supported. I suggest we keep as is for reduce max. There are separate efforts for improving error messages across the entire TF codebase.,This error message is consistent for all ops across TF when you try to run something on a dtype that isn't supported.
57363,singinwhale,1224044324,2022-08-23 13:02:49,"https://stackoverflow.com/questions/73416907/model-save-tried-to-export-a-function-which-references-untracked-resource-eve/73459151#73459151
The error is due to using a static class member instead of a normal attribute:
```python3
class DummyModel(tf.keras.Model):
feature_extractor = None # <- this is the problem
def __init__(self):
```",Using a static class member instead of a normal attribute.
51502,mohantym,1223809463,2022-08-23 9:30:49,It is still replicating in 2.9 and nightly. Attached [gist](https://colab.sandbox.google.com/gist/mohantym/6858ff76dd5f7e35d943683b4d8dd2f1/untitled79.ipynb#scrollTo=rt5eLIgAfLEU) for reference.,It is still replicating in 2.9 and nightly.
56255,yishuangP,1223579278,2022-08-23 5:50:42,"Hi really sorry for the late reply. I'm working on a fix. In the meantime, can you try tensorflow/lite/ios/build_frameworks.sh? You might need to add `--config=monolithic` when building TensorFlowLiteSelectTfOps_framework.","I'm working on a fix. In the meantime, can you try tensorflow/lite/ios/build_frameworks.sh? You might need to add --config=monolithic when building TensorFlowLiteSelectTfOps_framework."
57169,zzq96,1223503691,2022-08-23 3:58:40,"> self.call._stateful_fn._function_cache.primary.clear()
hi, i have tried it, but the memory isn't cleared:
<img width=""834"" alt=""image"" src=""https://user-images.githubusercontent.com/39215341/186066492-5b1c4acf-0e9b-449f-8b30-8d3a36483cc1.png"">","""I have tried it, but the memory isn't cleared"""
43497,curiarteb,1223370411,2022-08-23 0:34:27,"Yes! And it was exactly what I did. However, there are two big issues regarding this:
1) You need to know the size of ""b"" for reshaping both A and B.
2) The .fit function of Keras enables graph execution mode (it is much faster than eager execution). When constructing the graph,the reshape function breaks, since we are feeding tensors with ""None"" first axis dimensions.
Then, your reshaping only works in eager mode, which is much slower than graph mode.","""It was exactly what I did."""
55746,JunyoungLim,1223201103,2022-08-22 22:09:57,"This AMD ROCm build failure and ""Waiting for status to be reported"" blockers seem to be ubiquitous across all the PRs at the moment. I will try it internally if it works.","This AMD ROCm build failure and ""Waiting for status to be reported"" blockers seem to be ubiquitous across all the PRs at the moment."
57020,nouiz,1222522060,2022-08-22 15:30:32,@cheshire any idea why it isn't merged while the other PR got merged?,"""why it isn't merged while the other PR got merged?"""
22480,dydxdt,1221764615,2022-08-22 3:43:16,"This problem occurs when I used tf2.2, and I use tf2.3, it disappears","This problem occurs when I used tf2.2, and I use tf2.3, it disappears."
27318,beyarkay,1221294366,2022-08-20 11:05:14,"@jvishnuvardhan Can you confirm that there's no way for a custom callback to get access to the validation dataset being used by the model? For example, I want to log the per-class precision/recall on a multi-class classification problem but I can't do that without the data.
For future travellers, this [stack overflow](https://stackoverflow.com/a/61856587/14555505) answer has a workaround whereby you override `__init__` and pass in the validation data as a parameter",Can you confirm that there's no way for a custom callback to get access to the validation dataset being used by the model?
57020,nouiz,1220985696,2022-08-19 18:36:57,"> Infra got into a broken inconsistent state somehow.
> > Unfortunately, our external compiler warnings don't have -Werror, so got this:
> > ```
> ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]
> module->RemoveEmbeddedComputation(old_computation);
> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~
> ```
Fixed.","""Infra got into a broken inconsistent state somehow."""
57161,Jerry-Ge,1220944239,2022-08-19 17:46:19,"Alright, it seems the `Py+CPP` passed here. While `Windows Bazel GPU — Internal CI build failed
` is failing everyone else which I believe it's not due to my fault?",while  Windows Bazel GPU — Internal CI build failed  is failing everyone else which I believe it's not due to my fault?
57020,cheshire,1220932663,2022-08-19 17:40:08,"Infra got into a broken inconsistent state somehow.
Unfortunately, our external compiler warnings don't have -Werror, so got this:
```
ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]
module->RemoveEmbeddedComputation(old_computation);
^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~
```","""ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]"""
57161,Jerry-Ge,1220813138,2022-08-19 15:34:10,"Taking a note here: the large constant values has to be written like `0x5{{.*}}` in the mlir test files, otherwise it causes the pattern match failures.","Taking a note here: the large constant values has to be written like 0x5.* in the mlir test files, otherwise it causes the pattern match failures."
57222,stridge-cruxml,1220528527,2022-08-19 10:44:57,"@tilakrayal thanks for the quick response!
Yes, I found that work around. However this is not ideal as it loads as tensorflow model not keras model.
The other work around (if its just a shortcut layer) is to use `tf.keras.layer.Layer()` instead of Sequential().
But my purpose of reporting this, is this is a case that should work and should be supported but isn't.","""I found that work around"""
56745,sharadmv,1219947305,2022-08-18 20:49:24,I'm going to close this PR since after discussing offline w/ @hawkinsp we should avoid including CUDA in non-GPU builds.,I'm going to close this PR since after discussing offline w/ @hawkinsp we should avoid including CUDA in non-GPU builds.
56942,drivanov,1219870810,2022-08-18 19:30:27,"@bixia1 : I fixed the missing virtual destructor issue by excluding the virtual `AllowedDataType` method.
Now the allowed data types are determined by the second input parameter of the constructor:
```
explicit OpConverterBase(const OpConverterParams* params,
const std::vector<DataType>& data_types =
{DataType::DT_FLOAT, DataType::DT_HALF})
```","""I fixed the missing virtual destructor issue by excluding the virtual AllowedDataType method"""
57197,gzmkl,1219735297,2022-08-18 17:12:17,this workaround is not needed,This workaround is not needed.
55746,stewartmiles,1219707646,2022-08-18 16:41:09,"@JunyoungLim I'm now back to waiting on ""Code Check - Changed Files"" and ""Py+CPP Test Suite - Ubuntu CPU, Python 3.9"" to run but they look like they're hung. Is it possible those other two workflows depend upon AMD ROCm completing successfully?","I'm now back to waiting on ""Code Check - Changed Files"" and ""Py+CPP Test Suite - Ubuntu CPU, Python 3.9"" to run but they look like they're hung."
55766,Yulv-git,1219645913,2022-08-18 15:39:51,"> Please fix more than just one single letter typos.
Sorry, I have nothing else to fix at the moment.",Please fix more than just one single letter typos.
57161,Jerry-Ge,1219630143,2022-08-18 15:24:41,@jpienaar It fails the `ARM CI Extended / build (3.10) (pull_request) ` test where I saw a lot of FLAKY keywords there. Is there anyway to re-run those tests and could I have the permission to trigger the test runs here for efficiency? Tks,I saw a lot of FLAKY keywords there.
57146,bermeitinger-b,1219596622,2022-08-18 14:55:23,"PR #57186 fixes the compilation issue for me.
However, I don't know if this has any implication on the outcome.
I don't know if it's related or another issue but the output at import time is: ```
E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
```
(I have built it with cuda enabled as well.)",E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
57201,ejboettcher,1219572693,2022-08-18 14:33:55,"@tilakrayal I thought that you and colab were the same group. My bad.
**BUT** could you add a co-lab work around in your tutorial until co-labs get their act together.
Take care, Issue open for me but closed for you.","""co-labs get their act together"""
56692,nyngwang,1219452923,2022-08-18 12:49:24,"Hi, I'm not the staff of the Repo. but if you got the error because you put the `tf.recompute_grad` around the `__call__` or `call`(if you use `tf.keras`) method, then you might try to create a nested function inside `__call__` (or `call`) and wrap it with `tf.recompute_grad`:
```python
def call(self, x):
@tf.recompute_grad
def foo(x):
# original content
return foo(x)
```","if you got the error because you put the tf.recompute_grad around the __call__ or call(if you use tf.keras) method, then you might try to create a nested function inside __call__ (or call) and wrap it with tf.recompute_grad: python def call(self, x): @tf.recompute_grad def foo(x"
57064,bogdantmm92,1219405293,2022-08-18 11:59:38,"It looks that Conv2D filter doesn't work with the tflite option `options.setUseNNAPI(true)`. If I remove this line `options.setUseNNAPI(true),` the app runs fine",options.setUseNNAPI(true)
57201,ejboettcher,1219348749,2022-08-18 10:52:52,"@tilakrayal Please run your lab in google Co-lab with GPU enabled. It does not work in Google's colab.
That is the issue. It will only run in Google's colab if you switch to version 2.7 (with GPU) or run it without GPU in version 2.9. The only control I have in Co-lab is to run with and without GPU. I really do not have control of the environment :-(. I wish there was a requirement.txt file so that I can run it.","""I really do not have control of the environment :-(."""
57192,tilakrayal,1219105493,2022-08-18 6:54:42,"@sachinprasadhs,
I was able to reproduce the issue on tensorflow v2.9 and nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/775dc8ff4e3d8bb2b7c7dfbac78db7d3/untitled502.ipynb) and the issue was reproducible on all int datatype(**int8, int16, int32, int64)**, but was able to execute without any issue on other datatypes (**bfloat16, half, float32, float64, complex64, complex128)**","""I was able to reproduce the issue on tensorflow v2.9 and nightly"""
56519,enor2017,1219004673,2022-08-18 3:32:58,"@sachinprasadhs It's ok for me currently. However, if the issue is hard to fix, would you like to modify the documentation to remove support for the `half` type? That causes an inconsistency.",causes an inconsistency.
56860,njuhang,1218873420,2022-08-18 1:28:49,"Emm, there is nothing to do with pytorch. I need such a LSTM layer, when executing inference, it use functions defined in tensorflow/lite/kernels/lstm.cc. But currently I can only get Unidirectional LSTM layer, which use functions defined in tensorflow/lite/kernels/unidirectional_sequence_lstm.cc. My code has been attached above.
My question is how to get a .tflite model, with such a LSTM layer.","""I need such a LSTM layer, when executing inference, it use functions defined in tensorflow/lite/kernels/lstm.cc. But currently I can only get Unidirectional LSTM layer, which use functions defined in tensorflow/lite/kernels/unidirectional_sequence_lstm.cc. My code has been attached above. My question is how to get a .tflite model, with such a LSTM layer."""
55746,JunyoungLim,1218748411,2022-08-18 0:21:57,"Hi Stewart, I just checked that AMD ROCm error has been happening in other PRs as well. Although we might need to fix this eventually, for now the ROCm check failure is not blocking the PR to be submitted.
To unblock this PR from the remaining checks that are stuck, can you try pushing an empty commit to this PR so that we can re-trigger the builds in CI?
https://stackoverflow.com/a/58190576/6096472","""I just checked that AMD ROCm error has been happening in other PRs as well."""
56300,JXRiver,1218649702,2022-08-17 23:30:49,"`tape.batch_jacobian` and `tf.vectorized_map` (API of pfor) both don't support RaggedTensor at this moment. We are planning to support both of them. Before that happen, you may try to pad RaggedTensor to tensor.",tf.vectorized_map (API of pfor) both don't support RaggedTensor at this moment.
56471,LostBenjamin,1218351901,2022-08-17 18:22:24,"Hi,
Our automatic tool detected this statically. We manually checked it and identified it as a bug, because `self.node_index` is a `dict` (for example, see [this line](https://github.com/tensorflow/tensorflow/blob/87c892726d1ca89d24d4e384482a1c6bbbb4e1aa/tensorflow/python/autograph/pyct/cfg.py#L349)) and thus the correct operator should be `in`. I am afraid that we cannot add a test as we are not familiar with the codebase.
Best,
Jingxuan","""We manually checked it and identified it as a bug, because self.node_index is a dict (for example, see [this line](https://github.com/tensorflow/tensorflow/blob/87c892726d1ca89d24d4e384482a1c6bbbb4e1aa/tensorflow/python/autograph/pyct/cfg.py#L349)) and thus the correct operator"
57079,mihaimaruseac,1218289448,2022-08-17 17:16:20,"Please sing CLA, please change title to something not ""update <file>"", please revert trivial incorrect changes.
See https://cbea.ms/git-commit/ for how to better title the PR and the commits.","""Please sing CLA, please change title to something not ""update file>"", please revert trivial incorrect changes."""
40366,OrigamiDream,1218128353,2022-08-17 14:59:33,"I'm using TensorFlow 2.9.1, and I found a workaround.
Do not define the metrics within the scope.
```python
strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
model = ...
loss_fn = ...
optimizer = ...
# Out of scope, this works.
metrics = tf.keras.metrics.Mean(name='total_loss')
```
This works when you are using **custom training loop**.
I didn't test on `Model#fit()` because I have no enough time.
The model should not be compiled within the `MirroredStrategy` scope.","""I'm using TensorFlow 2.9.1, and I found a workaround"""
57146,bermeitinger-b,1218083227,2022-08-17 14:25:06,"I have the same issue but for `tensorflow-2.10.0-rc1`. Versions `2.9.1` and `2.8.2` compile fine. My GCC version is 9.4.0 (from Ubuntu 20.04).
The above hint with adding `--copt=-DINTEL_MKL_ML_ONLY` doesn't help, it can't find the file `""third_party/intel_mkl_ml/include/mkl_cblas.h""`","can't find the file ""third_party/intel_mkl_ml/include/mkl_cblas.h"""
57097,ibiglari,1217694266,2022-08-17 8:42:05,"@misterBart Yes that works. XNN can be switched off for ARM 7.
For other people who stumble across this, you need to install a version of GCC and G++ that matches what you have on your Raspberry Pi. After building the libraries using GCC11, I was not able to link against them on RPi because latest available version of GCC on RPi (at this time) is 10. I had to slightly modify the CMake script and download GCC10 and G++10 for AARCH64","""After building the libraries using GCC11, I was not able to link against them on RPi because latest available version of GCC on RPi (at this time) is 10. I had to slightly modify the CMake script and download GCC10 and G++10 for AARCH64."""
57103,afbpinheiro,1217524535,2022-08-17 6:39:09,"@TamirRozenfeld I had the problem you describe in the last image because I installed my python version via microsoft store. I uninstalled it and installed python via the usual means and it worked.
Note: I also needed to add the relevant miniconda folders to the system path",I had the problem you describe in the last image because I installed my python version via microsoft store.
57146,NeilPandya,1217511525,2022-08-17 6:21:52,"> I think you haven't installed intel_mkl_ml dependencies. Could you find ""mkl_cblas.h"" file in your system?
I searched for it, and it itsn't present. I was under the impression that I installed MKL libraries via Intel OneAPI, installed with the following command:
`sudo apt install intel-basekit`
as per [these instructions](https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl-download.html?operatingsystem=linux&distributions=aptpackagemanager).","I was under the impression that I installed MKL libraries via Intel OneAPI, installed with the following command: sudo apt install intel-basekit as per [these instructions](https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl-download.html?operatingsystem=linux&distributions=aptpackagemanager)."
56127,rdzhabarov,1217268588,2022-08-16 23:24:44,@mdfaijul the test was failing when i fixed that. So i expect it to fail with your change.,"""I expect it to fail with your change."""
56127,rdzhabarov,1217253198,2022-08-16 22:58:43,"@mdfaijul can you check tensorflow/core/transforms/remapper/tests/contraction.mlir?
EDIT: I changed `-remapper` to `-tfg-remapper` but it's still failing.",-remapper to -tfg-remapper but it's still failing.
57113,mseth10,1217223395,2022-08-16 22:13:27,"@nitins17 looks like it's missing CLA.. I checked and mine is present, looks like it's missing yours.
Also, after this PR is merged we need to enable more runners to share load. Do you have access to this page? https://github.com/tensorflow/tensorflow/settings/actions/runners/new?arch=arm64&os=linux","I checked and mine is present, looks like it's missing yours."
57020,nouiz,1217016137,2022-08-16 18:47:04,"The windows build was still failed.
So I rebased by PR. Maybe it will allow a clean run.
@cheshire can you approve again?",The windows build was still failed.
49202,nigelparsad,1216942729,2022-08-16 17:32:19,"**Error still persists in tensorflow-macos=2.9.2 and tensorflow-metal=0.5.0.**
Error occurred while attempting to run inference using a Tensorflow 2 version of the Mask R-CNN deep learning neural network architecture for instance segmentation:
2022-08-16 04:06:15.497243: F tensorflow/core/framework/tensor.cc:728] Check failed: IsAligned() ptr = 0x2d5a5cf60
Machine is an M1 Max MacBook Pro.
Message has been posted on Apple Dev Forum: https://developer.apple.com/forums/thread/696907",Error still persists in tensorflow-macos=2.9.2 and tensorflow-metal=0.5.0.**
52845,stephenonethree,1216810422,2022-08-16 15:39:00,"Looks like this problem also comes up if you want to run the Google Cloud Deep Learning Container (part of Vertex AI) `gcr.io/deeplearning-platform-release/tf-gpu.2-9` on an M1 Mac since it has Tensorflow. The error message is:
```
The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.
qemu: uncaught target signal 6 (Aborted) - core dumped
```","The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine."
56423,TimCargan,1216779710,2022-08-16 15:16:38,"Hi @jszaday, sorry for the delay. I have tested with `2.10.0rc0` and `2.10.0rc1` and the issue seems to be persisting. I have tested on configurations with both 1 and 2 A6000's and with 32G and 64G of ram, it always seemed to fail at the same point. I also tried explicitly distributing dataset as per your example code but that still has the same issue.",I have tested with 2.10.0rc0 and 2.10.0rc1 and the issue seems to be persisting.
52845,hanaa-rft,1216776257,2022-08-16 15:13:45,"I've tried `Tensorflow 2.3.1 ` and I still get `F tensorflow/core/lib/monitoring/sampler.cc:42] Check failed: bucket_limits_[i] > bucket_limits_[i - 1] (0 vs. 10)
qemu: uncaught target signal 6 (Aborted) - core dumped`
Any suggestions would be great - thanks.","""F"""
55608,Nyrio,1216651700,2022-08-16 13:38:16,"The pylint error is in the existing code, not in my branch. See #57178","The pylint error is in the existing code, not in my branch."
56970,Abdullah-208,1216638396,2022-08-16 13:27:17,"@rolfvandam actually I am training a custom object detection model to use it in android application.
I have already trained the model but can't export it in tflite format!","""Can't export it in tflite format!"""
56970,rolfvandam,1216629359,2022-08-16 13:19:59,"> I am currently facing this issue.
> any solution or alternative?
I've switched to a different platform for object detection (YOLOv5 which is pytorch based).",I am currently facing this issue.
57177,Abdullah-208,1216571597,2022-08-16 12:30:33,Closing this issue as duplicate,Closing this issue as duplicate.
51590,OllyJ918,1216487693,2022-08-16 11:07:36,"I had to do the following for it to work, I must have installed nightly builds and it resulted in mismatches, but this worked for me: ```
pip install tflite-model-maker==0.4.1
pip install tflite-support==0.4.1
```
I think I had a nightly version of tflite-support which was causing problems on writing the metadata to the model.",I think I had a nightly version of tflite-support which was causing problems on writing the metadata to the model.
57020,cheshire,1216403307,2022-08-16 9:40:31,"> `ERROR: T:/src/github/tensorflow/tensorflow/python/util/BUILD:189:27: Linking tensorflow/python/util/_pywrap_checkpoint_reader.so failed: (Exit 1120): link.exe failed: error executing command`
Windows build has failed. Let me try to rerun it.",Error: T:/src/github/tensorflow/tensorflow/python/util/BUILD:189:27: Linking tensorflow/python/util/_pywrap_checkpoint_reader.so failed: (Exit 1120): link.exe failed: error executing command
56604,namrata-ibm,1216403083,2022-08-16 9:40:19,"@gbaned @haozha111 This is yet to merge, could you please check once?","""This is yet to merge, could you please check once?"""
57155,bluetail14,1216329172,2022-08-16 8:46:45,"I have images in my train and test datasets,x_train and x_test, and 1 and 0 as labels in y_train and y_test. so I cant really remove the 0 labels?","""I have images in my train and test datasets,x_train and x_test, and 1 and 0 as labels in y_train and y_test. so I cant really remove the 0 labels?"""
57161,eric-k256,1216005246,2022-08-16 0:14:21,"I'd expect test_softmax_qi8 in tensorflow/compiler/mlir/tosa/tests/tfl-to-tosa-pipeline.mlir to fail with this change. The test code itself is fine, but the CHECK lines will need to be updated to match what the new softmax code is generating. The non int8 softmax should continue to work as is.","""The test code itself is fine, but the CHECK lines will need to be updated to match what the new softmax code is generating."""
36330,nyngwang,1215582505,2022-08-15 18:27:33,@rmothukuru The link you provided is not related to the title. This thread is about GPU memory usage while the linked answers are about how to visualize the computation graph of the given model.,The link you provided is not related to the title.
56127,rdzhabarov,1215297300,2022-08-15 16:25:41,"@mdfaijul these are the failures
```
tensorflow/core/transforms/remapper/remapping_helper.h:26 ClangTidy: do not use unnamed namespaces in header files
tensorflow/core/transforms/remapper/remapping_helper.h:65 ClangTidy: unused function 'GetTfgOpName'
```
Let's fix these. If there will be yet another failures I'll take a look and will be fixing internally myself.",let's fix these. If there will be yet another failures I'll take a look and will be fixing internally myself..
57064,bogdantmm92,1214777493,2022-08-15 9:04:25,Attaching a [link](https://drive.google.com/file/d/1SumAwiVQx36fkbBwZ9zW8Mcg7_3PDiE7/view?usp=sharing) to the tflite model that crashes on Android.,Attaching a [link](https://drive.google.com/file/d/1SumAwiVQx36fkbBwZ9zW8Mcg7_3PDiE7/view?usp=sharing) to the tflite model that crashes on Android.
20698,jiunyen-ching,1214438183,2022-08-14 19:49:22,@MatanSandori I think this method will work only if the inputs are of the same shape?,I think this method will work only if the inputs are of the same shape?
57013,mihaimaruseac,1214187656,2022-08-13 16:44:14,It did not show as merged here because copybara sync was broken. Fixed in big squash commit 49f97f135a2e1d5d22e60d2a80ec668d53f9708a,"""Copybara sync was broken"""
46776,GabrielGlzSa,1213636302,2022-08-13 2:34:33,"I am using a custom constraint with tf.reshape and it causes the error when using @tf.function.
https://colab.research.google.com/drive/1nD8j4j387lyPsrucTXw1wJoxUoPXM1nx?usp=sharing",I am using a custom constraint with tf.reshape and it causes the error when using @tf.function.
56485,bixia1,1213557911,2022-08-12 22:14:32,I am waiting for the author to address my comments.,I am waiting for the author to address my comments.
56127,rdzhabarov,1213432795,2022-08-12 19:09:21,"@mdfaijul i see some trivial failures, could you fix those?
* pass.cc:44 ClangTidy: do not use namespace using-directives; use using-declarations instead
* pass.cc:121 ClangTidy: missing [#include] <string> for 'std::string'
* remapping_helper.h:63 ClangTidy: missing [#include] <string> for 'std::string'","""I see some trivial failures, could you fix those?"""
57115,reedwm,1213370992,2022-08-12 17:56:38,"An internal test failed with the infamous ""No algorithm worked!"" error. I'll try to get an example to reproduce.
/CC @jlebar FYI (not sure why you weren't automatically CCed)","""No algorithm worked!"" error"
56228,aljungberg,1213284822,2022-08-12 16:13:24,"Just an idea but I don't think this looks right:
```
if shard_num and shard_index:
dataset = dataset.shard(shard_num, shard_index)
```
One of the shard indexes will be 0 (which evaluates as false), so it won't use any sharding for that one generator. As a result, some of your training data is lost because the unsharded generator outputs the beginning of the sequence, rather than a strided subset.",just an idea but I don't think this looks right:
57130,mihaimaruseac,1213231897,2022-08-12 15:25:42,I think these translations should go into tensorflow.org website. We cannot create readme files for each language in the world and expect them to be in sync.,I think these translations should go into tensorflow.org website. We cannot create readme files for each language in the world and expect them to be in sync.
57106,jarussell,1213214427,2022-08-12 15:06:31,"I tried rebuilding with `pip install numpy~=1.20` and using the same numpy version in my container with the same results. There is no need for `sudo` because this is being installed in a docker container that only has `root`. Apparently `1.22.4` is within the range of `~=1.20` so it did not change anything.
Edit: It looks like `numpy~=1.20` installs `1.23.x` in the build and that may lead to problems in running since later dependent packages use `1.22.4`. I will try rebuilding with `==1.21`.",I tried rebuilding with pip install numpy=1.20 and using the same numpy version in my container with the same results.
40457,carlthome,1213046147,2022-08-12 12:11:10,"Hmm, I also got tripped up on this now in 2.9.1. I'm using tf.data and return minibatches on the format `Tuple[Dict[str, tf.Tensor], Dict[str, tf.Tensor]]` with (features, targets).
Since that format ""just works"" with losses, metrics and optimizers, I expected it to also work for class_weight, but alas I need an additional `tf.data.Dataset.map(lambda features, targets: (features[input_key], targets[output_key]))` to use class_weight. Bit of a gotcha IMO.","""I'm using tf.data and return minibatches on the format Tuple[Dict[str], tf.Tensor], Dict[str, tf.Tensor]]"""
57120,bmharper,1213039776,2022-08-12 12:03:29,"Hi @mohantym,
I've tried 2.8.0 and 2.9.0 of the hexagon delegate, and I get the exact same error messages.
The GPU delegate runs without errors, but I specifically need the hexagon support.
My device is rooted, and I run with `setenforce 0`.
Like I said, I had this working for weeks.","""I've tried 2.8.0 and 2.9.0 of the hexagon delegate, and I get the exact same error messages."""
39632,HectorSVAI,1213030540,2022-08-12 11:51:14,"Getting now NotImplementedError: unable to open file: libtensorflow_io.so, from ... venv/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZNK10tensorflow10FileSystem8BasenameEN4absl12lts_2021110211string_viewE'] on tensorflow gpu nightly 2.10 self-compiled","Getting now NotImplementedError: unable to open file: libtensorflow_io.so, from ... venv/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZNK10tensorflow10FileSystem8BasenameEN4absl12lts_2021110211string_viewE'] on tensorflow gpu"
57103,TamirRozenfeld,1212962183,2022-08-12 10:27:15,"> Hi @TamirRozenfeld, Use Conda to install CUDA and cuDNN, which takes care the path setting.
> > ```
> conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0
> python3 -m pip install tensorflow
> # Verify install:
> python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""
> ```
I did but the gpu isn't compatible ![image](https://user-images.githubusercontent.com/108065131/184336220-f40c8c7e-bdbf-46d5-998d-5775b098a480.png)
i have some missing files",I did but the gpu isn't compatible
56997,MariumAZ,1212927398,2022-08-12 9:44:36,"@kfrancischen in my case the `tf.keras.models.load_model` doesn't solve the pb, for saving the model and loading it with tf 2.9 everything works fine but when I switch to tf 2.6 it is not the case .. for more info I am using the subclassing API to build the model.","""tf.keras.models.load_model"" -> ""doesn't solve the pb"""
57116,drivanov,1212701367,2022-08-12 3:45:35,"@bixia1: It is just a first draft. The test doesn't even work. In the meantime, it makes sense to review the refactoring of `tensorflow/python/compiler/tensorrt/test/BUILD`",The test doesn't even work.
57092,lzr-official,1212637924,2022-08-12 1:13:01,"It's not only the `DeregisterGraph`, there are also other methods without `CallOptions`.
`GRPC_FAIL_FAST` has also been tried, but it doesn't always work for unknown reason.","""it doesn't always work for unknown reason."""
57097,ibiglari,1212544821,2022-08-11 22:06:29,"@sachinprasadhs I can run those commands to create the library on Ubuntu, but the result is a library for the CPU architecture that's executing those commands, and not for ARM.","I can run those commands to create the library on Ubuntu, but the result is a library for the CPU architecture that's executing those commands, and not for ARM."
57092,haoyuz,1212535546,2022-08-11 21:55:52,"> The caller `DeregisterGraphAsync` doesn't have `CallOptions` in its parameters or in the request message.
Yes, but how could a deregister graph RPC fail with timeout? There is no blocking logic in handling `DeregisterGraph` RPCs, so I guess it's timing out because the remote worker is down? Should it set `GRPC_FAIL_FAST` in that case?",The caller DeregisterGraphAsync doesn't have CallOptions in its parameters or in the request message.
56127,rdzhabarov,1212489263,2022-08-11 21:02:39,"@penpornk could you help merging this? I do not see to have a permission to ""merge pull request"".","I do not see to have a permission to ""merge pull request""."
57082,Shinwazu,1212306156,2022-08-11 17:59:56,"The access link keeps changing. Please mail me or comment when you want access so that I can provide you with the latest link. I hope you understand. Kind regards,
SHINWAZU",The access link keeps changing.
57091,bixia1,1212149329,2022-08-11 15:30:50,"@DEKHTIARJonathan Can you double check your PR description, I think you meant
Behavior can be deactivated using export TF_TRT_EXPERIMENTAL_FEATURES= disable_logger_filtering","Can you double check your PR description, I think you meant Behavior can be deactivated using export TF_TRT_EXPERIMENTAL_FEATURES= disable_logger_filtering."
57103,TamirRozenfeld,1212118219,2022-08-11 15:05:21,"""Quick fix is to download 'cudart64_110.dll' from [dll-files.com ](https://www.dll-files.com/cudart64_110.dll.html)and paste it into cudatoolkit/bin file location."" **Done**
After I've seen your response I followed the instructions from [here](https://www.tensorflow.org/install/pip#windows) what i did is installing Cuda 11.2 and CudNN 8.1 and copy paste cudart64_110.dll.html to the bin file , Microsoft Visual up to date and NVIDIA® GPU drivers as well, I still have the same problem.",I still have the same problem.
57099,sandeep5193,1211971816,2022-08-11 13:12:52,"Hi @mohantym,
The document you have linked talks about using the delegate through org.tensorflow:tensorflow-lite-gpu module, while I am inclined with using them through [Play Services](https://www.tensorflow.org/lite/android/play_services). TensorFlow Lite available in the Google Play services API doesn't have any way to check if the device is compatible for GPU delegate.","""The document you have linked talks about using the delegate through org.tensorflow:tensorflow-lite-gpu module, while I am inclined with using them through [Play Services](https://www.tensorflow.org/lite/android/play_services). TensorFlow Lite available in the Google Play services API doesn't have any way to check if the device is compatible for GPU delegate."""
56088,mseth10,1211856818,2022-08-11 11:21:38,"@elfringham I see that we have removed `pad_op_test_{cpu/gpu}` from the arm skip-test list. While these tests pass on master branch, they are [failing](https://github.com/tensorflow/tensorflow/actions/workflows/arm-ci.yml?query=branch%3Ar2.10) on r2.10 branch. Is there a fix on master branch that needs to be backported to r2.10 branch for these tests?","""I see that we have removed pad_op_test_cpu/gpu from the arm skip-test list. While these tests pass on master branch, they are [failing]"""
57097,ibiglari,1211757568,2022-08-11 9:38:34,"> Without analyzing too deeply, I see in your first log: `arm-linux-gnueabihf-g++ -march=armv7-a -mfpu=neon-vfpv4` Which seems to imply you want to build for the Armv7a, which is a 32-bits platform, with hardware floating-point support. Yet in your 'Standalone code to reproduce the issue' I see you use `aarch64-linux-gnu-`, which is a 64-bits Arm compiler for the Armv8 platform. This may explain your ""Exec format error"".
I need both 32bit and 64 bits of the library, hence trying both commands.","""Exec format error"""
57097,misterBart,1211744255,2022-08-11 9:23:45,"Without analyzing too deeply, I see in your first log:
`arm-linux-gnueabihf-g++ -march=armv7-a -mfpu=neon-vfpv4`
Which seems to imply you want to build for the Armv7a, which is a 32-bits platform, with hardware floating-point support. Yet in your 'Standalone code to reproduce the issue' I see you use `aarch64-linux-gnu-`, which is a 64-bits Arm compiler for the Armv8 platform. This may explain your ""Exec format error"".","""Exec format error"""
57056,cantonios,1211049505,2022-08-10 17:49:13,"@olipinski which test is failing in your local install?
For your PR, gradients are only supported for floating-point types, so you'll need to modify the `testGradientsAll` test to something like:
```
def testGradientsAll(self):
for dtype in _TEST_DTYPES:
# Gradients not supported for integer types.
if not dtype.is_integer:
self._testGradientsSimple(dtype)
self._testGradientsSimpleVariable(dtype)
```",# Gradients not supported for integer types
57020,nouiz,1210759603,2022-08-10 14:30:40,"> No it was old, are you saying previously you saw PR being committed as multiple separate commits? I don't think that was ever the case?
Check this old PR of mine: https://github.com/tensorflow/tensorflow/pull/51558/
Its merge commit is: https://github.com/tensorflow/tensorflow/commit/f08d120097b070ad972948c7db3544aaaa6296a1
Then do: `git log --graph f08d120097b070ad972948c7db3544aaaa6296a1`
My individual commits are in the log","""I don't think that was ever the case"""
57064,bogdantmm92,1210591325,2022-08-10 12:16:27,"Hey @mohantym that doesn't seem to help. The app crashes when initializing the interpreter. Commenting out this line `options.setUseNNAPI(true)` fixed the crash but not the predictions.
I get the same random predictions. Could it be something in the format of the pixels fed to the tflite model?","""That doesn't seem to help"""
57010,maxhgerlach,1210520650,2022-08-10 11:06:39,"@gadagashwini, I see the same issue as OP, MacBook Pro 2021 with M1 Pro (aarch64), macOS 12.5.
TensorFlow is installed via the package `tensorflow-macos==2.9.2`, no `tensorflow-metal` needed.
The issue may very well be specific to arm64 (Apple Silicon). In that case you won't see it with an Intel CPU.","I see the same issue as OP, MacBook Pro 2021 with M1 Pro (aarch64), macOS 12.5."
57020,cheshire,1210462248,2022-08-10 10:13:55,"> I fixed all the comments in their respective commits and updated each commit description.
When the system imports the PR, it squashes all the commits into one. It squashes the commit descriptions as well. This usually results in very confusing commits messages. Would be best to have a single commit, with a single descriptive message.","""This usually results in very confusing commits messages."""
52357,Hastyrush,1210446728,2022-08-10 10:01:03,"Same issue with a Resnet based backbone feature extractor model initialized and trained from timm in pytorch. Pytorch > Onnx > Tensorflow PB with all minimal accuracy loss. Tflite FP32 and Dynamically quantized INT8 works fine as well, but statically quantized INT8 model totally fails with 0AP",Same issue with a Resnet based backbone feature extractor model initialized and trained from timm in pytorch.
57056,olipinski,1210341366,2022-08-10 8:32:08,"@cantonios I have added the split test, however it does not seem to pass, even on the release branch on my local install.","I have added the split test, however it does not seem to pass, even on the release branch on my local install."
55344,aaronhsueh0506,1210308845,2022-08-10 8:03:14,"Hi, I have same issue on ubuntu 18.04.
I try to follow the command on tensorflow.org
`cmake ../tensorflow_src/tensorflow/lite/c`
`cmake --build . -j`
And I get a libtensorflow_c.so in tflite_build folder.
But when I compile my main.cpp in my project, the error message like this issue, lots of undenfined reference.
Is this have any update?","""I have same issue on ubuntu 18.04"""
22770,ebrevdo,1210193276,2022-08-10 5:53:02,"(that said, the new tf2 code calls `tf.disable_v2_behavior()`, which drops you back in TF1-mode. I don't know the support story for this anymore...)","""That said, the new tf2 code calls tf.disable_v2_behavior(), which drops you back in TF1-mode."""
56863,eeDigitalSeal,1210123913,2022-08-10 3:44:49,"@cantonios Thanks for your reply. But I think if the `input1` is inappropriate, it should reject the input (like throwing exception) when computing gradients for `input1`?","""But I think if the input1 is inappropriate, it should reject the input (like throwing exception) when computing gradients for input1?"""
57070,osayes,1209970665,2022-08-09 22:54:18,"able to detect the gpu device but not use it, one reasonable reason is that the gpu has too low performance(384 pascal core with 64bit gddr5), and there are extra overhead when using gpu to compute, so cpu is a better choice for tf","able to detect the gpu device but not use it, one reasonable reason is that the gpu has too low performance(384 pascal core with 64bit gddr5), and there are extra overhead when using gpu to compute, so cpu is a better choice for tf."
56982,sachinprasadhs,1209947927,2022-08-09 22:18:24,"`tf.keras.losses.MSE` is doing the casting and it is converting to complex128 in both the cases, but in `grad_backward` when you calculate the gradient with respect to x, which is float32, it casts back to float32, that is the reason you're getting float32 in `grad_backward` and `complex128` in `res_forward`. To maintain the consistency, you can include manual casting to the `dtype` which you require.","""tf.keras.losses.MSE"" is doing the casting and it is converting to complex128 in both the cases, but in grad_backward when you calculate the gradient with respect to x, which is float32, it casts back to float32, that is the reason you're getting float32 in grad_backward and complex128 in res_forward. To maintain the consistency, you can include manual casting to the dtype which you require.."""
56852,Feynman27,1209832360,2022-08-09 20:18:28,"No, there shouldn't be any assert defined in any model layer.","No, there shouldn't be any assert defined in any model layer."
57044,penpornk,1209689295,2022-08-09 17:48:07,"~~It's failing again after everything is done though:~~
![Screen Shot 2022-08-09 at 10 47 01 AM](https://user-images.githubusercontent.com/38085909/183723926-cab57351-065c-43da-9e1a-eed0e4a1ff66.png)
~~I'll try testing it internally.~~
Edited to add: Sorry my page was stale. After refreshing, it also passed for me! :)",It's failing again after everything is done.
56628,cantonios,1209475737,2022-08-09 14:44:34,"@VictoriaGriffith yes, we're trying to decide what to do about that. Thanks for bringing this up. We have the same ""issue"" across all back-ends: that `gather_nd` supports a single index tuple, but `scatter_nd` does not. However, it's not currently a high priority for us, since there is a work-around.","""We have the same ""issue"" across all back-ends: that gather_nd supports a single index tuple, but scatter_nd does not."""
56766,Petros626,1208964802,2022-08-09 6:24:55,"right EfficientDet is for **TFLite**, but I think some users have tried to use for examplE `Faster R-CNN` in combination with the `graph_rewriter` in the specific `config file` to apply **aware-quantization**, but then fail on converting it from `.pb` file to `.tflite`.","""Fail on converting it from .pb file to .tflite."""
56974,rjchee,1208942747,2022-08-09 5:48:00,"`f` is a variable name, not the file name. If you do `f = tempfile.NamedTemporaryFile`, it creates a **new** file with a different name, so writes to it won't affect the first file. I modified your [gist](https://colab.research.google.com/drive/1_wx2Rdqpzt0Spbj5WIKdIB1_WstF1yMZ?usp=sharing) so that it prints the names of the files being used. I also added a version which doesn't create a new `NamedTemporaryFile`. Please refer to the last cell which demonstrates the bug.","f is a variable name, not the file name. If you do f = tempfile.NamedTemporaryFile, it creates a **new** file with a different name, so writes to it won't affect the first file. I modified your [gist](https://colab.research.google.com/drive/1_wx2Rdqpzt0Spbj5WIKdIB1_WstF1yMZ?usp=sharing) so that it prints"
56804,derrkater,1208546639,2022-08-08 19:55:31,"@haozha111 the model always operates on this datatype (float32) and shape ([1,3,416,416]). Onnx version works just fine and it breaks after being transferred to tflite.","""Onnx version works just fine and it breaks after being transferred to tflite."""
56974,rjchee,1208457553,2022-08-08 18:21:14,"It looks like in the gist, you use a different `NamedTemporaryFile` each time. These are different files with different names, so we would not expect any caching. The case I am looking at is if you use the **same** filename for two different `StaticHashTables`. Could you please check that case?","""The case I am looking at is if you use the **same** filename for two different StaticHashTables."""
56495,EnricoMi,1208248822,2022-08-08 15:04:49,I have removed `__exit__` from dispatcher and worker.,I have removed __exit__ from dispatcher and worker.
30418,nyngwang,1208244559,2022-08-08 15:01:34,"@ppham27 Could you provide a minimal example of how to use it in graph mode? I have read the test case provided in your repo, but I still have the same result as @Apprisco.
@sanatmpa1 I think this thread should be reopened because all the methods mentioned above not working on the latest version (2.9.1).","I have read the test case provided in your repo, but I still have the same result as @Apprisco."
56904,tolsicsse,1208181247,2022-08-08 14:09:58,"I reverted back to 2.8, then it work.","I reverted back to 2.8, then it work."
57037,vladimircape,1208069232,2022-08-08 12:33:30,"@tilakrayal please close issues, it was a problem not only with tensorflow, but also another pip packages","""it was a problem not only with tensorflow, but also another pip packages"""
57031,Raunak-Singh-Inventor,1208035875,2022-08-08 12:01:16,"I want to use my custom model that I have trained in TensorFlow. You can't do that in edge impulse unless you have an corporate account. I'm just a 9th grader, I don't have that privilege. Please suggest a native method through TensorFlow Lite Micro (without using any 3rd party library).","""You can't do that in edge impulse unless you have an corporate account"""
57030,Raunak-Singh-Inventor,1208035309,2022-08-08 12:00:40,"I want to import my custom model. You can't do that in edge impulse unless you have an corporate account. I'm just a 9th grader, I don't have that privilege. Please suggest a native method through TensorFlow Lite Micro (without using any 3rd party library).","""You can't do that in edge impulse unless you have an corporate account"""
57033,code4lala,1207980548,2022-08-08 11:05:21,"I think you may rearrange the dependencies tree to avoid double link, i.e. do not depend on builtin_ops_all_linked again if corss-compiling.","I think you may rearrange the dependencies tree to avoid double link, i.e. do not depend on builtin_ops_all_linked again if corss-compiling."
56918,gbaned,1207965277,2022-08-08 10:49:51,"@chxin66 It still shows CLA is pending, can you please make sure to use same GitHub username and email-id associated with it.","""still shows CLA is pending"""
51573,lailinbin,1207734634,2022-08-08 6:49:27,"I have the same issue. When I updata the python version to 3.9, the issue is still exist. In my opinion, this issue is related to MSVC compiler. Finally, I fit it by replacing the ""complex"" file on MSVC2019 with the same name on MSVC2017.",I have the same issue.
41448,pati-dev,1207459657,2022-08-07 18:15:46,"It has been over 2 years (w00t?!) since this issue was open, and sadly it's still unresolved. Can the team please provide any update on this?
P.S.: Have other folks found any reasonable solution to this as the team doesn't really seem to care about fixing this one anytime soon.","""still unresolved"""
56715,penpornk,1206831771,2022-08-05 20:20:04,@snadampal We are having trouble importing this PR. Could you please help rebase it? Sorry for the inconvenience. :(,:(
56708,Jiayuli-CU,1206178244,2022-08-05 8:19:44,"> @Jiayuli-CU Please try using `tf.experimental.tensorrt.Converter` to convert your saved model. Please take a look at the doc [here](https://www.tensorflow.org/api_docs/python/tf/experimental/tensorrt/Converter). Please let me know if you are still running into the same error. Thanks!
Got the same error message after using `tf.experimental.tensorrt.Converter` follwing the official doc.",Got the same error message after using tf.experimental.tensorrt.Converter follwing the official doc.
1065,shenh10,1206024226,2022-08-05 4:18:08,"> Known -- test is currently not expected to run on GPU. Will keep it open until we fix.
Does the issue be solved?","""Currently not expected to run on GPU"""
56804,derrkater,1205701754,2022-08-04 19:50:20,@mohantym @karimnosseir @sachinprasadhs is there any plan to go back to this problem? I'm stuck without support with this problem,I'm stuck without support with this problem.
56939,sachinprasadhs,1205564129,2022-08-04 17:33:58,"Since it is still not merged and the branch cut for 2.10rc0 happened 2 days ago, it will not make it to 2.10 release, but you can use tf-nightly till the next stable release is made","""It will not make it to 2.10 release, but you can use tf-nightly till the next stable release is made."""
56967,NoteDancing,1205188623,2022-08-04 12:28:09,"@gadagashwini I just tried it on version 2.8.2,I still can't load.","I just tried it on version 2.8.2,I still can't load."
56785,olipinski,1204920257,2022-08-04 8:15:39,"Sorry, I meant if it is not desirable to add it for a single use case of reducing memory consumption. I guess the main reason to do that would be for any local testing of models, which is what my issue was about. I cannot run my models locally, as they are too big in memory usage. As for casting — I'm using Ray, so all the code where I would have to cast it back and forth is not really accessible, without some major changes.
Hope this is clearer!","""I cannot run my models locally, as they are too big in memory usage."""
56885,chengmengli06,1204822143,2022-08-04 6:31:31,"https://github.com/alibaba/EasyRec/tree/fix_mirrored_bug, it can be reproduced with this branch using tensorflow 2.9.1 . The test is skipped in master branch temporarily.","""It can be reproduced with this branch using tensorflow 2.9.1 . The test is skipped in master branch temporarily."""
56967,NoteDancing,1204653727,2022-08-04 1:23:09,"> @7NoteDancing, We can see the file was loaded and saved on the drive. As the warning mentioned, please try to compile the file manually and try to load. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/70eae78f09906322deefbabbfe30f88b/untitled473.ipynb).
I find you used version 2.8.2.","""I find you used version 2.8.2."""
51799,mihaimaruseac,1204634204,2022-08-04 0:48:55,"@mohantym I just said I left TF, please don't reassign me!","I just said I left TF, please don't reassign me!"
56995,philipphack,1204561804,2022-08-03 22:53:45,"It seems that making `selected_algorithm` optional requires expanding the patterns into multiple lines. I previously attempted to use a regex which matches either the string or nothing, but apparently some regex operators are not recognized by FileCheck.","I previously attempted to use a regex which matches either the string or nothing, but apparently some regex operators are not recognized by FileCheck."
56995,jlebar,1204547108,2022-08-03 22:27:10,"I'm saying that we do not need to check for the presence of selected_algorithm on pre-Ampere GPUs. Rather than selectively removing it, we can make it optional in the test, or otherwise make it so that the test doesn't check for it.
Removing it with a complex search-and-replace on a string is complicated and not something I can approve, though.","""Removing it with a complex search-and-replace on a string is complicated and not something I can approve, though."""
56926,sjarus,1204417451,2022-08-03 20:01:59,@jpienaar (or @rsuderman as Jacques said he's going to be away) would you kindly unblock this ? Jerry caught a failing test and we just worked through its fix.,"""Jerry caught a failing test and we just worked through its fix."""
56928,sachinprasadhs,1204292170,2022-08-03 17:54:35,Could you try the option to `setUseNnapiCpu` to `true` in the `NnApiDelegate.Options` object and let us know if it is still crashing.,Could you try the option to setUseNnapiCpu to true in the NnApiDelegate.Options object and let us know if it is still crashing.
46178,rish2497,1203828660,2022-08-03 11:31:24,"Facing a similar issue created a new environment and installed all the required packages using conda-mini forge. Now the issue I'm getting is below.
`(TeleStoke) rish@DSIs-MBP Server % python app.py zsh: illegal hardware instruction python app.py
`",Facing a similar issue created a new environment and installed all the required packages using conda-mini forge. Now the issue I'm getting is below.
56860,njuhang,1203633766,2022-08-03 8:16:45,"BTW, I can't get a LSTM model from pytorch->onnx->tflite.","""I can't get a LSTM model from pytorch->onnx->tflite."""
54390,abattery,1203602581,2022-08-03 7:45:32,Could you sync again? The current sync head wasn't green enough.,The current sync head wasn't green enough.
56599,gowthamkpr,1203289972,2022-08-02 22:53:25,"@jhogg11 As mentioned [here](https://github.com/keras-team/keras/issues/16495#issuecomment-1150414919), KerasRegressor is deprecated and we are no longer maintaining/bug fixing it. Please forward this issue to https://github.com/adriangb/scikeras",KerasRegressor is deprecated and we are no longer maintaining/bug fixing it.
56119,bhack,1203109136,2022-08-02 19:09:02,"@gbaned We could not merge this.
@mdanatg We lost the PR... What you want to do?",We could not merge this.
56563,cantonios,1202863929,2022-08-02 15:44:48,"It shouldn't make a difference...
Looks like numpy is generating a `float64` tensor, so we have more precision bits to resolve the sum/mean. The tf version generates a `float32` tensor. Using `float32` with numpy will also lead to the same issue.",Using float32 with numpy will also lead to the same issue.
45663,Flamefire,1202749371,2022-08-02 14:55:07,"Test has only moved here: https://github.com/tensorflow/tensorflow/blob/v2.9.1/tensorflow/python/kernel_tests/array_ops/batch_scatter_ops_test.py#L86
And code looks still the same: https://github.com/tensorflow/tensorflow/blob/v2.9.1/tensorflow/core/kernels/scatter_nd_op_gpu.cu.cc#L201
https://github.com/tensorflow/tensorflow/blob/v2.9.1/tensorflow/core/framework/register_types.h#L195-L196
So yes the issue persists.",Test has only moved here: https://github.com/tensorflow/tensorflow/blob/v2.9.1/tensorflow/python/kernel_tests/array_ops/batch_scatter_ops_test.py#L86 And code looks still the same: https://github.com/tensorflow/tensorflow/blob/v2.9.1/tensorflow/core/kernels/scatter_nd_op_gpu.cu.cc#
56965,jasaw,1202150088,2022-08-02 8:00:05,"Turns out that I can't just change the default condition to `Aarch64` target because the bazel build script builds for k8 host as well.
I'm using a custom toolchain `//third_party/toolchains/cpus/aarch64:toolchain`.
What's the proper of detecting the current build target is aarch64 so I can set the `cross_arch_defines`?
Any help or pointers is much appreciated.","""Can't just change the default condition to Aarch64 target because the bazel build script builds for k8 host as well."""
47554,MrWangg1992,1202128352,2022-08-02 7:37:00,"Facing the same issues, tried several methods still not working. https://colab.research.google.com/drive/1LuNdGlQ2XdlCNyua_araGr6RYrwhiHPm?usp=sharing","Facing the same issues, tried several methods still not working."
56967,tilakrayal,1202102393,2022-08-02 7:08:24,"@7NoteDancing,
We can see the file was loaded and saved on the drive. As the warning mentioned, please try to compile the file manually and try to load. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/70eae78f09906322deefbabbfe30f88b/untitled473.ipynb).","""Please try to compile the file manually and try to load"""
56606,Flamefire,1202078152,2022-08-02 6:37:52,"Seems CI wants a slightly different formatting. Added that change (only), the other failures are unrelated","Added that change (only), the other failures are unrelated."
56563,cantonios,1202025839,2022-08-02 5:14:02,"Ugh, so I dug into this, and it's because in the first case we're doing a naive linear reduction and eventually run out of precision when adding up elements.
In the second case, when you split the channels, we're doing a more accurate tree reduction.
This is an Eigen ""bug"" - not bug as in it's wrong, but bug as in it could be doing something more accurate.
@rmlarsen it's the `InnerMostDimPreserver` that seems to always do a linear traversal.","Ugh, so I dug into this, and it's because in the first case we're doing a naive linear reduction and eventually run out of precision when adding up elements. In the second case, when you split the channels, we're doing a more accurate tree reduction. This is an Eigen ""bug"" - not bug as in it's wrong, but bug as in it could be doing something more accurate."
56575,cantonios,1201767977,2022-08-01 22:02:08,"Closing, as a work-around has been presented with no response.","Closing, as a work-around has been presented with no response."
56959,Cheeseboy8020,1201248957,2022-08-01 14:03:14,I followed all the steps on that site and still get the same issue. [Here](https://drive.google.com/file/d/17eXgipg8w6cd7Z1K_oi-H5ICTQ4U_bE-/view?usp=sharing) is a video of me following the steps.,I followed all the steps on that site and still get the same issue.
46356,AryanBisht-03,1201203854,2022-08-01 13:26:51,"I'm having this issue too.
Any updates? Please some one help us..!!",I'm having this issue too. Any updates? Please some one help us..!!
33131,aohan237,1200095696,2022-07-30 5:36:13,"@sachinprasadhs ok
as mentioned #56381
[jax like solutions](https://github.com/tensorflow/tensorflow/issues/56381#issuecomment-1166354173)
but
why dont you reply in this issue, so many devs watch this issue ? write down your roadmap on how to solve this issue.","""why dont you reply in this issue, so many devs watch this issue ?"""
34431,VinceJnz,1200079543,2022-07-30 3:37:32,"This introduced ""bug"" impacts a number of object_detection tools.
Is it going to be fixed or is it up to each individual to fix it in their local copy?","""Bug"" impacts a number of object_detection tools."
56949,nitins17,1200054392,2022-07-30 0:59:44,Closing this one as it is invalid. https://github.com/tensorflow/tensorflow/pull/56953 has the version changes for rc0,Closing this one as it is invalid.
56941,nfelt,1199992771,2022-07-29 22:27:13,"Sorry, I don't have the expertise in the profiler to review this code. Perhaps @yisitu can recommend someone on the profiler team to review?",I don't have the expertise in the profiler to review this code.
50575,Seioch,1199539977,2022-07-29 15:29:54,"I am also having this issue. I'm training a model using Attention. I'm running Ubuntu 20.04, Python 3.8.10, TF 2.5.3, NVIDIA GTX 1080 ti. ![image](https://user-images.githubusercontent.com/3291031/181793250-9b8bcf49-0144-414b-86f4-f240c4cc61e8.png)",I am also having this issue.
56623,psunn,1199271387,2022-07-29 13:13:39,"TESTs would pass when compiling with gcc>=8.1, it fails on `gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0`
code snippet to stand out the issue: https://godbolt.org/z/6o76cTM3W","TESTs would pass when compiling with gcc>=8.1, it fails on gcc (Ubuntu 7.5.0-3ubuntu118.04) 7.5.0"
56925,emenshoff,1199038596,2022-07-29 8:56:03,"--target_archs=x86_64,armv7,arm64 option does not work, because x86 platform for TensorFlowLiteSelectTfOps are not supported by Google anymore.","""x86 platform for TensorFlowLiteSelectTfOps are not supported by Google anymore."""
56393,Young768,1198731945,2022-07-28 23:50:22,"Hi @srujun This is the memory profile when running with batch_size=1024, using your method to capture.
![1024](https://user-images.githubusercontent.com/7083506/181655381-0d80f7ad-767d-4e49-86b8-9f0c78e73047.png)
Did you try running a larger ```--num_iter``` with batch_size=512. When I ran with batch_size=512., it hit OOM at around step 50-60. And with batch_size=1024., it hit OOM at around step 20-30.","""It hit OOM at around step 50-60."""
56906,Wanzizhu,1198682336,2022-07-28 22:14:02,"@sachinprasadhs , i am suggesting to change ` 0 < i` to`i < 0` in both `TF_ShapeInferenceContextGetInput` and `TF_ShapeInferenceContextSetOutput`, as `0 < i` is not out of range.","i am suggesting to change  0  i to i  0 in both TF_ShapeInferenceContextGetInput and TF_ShapeInferenceContextSetOutput, as 0  i is not out of range."
56653,bixia1,1198434599,2022-07-28 17:27:45,"@drivanov can you take care of this, it say cla/google failure.","""cla/google failure"""
56673,snadampal,1198386273,2022-07-28 16:40:46,"I rebased yesterday, looks like master moved. Will rebase it again.","I rebased yesterday, looks like master moved. Will rebase it again."
56935,hawkinsp,1198320873,2022-07-28 15:37:38,"@nouiz No, it wasn't. That symptom was memory corruption (missing synchronization?)","No, it wasn't. That symptom was memory corruption (missing synchronization?)"
56931,psunn,1197901121,2022-07-28 9:33:22,TESTs failures seem caused by recent [update of FlatBuffer](https://github.com/tensorflow/tensorflow/commit/625a4045bc0728c0f3d1b63e05749201f8b401dd).,TESTs failures seem caused by recent [update of FlatBuffer](https://github.com/tensorflow/tensorflow/commit/625a4045bc0728c0f3d1b63e05749201f8b401dd).
56925,emenshoff,1197886292,2022-07-28 9:19:30,"After removing -force_load option and adding framework manually (step 6) the project has been built successfully, but I've got runtime error when loading my .tflite model:
""Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.
TensorFlow Lite Error: Node number 0 (FlexResizeBilinear) failed to prepare.""
Seems like the Custom Ops framework was not initialised correctly on runtime :(","""Seems like the Custom Ops framework was not initialised correctly on runtime"""
37428,YouSenRong,1197790912,2022-07-28 7:50:37,"I try on Tensorflow 2.8.2 with the code follows, and can get the correct return 3.
```python
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
x = tf.Variable(3, name='x')
y = tf.get_default_graph().get_tensor_by_name('x:0')
init = tf.global_variables_initializer()
with tf.Session() as sess:
sess.run(init)
print(sess.run(y))
```","I try on Tensorflow 2.8.2 with the code follows, and can get the correct return 3."
56860,njuhang,1197713521,2022-07-28 6:20:03,"I have checked these links. But they can't help to solve my problem.
There is no bi-directional LSTM in my question, I just want to build a simple LSTM layer. But got Unidirectional LSTM instead, same as this [link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/Keras_LSTM_fusion_Codelab.ipynb). As you known, they are two different layers in tensorflow lite.","""But they can't help to solve my problem."""
56231,YouJiacheng,1197682150,2022-07-28 5:30:58,"Pylance auto-completion works before https://github.com/tensorflow/tensorflow/commit/e65b68a0914408118995d2f8b55c4286859362f8. Pylance does recognize the overwriting by the explicit import statement in the TYPE_CHECKING block
It doesn't work for release version because changes in https://github.com/tensorflow/tensorflow/pull/54104 haven't included in release yet.","""It doesn't work for release version because changes in https://github.com/tensorflow/tensorflow/pull/54104 haven't included in release yet."""
56906,Wanzizhu,1197611171,2022-07-28 3:25:50,"@mohantym, maybe it 's a typo in source code [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/ops.cc#L146) and [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/ops.cc#L166)","""Maybe it's a typo in source code"""
56747,sirakiin,1197563007,2022-07-28 1:58:14,"Hi @sdkdzq1 sorry for the delay.
For this model, the issue comes from the softmax node with a too large quantization range.
See hexagon [nnlib](https://source.codeaurora.org/quic/hexagon_nn/nnlib/tree/hexagon/ops/src/op_softmax_d32.c#n115) for the restrictions. Could you check your model quantization and see if it's expected?","""For this model, the issue comes from the softmax node with a too large quantization range."""
56903,mohantym,1197504764,2022-07-28 0:06:31,"Hi @NeuroRoboticTech !
Sorry for the late response. I could not find any issue with Python 3.7 and colab environment. It might be an environment issue ( CUDA 11.2 and CudNN 8.1 are tested configs for 2.8.2). Could you share the complete error log to help expedite the issue.","""I could not find any issue with Python 3.7 and colab environment."""
56572,reedwm,1197405706,2022-07-27 21:49:13,"This is failing an internal test with
```
error: non-constant-expression cannot be narrowed from type 'dnn::ActivationMode' to 'double' in initializer list [-Wc++11-narrowing]
```
I can get more context if you want.",error: non-constant-expression cannot be narrowed from type 'dnn::ActivationMode' to 'double' in initializer list [-Wc++11-narrowing]
44447,kun-lu20,1197374752,2022-07-27 21:13:35,"On TensorFlow v2.9.1, test case `//tensorflow/compiler/mlir/xla/tests/translate:while.hlotxt.test` passes with the Bazel build arg `--copt=-O -c opt` (optimized binary) but still fails with fastbuild binary. Looks like the cause is lacking certain operations in LLVM SystemZ backend.","On TensorFlow v2.9.1, test case //tensorflow/compiler/mlir/xla/tests/translate:while.hlotxt.test passes with the Bazel build arg --copt=-O -c opt (optimized binary) but still fails with fastbuild binary. Looks like the cause is lacking certain operations in LLVM SystemZ backend."
56088,nitins17,1196981544,2022-07-27 16:29:12,"Just a FYI, the Arm CD workflow [failed](https://github.com/tensorflow/tensorflow/runs/7535892666?check_suite_focus=true) yesterday because of an LLVM integrate. I believe https://github.com/tensorflow/tensorflow/commit/47c640a961874f644cd071752835c7b792450bb8 should fix it.","""I believe https://github.com/tensorflow/tensorflow/commit/47c640a961874f644cd071752835c7b792450bb8 should fix it."""
29198,mohantym,1196764007,2022-07-27 13:28:25,"I could replicate this issue in 2.8, 2.9 and nightly but was getting a different result when I used tf.shape(q) instead of q.shape. Attached[ gist](https://colab.sandbox.google.com/gist/mohantym/05441f9a8d07e06d17a7752fbdfedcdc/git_29198.ipynb#scrollTo=maKEhgFf0zVG) for reference.","""was getting a different result when I used tf.shape(q) instead of q.shape"""
56798,Sanjay2802,1196762073,2022-07-27 13:26:43,"I didnot get any response from forum,
Refer this project:
https://github.com/quantum6/Android-USB-OTG-Camera
How to implement in https://github.com/tensorflow/examples/tree/66f60ebc3dd2e8527b7bbbb280fe0657d54f20f4/lite/examples/object_detection/android
I suppose it's simple but , don't know how to do",I didnot get any response from forum
56904,zaccharieramzi,1196436020,2022-07-27 8:44:37,"After further investigation, I think that the memory leak actually comes from the use of `shuffle`, because I still have it even with a version of `RandomResizedCrop` not relying on `crop_and_resize`. The `shuffle` memory leak is explained here : https://github.com/tensorflow/tensorflow/issues/44176#issuecomment-783768033
However, I still think that this warning is a problem.","After further investigation, I think that the memory leak actually comes from the use of shuffle, because I still have it even with a version of RandomResizedCrop not relying on crop_and_resize."
56710,lapinskm,1196370490,2022-07-27 7:37:20,"Poiu19 is on vacation so we are filling in for him.
The values generated by nextafter are different than the original ones, which is as expected.
The problem is, that nextafter(x) > x should return True for every x (except for corner-cases).
In short, we would expect the result to be [True True True]
However, in this case 1.4012985e-45 > 0 returns False.
It is possible that operator Greater does not work correctly on denormals.","""The values generated by nextafter are different than the original ones, which is as expected."""
56791,gaurides,1196279241,2022-07-27 5:24:18,"> Metaoptimizer test failed
I have root caused the issue.. need some tests before i upstream. So, I can only do that tomorrow.","I have root caused the issue.. need some tests before i upstream. So, I can only do that tomorrow."
56890,sachinprasadhs,1196037207,2022-07-26 22:19:05,"@shkarupa-alex , It can be due to the large input given, could you please confirm if you face the same issue when the small input is given.","""It can be due to the large input given, could you please confirm if you face the same issue when the small input is given."""
55645,PatriceVignola,1196027905,2022-07-26 22:05:26,"I originally moved the files because the ""Android Demo"" check couldn't find them. This target was somehow depending on `kernels_experimental`, which in turn needs those files. Are there any reasons that Android build should depend on `kernels_experimental` at all, assuming this library's purpose is to be used by pluggable devices?
I'm going to try something else locally to fix this ""Android Demo"" build error, but I'm flying relatively blind if I can't repro the other failures locally :/","""Android Demo"" check couldn't find them."
45445,ghamkhari,1195996803,2022-07-26 21:23:07,"My pip version is 20.3.4. I run:
`pip install --no-cache-dir tensorflow`
However, the above command download and uses `grpcio-1.41.1.tar.gz`. As a result I receive the following error: `Running setup.py install for grpcio ... error`",I receive the following error: Running setup.py install for grpcio ... error.
56900,nitins17,1195874302,2022-07-26 19:07:58,Closing this one as #56901 has disabled the `quantization_ops_test`,Closing this one as #56901 has disabled the quantization_ops_test.
56898,Jwy-Leo,1195824715,2022-07-26 18:17:30,"Hi @sachinprasadhs ,
Directly using your recommendation command - tf.lite.OpsSet.TFLITE_BUILTINS is run on tf.int8.
But in my case, I want to run the inference_type=tf.int16.
So I try to cascade the flag, but it cannot work as I want.
`converter.target_spec.supported_ops = [
tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,
tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]`","Directly using your recommendation command - tf.lite.OpsSet.TFLITE_BUILTINS is run on tf.int8. But in my case, I want to run the inference_type=tf.int16. So I try to cascade the flag, but it cannot work as I want. converter.target_spec.supported_ops = [ tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_"
56898,sachinprasadhs,1195815710,2022-07-26 18:08:38,"Hi, Could you please try the below select TF options which will allow the failed ops to fallback to TF Ops and let us know if that works, since the Rsqrt is not supported in the mentioned select ops in your code. ```
converter.target_spec.supported_ops = [
tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
```","""Rsqrt is not supported in the mentioned select ops in your code."""
56088,mseth10,1195711555,2022-07-26 16:32:04,"> It looks like #56828 should fix this but we need to wait for it to be reviewed and merged so let's skip the test for now. We can remove it from the skip list after #56828 is merged.
@nitins17 removing the test from skip-list and adding a new one as it's been failiing because of a recent update. Please help merge this PR https://github.com/tensorflow/tensorflow/pull/56901","""It looks like #56828 should fix this but we need to wait for it to be reviewed and merged so let's skip the test for now."""
50304,sogartar,1195413070,2022-07-26 12:22:25,"> I am trying to add linting pre-commit in our developer container at #48371
> > If you use master you can already run the plain `pylint` command.
> > I think that you can also use `yapf` for autoformatting if you set the style as python Google style.
I want to resurrect this issue because
```
yapf --style google ...
```
is not the correct solution. For example the default indentation is 4 spaces where Tensorflow uses 2 spaces.",I am trying to add linting pre-commit in our developer container at #48371
56698,ViktorKhLoopMe,1195265020,2022-07-26 9:50:34,"Hi @mohantym
Where should exactly I need use `tf.global_variables_initializer().run() `?
running this line I got error :
`AttributeError: module 'tensorflow' has no attribute 'global_variables_initializer'`
Analysing relevant [thread](https://github.com/tensorflow/serving/issues/594) I decided update versions to 2.9.0
And After updating I got the same error","""AttributeError: module 'tensorflow' has no attribute 'global_variables_initializer'"""
55645,PatriceVignola,1194918547,2022-07-26 2:29:45,"@mihaimaruseac I'll remove `tensor_list.cc`, but obviously I won't know if it breaks something else by just running the OSS tests or fixes anything. What command can I run that could reproduce the errors above? I apologize for all the back and forth, I'm running the test suites that I'm aware of locally but it seems like there are a lot more tests than what the CI shows publicly.","I'll remove tensor_list.cc, but obviously I won't know if it breaks something else by just running the OSS tests or fixes anything. What command can I run that could reproduce the errors above?"
56088,mihaimaruseac,1194644658,2022-07-25 21:20:52,"I think it's a little bit more, but that issue should be deduplicated here","I think it's a little bit more, but that issue should be deduplicated here."
56785,cantonios,1194594596,2022-07-25 20:34:15,"@olipinski why do you need `int8` support?
We can easily add it, but doing so increases TF's already massive binary size. For this reason, we try to keep registrations minimal. Will casting to one of the supported types work for you, or are you actually dealing with very large datasets that `int8` is actually required?","""why do you need int8 support?"""
25231,tgeffroyModuleus,1193981088,2022-07-25 12:23:10,"Hi @Oktai15
I am facing an issue where it seems that the interpreter crashes while using an input tensor of shape length equaled to 5.
I don't find the documentation describing this tensor dimension constraint. Do you have one?
Also, is it normal that the interpreter just crash without any error message?
I think it should be logged.","""I don't find the documentation describing this tensor dimension constraint."""
56887,elfringham,1193947620,2022-07-25 11:48:03,"@d0k on AARCH64 the availability of _Float16 depends on if the target CPU includes the support or not. The documentation is sadly lacking in explaining this. So the default compile settings of '--copt=-march=armv8-a' result in the above failure. But if targetting a CPU with the needed support by '--copt=-march=armv8-a+fp16' then _Float16 is available and the code would compile, but then generate an illegal instruction trap if run on a CPU without the support.",The documentation is sadly lacking in explaining this.
56482,roserg,1193843194,2022-07-25 10:05:14,"Your unrolled model should be executed fully on gpu after this commit: https://github.com/tensorflow/tensorflow/commit/6bca0adc394a29eed9cd6e0a7d2143c46ba8e5b4
But this model still much slower on gpu, comparing to cpu
Not all models faster on gpu, gpu needs very big models to be efficient and faster than cpu","""But this model still much slower on gpu, comparing to cpu"""
56878,Apprisco,1193577184,2022-07-25 4:54:07,"It appears @tf.function placement on nested models matters greatly for gradient calculation: the lack of GPU vram usage is from missing gradients- although it doesn't explain the 700s batch times and clear cpu usage over GPU. Something is seriously wrong here, haha.","""It appears @tf.function placement on nested models matters greatly for gradient calculation: the lack of GPU vram usage is from missing gradients- although it doesn't explain the 700s batch times and clear cpu usage over GPU."""
50575,kkb0115-exem-ai,1193454039,2022-07-25 1:25:38,Same error/warning on training. I also want to know the status of this issue. ![image](https://user-images.githubusercontent.com/95392859/180675828-c0fdb736-96a0-483c-ab0c-0b750f4a320b.png),Same error/warning on training.
56878,Apprisco,1193435057,2022-07-25 0:41:36,"To be more specific, wrapping a training loop for a nested keras.model in any way will cause this issue. Wrapping each individual gradients functions in @tf.function prevents this issue, but is bad since we can't use mirroredstrategy.",Wrapping a training loop for a nested keras.model in any way will cause this issue.
40366,tylerpayne,1193205129,2022-07-23 23:28:03,I am also getting this error in Tensorflow 2.8.0 using `keras.metrics.Mean` within a `tf.distribute.MirroredStrategy`.,I am also getting this error in Tensorflow 2.8.0 using keras.metrics.Mean within a tf.distribute.MirroredStrategy.
43546,mariohgu,1193113814,2022-07-23 11:59:19,Hi I have the same problem with this model = [SSD MobileNet V2 FPNLite 640x640](http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz). It looks like this model needs more RAM (I only have 8 GB). I use the 320x320 version I was to able to train it with only 8 Gb.,I have the same problem with this model
56833,eeDigitalSeal,1193042435,2022-07-23 2:34:45,"@tilakrayal Please kindly note that this bug only happened when the `padding=""valid""`, not `padding=""same""`. In this case, the `pool_size = [2,1,1]` is invalid, so `MaxPooling3D` should throw `ValueError`. However now it excuted successfully when direct invocation and calculating gradient in reverse mode.","""However now it excuted successfully when direct invocation and calculating gradient in reverse mode."""
56867,AlbertoSinigaglia,1193007427,2022-07-22 23:42:30,"In my opinion is more related to the activation function than with the loss... probably all the relus are dead... I can reproduce your issue perfectly, but changing the activation function to `""selu""` every works fine",In my opinion is more related to the activation function than with the loss... probably all the relus are dead...
56167,SeeForTwo,1192867076,2022-07-22 19:18:01,"Sorry for the inconvenience, but could please update the commit description to reflect what it currently is, i.e. this is now a fix for TF_TString_GetDataPointer method for the OffsetType, and fix for TEST(TF_CTStringTest, OffsetType)?","""This is now a fix for TF_TString_GetDataPointer method for the OffsetType, and fix for TEST(TF_CTStringTest, OffsetType)?"""
55645,mihaimaruseac,1192713882,2022-07-22 15:53:03,"Please make sure all header inclusions are also paired with the corresponding BUILD rules. Internal tests are now failing because of
```
.../tensorflow/c/kernels_experimental.cc:30:10: error: module //.../tensorflow/c:kernels_experimental does not depend on a module exporting '.../tensorflow/core/kernels/tensor_list.h'; to fix run:
build_cleaner //.../tensorflow/c:kernels_experimental
#include "".../tensorflow/core/kernels/tensor_list.h""
```"," .../tensorflow/c/kernels_experimental.cc:30:10: error: module //.../tensorflow/c:kernels_experimental does not depend on a module exporting '.../tensorflow/core/kernels/tensor_list.h'; to fix run: build_cleaner //.../tensorflow/c:kernels_experimental #include "".../tensorflow/core/kernels/tensor_list"
56847,edwardyehuang,1192667389,2022-07-22 15:05:28,"Note that, I also tried
```
export PYTHONHASHSEED=0
```
and
```
tf.config.threading.set_intra_op_parallelism_threads(1)
tf.config.threading.set_inter_op_parallelism_threads(1)
```
However, none of them worked. I think some ops in TensorFlow are nondeterministic on TPU.","""I think some ops in TensorFlow are nondeterministic on TPU."""
56482,francescoyou97,1192587876,2022-07-22 13:41:08,@roserg. This is an unrolled model.,This is an unrolled model.
56859,jasaw,1192545326,2022-07-22 12:56:15,"@mohantym Cmake is only available for Tensorflow lite. I'm trying to cross compile Tensorflow with CUDA support, which means Bazel is the only choice.","""Cmake is only available for Tensorflow lite. I'm trying to cross compile Tensorflow with CUDA support, which means Bazel is the only choice."""
38409,Jorgvt,1192429515,2022-07-22 10:31:03,"Ran into this error as well and circumvented it doing a type check in my generator, just in case it helps anyone:
`path_to_file = path_to_file.decode('UTF-8') if isinstance(path_to_file, bytes) else path_to_file`","Ran into this error as well and circumvented it doing a type check in my generator, just in case it helps anyone: path_to_file = path_to_file.decode('UTF-8') if isinstance(path_to_file, bytes) else path_to_file."
56835,roserg,1192223899,2022-07-22 6:17:58,"Metal delegate supports only ""static"" batch. It means that batch size must be specified in the model. Dynamic batch(different batch sizes) with the same model not supported.","""Metal delegate supports only ""static"" batch. It means that batch size must be specified in the model. Dynamic batch(different batch sizes) with the same model not supported."""
56735,Yoline777,1192115060,2022-07-22 2:28:51,get the same problem,get the same problem
55645,mihaimaruseac,1191983011,2022-07-21 22:08:19,"It needs to be imported again, sorry, I didn't get a chance to look at it before (since I'm no longer in TF, reviewing TF PRs is a lower priority for me)","I'm no longer in TF, reviewing TF PRs is a lower priority for me."
56714,elfringham,1191321764,2022-07-21 10:32:32,@cantonios I'm guessing that the rollback was because of the pylint error? If so please merge #56730 first then resubmit this patch. The pylint error was pre-existing in one of the files touched by this change and was not caused by this change.,The pylint error was pre-existing in one of the files touched by this change and was not caused by this change.
56482,roserg,1191266583,2022-07-21 9:35:51,"Hi, can you share .tflite model? Empty/random weights are fine. If not, at least part of the tflite model with unpack reshape that do not supported on gpu.
Not all models faster on GPU delegate, especially when used cpu->gpu->cpu pipeline.","Not all models faster on GPU delegate, especially when used cpu->gpu->cpu pipeline."
56710,Poiu19,1191182089,2022-07-21 8:12:59,"@sachinprasadhs Yes, `assert all` expects all values to be `True`. The problem is that `Greater` returns `[False True True]` instead of `[True True True]`",assert all expects all values to be True. The problem is that Greater returns [False True True] instead of [True True True].
56685,gowthamkpr,1190805881,2022-07-20 22:01:47,@ddgonzal3 PocketFFt is still not integrated in tensorflow. jax has used PocketFFT as a workaround as mentioned [here](https://github.com/google/jax/issues/2952) but tensorflow has not.,"""PocketFFt is still not integrated in tensorflow"""
56832,abattery,1190794246,2022-07-20 21:48:57,"I suspect even for the TFLite floating point conversion, these kind of small numerical differences can be found.
There are many reasons to introduce such behaviors due to 1) conversion logic change 2) kernel logic change and so on. It is really impossible to guarantee the numerical exactness. For example, the numerical exactness is hard to achieve in (a+b) * c != a*c + b*c.
Those differences seem minor and looks like the behavior is working as intended.","I suspect even for the TFLite floating point conversion, these kind of small numerical differences can be found."
56782,gowthamkpr,1190767381,2022-07-20 21:18:10,@vgkortsas GPU kernels are way more slower than CPU cores but the main advantage of GPU is that you can run thousands of threads simultaneously. On CPU you're limited by number of cores and besides even if M1 has 8 cores only 4 of them can work simultaneously. Please take a look at the explanation [here](https://stackoverflow.com/questions/70653251/why-gpu-is-3-5-times-slower-than-the-cpu-on-apple-m1-mac),GPU kernels are way more slower than CPU cores
56302,nSircombe,1190690146,2022-07-20 19:50:33,"I'm getting the same error in ManyLinux2014 builds with Python 3.10, but not 3.8.","I'm getting the same error in ManyLinux2014 builds with Python 3.10, but not 3.8."
56464,mihaimaruseac,1190399727,2022-07-20 15:02:28,"This needs to be handled internally, after @learning-to-play finishes the migration to `cc_shared_library` and migrates protobuf to newer releases, both in these files and in `workspace?.bzl` ones.","This needs to be handled internally, after @learning-to-play finishes the migration to cc_shared_library and migrates protobuf to newer releases, both in these files and in workspace?.bzl ones."
46833,miguelusque,1190253050,2022-07-20 12:55:52,"Hi @tilakrayal , I would say that the issue still persists because it was raised on int32, and not uint32 or other datatypes.","""I would say that the issue still persists because it was raised on int32, and not uint32 or other datatypes."""
56815,chunduriv,1190055159,2022-07-20 9:38:05,"@celestinoxp,
>tensorflow 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.2 which is incompatible.
Protobuf 4.0 is incompatible with TF. For more details please take a look at https://github.com/tensorflow/tensorflow/issues/56815#issuecomment-1189277185 by @mihaimaruseac.
Thank you.","""Protobuf 4.0 is incompatible with TF"""
56649,kienerj,1189986518,2022-07-20 8:32:11,@chunduriv Sorry for late reply. It is an NVIDA A100 PCIe 40GB.,Late reply.
34491,yzhangswingman,1189851230,2022-07-20 5:40:48,"Can someone attend to this ticket, please? Can’t attach enough importance to correct loss computation.",Can’t attach enough importance to correct loss computation.
56088,nitins17,1189679338,2022-07-20 0:37:27,"@mseth10, @elfringham, @nSircombe Looks like there were no uploads to the `tf-nightly-cpu-aws` project on PyPI for the last few days as the Arm CD workflow has been [failing](https://github.com/tensorflow/tensorflow/actions/workflows/arm-cd.yml) due to a test failure (`tensorflow/python/kernel_tests/nn_ops:pooling_ops_3d_test_cpu`). fyi: @yarri-oss, @rishikasinha-tf","""Looks like there were no uploads to the tf-nightly-cpu-aws project on PyPI for the last few days as the Arm CD workflow has been [failing]"""
56610,wellerbii,1189499684,2022-07-19 20:03:26,"@chunduriv The other thread has been closed but I am still experiencing the same issue for v2.7, v2.8, and release 2.9 even after fresh installs.","The other thread has been closed but I am still experiencing the same issue for v2.7, v2.8, and release 2.9 even after fresh installs."
56799,adamjstewart,1189393013,2022-07-19 18:01:23,"Also, specifying `--macos_sdk_version` actually breaks TF 2.8.2/Bazel 4.2.2:
```
xcodebuild: error: SDK ""macosx12.4"" cannot be located.
```","Specifying --macos_sdk_version actually breaks TF 2.8.2/Bazel 4.2.2:  xcodebuild: error: SDK ""macosx12.4"" cannot be located. "
56822,Claeb101,1189376262,2022-07-19 17:42:12,Misread documentation. My matrices were not self-adjoint.,Misread documentation.
50575,ICEJM1020,1189331997,2022-07-19 16:55:27,"HI, same error in CPU training. It only appears in model with attention.
![image](https://user-images.githubusercontent.com/38009758/179806688-4058b8d8-b068-4dac-b699-76d710d81a8f.png)
But it seems only lead to cannot estimate the training time.","""It seems only lead to cannot estimate the training time."""
56815,mihaimaruseac,1189277185,2022-07-19 16:14:26,"For future: please use descriptive titles.
Regarding this issue: there is a big ABI/API breakage with protobuf 4 and TF cannot upgrade due to segfaults in Windows caused by ODR violations. @learning-to-play is working on `cc_shared_library` support and migration past these to then upgrade protobuf","""There is a big ABI/API breakage with protobuf 4 and TF cannot upgrade due to segfaults in Windows caused by ODR violations."""
55766,mihaimaruseac,1189272297,2022-07-19 16:12:28,"Brian does not work in TF for more than a year, removing assignment.","""Brian does not work in TF for more than a year, removing assignment."""
55766,mihaimaruseac,1189271389,2022-07-19 16:12:07,Please fix more than just one single letter typos.,"""Please fix more than just one single letter typos."""
55935,mihaimaruseac,1189267606,2022-07-19 16:10:39,"Please use better commit messages and PR titles, not ""update < file>"".","""update  file>"""
56804,derrkater,1188860198,2022-07-19 10:06:56,I don't have any idea how to go about debugging such model :/ Is it maybe possible to catch intermediate steps of tflite forward pass to check where exactly it fails?,I don't have any idea how to go about debugging such model :/
56804,derrkater,1188856973,2022-07-19 10:03:45,"@mohantym I don't actually have a tensorflow model .py file as it was converted from pytorch via onnx format. I can provide a pb file with variables - is that helpful?
https://drive.google.com/file/d/1oni3uxg_2gKJvF5zQMPH70l8061KLBr9/view?usp=sharing
https://drive.google.com/drive/folders/1WHRhOsEgUrPxHy7ZI3dnETZ7ERSanRVA?usp=sharing
I was using different input format - BCHW. Maybe that caused Dimension mismatch?","""I don't actually have a tensorflow model .py file as it was converted from pytorch via onnx format"""
56774,krehm,1187422215,2022-07-18 13:25:14,"@tilakrayal the problem seems to be this web page. If you click on the ""..."" for the code and select ""Edit"", then the code indentation is correct. In Edit mode I tried to add more blanks to each line to fix the indentation, but the web page just removes them again.","""The web page just removes them again."""
56804,derrkater,1187024306,2022-07-18 10:22:14,"@mohantym I should have mentioned it, but I use select ops syntax already. My conversion code is the same as you've posted and the problem persists :/","I should have mentioned it, but I use select ops syntax already. My conversion code is the same as you've posted and the problem persists."
56804,mohantym,1187020234,2022-07-18 10:17:41,"Hi @derrkater !
Could you put select ops syntax in TFLite conversion and let us know whether it resolves.
```
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.target_spec.supported_ops = [
tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)
```","""put select ops syntax in TFLite conversion and let us know whether it resolves"""
55534,elfringham,1187005524,2022-07-18 10:02:56,"@snadampal You need to address the buildifier failure on your PR. You have an added line that is not in alphabetical order in the list. This is why the required presubmit test ""Code Check - Changed Files"" is failing. See #56673 for the details on that.","""You have an added line that is not in alphabetical order in the list."""
56454,shenh10,1186764943,2022-07-18 4:45:54,"> @shenh10 Can you please modify the colab gist and provide the working gist. Thanks!
It's quite strange that I cannot use the gist to reproduce the problem. Could you please run the demo on your local machine? I cannot figure out how colab works with TF environment variables. Colab prints nothing even when I set TF_CPP_VMODULE=mark_for_compilation_pass=4 to os.environ or by %env methods thus I cannot make sure whether XLA is actually worked when running.","""I cannot figure out how colab works with TF environment variables."""
38257,innat,1186601059,2022-07-17 20:20:51,"@amahendrakar I've just faced this issue with tf 2.6. The TPU resource may be exhausted and cause such issue. But during the time of training, it's frustrating. Please reopen the ticket.",The TPU resource may be exhausted and cause such issue.
52913,johan-gras,1186586515,2022-07-17 18:35:44,"@gbaned unfortunately I am not working at Arm anymore, so I am unable to modify this PR. Someone from Arm would have to take a look at it. Maybe @Tessil?","""I am not working at Arm anymore, so I am unable to modify this PR."""
35010,ladi-pomsar,1186495112,2022-07-17 11:53:14,Can someone please finally look at this @tilakrayal ? It has been almost 3 years,Can someone please finally look at this @tilakrayal ? It has been almost 3 years.
30418,Apprisco,1186461340,2022-07-17 9:42:33,Update v2: turns out enabling gradient checkpoints in your manner will prevent model from training- it seems watch vars is simply not transferring.,enabling gradient checkpoints in your manner will prevent model from training- it seems watch vars is simply not transferring.
56722,jjl2,1186391895,2022-07-17 4:04:15,"@gadagashwini Thanks for your suggestion about installing TF 2.9 using pip, which of course I do know about. But I'll also want GPU support, and trying to get that done with pip means that I'll have to go back to dealing with CUDA / CUDNN / GPU drivers manually. That nightmare is the whole reason that I switched to Anaconda.
I will raise the issue with the conda-forge folks as you suggested. The post from @ngam suggests that they're already working on it.","""Trying to get that done with pip means that I'll have to go back to dealing with CUDA / CUDNN / GPU drivers manually."""
56694,VictoriaGriffith,1186313689,2022-07-16 22:39:45,"Hi @tilakrayal , thanks for your reply, indeed the gradient computation does not support `float64`. I just wonder why `tf.image.rgb_to_hsv` can expect a `float64` input if called directly (as in the snippet, and documentation also says it can accept a float64 input), but when I try to compute the gradient then it will complain about the input dtype. Wouldn't it be better if it can support `float64` both in the forward and backward pass?","""I just wonder why tf.image.rgb_to_hsv can expect a float64 input if called directly (as in the snippet, and documentation also says it can accept a float64 input), but when I try to compute the gradient then it will complain about the input dtype."""
56789,keith,1185838101,2022-07-15 19:21:35,"Added the patch for now, this actually broke when https://github.com/tensorflow/tensorflow/pull/56446 merged, which is why I didn't initially have this issue","Added the patch for now, this actually broke when https://github.com/tensorflow/tensorflow/pull/56446 merged, which is why I didn't initially have this issue."
56787,keith,1185672582,2022-07-15 15:49:07,AMD ROCm failure appears unrelated,AMD ROCm failure appears unrelated.
56707,PatriceVignola,1185635202,2022-07-15 15:05:34,"@gbaned @rohan100jain Could we get more eyes on this? This is an issue many users have faced since we released the preview of our plugin since most people use the `tensorflow` package, not `tensorflow-cpu`.","This is an issue many users have faced since we released the preview of our plugin since most people use the tensorflow package, not tensorflow-cpu."
56773,pimorandi,1185383167,2022-07-15 9:57:40,"Update:
Just found out that also the argument validation_steps needs to be set, now the training starts and does not give any error.
The problem now is that, no matter how many epochs and how many steps I set, the fit method performs only one iteration and then stops without any error.
Any advice on how to debug this?","""Just found out that also the argument validation_steps needs to be set, now the training starts and does not give any error."""
56422,rajeev921,1185335512,2022-07-15 9:01:06,"@gadagashwini @angerson @Arnold1
Hi,
Its been a long time on this thread without a reply. Can anyone look into that and help me.","""Its been a long time on this thread without a reply."""
30418,Apprisco,1185226717,2022-07-15 6:39:40,"Update: I was able to get it working on tensorflow 2.9.1, w/o jit_compile= True (It seems jit_compile=True will cause memory fragmentation errors that won't be resolved by CUDA Async Malloc.)","I was able to get it working on tensorflow 2.9.1, w/o jit_compile= True (It seems jit_compile=True will cause memory fragmentation errors that won't be resolved by CUDA Async Malloc.)"
48675,nfrumkin,1184552066,2022-07-14 14:58:25,I have an identical error with a model I am currently running,I have an identical error with a model I am currently running.
56768,JaimeArboleda,1184485190,2022-07-14 14:00:26,"Last thing I want to add: if you change the code by replacing tf.Variable with self.add_weight, it works in every case. This part of the documentation can be misleading (I must confess it was misleading for me).
https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#add_weight
In particular, when it says ""Note that the method add_weight() offers a shortcut to create weights:"". It's more than a shortcut, as the behaviour can be different.","Last thing I want to add: if you change the code by replacing tf.Variable with self.add_weight, it works in every case. This part of the documentation can be misleading (I must confess it was misleading for me). https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#add_weight In particular, when it says ""Note that the method add_weight() offers a shortcut to create weights:"" it's more than a shortcut, as the"
56743,pharmpy-dev-123,1184477550,2022-07-14 13:53:21,"Hi @mohantym. As I understand it, a `native` build would produce a less generic binary. I am looking for a more generic binary. I could find some references on how to obtain such a build (`manylinux`) for TensorFlow, but it seems instructions are missing for `tflite`.","""I could find some references on how to obtain such a build (manylinux) for TensorFlow, but it seems instructions are missing for tflite."""
56769,bhack,1184408781,2022-07-14 12:49:35,As we could see also on this last ticket we will have a more generic issue with the current Keras-cv logic and XLA `jit_compile` /cc @qlzh727,As we could see also on this last ticket we will have a more generic issue with the current Keras-cv logic and XLA jit_compile
56638,KingsleyLiu-NV,1184201060,2022-07-14 9:14:47,You guys have really bad management on the reported issues: no one is being responsible and no one is replying!,"""bad management"""
56696,KingsleyLiu-NV,1184199207,2022-07-14 9:12:53,"@chunduriv I don't think it should be a keras issue. `tf.sparse.reshape` cannot deal with `tf.keras.Input(shape=(3, 4), sparse=True, dtype=tf.int64)`, should not it be a TF issue?","""I don't think it should be a keras issue."""
32052,Jiangyu1181,1184155595,2022-07-14 8:30:28,"> I experience the same issue on 2.4.0.
I experience the same issue on 2.6.0.",I experience the same issue on 2.4.0. I experience the same issue on 2.6.0.
55645,mihaimaruseac,1183821041,2022-07-14 0:42:16,"Finally,
```
comparison of different enumeration types ('tensorflow::DataType' and 'TF_DataType') [-Werror,-Wenum-compare]
if (cc_a.dtype() == TF_VARIANT) {
~~~~~~~~~~~~ ^ ~~~~~~~~~~
1 error generated.
```","Finally,  comparison of different enumeration types ('tensorflow::DataType' and 'TF_DataType') [-Werror,-Wenum-compare] if (cc_a.dtype() == TF_VARIANT)    1 error generated. "
56664,rsanthanam-amd,1183568404,2022-07-13 18:55:04,"I finally found the correct problem: //tensorflow/core/grappler/optimizers:generic_layout_optimizer.
In this case, I have to make a change to the ROCm TF fork, so I will close this PR without merging.
Sorry for the confusion.","I finally found the correct problem: //tensorflow/core/grappler/optimizers:generic_layout_optimizer. In this case, I have to make a change to the ROCm TF fork, so I will close this PR without merging. Sorry for the confusion."
56687,sachinprasadhs,1183478453,2022-07-13 17:13:05,"May be it is because of the flag mentioned here.
```
# Flag, indicating that test should be run only with partial_pivoting=True
FLAG_REQUIRES_PIVOTING = ""FLAG_REQUIRES_PIVOT""
```","# Flag, indicating that test should be run only with partial_pivoting=True"
56753,JuanJoseMoralesCalvo,1183295108,2022-07-13 14:26:22,"Same error with those changes:
org.tensorflow.lite.examples.objectdetection E/tflite: Following operations are not supported by GPU delegate:
CUSTOM TFLite_Detection_PostProcess: TFLite_Detection_PostProcess
PACK: OP is supported, but tensor type/shape isn't compatible.
RESHAPE: OP is supported, but tensor type/shape isn't compatible.
111 operations will run on the GPU, and the remaining 46 operations will run on the CPU.","""Same error with those changes"""
55934,bmharper,1183293203,2022-07-13 14:24:47,"Hi @karimnosseir, I'm experiencing a strange problem now.
I've created a new project, and the SNAPSHOT dependencies aren't resolving.
This is the gradle error that I get:
```
Failed to resolve: org.tensorflow:tensorflow-lite:0.0.0-nightly-SNAPSHOT
Failed to resolve: org.tensorflow:tensorflow-lite-hexagon:0.0.0-nightly-SNAPSHOT
```
If I switch from SNAPSHOT to a real version (eg 2.9.0), then the packages resolve, but then I don't have a working hexagon delegate.","""I'm experiencing a strange problem now."""
56207,gowthamkpr,1183287869,2022-07-13 14:20:20,"@aliencaocao As mentioned [here](https://github.com/llvm/llvm-project/issues/55386), I think this might be a version issue of LLVM too. Can you downgrade the version of LLVM to 3.5 and see if the issue still persists?",I think this might be a version issue of LLVM too.
56755,Wyverald,1183287297,2022-07-13 14:19:52,"I briefly looked into this, and it seems that the culprit is this change: https://github.com/tensorflow/tensorflow/commit/ebbe9ef4ddd413f44b031da46ed32485a1009c5a#diff-e0345fa27516b1b540addbfb6cd863905cfebe045c336481f54af95361fff131
The `absl::string_view` argument isn't recognized by `llvm::formatv` -- but somehow this only breaks on Windows, and only on Bazel HEAD.","I briefly looked into this, and it seems that the culprit is this change: https://github.com/tensorflow/tensorflow/commit/ebbe9ef4ddd413f44b031da46ed32485a1009c5a#diff-e0345fa27516b1b540addbfb6cd863905cfebe045c336481f54af95361fff131"
56752,ZhinianQIN,1183067635,2022-07-13 10:48:08,Threads leak only in tf.distribute.MirroredStrategy,Threads leak only in tf.distribute.MirroredStrategy.
56747,sdkdzq1,1182897034,2022-07-13 8:01:33,"I used the conv + transfomer, to do the same work, it can run success.
so the error can find in some conv op
<img width=""1163"" alt=""f6d50095-4e04-4e14-9932-1487eae1e1e9"" src=""https://user-images.githubusercontent.com/31097104/178682132-bad1323e-153e-4a1d-ba41-b743118d3b8a.png"">
**Another thing**
the infer result is differ between CPU and DSP. as this
![image](https://user-images.githubusercontent.com/31097104/178682465-8464e20f-a2d3-4497-ae90-ea0ae0e003ce.png)","""So the error can find in some conv op"""
56649,kienerj,1182844664,2022-07-13 7:04:41,"I can add if I remove CUDA libraries, eg. limit tensorflow to CPU and no GPU device found, it works fine. If I add back the libraries, then even using `with tf.device('/CPU:0'):` blocks. Nothing happens. So the issue is certainly related to CUDA and GPU config.","I can add if I remove CUDA libraries, eg. limit tensorflow to CPU and no GPU device found, it works fine. If I add back the libraries, then even using with tf.device('/CPU:0'): blocks. Nothing happens. So the issue is certainly related to CUDA and GPU config."
56732,you74674,1182743393,2022-07-13 4:10:14,"I have similar issue with reduce_mean using OpenCL delegate.
for a (1 x 2 x 2 x 1) input, reduce across axis=2 gives wrong result. axis=1 or axis=(1,2) is fine.
also with keepDims=True the result is correct, so I think something might be wrong when keepDims=False. (I didn't check for other input size, though)
edit:
I just tested reduce_sum for same input, and the result is same as reduce_mean, keepDims=True is correct and keepDIms=False is wrong.","I have similar issue with reduce_mean using OpenCL delegate. for a (1 x 2 x 2 x 1) input, reduce across axis=2 gives wrong result. axis=1 or axis=(1,2) is fine. also with keepDims=True the result is correct, so I think something might be wrong when keepDims=False. (I didn't check for other input size, though) edit: I just tested reduce_sum for same input, and the result is same as reduce_mean, keepDim"
56747,sdkdzq1,1182714214,2022-07-13 3:10:13,"when i convert it to tflite, it has warnning: like this, WARNING:absl:Importing a function (__inference_inference_101322) with ops with custom gradients. Will likely fail if a gradient is requested.
convert code and saved model as this [Documents.zip](https://github.com/tensorflow/tensorflow/files/9098623/Documents.zip)",WARNING:absl:Importing a function (__inference_inference_101322) with ops with custom gradients. Will likely fail if a gradient is requested.
36981,Apprisco,1182655835,2022-07-13 1:19:30,This still doesn't seem to work... with a custom keras model.,"""still doesn't seem to work"""
56119,bhack,1182553641,2022-07-12 22:24:22,"/cc @learning-to-play Just a side note. Formally I am not changing an API here as in the signature. I am just changing the behaviour of an API with autograph to be aligned with the expected output we have in python.
Then I could not help to fix failures that are internal only as contributor cause I have no visibility.","""I could not help to fix failures that are internal only as contributor cause I have no visibility."""
54299,lgeiger,1182021452,2022-07-12 17:06:04,"@aaudiber To check whether this could somehow be a problem with Python's GC not cleaning up `data_iterator`, I also tested
```python
for epoch in range(3):
for batch in iter(dataset):
pass
```
and
```python
for epoch in range(3):
for batch in dataset:
pass
```
but both snippets show the same increasing memory usage as the original reproduction (tested on `f-nightly==2.10.0.dev20220711`).","""I also tested python for epoch in range(3): for batch in iter(dataset): pass  and python for epoch in range(3): for batch in dataset: pass  but both snippets show the same increasing memory usage as the original reproduction (tested on f-nightly==2.10.0.dev20220711)."""
54299,lgeiger,1181881473,2022-07-12 15:09:22,"@aaudiber were you able to reproduce this issue on your end?
I am still seeing the same behaviour in TF 2.9.1 which makes proper shuffling of datasets that do not fit into memory pretty much impossible together with Keras `model.fit`.
Let me know if I can provide you with more information about this problem.",I am still seeing the same behaviour in TF 2.9.1 which makes proper shuffling of datasets that do not fit into memory pretty much impossible together with Keras model.fit.
56687,enor2017,1181665258,2022-07-12 11:52:20,"@sachinprasadhs Please find the [gist here](https://colab.research.google.com/drive/1HmbKur0CX3oC40W5IyQN-b1JGwdF9MMg?usp=sharing) using nightly version against the master branch, the tests fail as before.
I noticed the last commit of this test file is on 1 Nov 2021, so perhaps it is not caused by a version mismatch?","using nightly version against the master branch, the tests fail as before."
40515,kshitij12345,1181581892,2022-07-12 10:16:37,Still the same issue on 2.8,Still the same issue on 2.8.
56709,zhihuiloke,1181318976,2022-07-12 5:06:39,"Also, compiling tflite 2.9.1 with cpp14 didn't help either.",compiling tflite 2.9.1 with cpp14 didn't help either.
56709,zhihuiloke,1181211477,2022-07-12 1:27:51,I don't think setting the Minimum iOS versioned to 10.0 worked. It is still having the same errors as above.,I don't think setting the Minimum iOS versioned to 10.0 worked. It is still having the same errors as above.
56731,cantonios,1180892766,2022-07-11 21:34:22,"@bhavani-subramanian can you try hacking this at the top of `tensorflow/core/kernels/eigen_spatial_convolutions.h`, before including Eigen's `Tensor` header?
```
// Hack to disable breaking AVX512 special GemmKernel.
// There is a conflicting specialization there causing build breakages.
#define GEMM_KERNEL_H disabled
```
I *think* this should solve it temporarily to unblock you, but I'm having trouble actually building this on our end.","I *think* this should solve it temporarily to unblock you, but I'm having trouble actually building this on our end."
56497,bixia1,1180578064,2022-07-11 15:49:09,waiting for author to fix the test failure.,waiting for author to fix the test failure.
56727,ChicchiPhD,1180459951,2022-07-11 14:10:33,"I forgot to mention that I've also tried different implementations of the representative dataset function, but I've always ended up with the same error.","I've also tried different implementations of the representative dataset function, but I've always ended up with the same error."
56708,Jiayuli-CU,1179891896,2022-07-11 2:19:32,@sushreebarsa Your code stops before converter.build() because certain packages are missing. You need to install TensorRT first. https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html,"""Your code stops before converter.build() because certain packages are missing."""
56697,baoachun,1179884415,2022-07-11 2:07:33,"~~I just found that changing the `mkl` of the bazel compilation option to `mkl_threadpool` and turning on `-O3` optimization has greatly improved the performance.~~
No effect.",No effect.
54479,bhack,1179579586,2022-07-09 17:39:36,"Some of these `Dynamic reshape is not implemented` are:
https://github.com/tensorflow/tensorflow/blob/262777e9f9304c7df6b694934af819c820954ef5/tensorflow/compiler/xla/literal.cc#L963-L965",Dynamic reshape is not implemented
46970,HridoyHazard,1179513950,2022-07-09 9:39:40,having the same issue even after using tf2 models,having the same issue even after using tf2 models
56231,GF-Huang,1179502753,2022-07-09 8:23:24,"Issue still, no IntelliSense for parameters.
It only show `Dense`, no parameters IntelliSense.
![image](https://user-images.githubusercontent.com/4510984/178097893-6f3c5ffe-0a0c-4700-932a-97508f864b34.png)
- TF 2.9.1
- Python 3.9.13 by MiniConda
- Win10 21H2
- VSCode 1.69.0
- Jupyter Extension v2022.6.1001902341
- Python Extension v2022.10.0","""It only show Dense, no parameters IntelliSense."""
56714,elfringham,1179501204,2022-07-09 8:09:28,"Sorry, found some new failures that I need to fix.",Found some new failures that I need to fix.
46052,cooperlab,1179419469,2022-07-08 23:04:43,I ran into this problem too and it's really annoying. TensorFlow's own dataset format can't be parsed in graph mode. We need a function analogous to exmple.ParseFromString that accepts non-eager tensors.,I ran into this problem too and it's really annoying.
55779,rahulbatra85,1179241017,2022-07-08 18:03:00,"@ezhulenev Hi Eugene,
This is passing all the CIs except ARM CI which is failing a conv_3d_ops. The failure looks to be completely unrelated to this commit as it is grappler/ROCm specific and adds some dependencies to the BUILD file. Also, for Windows Bazel GPU I can't see any details. Seems to me the CIs might be broken.
Given this can we merge this commit?",The failure looks to be completely unrelated to this commit as it is grappler/ROCm specific and adds some dependencies to the BUILD file.
21263,bingcheng1998,1179022803,2022-07-08 13:59:54,"This may be caused by homebrew installed [double-conversion](https://formulae.brew.sh/formula/double-conversion).
You can delete the double-conversion folder with `rm -r /usr/local/Cellar/double-conversion`, and then everything works well.","This may be caused by homebrew installed [double-conversion](https://formulae.brew.sh/formula/double-conversion). You can delete the double-conversion folder with rm -r /usr/local/Cellar/double-conversion, and then everything works well."
56609,rsanthanam-amd,1178889609,2022-07-08 11:41:44,Our Ubuntu-sanity test failed (the bazel query subtest specifically) if I didn't remove those dependencies. The conv_buffer_1x1 stuff was removed in some commit but it appears that commit failed to remove those [stale] refences in tf-lite.,"""It appears that commit failed to remove those [stale] refences in tf-lite."""
38158,seboz123,1178775741,2022-07-08 9:38:42,"Is there any resoultion to get tensorflow lite working for batches > 1?
I got the same error when I resize the input tensor",I got the same error when I resize the input tensor.
56712,Anbusekar-Scanflow,1178774879,2022-07-08 9:37:46,"Hi @mohantym !
Here my requirement is, I have to utilise both TensorFlowLiteTaskVision and TensorFlowLiteVisionSwift, If I give only TensorFlowLiteTaskVision, it works fine. but need to integrate both. while integrating shows error. is there another way to ignore duplicates?.","""I have to utilise both TensorFlowLiteTaskVision and TensorFlowLiteVisionSwift, If I give only TensorFlowLiteTaskVision, it works fine. but need to integrate both. while integrating shows error. is there another way to ignore duplicates?"""
56709,zhihuiloke,1178607739,2022-07-08 6:32:04,"@mohantym this is exactly how we are using the GPU delegate right now. Parts of it are redacted, but in general, the flow is exactly as described.
Update: using default options didn't help either",Using default options didn't help either.
56681,jungyin,1178460545,2022-07-08 2:12:26,"I tried again, and it seems that I can't reproduce this problem again. Although I'm very unwilling, I will remember this problem, and I will solve it when it reappears","I tried again, and it seems that I can't reproduce this problem again."
48195,very-meanly,1178368916,2022-07-07 23:31:56,"Bumping this to keep it open - still seeing this issue in tensorflow==2.9.1. Same pattern - no issue on MacOS, but as soon as I run on a Sagemaker notebook instance using Amazon Linux 2, the issue happens. The suggestion from @Jack-Fawcett does prevent the error from happening, for anyone else looking for a workaround (`tf.keras.backend.set_image_data_format(""channels_last"")`)","Bumping this to keep it open - still seeing this issue in tensorflow==2.9.1. Same pattern - no issue on MacOS, but as soon as I run on a Sagemaker notebook instance using Amazon Linux 2, the issue happens. The suggestion from @Jack-Fawcett does prevent the error from happening, for anyone else looking for a workaround (tf.keras.backend.set_image_data_format(""channels_last""))"
56655,sachinprasadhs,1178287302,2022-07-07 22:14:32,"This is because currently there is no Fill kernel for `GPU` for `dtype` bool, even though the ctx manager has both `CPU` and `GPU`, it uses `CPU` and only in case if it is required it will be copied to `GPU`.
Below comment and code explains the case where it is forced to run `with ops.device(""/device:CPU:0""):`.
https://github.com/tensorflow/tensorflow/blob/a113dd02168a01aba7aea99d8266630233d9269c/tensorflow/python/framework/constant_op.py#L318-L323","This is because currently there is no Fill kernel for GPU for dtype bool, even though the ctx manager has both CPU and GPU, it uses CPU and only in case if it is required it will be copied to GPU. Below comment and code explains the case where it is forced to run with ops.device(""/device:CPU:0""):."
56677,gowthamkpr,1178211130,2022-07-07 20:49:26,"@KingsleyLiu-NV` tf.keras.Input()` produces a symbolic tensor-like object (i.e. a placeholder) whereas you are directly passing in the data when you pass `tf.ones()` into the data and that is the reason why model is not getting executed as you are passing a placeholder, not the data itself. You can refer to this [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Input) for deeper understanding.","tf.keras.Input() produces a symbolic tensor-like object (i.e. a placeholder) whereas you are directly passing in the data when you pass tf.ones() into the data and that is the reason why model is not getting executed as you are passing a placeholder, not the data itself."
26842,cantonios,1178186341,2022-07-07 20:26:44,"Yes, this is still a problem - and it's an Eigen issue: [#2491](https://gitlab.com/libeigen/eigen/-/issues/2491)","Yes, this is still a problem - and it's an Eigen issue: [#2491](https://gitlab.com/libeigen/eigen/-/issues/2491)."
9234,cantonios,1178184891,2022-07-07 20:25:23,"> I started having this error ""/tensorflow/core/kernels/linalg/svd_op_impl.h:110] Eigen::BDCSVD failed with error code "" on Intel CPUs, in 2.7 and in 2.9.1 as well
That would be a different issue. Perhaps this one: #26842","I started having this error ""/tensorflow/core/kernels/linalg/svd_op_impl.h:110] Eigen::BDCSVD failed with error code "" on Intel CPUs, in 2.7 and in 2.9.1 as well"
56679,mihaimaruseac,1178029032,2022-07-07 18:15:03,"Yes, but this discussion confused @gadagashwini into thinking OP wanted to install via Conda, when Conda is just an orthogonal issue here.
To debug this further, we need the output of the two commands mentioned in https://github.com/tensorflow/tensorflow/issues/56679#issuecomment-1177881334:
```
pip show tensorflow
python -m pip show tensorflow
```","This discussion confused @gadagashwini into thinking OP wanted to install via Conda, when Conda is just an orthogonal issue here. To debug this further, we need the output of the two commands mentioned in https://github.com/tensorflow/tensorflow/issues/56679#issuecomment-1177881334:  pip show tensorflow python -m pip show tensorflow ."
56679,bhack,1178023873,2022-07-07 18:10:50,"Yes installing in a conda env doesn't mean install with coda.
I was always talking about preparing a fresh env with conda as it is our official doc now and not installing with conda.
Also I think that the first command/section is very confusing as It install something that It is used only in the metioned step-by-step section.",I was always talking about preparing a fresh env with conda as it is our official doc now and not installing with conda.
56679,bhack,1177946474,2022-07-07 17:09:02,"Not anymore (I meant to be sure you have a fresh env).
https://www.tensorflow.org/install/pip
For CPU It is only suggested to skip step 3 (Linux) and 4 (win).",Not anymore (I meant to be sure you have a fresh env)
56679,mihaimaruseac,1177932547,2022-07-07 16:58:31,No. Conda is used only to install the GPU libs (as that's easier than using the operating system's utilities). But TF official releases and guides are still pip based.,No. Conda is used only to install the GPU libs (as that's easier than using the operating system's utilities). But TF official releases and guides are still pip based.
56698,guanxinq,1177667566,2022-07-06 15:02:47,"@pindinagesh, could you redirect this question to TF repo? It looks more like a TF issue.","""It looks more like a TF issue."""
56609,rsanthanam-amd,1177624151,2022-07-07 13:34:30,This PR is failing to merge because of CI failures but I am unable to ascertain what the problem is. Can someone take a look?,unable to ascertain what the problem is.
56591,Flamefire,1177202308,2022-07-07 7:39:54,"Is this good to go or is there anything I need to do? The CI seems to be buggy (not caused by this PR) as all I see is
> tensorflow/tools/ci_build/pylintrc:1:0: E0015: Unrecognized option found: profile, cache-size, files-output, comment, zope, required-attributes, bad-functions, disable-report, no-space-check, ignore-iface-methods, short-func-length, deprecated-members, ignore-exceptions, copyright (unrecognized-option)","The CI seems to be buggy (not caused by this PR) as all I see is > tensorflow/tools/ci_build/pylintrc:1:0: E0015: Unrecognized option found: profile, cache-size, files-output, comment, zope, required-attributes, bad-functions, disable-report, no-space-check, ignore-iface-methods, short-func-length, deprecated-members, ignore-exceptions, copyright"
45285,M-R-Z-X,1176442107,2022-07-06 16:40:58,"Is this because my version is [wrong]
![NIPO9I ZEY`C0BJM{_8GYO6](https://user-images.githubusercontent.com/52115431/177600941-13752640-204c-4763-ba14-619306b16e9b.png)
![RG}$PY)Z(A%A~(8R(F623FE](https://user-images.githubusercontent.com/52115431/177600950-1fcee81e-4484-4b13-a6a2-cb03fa105a8b.png)",Is this because my version is wrong
56660,sachinprasadhs,1176433673,2022-07-06 16:31:52,Could you try again saving the model with `model.save` instead of `tf.saved_model.save` and follow the other steps as usual.,Could you try again saving the model with model.save instead of tf.saved_model.save and follow the other steps as usual.
56679,d287zhan,1176287029,2022-07-06 14:23:19,"Hi @gadagashwini, I understand that I can do that directly from Jupyter, but it doesn't make sense to me that it shows TensorFlow version 2.9.1, but I am using version 2.4.0. I checked and I only have 1 version of Python installed, so I am not sure why this is the case.","""I checked and I only have 1 version of Python installed, so I am not sure why this is the case."""
52988,GF-Huang,1176261950,2022-07-06 14:02:33,"Install `PyTorch` is so easy: `conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch`. It auto install cuda.
I don't know why tensorflow so difficult.",I don't know why tensorflow so difficult.
56664,hawkinsp,1176193381,2022-07-06 13:00:21,"@chr1sj0nes is the author of that change.
Something looks off here.
a) XLA shouldn't be depending on anything much in tensorflow/core/kernels.
b) in general targets should depend directly on what they need. Something is usually off if a top-level target is having dependencies added because an underlying target is missing them.",Something looks off here.
50487,suchunxie,1176094596,2022-07-06 11:12:38,"> In my case, I'd to specify `--distribution_strategy=one_device` here in my tests https://github.com/open-ce/tensorflow-feedstock/blob/main/tests/open-ce-tests.yaml#L22
@npanpaliya I'm using tensorflow model -garden, and tired your way to add strategy but tat parameter is not allowed in my case.","I'm using tensorflow model -garden, and tired your way to add strategy but tat parameter is not allowed in my case."
9234,DanielWicz,1175912943,2022-07-06 8:03:56,"I started having this error ""/tensorflow/core/kernels/linalg/svd_op_impl.h:110] Eigen::BDCSVD failed with error code "" on Intel CPUs, in 2.7 and in 2.9.1 as well","I started having this error ""/tensorflow/core/kernels/linalg/svd_op_impl.h:110] Eigen::BDCSVD failed with error code "" on Intel CPUs, in 2.7 and in 2.9.1 as well."
56671,mergian,1175806558,2022-07-06 5:45:57,"@agramesh1 no I'm using the official PYPI TensorFlow. I just created a blank VENV and only installed TF. It seems the second OpenMP output comes from a device support library we are using.
BUT I was able to reproduce the performance problems also with just a blank TF when running our pipeline on CPU, so I assume that the TF threading implementation itself might interfere with other libraries using OpenMP.","""I just created a blank VENV and only installed TF"""
56679,d287zhan,1175680536,2022-07-06 1:38:32,"Hi @bhack, I can try creating a condo env, but I download all these packages locally using pip; I install python packages through my terminal. To preface, I have tried uninstalling TensorFlow and reinstalling it, and it still said 'version 2.4.0' in my Jupyter and Terminal. Is there a way to uninstall a certain version of TensorFlow and retain the other? Could it also be related to my local python version?","I have tried uninstalling TensorFlow and reinstalling it, and it still said 'version 2.4.0' in my Jupyter and Terminal."
56665,njuhang,1175663485,2022-07-06 1:17:41,"I could pass example building. I check python:tflite_convert build info, it says 'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env ""PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/bin/python3"".
But still failed.","I could pass example building. I check python:tflite_convert build info, it says 'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env ""PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/bin/python3"" But still failed.."
56599,jhogg11,1175588593,2022-07-05 23:08:29,"> @jhogg11 Could you please confirm if you have posted the issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues). Thank you!
The Keras GitHub states to post it on the Tensorflow forum, and I did that, so that's one more venue where I've posted the question without a meaningful response.
https://discuss.tensorflow.org/t/basic-keras-model-underperforming-against-scikit-learn-mlpregressor/10485","""post it on the Tensorflow forum, and I did that, so that's one more venue where I've posted the question without a meaningful response."""
56674,markstrefford,1175427332,2022-07-05 19:38:52,"> Note that model.predict is not intended to use inside the loop or to iterate data and use it.
> It is intended for batch processing and large number of inputs.
> For details check [here](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly#predict).
Hi yes, I realise that I should use `prediction = model(input_image)` or similar. However, the `for` loop works and predicting on the dataset doesn't","""Note that model.predict is not intended to use inside the loop or to iterate data and use it."""
43497,MPKenning,1175075427,2022-07-05 13:41:32,"The other difficulty is that stacking the results of a for-loop is apparently forbidden by graph-execution mode: the interpreter complains of a matrix with changing dimensions, which is addressed by relaxing the shapes, but it is not a pretty solution nevertheless and certainly not efficient, as you say. This would be such a helpful innovation if it were implemented.","The other difficulty is that stacking the results of a for-loop is apparently forbidden by graph-execution mode: the interpreter complains of a matrix with changing dimensions, which is addressed by relaxing the shapes, but it is not a pretty solution nevertheless and certainly not efficient, as you say. This would be such a helpful innovation if it were implemented."
43497,MPKenning,1175032465,2022-07-05 13:00:21,"Is there interest in a sparse einsum? I have an interest in it myself. I want to multiply two matrices (k, v, v) and (k, v, f) together, the first of which is highly sparse, and so far I have found no way to do this efficiently. I have tried using a for-loop over the dimension k and stack the results, but it is neither pretty nor efficient.","I have tried using a for-loop over the dimension k and stack the results, but it is neither pretty nor efficient."
56677,KingsleyLiu-NV,1174993687,2022-07-05 12:19:17,"@tilakrayal I don't think you are answering my questions. I want to know whether there are differences for `model(inputs)` between when inputs are `TensorSpec` and when inputs are `Tensor`. Based on what I see in the log, they are different.
Besides, I don't think `call()` is used directly by `model(inputs)`. Instead, `model(inputs)` invokes `__call__`, which do some internal operations as well as the operations defined in `call()`.",I don't think you are answering my questions.
56674,tilakrayal,1174901316,2022-07-05 10:32:16,"@sachinprasadhs,
While I was trying to reproduce the provided code, the execution was taking longer than expected. Kindly find the [gist](https://colab.research.google.com/gist/tilakrayal/c6b64b5740e18886f8f9edc52417dc33/2-8dog_classifier.ipynb) of it [here](https://colab.research.google.com/gist/tilakrayal/dcfb4b657c010a678648c3d744498fc5/2-9dog_classifier.ipynb).","""The execution was taking longer than expected."""
56535,benkli01,1174835928,2022-07-05 9:28:09,"Hi @abattery. Thanks for looking into this. The issue here is that there is no ""strict mode"". We can not rely on the converter to produce a TFLite graph that only contains int8 operations (which might be required by the hardware).","""We can not rely on the converter to produce a TFLite graph that only contains int8 operations (which might be required by the hardware"")"""
56535,rino20,1174818719,2022-07-05 9:12:29,"Hi, @abattery It seems you are already assigned to this issue. Could you take a look at this issue? - This is a blocker of ARM side.",- This is a blocker of ARM side.
56673,elfringham,1174766638,2022-07-05 8:24:29,"@gadagashwini Unfortunately the colab gists referenced above by @mohantym are not relevant, you cannot build with --config=mkl_aarch64 on x86.
Full log from an AARCH64 build is here
https://ci.linaro.org/job/ldcg-python-manylinux-tensorflow-onednn-nightly/108/consoleText","""Unfortunately the colab gists referenced above by @mohantym are not relevant, you cannot build with --config=mkl_aarch64 on x86."""
56639,akowalsk,1173799458,2022-07-04 13:06:38,I cannot find any way to install that version on ubuntu 20.04. I tried installing per your comments here: https://github.com/tensorflow/tensorflow/issues/56432 but it didn't work.,I cannot find any way to install that version on ubuntu 20.04.
45200,hkayann,1173711658,2022-07-04 11:34:04,None of the above helped my case so I went with docker.,None of the above helped my case so I went with docker.
56669,Li-Jiajie,1173645777,2022-07-04 10:27:06,"> @Li-Jiajie In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thank you!
import tensorflow as tf
class TestModel(tf.keras.Model):
def __init__(self, *args, **kwargs):
super().__init__(*args, **kwargs)
self.dnn_model = self
print('here', self.trainable_variables) # here
test = TestModel(""test"")","""Please provide a code snippet to reproduce the issue reported here."""
53109,dzorwulu,1173631444,2022-07-04 10:13:52,"Hello butler -
The suggested build did not succeed. The LLVM package issue reappeared. Are the keys to these files wrong? Please see attached log file.",The suggested build did not succeed.
56665,njuhang,1173559120,2022-07-04 9:08:40,"I use ./configure to change python version from 3.6 to 3.8 now, but it doesn't work.","I use ./configure to change python version from 3.6 to 3.8 now, but it doesn't work."
56585,gadagashwini,1173450475,2022-07-04 7:26:40,"Hi @wangzy0327,
Delete the tensorflow repo, re-clone and try again, it starts compiling:
```
bazel clean --expunge
rm -rf /packages/tensorflow
!git clone tensorflow
cd /packages/tensorflow
./configure
bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
```","Delete the tensorflow repo, re-clone and try again, it starts compiling."
56665,njuhang,1173411209,2022-07-04 6:38:10,"I tried bazel5.0.0, but encounter some other errors","I tried bazel5.0.0, but encounter some other errors."
44602,NiuKeke,1173291128,2022-07-04 3:12:43,"> @sairmreddy Is this still an issue can you please try with latest stable version and let us know.
I got the same errors on my Ubuntu 20.04 with tensorflow 2.9.0 install.",I got the same errors on my Ubuntu 20.04 with tensorflow 2.9.0 install.
41169,nightpizza123,1173237106,2022-07-04 1:26:48,"sorry everyone, I don't know which comment should be responded. I find I upgraded requests using sudo pip install --upgrade requests and now I can't import it anymore.",I find I upgraded requests using sudo pip install --upgrade requests and now I can't import it anymore.
42701,MoFHeka,1172944630,2022-07-02 18:53:27,I also have this problem when develop user custom variable.,I also have this problem when develop user custom variable.
54390,abattery,1172788271,2022-07-01 23:51:33,Enabling both at once is really hard. You might want to land the new features without enabling by default and then you might be able to carefully turn on in the separate PRs for each repository.,Enabling both at once is really hard.
41403,deepquantum88,1172706738,2022-07-01 20:56:13,"@ymsaputra hey, did you able to solve the issue?
Because I also tried with multiple inputs, it throws the same error.","I also tried with multiple inputs, it throws the same error."
56484,bixia1,1172624504,2022-07-01 18:49:58,this is currently waiting on https://github.com/tensorflow/tensorflow/pull/56497,This is currently waiting on https://github.com/tensorflow/tensorflow/pull/56497.
56435,vsbc2010,1172543158,2022-07-01 16:59:37,environment variable TF_ENABLE_ONEDNN_OPTS to 1 for Python only. But not working with build param,"""But not working with build param"""
33975,Flamefire,1172166333,2022-07-01 9:56:06,"Hi @mohantym I just got around testing this with the latest Bazel and TF but the issue still persists.
Possible solutions is our long-term patch at https://github.com/easybuilders/easybuild-easyconfigs/blob/439c0f2cb41bcb8f11071457550a2786eabe76a8/easybuild/easyconfigs/t/TensorFlow/TensorFlow-2.1.0_fix-cuda-build.patch or https://github.com/tensorflow/tensorflow/pull/56360",I just got around testing this with the latest Bazel and TF but the issue still persists.
56088,elfringham,1172100494,2022-07-01 8:47:16,"I submitted the PR https://github.com/tensorflow/tensorflow/pull/56620 to remove uses of no_oss_py2, which has been approved but not merged yet, though not sure why.
So it seems that the ARM_CI and ARM_CD workflows are only running the pip tests. Are there plans to also run the nonpip tests on the source code directly?
The current test flags for these ARM workflows include ""-requires-gpu,-gpu,-tpu,"" which are not used in the x86 pip tests.","""I submitted the PR https://github.com/tensorflow/tensorflow/pull/56620 to remove uses of no_oss_py2, which has been approved but not merged yet, though not sure why."""
56636,Thf772,1172041005,2022-07-01 7:41:11,"I have tried building Tensorflow for Aarch64 (Raspberry pi 4), and this dependency was the last problem I encountered. I have manually altered the built wheels to remove it, so far without issues.
I think this dependency could (should?) be made optional, especially when building with --config=nogcp.","I have tried building Tensorflow for Aarch64 (Raspberry pi 4), and this dependency was the last problem I encountered."
42053,CJSeqtek,1171629623,2022-06-30 20:09:14,"I get a similar error, I have the latest version. Any help with this?","I get a similar error, I have the latest version. Any help with this?"
56466,rokopi-byte,1171293767,2022-06-30 14:30:29,"The code posted in the colab by @tilakrayal does not work, there are only definition there.. as soon as you call them you have the same error encountered by @gootacatchitall.
Look [here](https://github.com/USCDataScience/Image-Similarity-Deep-Ranking/issues/4#issuecomment-1171287956) for the solution.","The code posted in the colab by @tilakrayal does not work, there are only definition there.. as soon as you call them you have the same error encountered by @gootacatchitall. Look [here](https://github.com/USCDataScience/Image-Similarity-Deep-Ranking/issues/4#issuecomment-1171287956) for the solution."
55534,d0k,1171237194,2022-06-30 13:42:52,I don't have a setup to debug that build configuration and I'm still busy resolving breakages from your batch matmul change. What is broken for mkl_aarch64?,I don't have a setup to debug that build configuration and I'm still busy resolving breakages from your batch matmul change.
55534,snadampal,1171198029,2022-06-30 13:08:03,"I'm testing on aarch64 system, with https://github.com/tensorflow/tensorflow/commit/d74695c1097b181e4fe6b68e47b620053ce4a073 checked out, the default config builds fine, but not the '--config=mkl_aarch64'. would you be able to make sure mkl_aarch64 build is also tested before merging the PR. Recently we've setup aarch64 ci/cd for tensorflow
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/Dockerfile.cpu.arm64
Is there something I can help with?","""I'm testing on aarch64 system, with https://github.com/tensorflow/tensorflow/commit/d74695c1097b181e4fe6b68e47b620053ce4a073 checked out, the default config builds fine, but not the '--config=mkl_aarch64'. would you be able to make sure mkl_aarch64 build is also tested before merging the PR."""
49944,Flamefire,1171019026,2022-06-30 10:02:19,"No update on such a fundamental issue after more than 1 year?
Note that we have to fully disable the AVX512 optimization in order to use TensorFlow on such systems: https://github.com/easybuilders/easybuild-easyconfigs/blob/develop/easybuild/easyconfigs/t/TensorFlow/TensorFlow-2.5.0_disable-avx512-extensions.patch",No update on such a fundamental issue after more than 1 year?
56624,tilakrayal,1170989660,2022-06-30 9:33:22,"@gowthamkpr,
While I was trying to reproduce the issue on tensorflow v2.8, v2.9 and nightly, the code execution time was longer than expected. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/5560b8fd2223ffb0a584f4d2d253eefd/56624.ipynb).","""The code execution time was longer than expected"""
56454,shenh10,1170989027,2022-06-30 9:32:40,"> yes, did you reproduce the problem? I am not able to edit the gist.",I am not able to edit the gist.
55966,kaixih,1170854444,2022-06-30 7:14:30,I can see some tests are failed but the log shows the failure is unrelated to this PR. Can you help check? @reedwm,I can see some tests are failed but the log shows the failure is unrelated to this PR.
53101,JinXiaozhao,1170798808,2022-06-30 5:58:00,"@mohantym As gaonmaor said,""We can see that it uses unsupported operations based on the original post. As I've said, it used to work fine up until the previous version (2.6.2). You can try running the above code and see it yourself. It is a degradation which was introduced on version 2.7.0 and that's why we need to follow it to the relevant TensorFlow development group.""","""We can see that it uses unsupported operations based on the original post."""
56588,njuhang,1170686665,2022-06-30 2:41:30,"I followed this instruction to do conversion. But I got the whole graph in .tflite. I tried to specify some intermediate nodes as output_arrays but failed. No matter what I specify to parameter output_arrays, even if some meaningless name, there is no difference in .tflite file. What I want is to cut a piece from the whole graph.Thanks","""No matter what I specify to parameter output_arrays, even if some meaningless name, there is no difference in .tflite file."""
56530,xlnwel,1170601140,2022-06-29 23:48:44,"Hi @sachinprasadhs. I've read the link and understand your point. But I'm wondering if there is some way to avoid recomputing `inner` inside `outer` in the following link since `inner` may be performed multiple times before calling `outer` and repeating `inner` inside `outer` may be quite computationally expensive. Maybe with some lower-level techniques than `tf.function` in TF?
https://colab.research.google.com/drive/10V4V5Hr25NZehWE2TjFPD0-jtGpCFDMa?usp=sharing#scrollTo=O9G5leXLeexD",I've read the link and understand your point. But I'm wondering if there is some way to avoid recomputing inner inside outer in the following link since inner may be performed multiple times before calling outer and repeating inner inside outer may be quite computationally expensive. Maybe with some lower-level techniques than tf.function in TF?
31312,chongxiaoc,1170510460,2022-06-29 21:28:07,See similar large memory leakage issues in TF 2.6 when calling keras fit() function with tensorflow dataset.,See similar large memory leakage issues in TF 2.6 when calling keras fit() function with tensorflow dataset.
56405,gKurbis,1170496779,2022-06-29 21:10:53,"Hi @sachinprasadhs, Were you able to build this on your end?
I see there was a recent commit (commit 8eb1cba939c2ffaf0474d53f3d63debd458c9a3c)
that seems to address this, but I am still unable to build the benchmark framework on my end after applying these changes.","I see there was a recent commit (commit 8eb1cba939c2ffaf0474d53f3d63debd458c9a3c) that seems to address this, but I am still unable to build the benchmark framework on my end after applying these changes."
55959,DEKHTIARJonathan,1170200639,2022-06-29 16:20:25,"Depends on https://github.com/tensorflow/tensorflow/pull/56497
I need to wait on #56497 to rebase everything properly",Depends on https://github.com/tensorflow/tensorflow/pull/56497 I need to wait on #56497 to rebase everything properly.
56540,H-s-O,1169996272,2022-06-29 13:41:24,"The .tar.gz downloads are still not completely fixed; I get random download failures, then starts working again when retrying a few minutes later.","The .tar.gz downloads are still not completely fixed; I get random download failures, then starts working again when retrying a few minutes later."
48545,alexcoca,1169954614,2022-06-29 13:04:18,@nouiz according to https://github.com/tensorflow/tensorflow/issues/36465 I am in for bitter disappointment as this fix does not seem to solve a fundamental issue: deallocation of memory allocated to `tf` models once there are no more references to them. Is this feature impossible to implement for `tensorflow` @nouiz ?,"""I am in for bitter disappointment as this fix does not seem to solve a fundamental issue: deallocation of memory allocated to tf models once there are no more references to them."""
27298,mohantym,1169876881,2022-06-29 11:40:19,It is still replicating in [2.9 ](https://colab.sandbox.google.com/gist/mohantym/45b195bfa177a64af960f530308f8ad6/untitled56.ipynb#scrollTo=659SZpY7a5eu)and [2.10.0dev ](https://colab.sandbox.google.com/gist/mohantym/e4c752c98c44fda71e1514a91b4aa441/untitled56.ipynb#scrollTo=S-9S7X1kKXCH).,It is still replicating in [2.9](https://colab.sandbox.google.com/gist/mohantym/45b195bfa177a64af960f530308f8ad6/untitled56.ipynb#scrollTo=659SZpY7a5eu)and [2.10.0dev ](https://colab.sandbox.google.com/gist/mohantym/e4c752c98c
36465,andreped,1169851766,2022-06-29 11:12:09,"> Any update in 2022?
Doubt it will ever be fixed. Perhaps in TF 3, but doubt it.
I guess for now, don't use TF in deployment. Use OpenVINO/TensorRT with the ONNX format instead, but for training models TF works just fine.","""I guess for now, don't use TF in deployment."""
54899,hengshan123,1169724183,2022-06-29 9:07:29,"hi, I meet the same problem. I add github.com cer in java cacerts and set proxy. There is still has the problem:
""PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target"". How do you set your proxy ?","""I add github.com cer in java cacerts and set proxy. There is still has the problem: ""PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target""."
56255,dathudeptrai,1169664930,2022-06-29 8:06:38,"@gdannyliao I also had this issue. My model need to use flexDelegate so i added ""-force_load $(SRCROOT)/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps"" to ""Other Linker Flags"" but when i built an app there was many ""Undefined symbols for architecture arm64"" bug. Downgrade `TensorFlowLiteSwift ` and `TensorFlowLiteSelectTfOps ` to 2.6.0 work fine on IOS but my model need to use tf 2.9.x to be able to convert GroupConvolution to tflite.","""Undefined symbols for architecture arm64"" bug."
56422,rajeev921,1169620244,2022-06-29 7:15:43,@gadagashwini @angerson @Arnold1 Hi Will anyone help me to resolve this. It been long time but no reply ?,It been long time but no reply ?
56356,penpornk,1169250299,2022-06-28 21:14:58,"@kulinseth There seems to be an issue finding the `Expm1` op on M1 GPU. Could you please help take a look? ```
No registered 'Expm1' OpKernel for 'GPU' devices compatible with node {{node sub}}
. Registered: device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]
```
@AlbertoSinigaglia Have you installed the [`tensorflow-metal` plugin](https://developer.apple.com/metal/tensorflow-plugin/)? It is required for GPU usage.","No registered 'Expm1' OpKernel for 'GPU' devices compatible with node node sub . Registered: device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF] "
56276,bhack,1168877901,2022-06-28 15:32:47,"About failing copybara:
```bash
bazel test --test_env=TF_XLA_FLAGS=--tf_xla_auto_jit=2 --test_env=XLA_FLAGS=""--xla_dump_to=/tmp/generated"" --distinct_host_configuration=True tensorflow/python:bincount_ops_test_gpu --flaky_test_attempts=1
//tensorflow/python:bincount_ops_test_gpu PASSED in 11.0s
```
```bash ls /tmp/generated/*.ptx | wc -l
13
```","""Failing copybara: bash bazel test --test_env=TF_XLA_FLAGS=--tf_xla_auto_jit=2 --test_env=XLA_FLAGS=""--xla_dump_to=/tmp/generated"" --distinct_host_configuration=True tensorflow/python:bincount_ops_test_gpu --flaky_test_attempts=1"""
55941,learning-to-play,1168184114,2022-06-28 3:43:44,"As I shared with @trevor-m, multiple presubmits are failing including Windows GPU, Windows Bazel GPU, and so on as you can see in the table above. You need to pass all the presubmits. Furthermore, because the change touches many different directories, multiple owners have to approve the code. Feel free to sync and make sure all the presubmits pass. I can check if all the internal tests pass and also help add all the reviewers here (currently some of the comments are only visible internally).",multiple presubmits are failing
56404,roshan2908,1167691306,2022-06-27 18:01:52,"I am not able to reproduce the same on colab. Seems like it might be because of different version of keras. So uninsatalled keras and installed it again, it worked for me. pip uninstall keras
pip install keras --upgrade",I am not able to reproduce the same on colab.
48021,barabanus,1167621454,2022-06-27 17:05:52,"@ohadlights As I remember, this didn't work, so I had to open the project within Visual Studio and set config manually. Anyway, the problem is that by default it should be build as ""Release"".",I had to open the project within Visual Studio and set config manually.
17353,Devvvi1,1167545840,2022-06-27 16:08:08,"I am trying deepamc_maskrcnn and having the same issues.
https://github.com/tensorflow/models/tree/master/official/projects/deepmac_maskrcnn
![image](https://user-images.githubusercontent.com/103889847/175985384-d9c8cc1b-2d86-47a0-baba-2defd9d6ada8.png)
![image](https://user-images.githubusercontent.com/103889847/175985604-030f72d5-ed00-48c2-aec5-59fc58bdaf33.png)",I am trying deepamc_maskrcnn and having the same issues.
55959,bixia1,1167485341,2022-06-27 15:17:09,Can you squash the commits?,Can you squash the commits?
56584,FragrantRookie,1166796021,2022-06-27 3:29:59,Could not find file disco_f746ng_makefile.inc in folder named tflite-micro.,Could not find file disco_f746ng_makefile.inc in folder named tflite-micro.
55563,agostini01,1166639309,2022-06-26 20:39:06,"Hi, I had a similar issue but `bazel clean --expunge` did not fix my problem. Instead I had to use the workaround below
I had my env variables set CC=/usr/bin/clang CXX=/usr/bin/clang++
But bazel (v5.1.1) and tensorflow (93b2ef6e27c63f48f7bab67a630ee226b78e74c6) have some issues when compiling the with clang: https://github.com/bazelbuild/bazel/issues/15359
To fix this issue I cleared my CC and CXX env variables and compiled tensorflow with gcc9","""I had my env variables set CC=/usr/bin/clang CXX=/usr/bin/clang++"""
56381,pure-rgb,1166352991,2022-06-25 20:01:15,"@cantonios sorry I didn't wanted to create noise in issue. But I really think there should be intuitive approach to do item assignment. It doesn't need to be same as numpy. The jax also doesn't support item assignment but yet it provides some nice option, mentioned here https://github.com/google/jax/discussions/11035",I really think there should be intuitive approach to do item assignment.
55600,cantonios,1166330079,2022-06-25 17:28:02,Potentially. The user hasn't responded back to report that it works for them.,The user hasn't responded back to report that it works for them.
55522,impjdi,1165773296,2022-06-24 17:13:56,"Sorry, I'm not too familiar with lib.so setup; I only use statically linked binaries. In fact, I never got dynamically linked libraries to work with Bazel; I remember I gave up after a couple weeks of trying.
`undefined symbol: glFenceSync` is a linker error, so linking is failing due to not finding EGL / GLES libraries.","""I never got dynamically linked libraries to work with Bazel; I remember I gave up after a couple weeks of trying."""
56231,qlzh727,1165719345,2022-06-24 16:07:24,"The latest nightly should include this fix already. It should reach tf 2.10 release.
For auto complete ""from tensorflow.keras import layers"", I think it worked before 2.7 since keras was a python package located under tensorflow/python/keras. After we split the keras into a separate PIP package, i think we have some difficulty to build such hard deps in the tf/keras __init__ files. The import itself should still work as always, but the auto completion part is not.","""After we split the keras into a separate PIP package, i think we have some difficulty to build such hard deps in the tf/keras __init__ files."""
56547,renatobellotti,1165476810,2022-06-24 11:21:38,@sushreebarsa This is what I tried (see example code). The problem is that the sparse dense matrix multiplication implementation does not have a reproducible version.,The problem is that the sparse dense matrix multiplication implementation does not have a reproducible version.
56558,mohantym,1165439752,2022-06-24 10:32:17,"@FengMu1995 ! Few reasons not detecting GPU delegate be from below. 1. If you have not declared Gpu [dependencies](https://www.tensorflow.org/lite/performance/gpu#step_2_edit_appbuildgradle_to_use_the_nightly_gpu_aar) in build.gradle file.
2. If there is no Gpu on device Could you move this to closed status if it is resolved.","""Few reasons not detecting GPU delegate be from below."""
55522,BenjaminWegener,1165349148,2022-06-24 8:46:07,"@impjdi @sachinprasadhs @mohantym i built it again from 2.9 sources and even tested with tflite_runtime from pipy, same result. `undefined symbol: glFenceSync`
I guess there are two possibilities:
-missing library during build (android doesnt need gles libs for example)
-some error in the load_delegate function
I will dive deeper into it, during the weekend.",undefined symbol: glFenceSync
56432,Scheggetta,1165261461,2022-06-24 6:52:55,"So why do I have this `crosstool_wrapper_driver_is_not_gcc` error?
This error occured with both tf versions (i.e., 2.8 and 2.9).",So why do I have this crosstool_wrapper_driver_is_not_gcc error?
44129,FengMu1995,1165246732,2022-06-24 6:30:38,"@michaelnguyen11 Really? You could run tflite gpu delegates on Nvidia gpu,.
I tried it, but it failed. Failed to modify graph with delegate",Failed to modify graph with delegate.
55534,d0k,1164847919,2022-06-23 20:33:31,You should be able to see wrong results on `bazel test -c opt //tensorflow/compiler/xla/tests:dot_operation_test_cpu` https://source.cloud.google.com/results/invocations/e217cb81-ca26-408d-ac6f-b4f8da0e081c/log,"""should be able to see wrong results on bazel test -c opt //tensorflow/compiler/xla/tests:dot_operation_test_cpu https://source.cloud.google.com/results/invocations/e217cb81-ca26-408d-ac6f-b4f8da0e081c/log"""
56381,cantonios,1164774308,2022-06-23 19:12:35,"Duplicate of #33131 - yes, it is on the roadmap, but will not allow direct assignment like that since tensors are immutable.","Duplicate of #33131 - yes, it is on the roadmap, but will not allow direct assignment like that since tensors are immutable."
56452,meena-at-work,1164673814,2022-06-23 17:19:57,"> This is currently failed with some existing code and new checks :-( Basically, we will need to replace three absl::make_unique in this file with its std:: correspondent.
@bixia1 , I've made a separate commit with the absl replacements. Can you please review?",:-(
56276,bhack,1164672142,2022-06-23 17:18:09,"Honestly I don't know why we cannot align some these flag in our OSS public build (`-Werror,-Wunused-result`).","-Werror,-Wunused-result"
56533,amol-tw,1164174162,2022-06-23 9:29:25,"@mohantym I have updated the issue with the inference code and more details, but I am afraid that it's never called as the crash happens during the initialization.","""I have updated the issue with the inference code and more details, but I am afraid that it's never called as the crash happens during the initialization."""
55959,DEKHTIARJonathan,1163793852,2022-06-23 0:20:32,"@bixia1 I blocked my change for TRT < 8.0.0. Can you relaunch the tests internally ?
I'm still not able to reproduce with TRT 7.1.3 on my side",I blocked my change for TRT  8.0.0. Can you relaunch the tests internally ? I'm still not able to reproduce with TRT 7.1.3 on my side.
56537,impjdi,1163739383,2022-06-22 22:58:40,"As mentioned in the tflite_convert's error & warning messages, 6D tensors are not really supported in TFLite; only through Flex delegate which pulls in (non-mobile) TF code.
My team mates were able to get away with a hack folding / unfolding tensors into lower ranks to be able to use the GPU delegate. See whether you can do that. When you do it naively, you will indeed end up with 6D tensors which is not TFLite-friendly, especially not TFLite GPU.","""6D tensors are not really supported in TFLite; only through Flex delegate which pulls in (non-mobile) TF code."""
48167,lululxvi,1163505443,2022-06-22 19:11:44,"@rohitvuppala The error doesn't seem from DeepXDE, as there is only one place using NumPy
https://github.com/lululxvi/deepxde/blob/2ee292f0cbfb0aed69a786b02bc062a938907207/deepxde/optimizers/tensorflow/tfp_optimizer.py#L51","The error doesn't seem from DeepXDE, as there is only one place using NumPy."
55608,bixia1,1163391194,2022-06-22 17:05:42,Please squash the commits.,"""Please squash the commits."""
55534,d0k,1162886571,2022-06-22 9:47:00,"The merge conflict is in xla.proto, when another flag is added, I added another one yesterday, but it has been rolled back for now.
I don't seem to have access to the other CI builds either, it's just 404. Is there a way to retrigger them?","The merge conflict is in xla.proto, when another flag is added."
56518,FengMu1995,1162582354,2022-06-22 3:03:59,"I build the tflite2.8 by the following steps:
""""""
./configure
bazel build --config=monolithic --config=noaws --config=nohdfs --config=nonccl --config=v2 --define=tflite_convert_with_select_tf_ops=true --define=with_select_tf_ops=true //tensorflow/lite:libtensorflowlite.so
""""""
I'm not sure if the lib is built with NNAPI
if you have the built lib, please upload to somewhere, it is convenient to use for customers",""""""" I'm not sure if the lib is built with NNAPI if you have the built lib, please upload to somewhere, it is convenient to use for customers. """""""
56518,FengMu1995,1162531285,2022-06-22 1:47:39,"Well, I've uploaded my code in rvm_tflite2.zip. I tried to add your code to mine, but it reports: ""/rvm_tflite_2/main.cc:58: error: undefined reference to `tflite::NnApiDelegate()'
I am not sure if the needed .h files or .lib are included.
[rvm_tflite_2.zip](https://github.com/tensorflow/tensorflow/files/8953825/rvm_tflite_2.zip)",I am not sure if the needed .h files or .lib are included.
56527,rohitvuppala,1162528651,2022-06-22 1:41:43,"Accidentally opened an issue in the wrong repo where the comment originated, but this repo/package might be unrelated to the issue.","""Accidentally opened an issue in the wrong repo where the comment originated, but this repo/package might be unrelated to the issue."""
56430,ngam,1162429977,2022-06-21 22:34:40,"Hi @gadagashwini, I tried and it doesn't work. I am now going ahead and reverting daaed07d1056c522035f602e16196091eb371534 and 0b6cc85863c4484586d1317b08d7c42bc8a0640c to see if this helps. Is there anyway for us to hear from @majnemer about this? The author of these two commits may be able to diagnose this better, especially as these two commits weren't made in a PR (direct commits to master as far as I could tell)","I am now going ahead and reverting daaed07d1056c522035f602e16196091eb371534 and 0b6cc85863c4484586d1317b08d7c42bc8a0640c to see if this helps. Is there anyway for us to hear from @majnemer about this? The author of these two commits may be able to diagnose this better, especially as these two commits weren't made in a PR (direct commits to master as far"
56261,DemoLV,1162225559,2022-06-21 19:20:55,Is there a way @sachinprasadhs to post this problem somewhere else if no one is responding to this issue ?,No one is responding to this issue.
56372,hubingallin,1162180989,2022-06-21 18:45:22,"Hi @lightingghost, it's a known issue that DistributedIterator cannot be checkpointed. Support on this may require further discussion and design.","""It's a known issue that DistributedIterator cannot be checkpointed."""
44160,MarkDaoust,1162086258,2022-06-21 17:45:50,"> Preprocessing layers take long time to adapt the entire dataset, is there any solutions?
Sometimes people forget to batch the dataset before passing it to adapt. Using a large batch size instead of individual records can make a huge difference. If the problem is instead that you actually have a **large** dataset then you shouldn't be using `.adapt` you should use tensorflow_transform.","""Preprocessing layers take long time to adapt the entire dataset, is there any solutions?"""
55959,bixia1,1161976188,2022-06-21 16:21:38,This PR have some test failure that Jonathan is having problems to reproduce.,"""Having problems to reproduce"""
56505,FragrantRookie,1161553178,2022-06-21 10:20:32,"Hi @mohantym
My computer reported the same error as yours when using `tf.lite.TFLiteConverter.from_keras_model`.No matter using this codes or not:
```
converter.target_spec.supported_ops = [
tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]
converter.inference_input_type = tf.float32
converter.inference_output_type = tf.float32
converter.experimental_enable_resource_variables = True
converter.representative_dataset = representative_dataset
```","""No matter using this codes or not"""
46779,CuiYifeng,1161136666,2022-06-21 2:34:37,Close this PR due to similar fixes in https://github.com/tensorflow/tensorflow/pull/56139.,"""Close this PR due to similar fixes in https://github.com/tensorflow/tensorflow/pull/56139.."""
56231,laoshaw,1160967818,2022-06-20 23:59:17,"vim failed to autocomplete with keras as described here, waiting for a permanent fix.","vim failed to autocomplete with keras as described here, waiting for a permanent fix."
35160,maingoh,1160660073,2022-06-20 16:45:54,Can you reuse your [gist](https://colab.research.google.com/gist/mohantym/76d86dfc64f65b2bd90e0d5ebe3e5179/git_35160.ipynb) and just use tensorflow 2.9.1 instead of 2.8 ?,Can you reuse your [gist](https://colab.research.google.com/gist/mohantym/76d86dfc64f65b2bd90e0d5ebe3e5179/git_35160.ipynb) and just use tensorflow 2.9.1 instead of 2.8 ?
35160,maingoh,1160655294,2022-06-20 16:39:32,"Thank you @mohantym. This [PR](https://github.com/tensorflow/build/pull/57) changed the default ABI but I don't think it fixed the issue. If I use the code above I get a difference of RAM usage between tf 2.8 and 2.9. More people might complain now that the CXX11 ABI is used by default.
Note that my testing network has been trained with tf 1 but I don't think it change much (I don't have a tf 2 trained equivalent).","""I don't think it fixed the issue"""
56434,josuuribe,1160517133,2022-06-20 14:28:27,"Hi @mohantym I have read your comment and the stackoverflow thread, I do not hace such file **/etc/odbcinst.ini** because this is a ODBC specific configuration file, not related to my problem.","I do not hace such file **/etc/odbcinst.ini** because this is a ODBC specific configuration file, not related to my problem."
56412,oekosheri,1160144625,2022-06-20 8:36:13,"Hey, So 10 days have passed. Does this mean I get no response from Tensorflow?","""I get no response from Tensorflow"""
56465,tfeher,1159780356,2022-06-19 17:25:43,"@DEKHTIARJonathan Could you remove the last point in the description (""Cache device memory size""). That was separately added in #56268, so it is not part of this PR anymore.","""That was separately added in #56268, so it is not part of this PR anymore."""
56500,GravermanDev,1159523175,2022-06-18 17:47:52,"I found the issue, my environment had a bug and the observation was too big of a number","I found the issue, my environment had a bug and the observation was too big of a number."
56482,francescoyou97,1159395903,2022-06-18 8:24:12,"Sorry but in both the case i use tge GRU layers, but the results are almost the same. The RESHAPE and UNPACK ops have some problems.",The RESHAPE and UNPACK ops have some problems.
50705,thierryherrmann,1159303295,2022-06-17 23:34:30,"@sachinprasadhs I'm sorry to contradict you. `tf.saved_model.save()` is absolutely not deprecated.
It's not deprecated even as of TF2.9: https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/saved_model/save
And it is also described in details in the guide (at least as of today): https://www.tensorflow.org/guide/saved_model#saving_a_custom_model",I'm sorry to contradict you.
44061,sachinprasadhs,1159135159,2022-06-17 18:31:08,"`tf.contrib` module has been deprecated, as the error message suggests, use the OP which does not make use of OP from deprecated tf.contrib.","tf.contrib module has been deprecated, as the error message suggests, use the OP which does not make use of OP from deprecated tf.contrib."
56479,rsanthanam-amd,1158791154,2022-06-17 11:45:54,This has always passed on Nvidia. The enablement of this test on ROCm was done several weeks ago but needed the modification in this PR because of some other change. The same modification was done for Nvidia recently and that change needs to be replicated for CUDA.,The enablement of this test on ROCm was done several weeks ago but needed the modification in this PR because of some other change.
40309,Bourhano,1158708274,2022-06-17 9:54:13,"Same problem...
Update: Even though you print the tensor and get Tensor(""args_0:0"", shape=(), dtype=string) when you print but this does not repesent the actual values inside the tensor, i.e. if you just process your string tensor (or other tensor) appropriately, it will give you the good result, despite printing other stuff.
TL;DR: Don't trust the output of print() on a Tensor.",Same problem...
56386,EnricoMi,1158680325,2022-06-17 9:20:42,"You can call private method `_stop()` of `DispatchServer` and `WorkerServer`, which will stop the underlying servers.
The `__del__` method is calling that private method, so eventually, `_stop` gets called anyway.
There is no public / documented way to shut them down gracefully. And there is no explanation why.","""And there is no explanation why."""
54390,abattery,1158477137,2022-06-17 4:25:28,"For example,
```
[lut.h:47]:25: error: use of undeclared identifier 'optimized_ops'
uint8x16_t output = optimized_ops::aarch64_lookup_vector(table, input);
^
1 error generated.
```",[lut.h:47]:25: error: use of undeclared identifier 'optimized_ops'
55808,wenscarl,1158449159,2022-06-17 3:22:50,"> I would expect something along the lines of https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/compiler/jit/mark_for_compilation_pass_test.cc;l=180-197
Test fail with
`bazel test -test_env=TF_XLA_FLAGS=--tf_xla_cluster_exclude_ops=Where --config=cuda -c opt //tensorflow/compiler/jit:compilation_passes_test `
and pass with `bazel test --config=cuda -c opt //tensorflow/compiler/jit:compilation_passes_test `
which are as expected. @cheshire Could you review the changes?","""Test fail with bazel test -test_env=TF_XLA_FLAGS=--tf_xla_cluster_exclude_ops=Where --config=cuda -c opt //tensorflow/compiler/jit:compilation_passes_test  and pass with bazel test --config=cuda -c opt //tensorflow/compiler/jit:compilation_passes_test  which are as expected."""
56473,abhimanyuchadha96,1158440776,2022-06-17 3:06:07,"@kulinseth Yes it is the M1 machine.
I just did `import horovod.tensorflow` and the error appeared.
Are you referring to installation steps? To setup Tensorflow and Horovod on M1?",I just did import horovod.tensorflow and the error appeared.
33005,alexmil2019,1158364331,2022-06-17 1:10:12,"I have checked on nightly but still has the same issue. can you fix this?
``` tensorflow.python.framework.errors_impl.InvalidArgumentError: Current implementation only supports equal length strides in the row and column dimensions.
[[{{node model/conv/StatefulPartitionedCall/StatefulPartitionedCall/dw_conv2d_1/depthwise}}]] [Op:__inference_predict_function_43730]```","""InvalidArgumentError: Current implementation only supports equal length strides in the row and column dimensions."""
36465,nalane,1158139998,2022-06-16 21:19:37,"@j-mendez Right, so it looks like the wasm backend runs on the CPU. This issue is about how TF doesn’t release memory specifically on the GPU when a model drops out of memory. I suppose the two could be related, but I have personally never had these issues when running multiple models in sequence on a CPU with the usual Python bindings",TF doesn’t release memory specifically on the GPU when a model drops out of memory.
36465,j-mendez,1158075693,2022-06-16 19:55:57,"When using node.js it's almost impossible to clear the memory being held. It's not realistic to start a new sub process in node either to handle each request. If you try to clear any model including the default Mobilenet and CocoSSD etc, no luck. This issue is not related to gpu entirely and occurs on `wasm` backends too.","""It's not realistic to start a new sub process in node either to handle each request."""
56434,josuuribe,1157536331,2022-06-16 11:11:45,"@sushreebarsa thanks for your response, I have tried this it in a RPI with 8GB RAM, so I think this is not a memory constraint, in particular the model it's very simple and works perfectly except when I try to save it.
So the main problem is that seems is not possible use **model.save_weights(..)** function, this is a problem in most scenarios and I think not related to memory constraints.","""It's very simple and works perfectly except when I try to save it."""
53278,xsacha,1157476386,2022-06-16 10:03:57,"Any updates on this? How can it not be working for so long?
I just tried v2.9 and quantisation still does not work.
Again, converting to tensorflow lite works. It just exhibits weird behaviour when default optimisations are enabled.",I just tried v2.9 and quantisation still does not work.
52392,KangGrandesty,1157425755,2022-06-16 9:14:20,"We just use TF 2.4 and are not plan to upgrade version. @mohantym Even the test cases report errors, some of our models still work. And we make trade-off between other models and tflite.","""And we make trade-off between other models and tflite."""
56466,gootacatchitall,1157425039,2022-06-16 9:13:35,"@tilakrayal I tried ur method but come out with new bug...
![image](https://user-images.githubusercontent.com/50090044/174036284-a0bb5e9b-cff0-4d70-b185-6e947e373fb4.png)
I am using tf 1.x version because this bug [#2458](https://github.com/matterport/Mask_RCNN/issues/2458)
Btw,I just put my gist here so u can have full view on the code.
https://colab.research.google.com/drive/1CEogpAk3MAbF-3uDXVKiUd2h9rQXkLWU?usp=sharing",I tried ur method but come out with new bug...
56458,mohantym,1157198543,2022-06-16 3:43:01,@geraldstanje ! It is not crashing just skipping the statements from TF_SessionRun onwards.,It is not crashing just skipping the statements from TF_SessionRun onwards.
56474,weisrc,1157195559,2022-06-16 3:35:49,"Sorry, I will try to get the template filled when I have access to my computer.",I will try to get the template filled when I have access to my computer.
55808,wenscarl,1157125063,2022-06-16 1:09:09,"> Forking a process and writing to a global directory for a test is really not great. Maybe look at existing C++ tests for autoclustering?
Could you give me a pointer where to add the C++ test? How about `compilability_check_util_test.cc` ?",Forking a process and writing to a global directory for a test is really not great.
56408,mihaimaruseac,1157090005,2022-06-16 0:10:20,"Known internal breakage. Test disabled.
At the moment, once required builds passed and import/copybara is also green, you are waiting on internal review. Reviewer with approval rights has not yet acted on the internal version of this PR. You are in the bottom left corner here:
![life_of_pr](https://user-images.githubusercontent.com/323199/173962445-08056cd9-db00-49ed-a7f4-11c81eea556b.png)","Known internal breakage. Test disabled. At the moment, once required builds passed and import/copybara is also green, you are waiting on internal review. Reviewer with approval rights has not yet acted on the internal version of this PR. You are in the bottom left corner here: ![life_of_pr](https://user-images.githubusercontent.com/323199/173962445-08056cd9-db00-49ed-a7f4-11c81eea556b.png)"
56408,jamiecook,1157071986,2022-06-15 23:29:20,"Again CI doesn't seem to be building in an unrelated component (python //py_test_dir/tensorflow/python/distribute/failure_handling:failure_handler_test failed in 75% of cases) all other python tests passed: `Executed 821 out of 821 tests: 820 tests pass and 1 fails locally.`
![image](https://user-images.githubusercontent.com/151124/173958663-5b87f88a-052c-47f5-83ef-202410d36dee.png)",Again CI doesn't seem to be building in an unrelated component (python //py_test_dir/tensorflow/python/distribute/failure_handling:failure_handler_test failed in 75% of cases) all other python tests passed.
56157,impjdi,1157068996,2022-06-15 23:24:06,"Unassigned myself, because I'm not familiar with the Python code path =/","Unassigned myself, because I'm not familiar with the Python code path =/"
47802,desmondrous,1156925806,2022-06-15 20:57:31,I ran into this problem when trying to run Sonnet 2.0 with TensorFlow 2.9. This issue is quite similar to #32319. I fixed mine by updating `gast` with : `pip3 install 'gast==0.5.3'`,I ran into this problem when trying to run Sonnet 2.0 with TensorFlow 2.9.
56393,Young768,1156834329,2022-06-15 19:16:31,"@srujun Thanks for the fix. The code is good now. Right now I can increase the global batch size up to 1024 for 8 v100 GPUs.
However, after 30-40 steps, it hits OOM again. Would you be able to use this standard [script](https://github.com/tensorflow/models/blob/archive/research/inception/inception/data/build_imagenet_data.py) to generate TFRecord and reproduce the issue?
Btw, this OOM issue never happens when I use the very small dataset like mnist.","""After 30-40 steps, it hits OOM again."""
44264,shelper,1156831994,2022-06-15 19:13:41,"This is annoying , and causes issues
for instance, i am running ` tensorflow_datasets.load('cats_vs_dogs')`, and it downloads the data, not to my home directory, but to something like `project_root/~/tensorflow_datasets/cats_vs_dogs`, this is definitely not desirable.....","This is annoying , and causes issues for instance, i am running  tensorflow_datasets.load('cats_vs_dogs'), and it downloads the data, not to my home directory, but to something like project_root//tensorflow_datasets/cats_vs_dogs/, this is definitely not desirable."
56429,MoFHeka,1156823650,2022-06-15 19:03:35,"I should create a var_list for var in all devices, then wrap them with values.DistributedVariable. It's my fault.","I should create a var_list for var in all devices, then wrap them with values.DistributedVariable. It's my fault."
56432,Scheggetta,1156806299,2022-06-15 18:42:03,"Sorry for not having answered yet. I will try finding a way to install the right version of gcc. Thanks for the link, but it didn't work since that guide was published on June 4, 2020 when there was available only gcc 9.3.",I will try finding a way to install the right version of gcc.
56469,Enzo90910,1156799268,2022-06-15 18:33:38,"Not directly relevant to this project, but this bug cascades into the Transform component of TFX failing because it tries to delete non existing files and does not catch ""UnknownError"".","Not directly relevant to this project, but this bug cascades into the Transform component of TFX failing because it tries to delete non existing files and does not catch ""UnknownError""."
56393,srujun,1156797264,2022-06-15 18:31:15,"Ah my apologies, that particular line should be:
```
children = [_split(child, splits, axis + 1) for child in children]
```
We want to recursively split on each dimension of the input tensor. Does this fix make it work?"," children = [_split(child, splits, axis + 1) for child in children] "
33131,snarb,1156576679,2022-06-15 14:53:05,"The fact the tensors are immutable creates serious perfomance problems. I want to preallocate once big tensor and then fill it in the loop to avoid expensive memory allocations for small objects. In tensorflow I can't do this. For example I am straggling with taking a batch of crops from the same image as this creates a lots of small tensors inside the loop vectorized_map do not helping. All is super slow, And ability to change tensor would solve this issue.",The fact the tensors are immutable creates serious perfomance problems.
56422,rajeev921,1156564909,2022-06-15 14:43:22,"Hi @gadagashwini, I tried below steps -
1. clone the latest code.
2. git checkout v2.9.1 - facing the same issue, so 3. git checkout origin/master and when I tried same it is throwing the same issue.
Always making this link not found and then error and build stop.
https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/da74868905d61b0e82dace3f833e8f411fae27bc.tar.gz","""I tried below steps - 1. clone the latest code. 2. git checkout v2.9.1 - facing the same issue, so 3. git checkout origin/master and when I tried same it is throwing the same issue. Always making this link not found and then error and build stop."""
52607,pierocor,1156556050,2022-06-15 14:35:55,"I am encountering the same error.
System description:
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux SLE 15
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): v2.9.0-rc2-42-g8a20d54a3c1
Python version: 3.9.7
CUDA/cuDNN version: 11.4/8.2.4
GPU model and memory: V100 (32GB), A100 (40GB)",I am encountering the same error.
40680,prabhat00155,1156248159,2022-06-15 9:45:40,"> I face the same error. any body solved?
I was unable to solve it. Perhaps @talumbau could help you. Also, as this is an unresolved issue, this issue should be reopened.",I face the same error. any body solved?
56457,mohantym,1155951226,2022-06-15 3:48:02,"Hi @MM2932 ! I tracked the error [here](https://developer.android.com/ndk/reference/group/neural-networks#aneuralnetworksdevice_wait:~:text=ANEURALNETWORKS_DEAD_OBJECT) Here are a few things I need to know.
1. Could you confirm whether Android studio is detecting your device in a live state. 2. Commands to implement qti-dsp delegate specificially.","""I tracked the error [here](https://developer.android.com/ndk/reference/group/neural-networks#aneuralnetworksdevice_wait::text=ANEURALNETWORKS_DEAD_OBJECT)"""
36465,ctaoist,1155900429,2022-06-15 2:15:26,"> This has been a problem for two years now. Every once in a while, I'll come back to this thread to see if anything's been done, but nope. This is never getting fixed. Just use PyTorch.
I am running into this problem in jupyter notebook, but I have not used pytorch. Excuse me, does pytorch has no this problem?","This has been a problem for two years now. Every once in a while, I'll come back to this thread to see if anything's been done, but nope. This is never getting fixed. Just use PyTorch. I am running into this problem in jupyter notebook, but I have not used pytorch. Excuse me, does pytorch has no this problem?"
56072,DEKHTIARJonathan,1155773990,2022-06-14 22:32:57,"@bixia1 description added. I tried to find the source of this double logging, I was not able. Can be reproduced doing: `python tensorflow/python/compiler/tensorrt/test/topk_test.py 2>&1 | tee double_log_debug.log`","I tried to find the source of this double logging, I was not able. Can be reproduced doing: python tensorflow/python/compiler/tensorrt/test/topk_test.py 2>&1 | tee double_log_debug.log."
37284,Ir1d,1155614934,2022-06-14 19:10:59,"@mohantym I'm not aware of this HTTPS url in the doc. There are HTTP URLs and they work fine. However, note that the cert for https://download.tensorflow.org is still misconfigured.","I'm not aware of this HTTPS url in the doc. There are HTTP URLs and they work fine. However, note that the cert for https://download.tensorflow.org is still misconfigured."
40309,kaomoneus,1155537502,2022-06-14 18:19:17,"same here. TF Team, it's quite weird that you provided so many tools to convert python POD into tensors, and no way to get it back.","""it's quite weird that you provided so many tools to convert python POD into tensors, and no way to get it back."""
56422,svenstaro,1154907852,2022-06-14 8:58:36,Why highlight me even?,Why highlight me even?
56422,rajeev921,1154872166,2022-06-14 8:24:05,"Can someone please reply, How to resolve the issue as my whole development is stuck due to this build ?","Can someone please reply, How to resolve the issue as my whole development is stuck due to this build ?"
54456,LerysG,1154829412,2022-06-14 7:40:28,"Hi, got a similar problem (running on GTX 1080), using conv1d layers makes my kernel died every time, as my memory GPU is fully saturated even with very low dim input shpaes. I tried tensorflow version 2.5, 2.6, 2.7 and 2.9. I am running on CUDA v11.6 and CudNN v8.4.1 (windows). I could not find any thing to avoid crashing. If anyone has an idea, your very much welcome!","""I could not find any thing to avoid crashing."""
50637,grofte,1154819795,2022-06-14 7:30:03,"If it still causes you trouble then try `.batch(batch_size, drop_remainder=True)` since that's often a magic solve when you have training working for a couple of epochs and then suddenly not working. I don't know why.",I don't know why.
32489,U-Abhishek,1154672873,2022-06-14 3:36:20,@cyrus2018 still facing the same issue,still facing the same issue
51499,arthurmloureiro,1154433710,2022-06-13 20:59:48,"Oh, this is a bit sad because a lot of TensorFlow Probability needs these basic functions and running it on the GPU is much faster.
But as with many things Apple: there's no public `tensorflow-metal` repository (not that I can find) so an issue can be open there. These posts on the Apple Dev Forum seem to be mostly ignoring this issue for months... I wonder how many applications will break in the near future due to this issue.","""As with many things Apple: there's no public tensorflow-metal repository (not that I can find) so an issue can be open there."""
51499,muxamilian,1154429037,2022-06-13 20:53:46,"It's probably not a bug in Tensorflow but Apple's tensorflow metal plugin. See for example the following discussion https://developer.apple.com/forums/thread/689299 or this one https://developer.apple.com/forums/thread/697057?answerId=704830022#704830022
The solution is to run operations from `tf.random` and `tf.sort` and `tf.argsort` on the CPU like this:
```python
with tf.device('/cpu:0'):
tf.random.uniform((10,))
```","""It's probably not a bug in Tensorflow but Apple's tensorflow metal plugin."""
56432,Scheggetta,1154384066,2022-06-13 20:13:44,"I'm having difficulties to find `GCC 7.3.1` from PPAs for Ubuntu 20.04. As I have written in the first post, also tf v2.9.0 gives the same problem with `GCC 9.4.0`.
I'm also considering to try compiling tf with docker as witten [here](https://www.tensorflow.org/install/source#docker_linux_builds). Do you think I will get the same error?",I'm having difficulties to find GCC 7.3.1 from PPAs for Ubuntu 20.04
56408,mihaimaruseac,1154123913,2022-06-13 16:21:06,MLIR breakage at the point this PR was created.,MLIR breakage at the point this PR was created.
56435,vsbc2010,1154111842,2022-06-13 16:09:59,Nothing error message come with Win10 + vs2019 build. But seem not include oneDNN aways. Only show message with XNN after test load. Really working with oneDNN+ Windows ? Seem working with linux-x64 only.,Nothing error message come with Win10 + vs2019 build. But seem not include oneDNN aways. Only show message with XNN after test load. Really working with oneDNN+ Windows ? Seem working with linux-x64 only.
53271,elda27,1153896676,2022-06-13 13:11:40,"Is it applied stable release?
I tried on the v2.9.1 but auto completion is no longer working on VSCode.",I tried on the v2.9.1 but auto completion is no longer working on VSCode.
55808,cheshire,1153817106,2022-06-13 11:48:07,Forking a process and writing to a global directory for a test is really not great. Maybe look at existing C++ tests for autoclustering?,Forking a process and writing to a global directory for a test is really not great.
56422,rajeev921,1153665206,2022-06-13 9:05:03,"Hi @tilakrayal I already did. Even I deleted all the cache and using it. Getting the same issue.
I am following the steps as mentioned the the blog.
I am using tensorflow 2.5.3 from long time and it is working fine now my requirements made me to upgrade it to the latest one tensorflow 2.9.0 as it supports GCC 9.3.1 . I am able to build bazel 5.0.0 without any issue but only tensorflow didn't work.
Please let me know if you need additional info to help me to resolve the issue.",I already did. Even I deleted all the cache and using it. Getting the same issue.
19944,hdu-hh,1153651579,2022-06-13 8:51:01,"This issue is still valid (checked with the official libtensorflow-cpu 2.9.0 for x86_64). It reports `failed to add operation ""Gradients"": No gradient defined for op: Concat.`
This is sorely missing because many upstream models use concat and without gradient-support in the libtensorflow-cpu library they cannot be trained directly.","""failed to add operation ""Gradients"": No gradient defined for op: Concat"""
56419,FengMu1995,1153611823,2022-06-13 8:12:54,"I use the .Ref 2 that you provided, build on x86_64 machine
""""""
$ sudo bazel build \
--config=monolithic \
--config=noaws \
--config=nohdfs \
--config=noignite \
--config=nokafka \
--config=nonccl \
--config=v2 \
--define=tflite_convert_with_select_tf_ops=true \
--define=with_select_tf_ops=true \
//tensorflow/lite:libtensorflowlite.so
""""""
But error happens
ERROR: Config value 'noignite' is not defined in any .rc file",""""""" But error happens ERROR: Config value 'noignite' is not defined in any .rc file."
56242,LukeWood,1153568489,2022-06-13 7:25:19,reopened this because we should probably fully trace back the root cause of this issue,reopened this because we should probably fully trace back the root cause of this issue.
47032,kretes,1153527467,2022-06-13 6:33:27,"@sachinprasadhs you are referring to `specifying y separately`, but I can't see that in the code in the issue. Same is in the gist I prepared previously: https://gist.github.com/kretes/ca911085b2eb0fa3985894245ce3fd0c where the dataset yields a tuple of (x,y), and so is inline with documentation.
Can you try running the gist e.g. in collab to see if it fails on your side, and modify accordingly for it to pass?","""specifying y separately"""
56419,FengMu1995,1153499973,2022-06-13 5:50:57,"I modified the cmakelists.txt, like this,
""target_link_libraries(rvm_tflite_2
tensorflowlite
tensorflowlite_flex)""
but it is still reporting previous error","""but it is still reporting previous error."""
52342,amehrish,1153299514,2022-06-12 21:36:27,"If there is no support for TensorRT 8.x, please can you recommend the right version of tensorRT to be used with Tensorflow 2.7.0 on UBuntu 20.04?","No support for TensorRT 8.x, please can you recommend the right version of tensorRT to be used with Tensorflow 2.7.0 on Ubuntu 20.04?"
34346,ibkvictor,1153028596,2022-06-12 0:25:11,"> Issue is still replicating in [2.8 ](https://colab.sandbox.google.com/gist/mohantym/94a9597f1e8c9ee983bb35ad6f73b106/tpu-copy-of-cats-vs-dogs-without-data-augmentation.ipynb#scrollTo=qyHrknvL0pOu)version .
The issue is still persistent in TF 2.8 as stated by @mohantym.",The issue is still persistent in TF 2.8 as stated by @mohantym.
30047,whatdhack,1153019382,2022-06-11 23:33:26,"Debug build takes much more memory. In addition to the above, increasing the OS swap space, also helps.",Debug build takes much more memory.
41448,abdulbasit312,1152701056,2022-06-10 20:09:18,"Hi is there any update on this issue, seems to be around since the past 3 years and no fix yet! Would really appreciate if you could solve this soon.","""seems to be around since the past 3 years and no fix yet!"""
55776,tj-sun,1152549697,2022-06-10 16:46:40,"> @tj-sun This PR is in draft, any update on this? Please. Thank you!
Hi sorry, I realised that in order for this work, I need to add this op to XLA as well, which is probably not going to fly. So will close this for now I think.","I need to add this op to XLA as well, which is probably not going to fly."
56328,roserg,1152436734,2022-06-10 14:43:34,"From your model(resnext14_16x4d.tflite), I can see that you probably need grouped convolutions. We added support with this change(https://github.com/tensorflow/tensorflow/commit/8c2c575ab1090373873716afcd09ddd50dbce830).
So you probably need to fix your model and split will disappear at all.","""I can see that you probably need grouped convolutions."""
56418,innat,1152423023,2022-06-10 14:29:17,"@mohantym I need a solution that works less or equal to tf 2.8. And if there is any known bug in tflite (that fixes in tf 2.9), I like to adopt a workaround for tf <= 2.8.","I need a solution that works less or equal to tf 2.8. And if there is any known bug in tflite (that fixes in tf 2.9), I like to adopt a workaround for tf = 2.8."
56420,guweixin,1152113268,2022-06-10 8:30:20,"I met this issue on cmd, but did not meet it on git bash.
I don`t know why","I met this issue on cmd, but did not meet it on git bash."
31312,graille,1151722377,2022-06-09 23:51:53,I'm using Tensorflow 2.9.1 with Python 3.9 and the memory leak is still present,I'm using Tensorflow 2.9.1 with Python 3.9 and the memory leak is still present.
56279,hubingallin,1151621844,2022-06-09 21:03:30,The error is because of the [multiprocessing Threadpool](https://github.com/tensorflow/tensorflow/blob/9eb5fdf99053625f6e870e895a7cce6d1d3ed752/tensorflow/python/distribute/cross_device_ops.py#L1104) created is not closed properly before the program exits. A [fix](https://github.com/tensorflow/tensorflow/commit/3022b93691e00d82380cc3af9e1299feaae24432) has already been submitted,The error is because of the [multiprocessing Threadpool](https://github.com/tensorflow/tensorflow/blob/9eb5fdf99053625f6e870e895a7cce6d1d3ed752/tensorflow/python/distribute/cross_device_ops.py#L1104) created is not closed properly before the program exits.
42233,Rishav8989,1151582665,2022-06-09 20:24:02,still not working I've updated my TensorFlow to version 2.9.0,"""still not working"""
54481,sirakiin,1151426447,2022-06-09 17:51:27,"Hi @MM2932 ,
The depthwise conv issue is identified and it's a defect in Hexagon's depthwiseSupernode implementation. Unfortunately we can not modify hexagon nn lib. I'm trying to provide a workaround so we can avoid disabling it, but it requires some refactoring on the hexagon delegate code base. Will update in the thread next week.","""Unfortunately we can not modify hexagon nn lib."""
13096,eyaler,1151364945,2022-06-09 16:47:02,"@sushreebarsa no worries. i no longer have access to the original setup. and even if i had, the upgrade path would be difficult","i no longer have access to the original setup. and even if i had, the upgrade path would be difficult."
50108,chunduriv,1151074074,2022-06-09 12:44:45,"@dbl001,
Sorry for the late response. I tried with the latest version on MacOS Monetery 12.3.1 and it is working. Could you try with the latest tensorflow (TF2.9) as shown below ```
pip3 install tensorflow-macos
import tensorflow as rf
```",I tried with the latest version on MacOS Monetery 12.3.1 and it is working. Could you try with the latest tensorflow (TF2.9) as shown below  pip3 install tensorflow-macos import tensorflow as rf .
56376,skyientist,1150254287,2022-06-08 18:30:25,"bazel build --config=v1 [--config=option] //tensorflow/tools/pip_package:build_pip_package
ERROR: The 'build' command is only supported from within a workspace (below a directory having a WORKSPACE file).
See documentation at https://docs.bazel.build/versions/master/build-ref.html#workspace","""Error: The 'build' command is only supported from within a workspace (below a directory having a WORKSPACE file"")"""
6541,Andreas5739738,1150059960,2022-06-08 15:19:41,"Looks like things have improved significantly with the 2.9 release, although there still is a large gap to numpy and Jax:
```
seconds (lower is better):
Tensorflow 2.9.1 5.495112890999991
Tensorflow 2.9.1, double precision 7.629201937000033
Numpy: 2.1803204349999987
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
Jax: 1.4081462569999985
```","""There still is a large gap to numpy and Jax"""
56396,ghost,1150037889,2022-06-08 15:01:29,I created 2 issues in case it's irrelevant somewhere,I created 2 issues in case it's irrelevant somewhere.
12001,glenn-jocher,1149684021,2022-06-08 9:30:26,@ayushexel @sergiossm this is the main issue I found regarding DW Conv2d Transpose layers in TF (C++ conversion proposed by @Orpheus23),This is the main issue I found regarding DW Conv2d Transpose layers in TF (C++ conversion proposed by @Orpheus23).
56387,MovsisyanM,1149575660,2022-06-08 7:43:29,"This issue arises when using RNN-based cells (RNN, LSTM, GRU)","This issue arises when using RNN-based cells (RNN, LSTM, GRU)"
56387,MovsisyanM,1149568587,2022-06-08 7:35:38,"I tried running it on another device and it worked perfectly, but for some reason it doesn't work on this device which is a lambda workstation.","I tried running it on another device and it worked perfectly, but for some reason it doesn't work on this device which is a lambda workstation."
56391,mohantym,1149566250,2022-06-08 7:33:04,"@CORRELU ! It should be tf.TensorSpec instead of TensorSpec. Please let us know if it works.
```
tf_train = tf.data.experimental.load(
""D:/data/train_w512_small"",
element_spec=(tf.TensorSpec(shape=(None, None, 5), dtype=tf.float32), tf.TensorSpec(shape=(None, None, 5), dtype=tf.float32)),
compression=None,
reader_func=None,
)
```","""It should be tf.TensorSpec instead of TensorSpec."""
56398,xiongma,1149540210,2022-06-08 7:02:29,this scenario happen when you put some weights initial in build funtion,This scenario happen when you put some weights initial in build funtion.
56373,SnoopJ,1149538689,2022-06-08 7:00:37,"As described in #56399, this failure goes beyond the numpy-specific case.","As described in #56399, this failure goes beyond the numpy-specific case."
56176,snowuyl,1149392586,2022-06-08 2:44:58,"% cd examples/lite/examples/image_classification/ios % pod install
Analyzing dependencies
Downloading dependencies
Generating Pods project
Integrating client project
Pod installation complete! There is 1 dependency from the Podfile and 2 total pods installed.
open ImageClassification.xcodeproj
But Xcode build error with ""No such module TensorFlowLite""",no such module TensorFlowLite>
40680,tengshaofeng,1149364970,2022-06-08 1:57:11,I face the same error. any body solved?,I face the same error. any body solved?
56393,Young768,1148988616,2022-06-07 17:50:17,"Btw, I turned off mixed precision training and disabled fusion for BN layers, since it seems DTensor does not support these yet.","""DTensor does not support these yet."""
56177,Jesse-Kerr,1148971679,2022-06-07 17:34:24,This is occurring also when using tf.data.Dataset.save() on version '2.10.0-dev20220531',This is occurring also when using tf.data.Dataset.save() on version '2.10.0-dev20220531'.
54379,gzmkl,1148859514,2022-06-07 15:56:05,"@penpornk We have a better solution: skip the related test if it is running on AVX2 platform (when TestCPUFeature(...AVX512F) returns false) because oneDNN fused pad conv op is not supported on AVX2 or earlier instruction set. After our internal full-validation and code review, I will update this PR.","""We have a better solution: skip the related test if it is running on AVX2 platform (when TestCPUFeature(...AVX512F) returns false) because oneDNN fused pad conv op is not supported on AVX2 or earlier instruction set."""
56358,bhack,1148846609,2022-06-07 15:44:41,"@sachinprasadhs These metrics are third_party/deprecated.
You could use TF Addons if you don't want to wait Keras-cv porting:
https://www.tensorflow.org/addons/api_docs/python/tfa/metrics/F1Score","""these metrics are third_party/deprecated"""
40075,impjdi,1148838175,2022-06-07 15:38:07,"One thing I wanted to mention before we close the bug is that we cut corners and rely on undefined behavior in certain cases. If device-specific bug, there's a chance that this is not working out. As OpenGL is not relying on those features, I would try enforcing OpenGL for that particular device.","""If device-specific bug, there's a chance that this is not working out."""
56387,MovsisyanM,1148631193,2022-06-07 12:56:07,"Keywords to make it easier for other people with the same problem to find this:
```
InternalError: Graph execution error:
Failed to call ThenRnnForward with model config
```",Failed to call ThenRnnForward with model config
52944,wilecoyote2015,1148452160,2022-06-07 9:53:44,"Is there a technical reason why `tf.print` is not supported using XLA?
I am really missing this, especially for custom training loops.","I am really missing this, especially for custom training loops."
55934,bmharper,1148364434,2022-06-07 8:31:41,"For what it's worth.... I'm getting similar issues with a Snapdragon 662. On a Snapdragon 720G, all working fine.","I'm getting similar issues with a Snapdragon 662. On a Snapdragon 720G, all working fine."
55336,FengMu1995,1148261456,2022-06-07 6:45:14,"@mtamburrano I've combined all the libraries in /_deps into one folder, the number of errors reduced, but there are still 60 errors.
for example, have you met it?","I've combined all the libraries in /_deps into one folder, the number of errors reduced, but there are still 60 errors."
56333,kyamagu,1148197255,2022-06-07 4:59:43,"@sushreebarsa Yes, I want the documentation to be clear on when `InvalidArgumentError` happens. Target height or width can be invalid when certain positive value is given.","""InvalidArgumentError"""
56236,Snape3058,1148153954,2022-06-07 3:28:22,"My analyzer can only detect which PyObject is leaked. Besides, my research is not fixing the bug. For the sake of not introducing new bugs, I suggest related developers to take over the issue to fix these bugs.","Besides, my research is not fixing the bug."
55941,joker-eph,1148039973,2022-06-06 23:40:26,"It seems that this configuration will quickly get obsolete by the migration to cc_shared_library (as far as I understand), is this correct?
If so: - Is this PR gonna help the migration in any way?
- Otherwise, I'm concerned about the added complexity and how it'll get in the way of the current migration (by having more select/if and an extra config to maintain).
-",- I'm concerned about the added complexity and how it'll get in the way of the current migration (by having more select/if and an extra config to maintain).
56298,qlzh727,1147735122,2022-06-06 18:08:30,"dtensor API is only available at tf 2.9 or later, so it is expected to fail in tf 2.6.","dtensor API is only available at tf 2.9 or later, so it is expected to fail in tf 2.6."
56347,SnoopJ,1147727959,2022-06-06 18:00:36,"Note: title change and force-push are because the bug has been refiled as #56373 to comply with a request for using the issue template. Branch name is now slightly misleading, but semantics of the changes are unaffected by this name-juggling.","""Note: title change and force-push are because the bug has been refiled as #56373 to comply with a request for using the issue template."""
54379,ashiqimranintel,1147579948,2022-06-06 15:28:20,"@penpornk, could you re-run the test again?",Could you re-run the test again?
56231,prjrg,1147434422,2022-06-06 13:14:32,"For instance, this piece of code is marked by the IDEs as invalid. I have pylance in VS Code, yes.
```python
from tensorflow.keras import layers
```",python from tensorflow.keras import layers
56261,DemoLV,1147307525,2022-06-06 10:38:23,"So a little update. I implemented the WAKE LOCK function, following tutorial here [thread](https://www.youtube.com/watch?v=7Pp3E378JxY). I also put the TFL Detect application in my phone settings as (programms which can not be deactivated) and then pressed the Wake lock ON button in the app but when I lock the phone it still shuts down the application. Like it does not continue to detect the objects.
I wrote the code in CameraActivity.java and tfe_od_activity_camera.xml Waiting for your help !","I implemented the WAKE LOCK function, following tutorial here [thread](https://www.youtube.com/watch?v=7Pp3E378JxY). I also put the TFL Detect application in my phone settings as (programms which can not be deactivated) and then pressed the Wake lock ON button in the app but when I lock the phone it still shuts down the application. Like it does not continue to detect the objects. I wrote the code in CameraActivity.java and tfe_od_activity_camera"
56312,tilakrayal,1147165122,2022-06-06 8:03:44,"@gowthamkpr,
I was able to reproduce the issue on tensorflow-gpu [v2.8](https://colab.research.google.com/gist/tilakrayal/41ccd32091cfba1720c93fc30cbf306f/2-8_gpu.ipynb) and v2.9. In tensorflow-cpu [v2.8](https://colab.research.google.com/gist/tilakrayal/6ad3790641f539bedd86ab5338617585/2-8-56312.ipynb), i was not able to find the issue. Please find the gist.","""I was able to reproduce the issue on tensorflow-gpu [v2.8](https://colab.research.google.com/gist/tilakrayal/41ccd32091cfba1720c93fc30cbf306f/2-8_gpu.ipynb) and v2.9. In tensorflow-cpu [v2.8](https://colab.research.google.com/gist/tilakrayal/6ad"
22810,kognat-docs,1146402662,2022-06-03 22:09:53,No I have not I can spend some time if you want me to. I have been static linking under Linux as a workaround,No I have not I can spend some time if you want me to. I have been static linking under Linux as a workaround.
56341,bzdjordje,1146387530,2022-06-03 21:49:19,"@tilakrayal Unfortunately the error still seems to persist if I run the same script in v2.9 as presented below
![image](https://user-images.githubusercontent.com/4984500/171958551-8197cc91-2f19-4031-aaac-cc99482c5384.png)
![image](https://user-images.githubusercontent.com/4984500/171958562-3e646af7-1bee-4bd9-ad02-e2ab9d128031.png)","""Unfortunately the error still seems to persist if I run the same script in v2.9 as presented below"""
34742,tpham3783,1146375914,2022-06-03 21:28:41,"I tried those configs and it did not work.
The only way was to compile libtensorflow with monotholic option, which would build libtensorflow_framework and libtensorflow into one library. Just in case if someone else is also struggling ***
`bazel build --config=monolithic tensorflow`",I tried those configs and it did not work.
54146,joker-eph,1146156429,2022-06-03 16:30:40,This is missing tests I think.,This is missing tests I think.
56347,SnoopJ,1146029619,2022-06-03 14:36:43,"CI failure does not look related to these changes, seeing the same failure on #56345 (which has no code changes) so I assume this is noise.
```
//bazel_pip/tensorflow/python/kernel_tests/nn_ops:conv_ops_3d_test_cpu FLAKY, failed in 3 out of 33 in 300.3s
```",CI failure does not look related to these changes
23601,mdanatg,1145958077,2022-06-03 13:20:19,"Yes, this is still an open issue (I think @chunduriv was referring to autograph being available in its own namespace). Mainly because we need someone to set up and maintain a separate repo - otherwise, significant parts of the source code (tensorflow/python/autograph/pyct) are already independent of TF and could be mover rather mechanically.","""Mainly because we need someone to set up and maintain a separate repo - otherwise, significant parts of the source code (tensorflow/python/autograph/pyct) are already independent of TF and could be mover rather mechanically."""
56276,bhack,1145841517,2022-06-03 10:52:11,"On GPU I have a build error:
Do you understand what It means?
```
ERROR: /tf/tensorflow/tensorflow/compiler/tests/BUILD:2190:15: //tensorflow/compiler/tests:bincount_op_test_gpu: missing input file '//tensorflow/compiler/tests:bincount_op_test.py'
```","""I have a build error: Do you understand what it means?"""
56276,bhack,1145821512,2022-06-03 10:24:05,"As you see with the few performance tests I've added on CPU we are passing 10% gain for medium and large and small input (int64).
It cannot achieve a 10% gain with small input (int32).
The pointer to the log is:
https://source.cloud.google.com/results/invocations/eb6fa396-382b-464b-847d-10a4dfdd3658/targets/pkg%2Fpip_and_nonpip_tests/tests","""Cannot achieve a 10% gain with small input (int32)"""
56183,khanhlvg,1145516446,2022-06-03 2:06:31,"TVM isn't part of the TensorFlow ecosystem so unfortunately we don't have anyone familiar with it at Google. If you want to run inference with TF models, I'd suggest checking out:
1. TF Serving if you want to run inference on the server.
2. TFLite if you want to run inference on edge devices.","""TVM isn't part of the TensorFlow ecosystem so unfortunately we don't have anyone familiar with it at Google."""
56327,tsindhuja,1145205271,2022-06-02 18:55:36,"It isn't working because the input to the tokenize function is a list of sentences. char_level=True helps only if it is a string, from my experience.
You can concatenate the list to a string and then tokenize with char_level=True
```
def convert_list_to_string(sentences): text = """" for s in sentences:
text += s + "" ""
return text
```
Hope this helps.","""It isn't working because the input to the tokenize function is a list of sentences. char_level=True helps only if it is a string, from my experience."""
54155,karimnosseir,1144226450,2022-06-01 23:02:12,"@joker-eph The model shared here fails during saved model to TF importing, looks like coming from a composite tensor.
Mehdi can you please have a look ?
Thanks","""The model shared here fails during saved model to TF importing, looks like coming from a composite tensor."""
40366,gtuzi,1144191807,2022-06-01 22:09:37,Facing the same issue with Tensorflow 2.8.0 using `metric = tfk.metrics.Mean` with `MirroredStrategy` when calling `metric.update_state(value)`. This is a basic functionality for the Keras echo system. Strange that this is a persisting bug for so many versions.,Facing the same issue with Tensorflow 2.8.0 using metric = tfk.metrics.Mean with MirroredStrategy when calling metric.update_state(value).
56319,0x0L,1144122833,2022-06-01 20:52:06,"Reproduced on macos python 3.10.4 (conda-forge) tf 2.9.1 (pypi)
This seems to depend on the size of the data and works ok as long as the total size is < 2**31 - 1.
e.g. `x.shape == (2 ** 31 - 2, 1)` is ok but `x.shape == (2 ** 30, 2)` is not
However for `x.shape == (2 ** 31, 1)` the construction of the RaggedTensor throws an exception complaining that `Shape output type is 32-bit but dim 0 is 2**31 [Op:Shape]`. It does not work but at least it does not produce a random output silently","x.shape == (2 ** 31 - 2, 1)"
56312,Mirandatz,1144071476,2022-06-01 19:55:15,I also asked for help on TensorFlow and it appears [other have also reproduced the issue](https://stackoverflow.com/questions/72450109/training-the-same-model-on-the-same-data-yielding-extremely-different-test-accur).,I also asked for help on TensorFlow and it appears [other have also reproduced the issue](https://stackoverflow.com/questions/72450109/training-the-same-model-on-the-same-data-yielding-extremely-different-test-accur).
56321,bjacob,1144013024,2022-06-01 18:51:49,I wasn't able to add @sjarus as reviewer so here's a FYI.,I wasn't able to add @sjarus as reviewer so here's a FYI.
54374,Nyrio,1143963435,2022-06-01 18:00:29,"We decided that we don't need this functionality at the moment, so let's close this PR instead of reworking it using the function names.","""let's close this PR instead of reworking it using the function names."""
22623,DryCell-x,1143613603,2022-06-01 13:28:31,"Same issue happen to me, RTX 3050 with 4GB memory but Tensorflow only use 1673MB memory. Any solution yet?","""RTX 3050 with 4GB memory but Tensorflow only use 1673MB memory"""
56313,MATTYGILO,1143548758,2022-06-01 12:31:29,@mohantym Ok the only problem is that I require integer quantisation for tflite micro. So I'm a bit stuck I guess.,I'm a bit stuck I guess.
56313,MATTYGILO,1143540741,2022-06-01 12:24:39,Ok using the `data.astype(np.int8)` I get a `ValueError: Cannot set tensor: Got value of type UINT8 but expected type FLOAT32 for input 0`,ValueError: Cannot set tensor: Got value of type UINT8 but expected type FLOAT32 for input 0
56076,Nyrio,1143462342,2022-06-01 11:07:16,@gbaned The other PRs in the series need to be reviewed first (cf description),The other PRs in the series need to be reviewed first (cf description)
56309,awf,1143459469,2022-06-01 11:04:13,"This is a feature for tensorflow developers following the instructions at https://www.tensorflow.org/install/source#cpu-only
Currently, as a developer, I cannot debug tensorflow using gdb inside the developer docker container.","Currently, as a developer, I cannot debug tensorflow using gdb inside the developer docker container."
52167,goncinious,1143454425,2022-06-01 10:59:59,I can confirm that this is still an issue in TF 2.9.1. Tested with `tensorflow/tensorflow:latest-gpu` (`sha256:a34c2420739cd5a7b5662449bc21eb32d3d1c98063726ae2bd7db819cc93d72f`) on a NVIDIA A10G GPU.,I can confirm that this is still an issue in TF 2.9.1.
56171,bulvara,1143427304,2022-06-01 10:32:13,"> Could you try to install with `GCC 9.3.1 and Bazel 5.0.0` which is compatible for tensorflow latest stable v2.9.
> > Version	Python version	Compiler	Build tools
> tensorflow-2.9.0	3.7-3.10	GCC 9.3.1	Bazel 5.0.0
Great, but I'm compiling git master and not a stable version.
I've tried with https://aur.archlinux.org/packages/gcc9 and it's giving the exact same failure with my LLVM, which is needed for _XLA_.","""I'm compiling git master and not a stable version"""
56313,MATTYGILO,1143400614,2022-06-01 10:07:17,@mohantym I'm really stuck is it possible if you could provide an example for a working full int 8 quantisation of a yamnet,I'm really stuck is it possible if you could provide an example for a working full int 8 quantisation of a yamnet.
53144,alkatar21,1143385976,2022-06-01 9:54:10,"@qlzh727 The PR only solves part of the problem, as @bermeitinger-b already notes.
I'm not sure which issue I should best respond to that won't be overlooked?
I decided to wrote a comment in the other issue (https://github.com/tensorflow/tensorflow/issues/56231#issuecomment-1143377838) that should hopefully help understand and solve the problem. It contains a report of what works in which verseion and should explain which autocompletions should work.",I'm not sure which issue I should best respond to that won't be overlooked.
53144,Yuri-Su,1143359186,2022-06-01 9:29:18,"> It works with `tf-nightly` (or applying the patch on the 2.9.1 source) when using the toplevel import `import tensorflow as tf` and then do everything with `tf.keras....`.
> > However, some things still don't work:
> > * `from tensorflow.keras import layers as L` (shown as error)
> * `from tensorflow.keras import backend as K` (shown as error)
> * `from tensorflow import keras` (no error shown, but no autocompletions)
sure , why google do not fix it ??","""why google do not fix it"""
53144,bermeitinger-b,1143349156,2022-06-01 9:19:40,"It works with `tf-nightly` (or applying the patch on the 2.9.1 source) when using the toplevel import `import tensorflow as tf` and then do everything with `tf.keras....`.
However, some things still don't work:
* `from tensorflow.keras import layers as L` (shown as error)
* `from tensorflow.keras import backend as K` (shown as error)
* `from tensorflow import keras` (no error shown, but no autocompletions)",some things still don't work
47554,Ammar4774,1143334774,2022-06-01 9:07:17,Still not solved?,Still not solved?
55919,muxamilian,1143221863,2022-06-01 7:34:59,"@Petros626 One thing you can do is to train Spaghettinet normally and then use post-training quantization. This should always work. But training-aware quantization (another quantization method) doesn't work without this patch. However, I'm not the author of Spaghettinet, I just reported the bug. @hermitman, who wrote this patch originally, is a coauthor of the Spaghettinet, so I guess it's better to ask him.","""training-aware quantization (another quantization method) doesn't work without this patch"""
56231,mshijie,1143177431,2022-06-01 6:40:43,"<img width=""602"" alt=""image"" src=""https://user-images.githubusercontent.com/2002700/171343234-ad911e0f-9677-4fd4-9a2e-dbf9741edb87.png"">
PyCharm 2021.3 . No code completion.",No code completion.
55919,Petros626,1143114763,2022-06-01 4:50:48,"@muxamilian I want to use the Google Coral so I need a TensorFlow lite model. So you revise the files from the TF1 Object Detection API to have ""real"" quantization. Is this the only way to do it, I mean isn't there a working converter for it (dont found a working one). So your way is needed if you have a non quantized model from beginning, but if I train a already quantzied (Model Zoo 1) is it not ""real"" quantized or where is the point?","""I want to use the Google Coral so I need a TensorFlow lite model. So you revise the files from the TF1 Object Detection API to have ""real"" quantization. Is this the only way to do it, I mean isn't there a working converter for it (dont found a working one). So your way is needed if you have a non quantized model from beginning, but if I train a already quantzied (Model Zoo 1) is it not ""real"" quantized or where is the point?"""
56281,drivanov,1143098857,2022-06-01 4:15:56,"I am closing that one because [PR#56295](https://github.com/tensorflow/tensorflow/pull/56295) contains these three (`>`, `<`, `==`) and two more (`>=`, `<=`) operations.","I am closing that one because [PR#56295](https://github.com/tensorflow/tensorflow/pull/56295) contains these three (>, , ==) and two more (>=, ==) operations."
54173,impjdi,1142648249,2022-05-31 21:17:32,"14dd3a808ded246021a110ee7cf8b174214ed6c2 should have fixed it, right?","""should have fixed it, right?"""
56165,Nyrio,1142524382,2022-05-31 18:46:50,"@bixia1 Is there a way to see the details of the test failures in the ""ARM CI"" job? I can only see the bazel logs but not the `test.log` files, and cannot reproduce these failures.","I can only see the bazel logs but not the test.log files, and cannot reproduce these failures."
56259,kushanam,1142520198,2022-05-31 18:41:30,"> @sachinprasadhs,
> While I was trying to reproduce the issue in google colab, the code was executing more than 3 hrs which was longer than expected. Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/85a9bd43309f44ed92cd4611c13233f5/56259.ipynb).
Made a smaller dataset. could you have another look please?","""The code was executing more than 3 hrs which was longer than expected."""
56125,mrdeveloperdude,1142347060,2022-05-31 16:23:05,"Can confirm this is a problem on Linux 5.10.0-13-amd64 #1 SMP Debian 5.10.106-1 (2022-03-17) x86_64 GNU/Linux.
Renaming changing from 'common.c' to 'common.cc' in tensorflow/lite/c/CMakeLists.txt fixes the problem.",Can confirm this is a problem on Linux 5.10.0-13-amd64 #1 SMP Debian 5.10.106-1 (2022-03-17) x86_64 GNU/Linux. Renaming changing from 'common.c' to 'common.cc' in tensorflow/lite/c/CMakeLists.txt fixes the problem.
56231,alkatar21,1142168772,2022-05-31 13:54:49,"This has been a problem for a while now, see #53144. It would be really nice if this would be fixed. It can't be that everyone who uses an IDE and code completion, which should be normal by now, has to hack around in his installation.","This has been a problem for a while now, see #53144."
56302,elfringham,1141887483,2022-05-31 9:20:35,"@mohantym @gadagashwini The referenced gist above does not match what I am seeing, not sure why that would be, possibly a different default in your environment. A full build log can be seen at
https://ci.linaro.org/view/All/job/ldcg-python-manylinux-tensorflow-onednn-nightly/68/consoleText","not sure why that would be, possibly a different default in your environment"
56231,Yuri-Su,1141695387,2022-05-31 5:54:35,"> @Yuri-Su try this: [#53144 (comment)](https://github.com/tensorflow/tensorflow/issues/53144#issuecomment-985179600)
![image](https://user-images.githubusercontent.com/45455052/171102216-7c26a877-6371-4d1c-98ac-257deb934b2f.png)
I have made the changes as suggested and cleared the cache, but it still doesn't work.","I have made the changes as suggested and cleared the cache, but it still doesn't work."
46635,misha-antonenko,1141297176,2022-05-30 15:47:31,"Hello. Could you please inform if there has been any progress on the issue?
For me, it resulted in a sudden sixfold increase of the train step duration due to having to move to CPU (compared to residing to a contrived solution with non-ragged tensors)",Having to move to CPU (compared to residing to a contrived solution with non-ragged tensors)
56299,misha-antonenko,1141125725,2022-05-30 12:56:02,"This `block` refers to an instance of a `ResidualAttentionBlock`. Please excuse me for missing this:
```
block = ResidualAttentionBlock()
x = tf.RaggedTensor.from_row_lengths(foo(6, 2), [1, 2, 3])
block(x)
```","This block refers to an instance of a ResidualAttentionBlock. Please excuse me for missing this:  block = ResidualAttentionBlock() x = tf.RaggedTensor.from_row_lengths(foo(6, 2), [1, 2, 3]) block(x) ."
55934,amitmate,1141080930,2022-05-30 12:11:22,@karimnosseir i am also seeing the same issue as reported by @bmharper. its not solved or we are missing something in the build,i am also seeing the same issue as reported by @bmharper. its not solved or we are missing something in the build.
56287,w3sip,1141059037,2022-05-30 11:47:46,"@mohantym - can't try 2.9 because of https://github.com/tensorflow/tensorflow/issues/56250 Considering we're on iOS, providing a standalone sample may end up being a project. Is there a sample/demo project for IOS that could be used for that?",can't try 2.9 because of https://github.com/tensorflow/tensorflow/issues/56250
56258,tilakrayal,1140611442,2022-05-30 2:15:16,"@hkvision,
I was facing different error while trying to execute the mentioned code. Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/477ac53b13c581843b3da03b4b6745b4/untitled357.ipynb).","""I was facing different error while trying to execute the mentioned code."""
8033,zkxvb,1140180745,2022-05-28 5:51:58,"> No, I should have been more specific. I meant that I'm no longer working on improving C++ API usability, in particular the Tensor interface, meaning there's nothing to do for this issue anymore (this functionality is currently possible in both the C API and using the Tensor C++ interface directly).
@skye have examples about how to use this functionality with C++ api?","""No, I should have been more specific"""
56088,mihaimaruseac,1138701954,2022-05-26 15:29:18,"Segfaults arising from C++17 switch are mostly due to `std::string_view(nullptr)`. I'm currently hunting for these and fixing, but will take a while to get to all of them","""Segfaults arising from C++17 switch are mostly due to std::string_view(nullptr)"""
56259,tilakrayal,1138599489,2022-05-26 13:48:54,"@sachinprasadhs,
While I was trying to reproduce the issue in google colab, the code was executing more than 3 hrs which was longer than expected. Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/85a9bd43309f44ed92cd4611c13233f5/56259.ipynb).","""The code was executing more than 3 hrs which was longer than expected."""
47086,tanmaysachan,1138548546,2022-05-26 12:57:57,"@ckkuang Can the commit fixing this issue be mentioned? Currently stuck with tf 2.4, and need to figure out a workaround for this issue.","Currently stuck with tf 2.4, and need to figure out a workaround for this issue."
56088,nSircombe,1138547357,2022-05-26 12:56:26,"I think this is a bit of a mangling of `-no_oss_py2`, it's set as `-no_oss_py${py_ver}` - I don't think it does anything and is just ignored, but I may be wrong.","I think this is a bit of a mangling of -no_oss_py2, it's set as -no_oss_py$py_ver - I don't think it does anything and is just ignored, but I may be wrong."
54479,bhack,1138544904,2022-05-26 12:53:24,"Just another info. I see that iterating the development over this TF2XLA kernels is quite slow also on a GCP instance with a modern CPU. E.g. with every single edit in `unique_op.cc`:
```bazel test -k tensorflow/python/eager:def_function_xla_jit_test --flaky_test_attempts=1 --test_filter=*testUniqueCompilability*```
```Elapsed time: 48.398s, Critical Path: 40.65s```",I see that iterating the development over this TF2XLA kernels is quite slow also on a GCP instance with a modern CPU. E.g. with every single edit in unique_op.cc: bazel test -k tensorflow/python/eager:def_function_xla_jit_test --flaky_test_attempts=1 --test_filter=*testUniqueCompilability* Elapsed time: 48.398
56088,elfringham,1138337442,2022-05-26 9:22:51,"Hi @milpuz01 those unit tests started to fail with commit https://github.com/tensorflow/tensorflow/commit/8ea5ed0c392b329a3e0481a3f1f7b0ca86821b84
I can only reproduce them inside the manylinux2014 docker container at the moment. Outside of that those tests pass.",I can only reproduce them inside the manylinux2014 docker container at the moment.
22214,GPhilo,1138185611,2022-05-26 6:02:27,"@gadagashwini the problem is not the inability to use a float, but rather the fact that if you pass an int instead of a float you won't know your program is broken until it crashes during runtime.
Seeing how old this issue is, however, I don't know if it's still the case in the current version of Tensorflow and I'm not able to check, so I'll just close this.","""Seeing how old this issue is, however, I don't know if it's still the case in the current version of Tensorflow and I'm not able to check, so I'll just close this."""
41375,gowthamkpr,1138061977,2022-05-26 1:59:42,"@1340896123 I think pip is installing it for python 2 and then you are running in python 3. Use pip3 while installing tensorflow.
Also, if the above solution doesn't work, Please go through these [solutions](https://stackoverflow.com/questions/41415629/importerror-no-module-named-tensorflow-python)","""I think pip is installing it for python 2 and then you are running in python 3"""
56210,sachinprasadhs,1138011983,2022-05-26 0:27:40,"@harrylincoln , To make it more generic changing it to `models_file_path` from `models_path` would be more relevant.
Also, have you tried the below option with `model_file_path`?
`# classifier = audio.AudioClassifier.create_from_file(model_path)`",# classifier = audio.AudioClassifier.create_from_file(model_path)
56256,curioyang,1137978289,2022-05-25 23:47:25,"@mohantym It will create many quant nodes such as ![image](https://user-images.githubusercontent.com/39184746/170387183-cc1801a3-99a8-45cb-bdcd-bed44e0cd99b.png)
Conv2D and DepthwiseConv2D are fp32 nodes with fp32 input and output. I just want to remove these ""Quantize & Dequantize""","""Quantize & Dequantize"""
56088,mseth10,1137810255,2022-05-25 20:19:33,"The nightly pipeline seems to be broken today. Earlier only some tests were failing as mentioned by @milpuz01 , but today the build seems to be failing as well. Does anyone have pointers as to what might have caused it (should be one of yesterday's commits) and how to fix it? Thanks! @mihaimaruseac @learning-to-play @nitins17 https://github.com/tensorflow/tensorflow/actions/runs/2383006475","""The nightly pipeline seems to be broken today."""
56157,Sara980710,1137805297,2022-05-25 20:13:36,"I have checked the links before, I am however using a satellite computer with AMD64 architecture and ubuntu, not android or ios. For C++, I have managed to get the GPU to work, but for Python I cannot find a way to choose a device.","I am however using a satellite computer with AMD64 architecture and ubuntu, not android or ios."
48056,bhack,1137779594,2022-05-25 19:43:10,"I think this was merged with https://github.com/tensorflow/tensorflow/commit/299cb76dd913e7bb0349a13c1165459dac4ea81e but the PR is not closed and put in a merged status.
We have collected this case in https://github.com/tensorflow/community/issues/413",I think this was merged with https://github.com/tensorflow/tensorflow/commit/299cb76dd913e7bb0349a13c1165459dac4ea81e but the PR is not closed and put in a merged status.
56241,Petros626,1137151887,2022-05-25 11:59:23,should i delete this issue and create a new one with the template?,should i delete this issue and create a new one with the template?
56255,sschaetz,1137103854,2022-05-25 11:03:59,If I force the pods to 0.0.1-nightly.20201122 it works fine. So some time between `20201122` and `20220524` things broke.,20201122 and 20220524
53144,bermeitinger-b,1136970981,2022-05-25 8:48:26,"As a working workaround, you can do the following: (no need to recompile).
Use the fix from https://github.com/tensorflow/tensorflow/issues/53144#issuecomment-985179600 (you need to modify it a bit for the current versions.)
Then, in your code:
```python
import tensorflow as tf
keras = tf.keras
K = keras.backend # example
```
You have to ""Invalidate Caches & Restart"" in PyCharm and let it regenerate the indices which will take a few minutes.
With this, I have full autocomplete as it should be.","""I have full autocomplete as it should be.."""
56253,fjzhangcr,1136953412,2022-05-25 8:32:04,"btw, i have make some experiments, they shows that tf.shape, tf.meshgrid, tf.range are all supported well by intrpreter.invoke(), but when use together, the bug appears~~~
such as the code below works well:
```
def pre_process(x):
grid_x,grid_y=tf.shape(x)[1:3]
x_grid=tf.ones([grid_x,grid_y])
y_grid=tf.ones([grid_x,grid_y])
b=tf.stack([x_grid,y_grid],axis=-1)
y=x+tf.cast(b,tf.float32)
return y
```","btw, i have make some experiments, they shows that tf.shape, tf.meshgrid, tf.range are all supported well by intrpreter.invoke(), but when use together, the bug appears"
56179,gowthamkpr,1136540656,2022-05-24 23:51:15,@south-ocean Tensorflow doesn't officially support ROCm. You can follow the instructions [here](https://rocmdocs.amd.com/en/latest/Deep_learning/Deep-learning.html). Also this is similar to these issues [issue1 ](https://github.com/tensorflow/serving/issues/1604)and [issue2](https://github.com/tensorflow/tensorflow/issues/25323). Follow all the suggestions in issue2 but this is all I can do as we don't support AMD ROCm,"""Tensorflow doesn't officially support ROCm"""
56231,Dadealos,1136380661,2022-05-24 20:05:13,"A side question - does everyone just use Notepad++? This has been a persistent issue with VSCode and Pycharm since at least 2017. I'm baffled that it's not fixed, but I don't understand it well enough to fix it myself.
I would happily just buy a different IDE if there's one that supports code completion and doesn't break on every new TF release.",This has been a persistent issue with VSCode and Pycharm since at least 2017.
56231,Dadealos,1136371814,2022-05-24 19:54:45,"So... there's no ""error log"" for code completion failures. As Yuri-Su mentioned, everyone is having the same issue.
Steps to reproduce:
1. from tensorflow.keras import Model
That's literally it. If you want to rule out other IDEs, configuration issues, etc etc, just go to https://www.tensorflow.org/tutorials/quickstart/advanced
Run the first cell:
![image](https://user-images.githubusercontent.com/36662766/170120811-739ade36-f3cf-4da8-9e7e-ba263a91fea1.png)","""There's no ""error log"" for code completion failures"""
41448,qlzh727,1136121755,2022-05-24 16:09:22,Reassign to myself since Tom has left the team.,Tom has left the team.
40075,impjdi,1136108548,2022-05-24 15:57:18,"Hm, we don't have a Samsung Galaxy Tab S6 to take a look at that device-specific issue :( As sushreebarsa suggested, I would start with a TF upgrade.","""We don't have a Samsung Galaxy Tab S6 to take a look at that device-specific issue"""
56242,bhack,1136042836,2022-05-24 15:00:13,"@kaitolucifer You need to open this type of tickets in the [Keras repository](https://github.com/keras-team/keras/). But I suppose it could be caused by the fallbacks introduced at:
https://github.com/keras-team/keras/commit/dbc4526978865b372e794accbaa7c584f9c86a0f
It is already partially tracked at:
https://github.com/tensorflow/tensorflow/issues/55639 (/cc @wangpengmit @rohan100jain)
https://github.com/keras-team/keras-cv/issues/291 (/cc @LukeWood @qlzh727)","""But I suppose it could be caused by the fallbacks introduced at: https://github.com/keras-team/keras/commit/dbc4526978865b372e794"""
56240,ghost,1135822672,2022-05-24 11:55:56,"The PyFunc kernel corresponds to a [tf.py_func](https://www.tensorflow.org/api_docs/python/tf/py_func) in your model, which requires a Python interpreter to execute.
So, it seems the model you're using requires the Python interpreter, which is not linked into the Java API and hence you see the error.
You'd want to figure out where in your model tf.py_func is being used replace the computation expressed as a Python function in your model with something that uses TensorFlow operations.","The PyFunc kernel corresponds to a [tf.py_func](https://www.tensorflow.org/api_docs/python/tf/py_func) in your model, which requires a Python interpreter to execute. So, it seems the model you're using requires the Python interpreter, which is not linked into the Java API and hence you see the error. You'd want to figure out where in your model tf.py_func is being used replace the computation expressed as a Python function"
34444,feliwir,1135643466,2022-05-24 9:26:26,I'm getting a similar issue now with TF 2.9. The crash happens in `NextIndex` called by `EvalMeanReferenceOps`,I'm getting a similar issue now with TF 2.9. The crash happens in NextIndex called by EvalMeanReferenceOps.
56216,snowuyl,1135477309,2022-05-24 6:48:53,"image classifiication project also has this issue too.
cd examples/lite/examples/image_classification/iOS
open ImageClassification.xcodeproj
Xcode build failed with No such module 'TensorFlowLite'.",Xcode build failed with No such module 'TensorFlowLite'.
56216,snowuyl,1135299116,2022-05-24 1:24:21,"pod install command can run successfully after modifying Podfile according your suggestion. But Xcode build failed with No such module 'TensorFlowLite'. The following is content of TFLiteSwiftApp.swift.
cat TFLiteSwiftApp.swift //
// TFLiteSwiftApp.swift
// TFLiteSwift
//
// Created by snowuyl on 2022/5/23.
//
import SwiftUI
import TensorFlowLite
@main
struct TFLiteSwiftApp: App {
var body: some Scene {
WindowGroup {
ContentView()
}
}
}",Pod install command can run successfully after modifying Podfile according your suggestion. But Xcode build failed with No such module 'TensorFlowLite'.
30418,nyngwang,1134781504,2022-05-23 14:55:24,"@ppham27 I just updated my last comment I was writing it in a hurry.
> I don't see a lot of value in rewriting it, though.
It seems that it depends on some modules that have been removed in TF2, so I was asked to do so.",I was writing it in a hurry.
56121,tilakrayal,1134757898,2022-05-23 14:34:47,"@gowthamkpr ,
I was able to reproduce the issue in tensorflow [v2.8](https://colab.research.google.com/gist/tilakrayal/1e3b0be6e3530f5bd0a50ce0e88da509/text_generation.ipynb), and in [v2.7](https://colab.research.google.com/gist/tilakrayal/8016913e447c36af168fbf85f98e8d5e/text_generation.ipynb) and nightly the code was failing with different error. Please find the gist.","I was able to reproduce the issue in tensorflow [v2.8](https://colab.research.google.com/gist/tilakrayal/1e3b0be6e3530f5bd0a50ce0e88da509/text_generation.ipynb), and in [v2.7](https://colab.research.google.com/gist/tilakrayal/8016913e447c36af168fbf85f98e"
56181,mdanatg,1134756107,2022-05-23 14:33:05,"Right, so that's my point - the optimizations should never be approximating. It's more like transformations of the kind `exp(log(x)) -> x`, or `3 * x / x -> 3`. So if we see discrepancies, it's likely that the eager code is numerically unstable, whereas the optimized code is more correct. That's why disabling the optimizer should only be used for testing.","Right, so that's my point - the optimizations should never be approximating. It's more like transformations of the kind exp(log(x)) -> x, or 3 * x / x -> 3. So if we see discrepancies, it's likely that the eager code is numerically unstable, whereas the optimized code is more correct. That's why disabling the optimizer should only be used for testing."
30418,ppham27,1134698389,2022-05-23 13:43:54,"I am not too familiar with that repository, unfortunately. I don't see a lot of value in rewriting it, though. If I were to keep pursuing this, I'd be more inclined to merge this into TF Core as a fix for https://www.tensorflow.org/api_docs/python/tf/recompute_grad.","I am not too familiar with that repository, unfortunately."
56109,bleedingfight,1134476983,2022-05-23 10:10:11,"@gadagashwini I try to build tensorflow in 11.2.0-cudnn8-devel-ubuntu20.04 from nvidia.My tensorflow's latest commit is f807fbabc73 with master branch.I build tensorflow no error for this env:
1. cuda:`Build cuda_11.2.r11.2/compiler.29618528_0`(cuda-11.2xxx.run)
2. bazel:5.1.1
3. cudnn:8
4. gcc: `gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0`
it't ok.I think that my manjaro's gcc is ok because i build it last three month ago.maybe is'nt tensorflow's bug.i don't know.","""it't ok"""
56207,aliencaocao,1134319225,2022-05-23 8:01:00,"The complete code is very long as it involves loading of datasets which I cannot make public, and various preprocessing functions. What I have provided is a line that when removed, solves the issue.","The complete code is very long as it involves loading of datasets which I cannot make public, and various preprocessing functions. What I have provided is a line that when removed, solves the issue."
56208,faysalhossain2007,1133909981,2022-05-22 14:34:29,"I even try to increase GPU growth ```
config = tf.compat.v1.ConfigProto(gpu_options=tf.compat.v1.GPUOptions(allow_growth=True))
sess = tf.compat.v1.Session(config=config)
```",I even try to increase GPU growth
41836,gowthamkpr,1133785729,2022-05-21 23:58:04,"@ziofil This is not yet possible.
But you have two choices:
a) Use Tensorflow ufuncs for your loss function, for instance not using .numpy() and replacing np.sum() by tf.reduce_sum()
b) Use NumPy ufuncs, but train eagerly, by passing run_eagerly=True in model.compile()","""This is not yet possible"""
46757,st--,1133658865,2022-05-21 15:48:39,@Cheril311 @random-forests I've tried fixing the same issue in #55697 but I've got issues with the CLA system refusing to accept my username. But maybe it'll help you figure out what the issue is & you can re-do the docfix on your own.,I've tried fixing the same issue in #55697 but I've got issues with the CLA system refusing to accept my username.
54498,njzjz,1133490000,2022-05-21 1:07:27,"I got the same issue when building TensorFlow 2.9 with CUDA 10.1, among a lot of `.cu.cc` files. Finally, I decided to use CUDA 10.2 instead, and the errors disappeared. It's still quite confusing why 10.2 doesn't have this issue.","I got the same issue when building TensorFlow 2.9 with CUDA 10.1, among a lot of .cu.cc files."
56119,bhack,1133374790,2022-05-20 21:05:01,"To summarize I think that all is working correctly and the code in master was working cause the old local scope of the lambda args but it was ""wrong"".","""wrong"""
56180,sachinprasadhs,1133222249,2022-05-20 19:06:26,"Hi, Currently NNAPI acceleration is not supported when dynamic sized tensors are used, you can find the details from the tensorflow document [here](https://www.tensorflow.org/lite/android/delegates/nnapi#use_supported_models_and_ops).
The only way now would be to have workaround solution like you're doing.",Currently NNAPI acceleration is not supported when dynamic sized tensors are used
55971,sachinprasadhs,1133217461,2022-05-20 18:59:18,"Hi, It can be due to some of the High level TF APIs which doesn't support dispatching which means you cannot pass `KerasTensors`. For more details you can look at the detail here.
https://github.com/keras-team/keras/blob/4098cefe29aea05ba80faff878cec464f1b3a2e9/keras/engine/keras_tensor.py#L77-L93","""It can be due to some of the High level TF APIs which doesn't support dispatching which means you cannot pass KerasTensors."""
56169,bhack,1133004351,2022-05-20 14:54:42,P.s. take care if/when you will work in graph mode with your initial syntax. We are working on fixing/align it https://github.com/tensorflow/tensorflow/issues/56089,P.s. take care if/when you will work in graph mode with your initial syntax. We are working on fixing/align it https://github.com/tensorflow/tensorflow/issues/56089.
56179,south-ocean,1132934761,2022-05-20 14:00:24,"@sushreebarsa I can try this, but TF v2.7.0 is the version what I need despite the outcome of TF2.9, on the same time ,I had build TF1.15 success with of the same environment of the TF v2.7.0 , So what I want to solve is this error on the version of TF v2.7.0, So can you help me to solve this error, thanks.","Despite the outcome of TF2.9, on the same time ,I had build TF1.15 success with of the same environment of the TF v2.7.0, So what I want to solve is this error on the version of TF v2.7.0, So can you help me to solve this error, thanks."
56171,bulvara,1132821979,2022-05-20 12:06:03,"Hi Tilakrayal,
do you know what the _true_ upper limit for GCC version is?
* You are testing with GCC 9.3.1
* ArchLinux is successfully [using GCC 11](https://github.com/archlinux/svntogit-community/blob/2d998daafcc3c58749b4b2d585c7fa62250d3db3/trunk/PKGBUILD#L97)
** I'm having the same MLIR issue even with GCC11 :(",I'm having the same MLIR issue even with GCC11 :(
56157,Sara980710,1132787044,2022-05-20 11:23:44,"When I test the code you suggested, it shows that it does not have a GPU even if I do not set os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1"", but it still is as fast as when I run with C++ GPU delegate. Are you sure that it works with AMD GPU:s as well? When i use c++:
-GPU: 2 min
-CPU: 5 min
When I use python:
- 2 min (should be using GPU right? )","""It shows that it does not have a GPU even if I do not set os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1"", but it still is as fast as when I run with C++ GPU delegate."""
56121,Jalil-Mahdizadeh,1132580618,2022-05-20 7:37:05,"Hi It is still not working when I tried to batched generate text by ""one_step_reloaded"" [https://colab.research.google.com/gist/tilakrayal/1e3b0be6e3530f5bd0a50ce0e88da509/text_generation.ipynb](url)","""still not working"""
56137,kulinseth,1132455690,2022-05-20 4:43:56,"> @sachinprasadhs / @kulinseth I see that `tensorflow-macos` 2.9.0 was released, but there is not a 3.10 wheel:
> > * https://pypi.org/project/tensorflow-macos/2.9.0/#files
We have planned for py38/py39 releases so far. We can look into adding 3.10 support.","""I see that tensorflow-macos 2.9.0 was released, but there is not a 3.10 wheel:"""
56081,impjdi,1132306528,2022-05-19 23:51:15,"TFLite GPU is not 100% compatible with all the ops & options. As the error message says:
> Slice does not support shrink_axis_mask parameter.
You can look for that string in `//tensorflow/lite/delegates/gpu/common/model_builder.cc`.",TFLite GPU is not 100% compatible with all the ops & options.
56137,johnthagen,1132077587,2022-05-19 18:51:37,"@sachinprasadhs / @kulinseth I see that `tensorflow-macos` 2.9.0 was released, but there is not a 3.10 wheel: - https://pypi.org/project/tensorflow-macos/2.9.0/#files","I see that tensorflow-macos 2.9.0 was released, but there is not a 3.10 wheel: - https://pypi.org/project/tensorflow-macos/2.9.0/#files."
56019,tilakrayal,1131272510,2022-05-19 6:25:27,"@JamesJacquesDiego ,
I was facing different error while executing the mentioned code.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/f7d41bd80ea5b9b250d4e357bb0e3694/untitled346.ipynb).","""I was facing different error while executing the mentioned code."""
56149,gauravxgolu,1131229965,2022-05-19 5:37:45,"I have got the reason to throw this error. Actually, I am trying to convert tf1 generated model in tf2 installed version.","I have got the reason to throw this error. Actually, I am trying to convert tf1 generated model in tf2 installed version."
55966,kaixih,1130380196,2022-05-18 18:42:15,Force pushed the PR to resolve the conflict.,Force pushed the PR to resolve the conflict.
55812,cantonios,1130307106,2022-05-18 17:41:51,"I think you want a combination of `tf.gather_nd` and `tf.tensor_scatter_nd_update`:
```
# Get the list of indices for some condition mask = tf.where(y_true > 5)
# Extract the values from another tensor at those indices
vals = tf.gather_nd(error, mask)
# Update values at the supplied indices based on the old values
new_error = tf.tensor_scatter_nd_update(error, mask, 5 * vals)
```","I think you want a combination of tf.gather_nd and tf.tensor_scatter_nd_update:  # Get the list of indices for some condition mask = tf.where(y_true > 5) # Extract the values from another tensor at those indices vals = tf.gather_nd(error, mask) # Update values at the supplied indices based on the old values new_error = tf.tensor_sc"
56093,ozencgungor,1130180056,2022-05-18 15:38:29,@gadagashwini My `$LD_LIBRARY_PATH` variable already contains the relevant paths for cuDNN and CUDA libraries but for some reason `configure` fails to find them. `export TF_CUDA_PATH` was more reliable for me for some reason but thanks for the link.,"""for some reason configure fails to find them"""
56104,yakupakkaya,1130137260,2022-05-18 15:08:37,@mohantym export_tflite_ssd_graph.py does not work for faster_rcnn. How can I convert my custom model? Do you have any alternative solution?,"""export_tflite_ssd_graph.py does not work for faster_rcnn"""
53144,bermeitinger-b,1130130653,2022-05-18 15:02:48,"The same issue is still happening with 2.8.0, 2.8.1, 2.9.0, and any combination with Python/Conda versions 3.7, 3.8, 3.9, or 3.10.","The same issue is still happening with 2.8.0, 2.8.1, 2.9.0, and any combination with Python/Conda versions 3.7, 3.8, 3.9, or 3.10."
55970,CaptainDario,1129217925,2022-05-17 19:04:38,@mohantym I tried it with vs 2019 and cmake 3.23 and it is still stuck at `generating code...`,I tried it with vs 2019 and cmake 3.23 and it is still stuck at generating code....
56135,BasanthVerma,1129110801,2022-05-17 17:05:33,"I also couldn't run the colab example on this link, the github repo link also seems to be broken https://www.tensorflow.org/lite/tutorials/model_maker_object_detection","I also couldn't run the colab example on this link, the github repo link also seems to be broken"
55931,mihaimaruseac,1129001773,2022-05-17 15:18:39,"Sadly, I don't know what error occurred during that fetch. Might have been just a transient error and retrying would work.","Sadly, I don't know what error occurred during that fetch."
56099,chunduriv,1128948526,2022-05-17 14:33:12,"@zurgeg, Thank you for reporting the issue. Unfortunately I am unable to replicate the issue using `Firefox 96.0.3`. Please refer to the screenshot in below
![image](https://user-images.githubusercontent.com/74177924/168835495-0b7f0788-c88c-4335-9462-06bc55c89c9c.png)
Generally `503 service unavailable` caused by a high number of requests sent to the server, which exhausts available resources.
Please can you confirm if you are facing any issues using `Firefox 96.0.3`? Thank you.",unable to replicate the issue using Firefox 96.0.3
56125,feliwir,1128763078,2022-05-17 11:39:38,"I get the same issue when compiling on x86 Linux. Inside the `contrib/tensorflow/tensorflow/lite/c/CMakeLists.txt:63` the non-existing file `common.c` is added. The file is now named `common.cc`
This only happens when using the C API though as described here: https://www.tensorflow.org/lite/guide/build_cmake#build_tensorflow_lite_c_library",I get the same issue when compiling on x86 Linux.
53099,ArtanBerisha1,1128496155,2022-05-17 7:09:13,"Hi @sachinprasadhs , Can you be more specific on what details do you need ? I have already provided the version of Hexagon `v1.20.0.1` .
The issue that you linked was solved by disabling SELinux: `adb root` and `adb shell setenforce 0`, which I already did but it didn't work for me :/","""I have already provided the version of Hexagon v1.20.0.1 . The issue that you linked was solved by disabling SELinux: adb root and adb shell setenforce 0, which I already did but it didn't work for me :/."""
54104,nyngwang,1128366267,2022-05-17 3:24:20,"FYI: While this update has been merged, `tensorflow-macos` users cannot benefit from this update :(",tensorflow-macos users cannot benefit from this update :(
56119,bhack,1128187727,2022-05-16 22:11:39,"P.s. I think that there was already your TODO:
https://github.com/tensorflow/tensorflow/blob/79eee8e0493fb443a3fe6f247c3f55cec40b11e7/tensorflow/python/autograph/operators/control_flow.py#L196",I think that there was already your TODO.
55930,John-yovoa,1128127944,2022-05-16 20:57:42,I've installed tensorflow GPU and cpu both 2.8.0 but didn't helped :(,I've installed tensorflow GPU and cpu both 2.8.0 but didn't helped :(
55231,cantonios,1128111543,2022-05-16 20:36:37,I think a test or two timed out. Doesn't look related.,I think a test or two timed out.
56075,josephrocca,1128049556,2022-05-16 19:24:32,"> tflite is working exactly as intended and that file size is optimal given tflites design choice to materialise arrays.
@sachinprasadhs A 30x size increase over the TensorFlow SavedModel format in a format specifically designed for microcontrollers, mobiles, and other edge devices definitely doesn't seem optimal to me. Within the context of a design choice, it might make sense, but the design choices can be bad.
I'll await further feedback/comments on this.","A 30x size increase over the TensorFlow SavedModel format in a format specifically designed for microcontrollers, mobiles, and other edge devices definitely doesn't seem optimal to me."
55808,cheshire,1127753715,2022-05-16 14:33:01,"Do you think we could do something much simpler, without a flag, and just disabling tf.where?","Do you think we could do something much simpler, without a flag, and just disabling tf.where?"
56117,Sanjay2802,1127719636,2022-05-16 14:05:33,"Hi @mohantym , https://colab.research.google.com/drive/1qdcEpYXlu-K5eYC9ARtyJaiQjlhT63kT?usp=sharing
You can see that i interrupted the training , but after that i need to continue from last checkpoint epoch , but error arises , please help me to solve this brother","""Error arises"""
56116,EnricoMi,1127537255,2022-05-16 11:11:25,"Same for Ubuntu 20.04.
Last version that worked was `tf_nightly_gpu-2.10.0.dev20220427`.
First version this fails is `tf_nightly_gpu-2.10.0.dev20220514`.
https://buildkite.com/horovod/horovod/builds/7748#c5793a34-ec06-4ca8-aa26-b4491ea3fbca/224-470",Same for Ubuntu 20.04
55549,TheBlackPlague,1127482048,2022-05-16 10:11:59,Hello. It seems for me it is still not utilizing the GPU.,It seems for me it is still not utilizing the GPU.
55931,wanderscrow,1127282531,2022-05-16 6:42:06,"I've also encountered this error (GPU support tho, and for Windows), both with bazel 4.2.1 and 4.2.2. when I tried what @mihaimaruseac suggested and tried bazel 5.1.1, I got
`ERROR: An error occurred during the fetch of repository 'local_config_cuda':`
this is using a tested build configuration, so I don't know what is up with this. please help.","""Error: An error occurred during the fetch of repository 'local_config_cuda':"""
56104,mkxyh,1127114152,2022-05-16 1:10:57,"Hey I would change the script back to the original values uint8 and try running it on one gpu without batching, Im sorry to confuse you I hope someone else can help.",I hope someone else can help.
40568,MUNI9849,1126984921,2022-05-15 17:41:00,"Hi Sir,
I am not able to build bazel for pynq-z2. I am getting java.lang.OutOfMemoryError: Java heap space even though I use ${JAVAC}"" -J-Xms750m -J-Xmx1024m -J-Xms750m. Could you please help me in this?
![bazel_build_error](https://user-images.githubusercontent.com/70973340/168486426-44076fca-0324-475d-8138-1dfc9c6c2f88.png)","""I am not able to build bazel for pynq-z2"""
45795,gowthamkpr,1126834026,2022-05-15 0:44:21,@TheBeaNerd The error occurs because the loss that has been calculated is not a scalar but an n dimensional tensor. Try reducing your loss to a scalar as mentioned in this [issue](https://stackoverflow.com/questions/69643143/raise-shapes-must-be-equal-rank-when-adding-regularizers-to-keras-layers) and it should work with regularizers.,The error occurs because the loss that has been calculated is not a scalar but an n dimensional tensor.
43732,gowthamkpr,1126831227,2022-05-15 0:17:49,@georgwiese Tried running your code with latest version of tensorflow (tf 2.8.0). Ran into a different error. Here's the [colab](https://colab.research.google.com/gist/gowthamkpr/3c0a1349de501cf89d15a62f06c0dae8/untitled.ipynb). Pls take a look at the solution provided [here](https://stackoverflow.com/questions/61910143/how-to-use-tf-function-with-keras-sequential-apis) to solve the new error.,"""Tried running your code with latest version of tensorflow (tf 2.8.0)"""
43665,gowthamkpr,1126828630,2022-05-14 23:55:37,@maifeeulasad @RicDen Please take a look at this [answer](https://stackoverflow.com/questions/62009497/tensorflow-sees-gpu-but-only-uses-xla-cpu-and-crashes-when-told-to-use-xla-gpu) and let me know if the issue still persists. Thanks!,"""sees gpu but only uses xla-cpu and crashes when told to use xla-gpu"""
37580,gowthamkpr,1126821842,2022-05-14 22:37:28,This still persists in tensorflow 2.8.0,This still persists in tensorflow 2.8.0.
56093,ozencgungor,1126777374,2022-05-14 17:09:12,@sushreebarsa I will try but the cluster I'm working with has Bazel v3.7.2 as the latest version. I'll try and update the Bazel version and build Tensorflow v2.8.0. The issue seems that my cuDNN library files are not under the CUDA directory `/usr/local/easybuild/software/CUDA/11.3.1` but rather at `/usr/local/easybuild/software/cuDNN/8.2.1.32-CUDA-11.3.1`. I do not have `sudo` access to create symlinks but maybe the newer bazel version and the newer tensorflow version will solve this issue.,I do not have sudo access to create symlinks.
56089,mdanatg,1126575570,2022-05-13 23:24:16,"Ah, that's not intended. Can add it to the build file and mark the failing ones with self.skipTest, then file an issue to get them to pass?
For the transformation, I'm not sure, the rules are quite finnicky, and I'm not sure I'd change them without extensive testing. Likely still safer to manually add the iterate to the list of loop_vars.","""Can add it to the build file and mark the failing ones with self.skipTest, then file an issue to get them to pass"""
56073,Maratyszcza,1126368630,2022-05-13 19:16:57,@gbaned Could you merge the PR?,Could you merge the PR?
56100,nochichi,1126357842,2022-05-13 19:00:39,"I may be facing the same problem.
I have not confirmed the value of the tensor board, but I think that it is not learning correctly.
In my case, the value of loss gradually increases to a very large value, eventually becoming nan.
Until the day before yesterday, the same code worked correctly.","I have not confirmed the value of the tensor board, but I think that it is not learning correctly."
29472,ManishwarG,1126214869,2022-05-13 16:03:14,"![image](https://user-images.githubusercontent.com/105509376/168322331-b6a07cd5-5ca6-4cd6-ac5f-f787eff4bebd.png)
I am facing the same problem even after setting the `experimental_new_converter` to `True`. Can anyone help me ?
I am trying to quantize a custom layer (Full-int8)",I am facing the same problem even after setting the experimental_new_converter to True.
56044,Assia17,1126137544,2022-05-13 14:45:25,Any solution ? I am facing the same problem,I am facing the same problem.
42424,mohantym,1126005848,2022-05-13 12:30:36,Sorry @fsx950223 ! I thought disabling eager execution in v2 code will give more accurate results. But it is still replicating in [2.8](https://colab.sandbox.google.com/gist/mohantym/649e6467c6caa1b404ac258eae79a378/git_42424.ipynb#scrollTo=HnkBabLwXllb) and [nightly](https://colab.sandbox.google.com/gist/mohantym/8df6343819ea8e8b8be1b9d626afb913/git_42424.ipynb#scrollTo=HnkBabLwXllb) version. Thanks for pointing out.,I thought disabling eager execution in v2 code will give more accurate results. But it is still replicating in [2.8](https://colab.sandbox.google.com/gist/mohantym/649e6467c6caa1b404ac258eae79a378/git_42424.ipynb#scrollTo=HnkBabLwXllb) and [nightly](https://colab.sandbox.google.
55960,joker-eph,1125950529,2022-05-13 11:22:59,The error is one of binary getting to big to link in this code model. Any commit can trigger it really when we're close to the limit.,The error is one of binary getting to big to link in this code model.
47554,hablee,1125936465,2022-05-13 11:06:06,"have it solved now? i have the same issue, with tf==2.7.1, and i can not save my model, neither through model.save() nor ModelCheckpoint callback, i am runing swin-transformer","i have the same issue, with tf==2.7.1, and i can not save my model, neither through model.save() nor ModelCheckpoint callback, i am runing swin-transformer."
55973,codeislife99,1125643074,2022-05-13 4:22:52,Hi @gadagashwini were you able to replicate the issue? This is a regression from TF2.6 and it seems to be really problematic as an user.,This is a regression from TF2.6 and it seems to be really problematic as an user.
44539,mihaimaruseac,1125525936,2022-05-12 23:53:17,This stalled at the moment as we;re trying to find a best way for steering non-python language bindings.,stalled at the moment as we;re trying to find a best way for steering non-python language bindings.
56082,AlbertoSinigaglia,1125415503,2022-05-12 20:58:16,"Tried many other thing, the only solution for the moment is to uninstall `tensorflow-metal` (I tried all the versions, all have the same issue), but it means that everything runs on the CPU (even though for the project I'm doing, the performances are more or less the same, for some reason)","Tried many other thing, the only solution for the moment is to uninstall tensorflow-metal (I tried all the versions, all have the same issue), but it means that everything runs on the CPU (even though for the project I'm doing, the performances are more or less the same, for some reason)."
56073,lgeiger,1125297365,2022-05-12 18:25:44,"> Needs unit tests
@Maratyszcza Sorry about that. I wasn't sure whether there is a concrete reason why reshapes are not yet delegated to XNNPACK before adding tests. I updated the PR to add tests for signed and unsigned reshapes similar to the floating point tests.",I wasn't sure whether there is a concrete reason why reshapes are not yet delegated to XNNPACK before adding tests.
56077,haberman,1125199927,2022-05-12 16:32:30,"> `_message.Message._CheckCalledFromGeneratedFile()`
This indicates that you are using generated code from an old protoc.
Per [the announcement](https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates), Protobuf 4.21.0 requires that the `_pb2.py` files were generated by protoc >= 3.19.0.
Regenerating the `_pb2.py` files from a new protoc should fix this problem.",_message.Message._CheckCalledFromGeneratedFile()
52064,MrSherish,1125107874,2022-05-12 15:04:34,"I've noticed very similar behaviour exists in TF 2.8 (moving from 2.5). In my case it was that one Conv2D was converted to Flex op for some reason. Using build() method instead of random inputs solved it as workaround. Still, this cannot be used for models with multiple inputs.",I've noticed very similar behaviour exists in TF 2.8 (moving from 2.5)
55697,st--,1124702745,2022-05-12 8:46:24,"Could you confirm that I understand correctly, it is ONLY possible to contribute to tensorflow (/ other Google projects) if I make my email address public in the git commit, and I cannot propose commits under my github-users-noreply address?
I've attached the error message I get when trying to sign the CLA.
![image](https://user-images.githubusercontent.com/5763727/168030545-ca833433-1449-41cf-becd-84d44d588db1.png)","""I've attached the error message I get when trying to sign the CLA."""
55973,codeislife99,1124542226,2022-05-12 5:23:37,"Hi, No this is not the issue. Can you run it on a GPU? Right now the script is not able to run because you are running it on a CPU. You can find the CUDA versions in the issue description but that shouldn't matter for the error.","""Right now the script is not able to run because you are running it on a CPU."""
53767,lgeiger,1124162232,2022-05-11 18:35:56,@JunyoungLim I retested the above example `2.10.0-dev20220427` and the converter still segfaults.,I retested the above example 2.10.0-dev20220427 and the converter still segfaults.
56060,yongtang,1123887381,2022-05-11 14:55:35,@mihaimaruseac There a quite a few vulnerabilities in curl 7.83.0. I don't know if the update has already been applied internally in google or not. Please feel free to close the PR if it has been taken care of.,I don't know if the update has already been applied internally in google or not.
56021,kobrineli,1123855406,2022-05-11 14:34:53,"@rockspring I've got the way to resolve problem.
Try not to build with `--copt='-UNDEBUG'` or build with `--copt='-DNDEBUG'`.
But while building with -UNDEBUG there is still a problem since the moment when LsbMask in xla/util.h became `constexpr`.","""But while building with -UNDEBUG there is still a problem since the moment when LsbMask in xla/util.h became constexpr."""
56054,areiner222,1123623438,2022-05-11 11:32:49,"@tilakrayal Sorry about including the botched strings - I did it to quickly visualize the output without having to navigate to a gist.
If you remove the strings (like @bhack did) and run the code, you should see the unexpected behavior.","""I did it to quickly visualize the output without having to navigate to a gist."""
56019,ambitious-octopus,1123386411,2022-05-11 8:53:57,"The lines you added have no effect, I assume you read [this issue](https://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory), you need to use the instructions related to TensorFlow 2. However, it seems that you don't have enough memory on the GPU. To test this assertion you could change the input size( e.g. `x = np.random.rand(1000, 200, 256, 3)` to `x = np.random.rand(1000, 100, 100, 3)` ) and see if the error persis","""You need to use the instructions related to TensorFlow 2"""
55713,mosicr,1123347193,2022-05-11 8:30:22,"Hi,
TF record is 500M, can't upload to github compressed. I enabled public access to gs://avwaymo/training_tfexample.tfrecord-00000-of-01000
You should be able to download from the Google Cloud ( gsutil cp gs://avwaymo/training_tfexample.tfrecord-00000-of-01000 . ); or perhaps use GCP Storage Browser and download the file .","""TF record is 500M, can't upload to github compressed"""
56025,nSircombe,1123214421,2022-05-11 5:53:53,Could they not simply be listed explicitly in TF_TEST_TARGETS with the other test exclusions?,Could they not simply be listed explicitly in TF_TEST_TARGETS with the other test exclusions?
56057,RuofanKong,1123213753,2022-05-11 5:52:34,"@edwardyehuang `pip install tensorflow==2.5` installs 2.5.0 only, while the need here is 2.5.3.","pip install tensorflow==2.5 installs 2.5.0 only, while the need here is 2.5.3."
56025,mseth10,1123074239,2022-05-11 1:15:15,"> This is not the right way to temporarily exclude these tests. It hides the exclusion away in multiple places and also prevents these tests from being run for the default Eigen build where they are passing.
Hi @elfringham , I agree with your concern. We can introduce a new tag like `no_aarch64_onednn_acl` and use it to exclude these tests. Do you think that's an acceptable solution?",This is not the right way to temporarily exclude these tests. It hides the exclusion away in multiple places and also prevents these tests from being run for the default Eigen build where they are passing.
54155,raminmohammadi,1123056133,2022-05-11 0:47:53,"I have tested the model in TF and it does work, I only have problem with the conversion.","I have tested the model in TF and it does work, I only have problem with the conversion."
31932,peastman,1122542343,2022-05-10 15:25:47,"I don't think the issue should be closed until it's actually fixed. The comment you linked above doesn't indicate a fix. It just suggests a very complicated workaround. I'm no longer using TensorFlow, but this is still a serious problem that will affect other people who are.","""The comment you linked above doesn't indicate a fix"""
55246,joelberkeley,1122424002,2022-05-10 13:49:19,"I have remembered that it may be possible to use `xla::While` with `xla::Slice` and `xla::DynamicUpdateSlice`, thus avoiding the concatenation at the end. I don't know if this would be performant.
**EDIT** Having spoken to the XLA devs, this would apparently **not** be performant","I have remembered that it may be possible to use xla::While with xla::Slice and xla::DynamicUpdateSlice, thus avoiding the concatenation at the end. I don't know if this would be performant. **EDIT** Having spoken to the XLA devs, this would apparently **not** be performant."
55945,catqaq,1121935216,2022-05-10 5:03:59,"Hi, of course I did, but failed for network error.
> Hi, Did you follow [this](https://www.tensorflow.org/lite/android/lite_build#set_up_build_environment_without_docker) document to setup build without docker.","""Failed for network error"""
56019,JamesJacquesDiego,1121762007,2022-05-10 1:11:29,"Thank you for your suggestions. But I've checked gpu usage in task manager when script was running. And the usage was from 1% to 5%. It looks like tensorflow is not using gpu memory at all.
In the info there was information that tensorflow tried to allocate about 10GB of memory (but I have only 6GB gpu). I've also tried memory_limit=4096 and allow_growth=True.
PS
When I use the same code but with cpu there is no problem.","""I've also tried memory_limit=4096 and allow_growth=True"""
55807,duncanriach,1121593282,2022-05-09 21:19:59,"The four tests that compared the gradients between the CPU and GPU were being run even when there was no GPU available, which would have compared the CPU gradients with themselves, always passing. The tests were being run under both `depthwise_conv_op_test.py` and `depthwise_conv_op_d9m_test.py`, so the most recent commit removes eight tests when there is no GPU available.","The four tests that compared the gradients between the CPU and GPU were being run even when there was no GPU available, which would have compared the CPU gradients with themselves, always passing."
55934,bmharper,1121500340,2022-05-09 19:38:27,"OK... I've tried minSdkVersion at 16 and 18, but I still get the same issue.","I've tried minSdkVersion at 16 and 18, but I still get the same issue."
50735,rsnitsch,1121486840,2022-05-09 19:25:21,Addendum: I just tested the same container image on a p3.2xlarge instance and the error did **not** occur. To confirm this I switched back to p2 and the error reappeared...,"""I just tested the same container image on a p3.2xlarge instance and the error did **not** occur."""
31932,peastman,1121351503,2022-05-09 17:02:44,"I long ago gave up on trying to support TensorFlow. And there's no longer any demand for it. In the field I work in (molecular simulation), nearly everyone has moved to PyTorch.","I long ago gave up on trying to support TensorFlow. And there's no longer any demand for it. In the field I work in (molecular simulation), nearly everyone has moved to PyTorch."
55972,mohantym,1121340670,2022-05-09 16:51:01,@Sanjay2802 ! This approach won't work on TFlite model maker . @sachinprasadhs ! Could please you look at this issue? Thanks,"""This approach won't work on TFlite model maker"""
56025,elfringham,1121303627,2022-05-09 16:14:59,"I think the use of no_aarch64 tag to exclude tests that fail when TF_ENABLE_ONEDNN_OPT=1 is not the best thing to do. The flag is currently used to exclude tests that can never work on AARCH64. This would add a group of tests that we hope are only temporarily broken and so would have their no_aarch64 tag removed at some later point. It also stops those tests being run at all on AARCH64 even for the Eigen build, where they currently pass. I think some other mechanism would be better.","""I think the use of no_aarch64 tag to exclude tests that fail when TF_ENABLE_ONEDNN_OPT=1 is not the best thing to do."""
26842,amilsted,1121133108,2022-05-09 13:53:59,"Looks like this is still a problem, judging by the recent comments.","Looks like this is still a problem, judging by the recent comments."
18356,gsoykan,1121004961,2022-05-09 11:57:39,"I had the same issue. After so many trials and continuous errors, I've finally generated the framework simply by updating TFL_MINIMUM_OS_VERSION = ""12.0"" from ""9.0"" in path 'tensorflow/lite/ios/ios.bzl' file.",After so many trials and continuous errors.
55970,CaptainDario,1120925582,2022-05-09 10:29:09,"I can try this but I also tried building it on github actions with no success.
The workflow can be found [here](https://github.com/CaptainDario/TfLite-Binaries/blob/main/.github/workflows/build_windows.yaml). There it also just stops.",I can try this but I also tried building it on github actions with no success.
55945,catqaq,1120812104,2022-05-09 8:38:39,"@karimnosseir @sachinprasadhs @mohantym Hi, Can we download all dependencies at once and load them locally? Currently I can only handle network errors for each package one by one, download the package first, and change the url to a local path, which is too time consuming.","Currently I can only handle network errors for each package one by one, download the package first, and change the url to a local path, which is too time consuming."
55812,Muhammad-Kaleem-Ullah,1120401606,2022-05-08 11:34:44,"I used run_eagerly in model compile and then call the custom loss function. Then, I just used the above calculation for Tensors. And I got stuck there. Regarding your suggestion to use `scatter_update`, it did not work here.","I used run_eagerly in model compile and then call the custom loss function. Then, I just used the above calculation for Tensors. And I got stuck there. Regarding your suggestion to use scatter_update, it did not work here."
12001,glenn-jocher,1120150874,2022-05-07 7:05:14,"@Orpheus23 hmm, yes that's what I was worried about. We've been running YOLOv5 experiments with these layers in PyTorch, and they seem to export well everywhere except TF. We build the TF models natively rather than go through ONNX, and currently there seems to be no efficient solution to build TF models with these layers. In PyTorch it's pretty simple as you can just set groups to equal the input/output channel counts to create depthwise conv2dtranspose layers.","""Currently there seems to be no efficient solution to build TF models with these layers."""
55576,HedgeHao,1119634790,2022-05-06 13:39:45,@terryheo I use debug build but the debugger stuck at InterpreterBuilder(). I can't step into tflite function to debug. Can anyone provide the sample code and prebuilt dylib that can work. I've been stuck on this very beginning sample for a month..,I've been stuck on this very beginning sample for a month.
55940,yimeisun123,1119137754,2022-05-05 23:31:21,"Two test are failed, I am trying to look at the details, but not sure if the failure is related to this PR change. For the Code Check-Changed Files, I couldn't see what exactly failed.
For the Py+CPP Test Suite, I see //tensorflow/c/eager:c_api_test_gpu failed, not sure if it is from this PR change.
Please let me know if I miss anything. Thanks.","""I am trying to look at the details, but not sure if the failure is related to this PR change."""
55807,duncanriach,1119116020,2022-05-05 23:01:38,"> This needs to get fixed, as we are not testing depthwise convolutions currently. @duncanriach, do you want to fix this? A quick fix would be to add `@unittest.skip(""Test currently fails"")` for the two failing test methods.
Yes, I'll work on this. It's weird that these tests are failing because I thought I ran them successfully for PR [55657](https://github.com/tensorflow/tensorflow/pull/55657).","""It's weird that these tests are failing because I thought I ran them successfully for PR [55657](https://github.com/tensorflow/tensorflow/pull/55657)..."""
55915,gowthamkpr,1119082611,2022-05-05 21:57:35,"@MarkTension This is not specific to MacOS. The `tf.train.Checkpoint()` receives `kwargs` paras which are key-object. And the object must be a trackable object(`RMSprop` is not a trackable object)
If you replace optimizer `RMSprop` with `tf.keras.optimizers.RMSprop(0.1)`, it works. Heres the [code](https://colab.research.google.com/gist/gowthamkpr/9cd79dc070b64c02f2f3acbfb74a8de5/checkpoint.ipynb).","""This is not specific to MacOS"""
55207,whatdhack,1118922652,2022-05-05 18:36:22,"That dump failed because the TF_DUMP_GRAPH_PREFIX was not populated. I do not think these logs can be ignored. My suspicion is under the hood XLA is enabled, even though it was not supposed to be.",That dump failed because the TF_DUMP_GRAPH_PREFIX was not populated.
46939,DNXie,1118740036,2022-05-05 15:59:21,"@saikumarchalla I think the gist you referred to is a wrong API. The bug still exists, see [gist](https://colab.research.google.com/drive/15PAAdaxZXoJvtuZ0joDwOBpTTc2crDQG?usp=sharing).","""The bug still exists"""
46899,DNXie,1118692762,2022-05-05 15:23:34,"The crash in `tf.ragged.range` still exists, see [gist](https://colab.research.google.com/drive/1bDowh0LgIL5WuYmn67UHgZgEwB185L-l?usp=sharing). @yongtang","""The crash in tf.ragged.range still exists"""
46913,DNXie,1118661417,2022-05-05 14:59:22,"This second input still crashes in the nightly version, see [gist](https://colab.research.google.com/drive/1eGLreialnwDETqk5LipkD1mLd7v5N9vP?usp=sharing) @yongtang ```
import tensorflow as tf
import numpy as np
tf.keras.layers.RepeatVector(n=9223372036854775807)(np.ones((0,0)))
```","""This second input still crashes in the nightly version, see [gist](https://colab.research.google.com/drive/1eGLreialnwDETqk5LipkD1mLd7v5N9vP?usp=sharing)"""
55815,michaeldietz,1118352991,2022-05-05 9:29:46,"@sushreebarsa The error in your gist happens because it uses an older version of tensorflow-datasets. I already tried to replicate the problem on colab with this [gist](https://colab.research.google.com/drive/1WB3ZMoPawC8UZEKQFUd7g_piHUxo2ajG?usp=sharing) but it does not happen there. As mentioned, it only happens on systems with a Windows operating system.","""The error in your gist happens because it uses an older version of tensorflow-datasets"""
55645,PatriceVignola,1118148892,2022-05-05 4:26:27,Is there anything I can do to satisfy the failing checks or are they infra issues? They don't seem related to my changes.,Failing checks or are they infra issues? They don't seem related to my changes.
55849,drivanov,1118118765,2022-05-05 3:01:01,"@bixia1 : I fixed it. In my previous check-in I was using ```
#if IS_TRT_VERSION_GE(8, 2, 0, 0)
return ValidateImpl(....);
#else
return errors::Unimplemented(""Boolean op: "", params_->node_def.op(),
"" is not supported in TRT version < 8.2"");
#endif
```
for the wrong class (`ConvertBinary` instead of `ConvertBooleanBinary`)","""I fixed it. In my previous check-in I was using  #if IS_TRT_VERSION_GE(8, 2, 0, 0) return ValidateImpl(....); #else return errors::Unimplemented(""Boolean op: "", params_->node_def.op(), "" is not supported in TRT version  8.2""); #endif  for the wrong class (ConvertBinary instead of ConvertBooleanBin"
55919,mihaimaruseac,1117973352,2022-05-04 21:57:59,"We no longer patch 1.15. We don't have access to VMs on which to build it, our end-of-life-policy is that each version is available for ~1 year. We are currently releasing TF 2.9 so this means versions TF 2.6, TF 2.7, TF 2.8 and TF 2.9 are the only versions that we support.","""We don't have access to VMs on which to build it, our end-of-life-policy is that each version is available for 1 year."""
48249,haitong,1117668579,2022-05-04 18:28:39,"For example, tfserving build is failing due to this https://github.com/tensorflow/serving/issues/1958#issue-1092136168",tfserving build is failing due to this
55817,bani-intelaipg,1117600382,2022-05-04 17:12:44,"@penpornk I believe this has been broken for a while. I tried to go back to find a passing build with ""--config=mkl"" setting, but haven't found one with my limited rebuild effort. I can use some guidance on a potential analysis of the why this import is potentially failing in Windows. Do we have to explicitly set some PATH/PYTHONPATH env var to help the build find the module? The pipeline you mentioned was a recent temporary effort and not a mainline pipeline.","""I believe this has been broken for a while"""
55495,impjdi,1117526194,2022-05-04 15:58:23,"Uh, I'm not familiar with the Maven repository. When I go to search.maven.org, I only get 3 search results and neither libs are there =/",I'm not familiar with the Maven repository.
41448,pltnk,1117204994,2022-05-04 11:33:13,The issue is still present in TF 2.8.0.,The issue is still present in TF 2.8.0.
55840,bhack,1117200361,2022-05-04 11:27:02,"> `tf2.8 reports error: Segmentation Fault (core dumped)` I cannot reproduce this on Colab GPU (TF 2.8)
Please note also that usually the fix in the older versions (<=1 year) will be done if that is a security bug or if the bug is critical and impacting larger community. In this case it is working fine or not reproducible with Tensorflow 2.8 you need to continue using this version.",tf2.8 reports error: Segmentation Fault (core dumped)
55495,nhatuan84,1117126225,2022-05-04 9:51:49,I am facing the same issue with 2.8.,I am facing the same issue with 2.8.
55817,mihaimaruseac,1116764755,2022-05-03 23:43:06,"Penporn (@penpornk), can you bisect this if this still happens? I couldn't find a cc_shared_library last week last week that could have cause this",I couldn't find a cc_shared_library last week last week that could have cause this.
55550,bhack,1116447133,2022-05-03 18:50:52,"> tf.image.rgb_to_grayscale(img) also does not yield the same result as the decode_image version.
Are you sure that the weights are exactly the same?
As `decode_image` in your case is `libpng`:
https://github.com/glennrp/libpng/blob/master/pngrtran.c#L1013-L1049
Instead `tf.image.rgb_to_grayscale(img)` is:
https://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850e/tensorflow/python/ops/image_ops_impl.py#L2514",tf.image.rgb_to_grayscale(img) also does not yield the same result as the decode_image version. Are you sure that the weights are exactly the same?
55799,hartmannma,1116420798,2022-05-03 18:26:15,"Yes, I agree with you. When the error does not occur, differences are in the order of 10e-2. But in the error case differences were above 90%, which is basically a completley different output: `relative error: tf.Tensor(0.9494969, shape=(), dtype=float32) %`
I assumed, @mohantym did also get an error in this order of magnitude in TF 2.8. Maybe I misinterpreted his comment?","relative error: tf.Tensor(0.9494969, shape=(), dtype=float32) %"
55752,hawkinsp,1116414281,2022-05-03 18:19:14,I think you actually need to fix the `std::string` references before merging: they cannot appear in a C API.,I think you actually need to fix the std::string references before merging: they cannot appear in a C API.
55628,kaixih,1116347799,2022-05-03 17:16:18,"IIRC, the `AMD ROCm -- Community CI Build` will fail if those TENSORFLOW_USE_ROCM is there. It usually was some dependence issue but I didn't dive deep into the issue. @rahulbatra85","IIRC, the AMD ROCm -- Community CI Build will fail if those TENSORFLOW_USE_ROCM is there."
46342,Anmol-Sharma,1116134190,2022-05-03 13:59:37,"Hi @sushreebarsa I don't this is still an issue. At that time when I opened this issue, it was present and I raised a fix as well [Here](https://github.com/tensorflow/estimator/pull/59) but due to very large merge conflicts (when the time to merge came) the changes were skipped.
So, I think this ticket can be closed, as mixing of Estimators with TF2 may not be a very good idea, so I'm closing this.",I don't this is still an issue.
55754,ahlzouao,1116072167,2022-05-03 13:02:22,"Hello @haozha111, Have you been able to reproduce this issue on your side ? For your information, I tried setting **input_tensor->data.data** which is a void* and it doesn't seem to work either. This is what I tried to do:
```
TfLiteTensor* input_tensor = restore_runner->input_tensor((std::string(input_names[0])).c_str());
input_tensor->data.data = (void *)restore_path;
```
Although, I am pretty convinced that the issue is related to this variable assignement.","""I am pretty convinced that the issue is related to this variable assignement."""
55550,StefRommes,1115977964,2022-05-03 11:08:45,Visually there are very minimal to no differences but numerical there are.,Visually there are very minimal to no differences but numerical there are.
55821,Wei-JL,1115977474,2022-05-03 11:08:06,"Now I have updated the progress, may I ask why this error is reported?
```
""tf.GradientTape.gradients() does not support graph control flow ""
NotImplementedError: tf.GradientTape.gradients() does not support graph control flow operations like tf.cond or tf.while at this time. Use tf.gradients() instead. If you need this feature, please file a feature request at https://github.com/tensorflow/tensorflow/issues/new
```","""tf.GradientTape.gradients() does not support graph control flow"""
55819,dgrnd4,1115925260,2022-05-03 10:00:24,"@mohantym cannot use it on versions > 2.7 because this NET will be implemented on a microcrontroller that doesn't support these new versions. However because of privacy and stuff I cannot share the Dataset but it's a dataset of images with 4 classes!
So whatever dataset you find with just 4 classes it's ok!
There are 4k images for the TR and 1k images for the TE.","""Cannot use it on versions > 2.7 because this NET will be implemented on a microcrontroller that doesn't support these new versions"""
55835,leeflix,1115877483,2022-05-03 8:59:50,When using Select Ops I have to link way bigger binaries.,I have to link way bigger binaries.
55821,Wei-JL,1115745721,2022-05-03 5:12:37,"I have rewritten the network, using the tensorflow.keras component of the network can normally output the entire network structure, but the 'x' and 'y' parameters needed in model.fit need to be generated by what method of dataset, has been stuck here for a long time. Tried all the data types given by the official 1.15 API, reported all kinds of strange errors. Using the keras.model class to build a network doesn't have this problem.",Using the keras.model class to build a network doesn't have this problem.
55505,372046933,1115560018,2022-05-03 2:45:50,"Copy `genrule` from the above comment. https://github.com/tensorflow/tensorflow/issues/55505#issuecomment-1114333283
Build results in the following error.
```
ERROR: /xxx/yyy/BUILD:330:8: in cmd attribute of genrule rule //:tf_wheel: label '//:build_pip_package' in $(location) expression is not a declared prerequisite of this rule
ERROR: /xxx/yyy/BUILD:330:8: Analysis of target '//:tf_wheel' failed
ERROR: Analysis of target '//:tf_wheel' failed; build aborted: ```",genrule:
55829,Nayana-ibm,1115218742,2022-05-02 18:24:12,"Tried with installing typing, typing-extensions but not resolved","Tried with installing typing, typing-extensions but not resolved."
55222,mihaimaruseac,1115086903,2022-05-02 16:20:11,Since https://github.com/tensorflow/tensorflow/pull/55222#issuecomment-1084771301 showed up in #53828: the comment here is to help in solving the issue properly: you need to subclass the methods for the ragged tensor and attach docstring to the new methods.,Since https://github.com/tensorflow/tensorflow/pull/55222#issuecomment-1084771301 showed up in #53828: the comment here is to help in solving the issue properly: you need to subclass the methods for the ragged tensor and attach docstring to the new methods.
55814,mdanatg,1115034411,2022-05-02 15:27:12,"I suspect the non-nan results for loss_object are thanks to arithmetic simplifications made by TF. Without those simplifications, you might hit numerical instability, especially in FP32. In eager, there are no such optimizations, and if you insert tf.print in graph mode, that would block such optimizations as well. Try disabling the optimizer to see if the inconsistency persists.
See https://www.tensorflow.org/api_docs/python/tf/config/optimizer/set_experimental_options","I suspect the non-nan results for loss_object are thanks to arithmetic simplifications made by TF. Without those simplifications, you might hit numerical instability, especially in FP32. In eager, there are no such optimizations, and if you insert tf.print in graph mode, that would block such optimizations as well."
55814,bhack,1114964710,2022-05-02 14:32:21,Yes sorry I supposed to have a `tf.print` also in eager.. probably there was some copy-paste last minute change. But can you see that with `tf.float64` we have the same results in eager and graph mode?,tf.print also in eager..
55809,innat,1114861573,2022-05-02 13:18:23,@sushreebarsa adding `keras` tag may not be proper. Please read this https://github.com/keras-team/keras/issues/16260#issuecomment-1070950476 and https://github.com/keras-team/keras/issues/16260#issuecomment-1071146844.,"""Adding keras tag may not be proper"""
55811,mcourteaux,1114680986,2022-05-02 9:57:17,Although I still think the TensorArray/tf.while_loop 4-byte copy is a performance bug.,Although I still think the TensorArray/tf.while_loop 4-byte copy is a performance bug.
55755,GosuPaper,1114665653,2022-05-02 9:33:13,"Since it appears that freeze_save_model.h and .cc are not in the build library, I have managed a simple workaround for those two :
Include them directly in the project next to my main and obviously in the makefile. This way I am able to use the function FreezeSavedModel without any trouble and it is working great.
Still, I would really like to know how to save and load a model using the C++ API in TF2.x. There has to be a way included in the library !","""There has to be a way included in the library!"""
55817,bani-intelaipg,1114542170,2022-05-02 6:35:16,Same error is there with commit# b5fa6a4eecdcf69408708e98ba4de6debb880596,Same error is there with commit# b5fa6a4eecdcf69408708e98ba4de6debb880596.
55817,mihaimaruseac,1114334113,2022-05-01 21:01:43,"We were using a non-LTS Bazel at that commit, to work around some other `cc_shared_library` issues. Can you try with 3637a0245a2b44f2de2aa84b6f8f40cac5600109 (the commit after the one you tested, which reverts back to LTS Bazel)? This way we'd know if the issue is Bazel or some `cc_shared_library` changelist.","""We were using a non-LTS Bazel at that commit, to work around some other cc_shared_library issues."""
55817,mihaimaruseac,1114332227,2022-05-01 20:50:14,"Rostam, this looks like another cc_shared_library issue",This looks like another cc_shared_library issue.
55803,rudimytro,1114174882,2022-05-01 8:56:42,"**Update**
Despite throwing errors my command generates the required .so file. But, when I try to load it with `tf.load_op_library` I get `tensorflow invalid ELF header` error.","Despite throwing errors my command generates the required .so file. But, when I try to load it with tf.load_op_library I get tensorflow invalid ELF header error."
55810,mihaimaruseac,1114083662,2022-05-01 0:30:57,2022-04-30 06:07:54.980435: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.,W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries.
55703,kofimokome,1113667384,2022-04-29 19:50:32,The issue has not been resolved. But I found an alternative method to save the model,The issue has not been resolved.
45946,isabellahuang,1113546656,2022-04-29 17:19:39,Running into the same issue as well.,Running into the same issue as well.
55797,elfringham,1113521556,2022-04-29 16:50:36,This should have been resolved by https://github.com/tensorflow/tensorflow/commit/a969f2bf9196b534f2acc47ca11e963c729bb80d,This should have been resolved by https://github.com/tensorflow/tensorflow/commit/a969f2bf9196b534f2acc47ca11e963c729bb80d.
55505,mihaimaruseac,1113457426,2022-04-29 15:37:24,"If you are building a target from an external repo you have to use `@` syntax.
I think this is more and more becoming an issue for Bazel, not TF. There's no breakage that can be root caused to TF itself.","I think this is more and more becoming an issue for Bazel, not TF. There's no breakage that can be root caused to TF itself."
55752,jpienaar,1113259844,2022-04-29 12:32:49,"SG, I didn't see any of those failing here (I mentioned the other set yesterday and those you seem to have fixed but kokoro pending).",I mentioned the other set yesterday and those you seem to have fixed but kokoro pending.
55752,lipracer,1113253902,2022-04-29 12:26:20,"I have resubmitted jax's [patch](https://github.com/google/jax/pull/10482), but it has not been merged yet.Integration testing may also be affected.","I have resubmitted jax's [patch](https://github.com/google/jax/pull/10482), but it has not been merged yet."
55575,jerryIsHere,1113207199,2022-04-29 11:44:11,"This happened in version 2.6.3 `tfds.load('cifar100',split=['train', 'test'],as_supervised=True)`","This happened in version 2.6.3 tfds.load('cifar100',split=['train', 'test'],as_supervised=True)."
55756,xianzhuo-sky,1113145839,2022-04-29 10:19:33,"@sushreebarsa I change to the latest nightly build version 'tf-nightly-2.10.0.dev20220427', and can't use tf.data.Dataset.load .
`AttributeError: type object 'DatasetV2' has no attribute 'load'`","I change to the latest nightly build version 'tf-nightly-2.10.0.dev20220427', and can't use tf.data.Dataset.load . AttributeError: type object 'DatasetV2' has no attribute 'load'"
55755,GosuPaper,1113035874,2022-04-29 8:38:14,@tilakrayal I did try to use this version of gcc... Still exactly the same log error,I did try to use this version of gcc... Still exactly the same log error.
55752,lipracer,1112856521,2022-04-29 4:04:54,"Yes. I just run with command:
```bazelisk test tensorflow/compiler/mlir:all --cache_test_results=no```
and all pass, It seem some test no run.","I just run with command: bazelisk test tensorflow/compiler/mlir:all --cache_test_results=no and all pass, It seem some test no run."
55466,Jiaao-Bai,1112791045,2022-04-29 1:04:47,"i'm seeing the same issue too, what can i do if i build op only using gcc 9.3? Delete /root/.cache/bazel is not ok","i'm seeing the same issue too, what can i do if i build op only using gcc 9.3?"
55792,kaixih,1112640428,2022-04-28 20:47:39,Initial investigation shows that [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/remapper.cc#L401): the returned `input_props` of the conv2d node is an empty vector and further [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/costs/graph_properties.cc#L2833): the member `input_properties_` is also an empty map. vis. @nluehr @wenscarl,Initial investigation shows that [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/remapper.cc#L401): the returned input_props of the conv2d node is an empty vector and further [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/costs/graph_
55610,mohitrajpal1,1112635663,2022-04-28 20:41:43,This is also causing an issue for me. Do you have a pointer in TF code where I can hack together an addition to not compile tf.where?,This is also causing an issue for me.
55655,bixia1,1112521031,2022-04-28 18:17:29,"Many tests failed, will email a log",Many tests failed.
55432,PhillipMaire,1112477901,2022-04-28 17:31:43,I am using colab for my project and they have TF 2.8 GPU installed and I can not get a working V2.7 GPU working on colab. Is it unreasonable to ask for a working TF 2.7 version mobilenet model function perhaps like `applications._MobileNetV3Large()` or something? or are there too many dependancies and changes for that to work?,I am using colab for my project and they have TF 2.8 GPU installed and I can not get a working V2.7 GPU working on colab.
53613,Thf772,1112456708,2022-04-28 17:12:19,"I understand, but the whole point of this issue was to avoid having to use the `cache` hack for this kind of situation. There may be other scenarii in which it is useful to immediately get an in-memory dataset from a saved file.
If you do not wish to expand the capabilities of the API, maybe it would be wise to document the fact that `load` only performs lazy loading, and to describe the `cache` hack for the cases where eager loading is required.","I understand, but the whole point of this issue was to avoid having to use the cache hack for this kind of situation."
55592,zanussbaum,1112189377,2022-04-28 13:15:00,any update? this is a blocking issue,blocking issue>
55495,lakshya-4gp,1111777134,2022-04-28 5:55:39,"No, I'm not instantiating even 1 interpreter. I only added single line `auto* delegate = TfLiteGpuDelegateV2Create(/*default options=*/nullptr);` in a default native application, and it gives resulted in error. Please note that I downloaded the libs , tflite-2.8 and tflite-gpu-2.8, from the maven repository(https://search.maven.org/). Could there be difference.","""I only added single line auto* delegate = TfLiteGpuDelegateV2Create(/*default options=*/nullptr);"""
55432,PhillipMaire,1111538923,2022-04-27 22:23:46,"any mobility on this issue? I saw the commit failed some tests, can anyone elaborate on why changing the mobile net weights file to what it was in 2.7 causes failures?","I saw the commit failed some tests, can anyone elaborate on why changing the mobile net weights file to what it was in 2.7 causes failures?"
55610,wenscarl,1111404340,2022-04-27 19:35:55,"@cheshire > Actually do you want to send a patch disabling tf.where autoclustering?
Yes. `tf_xla_ops_to_cluster` serves a similar purpose but not quite the granularity we want. I would like to propose a `tf_xla_ops_cluster_blacklist` under `TF_XLA_FLAG` which takes comma separated op names like where or unique. Does that sound good?",tf_xla_ops_cluster_blacklist
54374,bixia1,1111317104,2022-04-27 18:00:50,We discussed this at meetings. This solution doesn't work because we currently allow TF-TRT to perform conversion on functions.,This solution doesn't work because we currently allow TF-TRT to perform conversion on functions.
52155,mariecwhite,1111315834,2022-04-27 17:59:28,"I ran into a similar error when building `benchmark_model` with OpenCL on x86. This command worked for me:
```
bazel build -c opt --copt=-DCL_DELEGATE_NO_GL --copt=-DMESA_EGL_NO_X11_HEADERS=1 tensorflow/lite/tools/benchmark:benchmark_model
```",I ran into a similar error when building benchmark_model with OpenCL on x86.
55754,haozha111,1111245927,2022-04-27 16:59:49,"The Loss value from the initial training on Google Colab seems to end at 0.241, but it starts at 0.0003855.
Hi,
From your C++ code, I didn't see you load the initial checkpoint from the previous training in colab. This will cause the model to use random weights. Could you try calling the restore signature as described in this article?
https://www.tensorflow.org/lite/examples/on_device_training/overview#restore_the_trained_weights","""The Loss value from the initial training on Google Colab seems to end at 0.241, but it starts at 0.0003855."""
55733,onurberkay,1111242057,2022-04-27 16:55:30,btw I have same problem with master branch,I have same problem with master branch.
55561,Yulv-git,1111187769,2022-04-27 16:08:59,"> This seems to have trouble landing. Can you split it into multiple PRs, one per top-level directory, please?
I have created multiple new PRs based on multiple branches to fix typos.",I have created multiple new PRs based on multiple branches to fix typos.
55725,mihaimaruseac,1111103091,2022-04-27 14:54:08,We had some issues hitting the size limit on the `tensorflow-cpu` project. We asked for a size limit bump at https://github.com/pypa/pypi-support/issues/1870,We had some issues hitting the size limit on the tensorflow-cpu project.
55658,zjk000,1111058240,2022-04-27 14:16:04,"Hi, it was some path issue, I modified the code a little bit, now it should be able to replicate the error, https://drive.google.com/drive/folders/1uNZ4ccKKX2p2yIoL3uH01dQDq07VQMdf?usp=sharing. And can you try again with python 3.8? The default python version of Colab is python 3.7, which may cause some error with pickle.","""I modified the code a little bit, now it should be able to replicate the error, https://drive.google.com/drive/folders/1uNZ4ccKKX2p2yIoL3uH01dQDq07VQMdf?"""
55550,StefRommes,1110746089,2022-04-27 8:58:37,"@sushreebarsa this isn't solved, the bug/quirk in Tensorflow persists, I just patched a small error in your gist so you can easier replicate.","""this isn't solved, the bug/quirk in Tensorflow persists"""
54559,jinfagang,1110666495,2022-04-27 7:50:19,"Same questoin, 2022 years, tflite still can not fully do fp16 inference on armv8.2 above CPU? Even on CPU doesn't support fp16 natively, it should also using simulation to support fp16. Please add cpu fp16!","""tflite still can not fully do fp16 inference on armv8.2 above CPU?"""
55561,Yulv-git,1110637654,2022-04-27 7:21:54,"> This seems to have trouble landing. Can you split it into multiple PRs, one per top-level directory, please?
I don't know how to split this PR into multiple PRs.
Perhaps, I can create multiple branches based on the current official latest master branch to create multiple new PRs.",I don't know how to split this PR into multiple PRs.
55296,wangpengmit,1110437154,2022-04-27 1:35:48,"Hi @LJKS, thanks for reporting! I think there are two issues here: (1) TF doesn't support differentiating through variable writes; (2) your workaround doesn't work for its own reason. About (2), because `copy_gradient`'s first return value isn't used, `var_up` in `grad` will also be zeros.
I can't think of a working workaround though. So the current situation is that differentiating through variable writes is impossible in TF. We are exploring some ways to properly support it.",TF doesn't support differentiating through variable writes; (2) your workaround doesn't work for its own reason.
55730,mihaimaruseac,1110291230,2022-04-26 21:59:10,"So, windows GPU failure:
```
T:\src\github\tensorflow>CALL tensorflow\tools\ci_build\windows\gpu\pip\run.bat --enable_remote_cache ""T:\src\gfile\bazel_wrapper.py"" 1>tensorflow/tools/ci_build/builds/win.out 2>&1 ERROR: Aborting VM command due to timeout of 10800 seconds
```
This can be ignored, I see the other one passed, so we should be good.","So, windows GPU failure: "
54650,christopherbate,1110264047,2022-04-26 21:24:59,"I fixed the issue. I also rebased on top of tree, but GitHub is showing extra commits from main branch","I also rebased on top of tree, but GitHub is showing extra commits from main branch."
55330,richhx,1110259281,2022-04-26 21:19:11,Anything I need to do to make the build pass or get this reviewed? Not sure why it's failing since the link doesn't work for me.,Not sure why it's failing since the link doesn't work for me.
55655,bixia1,1110104681,2022-04-26 18:12:37,@[meena-at-work](https://github.com/meena-at-work) Would you please make the PR description more descriptive and put the examples outside the PR description?,"""Would you please make the PR description more descriptive and put the examples outside the PR description?"""
55702,chunky,1110088846,2022-04-26 17:55:24,"I manually set the tensorflow_lite target to be built using stdc++20
I then subsequently got errors about __PRETTY_FUNCTION__ not defined, so I edited types.h, and inside the tflite namespace, I added:
```
#if !defined(__PRETTY_FUNCTION__) && !defined(__GNUC__)
#define __PRETTY_FUNCTION__ __FUNCSIG__
#endif
```
[Borrowed from here: https://stackoverflow.com/questions/48857887/pretty-function-in-visual-c ]
I haven't tested, but it did at least output Release/tensorflow-lite.lib without errors","I manually set the tensorflow_lite target to be built using stdc++20 I then subsequently got errors about __PRETTY_FUNCTION__ not defined, so I edited types.h, and inside the tflite namespace, I added:  #if !defined(__PRETTY_FUNCTION__) && !defined(__GNUC__) #define __PRETTY_FUNCTION__ __FUNCSIG__ #endif "
55561,mihaimaruseac,1110007779,2022-04-26 16:33:09,"This seems to have trouble landing. Can you split it into multiple PRs, one per top-level directory, please?","""Trouble landing"""
51429,mizabrik,1109913463,2022-04-26 15:06:31,"We faced an issue with tiled mean too; unfortunately, I can not provide a model either :(","We faced an issue with tiled mean too; unfortunately, I can not provide a model either."
55703,kofimokome,1109865691,2022-04-26 14:26:41,I finally saved the model without the .h5 extension and it works now. I don't know why models with .h5 do not work.,I don't know why models with .h5 do not work.
55752,jpienaar,1109838796,2022-04-26 14:03:26,"We don't expect it to be different with same input state no:
""The output is guaranteed to be a deterministic function of the initial state but it is not guaranteed to be deterministic between backends and different compiler versions.""
https://www.tensorflow.org/xla/operation_semantics#rngbitgenerator
If you wanted different numbers you have to use returned updated state from output of one as input to the other.","""The output is guaranteed to be a deterministic function of the initial state but it is not guaranteed to be deterministic between backends and different compiler versions."""
55747,tilakrayal,1109825492,2022-04-26 13:51:54,"@sachinprasadhs ,
I was facing different error while trying to execute the given code.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/238a0b0ad41c64e1592fc42511f932d2/55747.ipynb).","""I was facing different error while trying to execute the given code."""
55707,hijkzzz,1109793642,2022-04-26 13:24:01,"> Hi @hijkzzz Thanks for reporting this issue. Can you try the workaround mentioned on similar [thread](https://developer.apple.com/forums/thread/693292?page=2). Thanks!
Hi, they are not the same problem.
My bug can be fixed by replacing the Adam Optimizer with SGD Optimizer.
So the problem is that tensorflow-metal is not compatible with Adam Optimizer ?
The key error is "" 93125 segmentation fault""","""So the problem is that tensorflow-metal is not compatible with Adam Optimizer ?"""
44868,Ashment,1109358400,2022-04-26 5:25:34,"Hello @ghanko I have the same problem as in this thread, where I get this error:
```
Feature generation failed
Requested feature_data_ size 536907080 doesn't match 1960
```
I've tried your solution by adding the conv2d op to the micro op resolver, and it did not fix the issue. I've also tried increasing the arena size, as well as swapping to an all ops resolver, neither of which fixed the issue either. Maybe you have more insight into how to fix this? Thanks!","I've tried your solution by adding the conv2d op to the micro op resolver, and it did not fix the issue."
55724,iwen-kang,1109145987,2022-04-25 23:45:24,"@sushreebarsa I did provide code snippet in my PR, not sure why it disappeared, here it is:
pip install tensorflow==2.6
pip install keras==2.6.0
pip install --upgrade tensorflow-hub
pip install tensorflow_datasets
pip install tensorflow-text==2.6 --no-dependencies
pip install --upgrade tensorflow-estimator==2.6.0
python -c ""import tensorflow as tf; import tensorflow_datasets as tfds; print('tfds.__version__:' + tfds.__version__)""","""not sure why it disappeared"""
55657,reedwm,1109123881,2022-04-25 23:07:02,"The CI complains when a test with a timeout of ""long"" is added, but gives us the option of ignoring it when merging. Since you just split a test and didn't add a new test with a timeout of ""long"", we should ignore it. It does say that tests with a timeout of ""long"" are not automatically run by the CI, so it turns out `depthwise_conv_op_test` was not and is not run by the CI. We should probably fix this later.
There's also a merge conflict, but I'll manually resolve it.","The CI complains when a test with a timeout of ""long"" is added, but gives us the option of ignoring it when merging."
55745,stewartmiles,1109113289,2022-04-25 22:46:11,"Looks like some C99 (which, AFAIK, didn't appear until C++20) has crept into TF Lite.
https://github.com/tensorflow/tensorflow/blob/0cdb708bf66dde960e4ff9d6e8bf304d7d2b6d7c/tensorflow/lite/delegates/external/external_delegate.cc#L157","Looks like some C99 (which, AFAIK, didn't appear until C++20) has crept into TF Lite."
55745,stewartmiles,1109111463,2022-04-25 22:42:24,"After fixing that issue, there is another problem:
```
external_delegate.cc
src\tensorflow\tensorflow\lite\delegates\external\external_delegate.cc(158,11): error C7555: use of designated initializers requires at least '/std:c++20' [src\tensorflow\tensorflow\lite\cmakebuild\tensorflow-lite.vcxproj]
```
has TF _really_ moved to C++20? It doesn't seem great for compatibility.","after fixing that issue, there is another problem"
55696,sachinprasadhs,1108959641,2022-04-25 19:33:08,"`embedding_lookup_sparse` is not registered with XLA, `XLA_TPU_JIT` and `XLA_GPU_JIT` as well.
As the error message suggests you can perform the unsupported OPs compilation in CPU by setting `tf.config.set_soft_device_placement(True)`.
In your case, since you are using only one operation inside tf.function, it makes sense if you use only eager mode, also `experimental_get_compiler_ir` is limited to XLA operations only.","embedding_lookup_sparse is not registered with XLA, XLA_TPU_JIT and XLA_GPU_JIT as well. As the error message suggests you can perform the unsupported OPs compilation in CPU by setting tf.config.set_soft_device_placement(True). In your case, since you are using only one operation inside tf.function, it makes sense if you use only eager mode, also experimental_get"
47554,DalilaKhettaf,1108803674,2022-04-25 16:44:40,I'm having the same issue with Tensorflow 2.8.0. I got the warning while saving the model and couldn't reload it.,I got the warning while saving the model and couldn't reload it.
55623,Mastiff37,1108763535,2022-04-25 16:05:10,"I am unable to replicate with a toy problem in a different environment. Must be specific to the exact versions of things I have installed or some other factor. If someone knows the answer just from the symptoms, I'd like to hear, but I'll close this since there's nothing concrete for devs to go from.",I am unable to replicate with a toy problem in a different environment.
55645,mdanatg,1108726642,2022-04-25 15:31:02,cc @mihaimaruseac for a security assessment into the ability to inject arbitrary code into the computation.,cc @mihaimaruseac for a security assessment into the ability to inject arbitrary code into the computation.
55725,EnricoMi,1108495403,2022-04-25 12:16:25,"@tilakrayal you probably run macOS with Python 3.7 or 3.8, or Linux with Python 3.7. All other Python ad OS versions do not work.","""All other Python ad OS versions do not work."""
48068,jinfagang,1108454458,2022-04-25 11:32:12,"Hi, does there a way in CMake? I am using the minal example to build my app, I don't want change any code in tensorflow_src, is there a way to do it to shinrk down the tensorflowlite.so size? BTW, I using staticlib for tflite.a, it was 780M huge!!","I am using the minal example to build my app, I don't want change any code in tensorflow_src, is there a way to do it to shinrk down the tensorflowlite.so size? BTW, I using staticlib for tflite.a, it was 780M huge!"
53869,pure-rgb,1107918469,2022-04-24 21:07:06,"Any update? Why it's always required for end-user to update path config manually for installing TensorFlow with GPU support? Can't it be made automatic, like in a torch?","Why it's always required for end-user to update path config manually for installing TensorFlow with GPU support? Can't it be made automatic, like in a torch?"
42004,HJasperson,1107031695,2022-04-22 23:21:30,"To save any future readers my pain, point 1 in [the comment above](https://github.com/tensorflow/tensorflow/issues/42004#issuecomment-669118318) is trying to say that you should change any instances of `tf.keras.Model` to `tf.keras.layers.Layer`. My issue was slightly different, but ultimately came down to incorrect/missing concrete functions and point 1 fixed the problem.","point 1 in [the comment above](https://github.com/tensorflow/tensorflow/issues/42004#issuecomment-669118318) is trying to say that you should change any instances of tf.keras.Model to tf.keras.layers.Layer. My issue was slightly different, but ultimately came down to incorrect/missing concrete functions and point 1 fixed the problem."
55664,lu-wang-g,1107007416,2022-04-22 23:09:08,"I think it's the same issue as https://github.com/tensorflow/tensorflow/issues/47595, where something went wrong during conversion. The model should only have 4 outputs instead of 8. Circle back to @karimnosseir to double check the conversion step.","I think it's the same issue as https://github.com/tensorflow/tensorflow/issues/47595, where something went wrong during conversion."
55686,bixia1,1106031966,2022-04-22 5:44:07,@DEKHTIARJonathan It is nice to show an example of the graph here. But would you please move the graph example out of the PR description and put it in a comment block?,"""It is nice to show an example of the graph here. But would you please move the graph example out of the PR description and put it in a comment block?"""
55695,hontimzam,1106008291,2022-04-22 5:01:02,"Hello, I am sorry for the wrong click ""closed"" issue. For the tf version. I am sorry for the wrong information. we are using Tension flow version of **1.15.x** . In this case, do we able to solve the non-deterministic behavior on different GPU? We do not upgrade from 1.15.x to 1.2.x because of some compatible issue.","""We do not upgrade from 1.15.x to 1.2.x because of some compatible issue."""
55268,sanghun8741,1105979050,2022-04-22 4:01:24,"I also have a similar problem.
Unable to create symblic link in /usr/bin path on macOS.
`
exec env - .....
`
Because of that part, the environment variable disappears.
And if the environment variable disappears, python cannot be used, only python3 can be used.
This is my guess, but because of this I can't run bundletool during build.
Is there a way to fix the build other than bypassing the OS?",exec env - .....
55207,sachinprasadhs,1105451967,2022-04-21 16:37:12,"It is mentioned as failed to dump `encapsulate_xla_computations` for before, halfway and after in the warning log, it means it is not making use of XLA. You can ignore these warnings.","Failed to dump encapsulate_xla_computations for before, halfway and after in the warning log."
53443,rsandler00,1105177126,2022-04-21 13:00:24,"Hi, can this issue please be reopened. The solution @ArtyomZemlyak posted only works for TF1 (if you look at his source, it was posted in Posted on 2017-12-13, years before TF2 was released). Can we have a solution for TF2?","The solution @ArtyomZemlyak posted only works for TF1 (if you look at his source, it was posted in Posted on 2017-12-13, years before TF2 was released). Can we have a solution for TF2?"
55700,nSircombe,1105038785,2022-04-21 10:37:21,"@penpornk - here's a PR to avoid the...
```
//tensorflow/core/kernels:no_mkldnn_contraction_kernel
Multiple matches are not allowed unless one is unambiguously more specialized.
```
...error when running `bazel test` on a build with `--config=mkl_aarch64`.
Is there any chance this could be cherry-picked for the next TF 2.9 RC?",...error when running bazel test on a build with --config=mkl_aarch64.
55697,st--,1104830877,2022-04-21 7:49:16,"@gbaned Hi, I am not able to sign the Google CLA because the form claims that my GitHub username is invalid!
(It may be because I've got a very old GitHub account from before they changed it to disallow final dashes...)","""I am not able to sign the Google CLA because the form claims that my GitHub username is invalid!"""
46934,qiantanjingtao,1104673795,2022-04-21 3:37:47,"> I faced the same problem with Python 3.8.5 This works for me: enable ""long path"" policy using ""gpedit.msc"", then execute ""pip install --upgrade tensorflow""
> See also https://stackoverflow.com/questions/66672519/could-not-install-packages-due-to-an-oserror-errno2-no-such-file-or-directory
1. follow this https://www.howtogeek.com/266621/how-to-make-windows-10-accept-file-paths-over-260-characters/ to enable long path
2. pip install --upgrade tensorflow","""I faced the same problem with Python 3.8.5"""
55562,bhack,1104558310,2022-04-20 23:47:22,"@LukeWood too late, It was already merged. If @wangpengmit is ok with this additional API change I could eventually open a new one.","too late, It was already merged."
55525,Soleimani64,1104434569,2022-04-20 20:37:11,"@Maratyszcza , yes to both questions. I can see improvement in the latency of both tf32 and int8 models when comparing XNN pack with regular tensorflow. but I don't see any difference in the latency of the int8 models if I use --define tflite_with_xnnpack_qs8=true and --define tflite_with_xnnpack_qu8=true compared to when I don't use them.","""I can see improvement in the latency of both tf32 and int8 models when comparing XNN pack with regular tensorflow. but I don't see any difference in the latency of the int8 models if I use --define tflite_with_xnnpack_qs8=true and --define tflite_with_xnnpack_qu8=true compared to when I don't use them."""
55382,penpornk,1104231514,2022-04-20 17:40:48,"@gbaned The TF_AssignRefVariable PR just went in (https://github.com/tensorflow/tensorflow/pull/55640). As for this PR, it can't be merged until we investigate the failure of https://github.com/tensorflow/tensorflow/pull/53561. So I'm temporarily closing it until we have time to revisit.","The TF_AssignRefVariable PR just went in (https://github.com/tensorflow/tensorflow/pull/55640). As for this PR, it can't be merged until we investigate the failure of https://github.com/tensorflow/tensorflow/pull/53561. So I'm temporarily closing it until we have time to revisit."
55670,cantonios,1104100192,2022-04-20 15:56:11,"Hi @maxiwell, I just want to confirm this is actually what IBM wants. It was removed by default because it's broken on system-installed compilers on many systems (e.g. ubuntu), since it requires a specific ld version that we cannot detect from within Eigen. The thinking was that build systems might be able to detect this and turn on the flag if desired.","""It was removed by default because it's broken on system-installed compilers on many systems (e.g. ubuntu), since it requires a specific ld version that we cannot detect from within Eigen."""
47554,jakubvedral,1103811242,2022-04-20 11:12:08,i am having the same issue,I am having the same issue.
55621,gadagashwini,1103760271,2022-04-20 10:13:54,"@OrazioLombardi,
```
ERROR: C:/users/priva/tensorflow/tensorflow/core/kernels/BUILD:1222:18: C++ compilation of rule '//tensorflow/core/kernels:inplace_ops_gpu' failed (Exit 1): python.exe failed: error executing command
cd C:/users/priva/_bazel_priva/5lyc5377/execroot/org_tensorflow
```
Looks issue with MSVC2019.
Make sure you followed steps mentioned [here](https://www.tensorflow.org/install/source_windows) and install required packages. Thanks !",ERROR: C:/users/priva/tensorflow/tensorflow/core/kernels/BUILD:1222:18: C++ compilation of rule '//tensorflow/core/kernels:inplace_ops_gpu' failed (Exit 1): python.exe failed: error executing command cd C:/users/priva/_bazel_priva/5lyc5377/execroot/org_tensorflow  Looks issue with MS
55664,iamdanniemirz,1103608940,2022-04-20 8:22:29,"@mohantym Yes, I have tried to quantize the model using TF 2.3 aswell. But the same thing happened again. Also tried to write metadata as well using Luwang's [metadata writer](https://stackoverflow.com/a/64493506/11530462) but this code does not work for 8 outputs.","""The same thing happened again"""
55197,Petros626,1103576466,2022-04-20 7:46:10,"Yes I've read this one, but I'm using this script to generate my TFRecord files (sorry wasn't from the Object Detection API). It uses jpg Images. (https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/blob/master/generate_tfrecord.py). So I think I must change line 62, 66, 67 and 80 ?","I'm using this script to generate my TFRecord files (sorry wasn't from the Object Detection API) It uses jpg Images. (https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/blob/master/generate_tfrecord.py). So I think I must change line 62, 66, 67 and 80 ?."
39103,Tanxj,1103427839,2022-04-20 3:15:49,"I've got the same WARNING, the reason is the tf versions for training and saving are different.","I've got the same WARNING, the reason is the tf versions for training and saving are different."
54231,Cheril184,1103399656,2022-04-20 2:29:08,"@mihaimaruseac for the py+cpp test suite I am getting a build error because the array in the test case does not have a rank of 2k, so my modified code is throwing a value error as written in the code, and in the issue and documentation of TF, it says the rank should 2k itself, so can you see if this an error in writing tests or the issue and docs are wrong?","""I am getting a build error because the array in the test case does not have a rank of 2k, so my modified code is throwing a value error as written in the code, and in the issue and documentation of TF, it says the rank should 2k itself, so can you see if this an error in writing tests or the issue and docs are wrong?"""
55525,Soleimani64,1103205039,2022-04-19 21:49:19,"@Maratyszcza I did that, it doesn't improve the latency.","I did that, it doesn't improve the latency."
36833,jss367,1103157490,2022-04-19 20:54:58,"This could be caused by having `tf.debugging.experimental.enable_dump_debug_info(my_path)` in your code. If you do, try removing it.","This could be caused by having tf.debugging.experimental.enable_dump_debug_info(my_path) in your code. If you do, try removing it."
53396,apivovarov,1102976222,2022-04-19 18:50:18,Can it be backported to r2.7? (since the issue was introduced in v2.7.0 first),Can it be backported to r2.7? (since the issue was introduced in v2.7.0 first)
51659,abwilf,1102969088,2022-04-19 18:41:25,"Hi was there ever an update on this? I've been trying to pip install tensorflow within this image, `docker://nvidia/cuda:11.4.0-cudnn8-runtime-ubuntu20.04`, but I keep getting these errors: https://github.com/google/jax/issues/7239, https://github.com/google/jax/discussions/6843. It'd be really helpful if you could put out an image with tensorflow and jax already inside!","I've been trying to pip install tensorflow within this image, docker://nvidia/cuda:11.4.0-cudnn8-runtime-ubuntu20.04, but I keep getting these errors: https://github.com/google/jax/issues/7239, https://github.com/google/jax/discussions/6843."
54448,andmis,1102940745,2022-04-19 18:08:23,"That document is about installing TensorFlow, not building it, and as we have already discussed in this GitHub thread, the OP is asking about building TF, not installing it.","That document is about installing TensorFlow, not building it, and as we have already discussed in this GitHub thread, the OP is asking about building TF, not installing it."
48084,mihaimaruseac,1102820907,2022-04-19 15:55:54,https://github.com/tensorflow/tensorflow/pull/48084#pullrequestreview-679861204 is still not addressed. Reviewing it now means giving the exact same comment and just wasting time.,"""still not addressed"""
54519,IAL32,1102621510,2022-04-19 12:58:18,"> @IAL32 This PR is in draft, any update on this? Please. Thank you!
Hi @gbaned , still no update on this from my side. As in https://github.com/tensorflow/tensorflow/pull/53385 it might take some time for me to get back to this.","""still no update on this from my side"""
55621,OrazioLombardi,1102536683,2022-04-19 11:38:03,"Hi @sushreebarsa. I have GCC installed by Cygwin.
Today I update GCC to the latest version and try to build tensorflow but nothing seems to change in the output.
By the way, in the tested build configuration for Windows, GCC is not metioned (GCC is mentioned only in Linux guide), are you sure that this could influence the building process?
How can I check that the right version of GCC is installed on my computer and that works correctly with MSVC2019?","""I have GCC installed by Cygwin. Today I update GCC to the latest version and try to build tensorflow but nothing seems to change in the output."""
55651,rohanmuplara,1102152659,2022-04-19 6:42:39,@sushreebarsa I actually think this more of a tensorflow core issue. I can remove it from there if you prefer as this is a problem I am seeing in tensorflow core and people might encounter this even if they are not using tensorflow serving;,I can remove it from there if you prefer as this is a problem I am seeing in tensorflow core and people might encounter this even if they are not using tensorflow serving;
55528,zhucan,1102117349,2022-04-19 5:49:19,"But with the v2.8.0, I hadn't fixed @sushreebarsa",I hadn't fixed @sushreebarsa.
46356,lu-wang-g,1102096708,2022-04-19 5:36:05,"Sorry that I just noticed this issue. Seems like it's due to that Sceneform repackaged FlatBuffers, which conflicts with the official Flatbuffer Java time. I'll loop in the Sceneform team to take a look at this issue.","""Seems like it's due to that Sceneform repackaged FlatBuffers, which conflicts with the official Flatbuffer Java time."""
32052,no1nemo,1101618899,2022-04-18 18:09:10,I am having the same issue when running the trainer.py from the keypose repo from Google. [https://github.com/google-research/google-research/blob/master/keypose/trainer.py] I am running TF 2.8.0 and in conda,I am having the same issue when running the trainer.py from the keypose repo from Google.
55613,chsigg,1101586303,2022-04-18 17:25:18,Wouldn't this break [this](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/tools/toolchains/remote_config/rbe_config.bzl;drc=af542fe44674dd8c86159cd2f51782e75140ae8b;l=40) config running on e.g. sm_52?,Wouldn't this break [this](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/tools/toolchains/remote_config/rbe_config.bzl;drc=af542fe44674dd8c86159cd2f51782e75140ae8b;l=40) config running on e.g. sm_52?
54484,edloper,1101519096,2022-04-18 15:55:56,"I'm not familiar with this code, so I'm probably not the right person to review it. But it looks like the original change was automatically rolled back because it caused a test in this file to fail: https://github.com/tensorflow/tensorflow/blob/v2.8.0/tensorflow/compiler/tests/image_ops_test.py. (Unfortunately, since it was rolled back several months ago, the logs from the failed test aren't available anymore.)","""I'm not familiar with this code, so I'm probably not the right person to review it."""
55615,rohanmuplara,1101364567,2022-04-18 12:34:05,You have to upload a sample image for that cell because it reads it locally. Wasn't sure how it was supposed to work otherwise. https://user-images.githubusercontent.com/47044724/163808624-2f435116-f8cd-4cb2-9be0-116c16fe56fe.png,"""It reads it locally"""
55615,rohanmuplara,1101206080,2022-04-18 8:17:48,@tilakrayal sorry for repining but I haven't been able to get it to work for any other image formats. I have also tried other formats here and they are still not working. I have added this https://colab.research.google.com/drive/13Y4ET8jAHZDq7qwUYgg8nuBkxqniUqGu#scrollTo=kTeKBHg-kcaG.,I haven't been able to get it to work for any other image formats.
55563,gadagashwini,1101187984,2022-04-18 7:50:02,"@ayaka14732,
Can you try to clear the bazel cache before building Tensorflow
`bazel clean --expunge`",Can you try to clear the bazel cache before building Tensorflow bazel clean --expunge
55524,gadagashwini,1101042660,2022-04-18 2:53:53,"@zhucan, Tensorflow v2.8 has no `--config==verbs`.
Tensorflow v2.0 supports `--config==verbs`","""Tensorflow v2.8 has no --config==verbs."""
55549,TheBlackPlague,1100766434,2022-04-16 22:50:15,"<img width=""296"" alt=""image"" src=""https://user-images.githubusercontent.com/44123859/163693552-e0b8b90f-8bf9-45aa-bfbf-c9c597947dd3.png"">
@gadagashwini I was able to use the Profiler and found out that no operations are running on GPU. Which was something I thought might be happening. No clue why...",No clue why...
55364,karimnosseir,1100233006,2022-04-15 17:04:42,"One more thing for pixel phones DSP access is disabled so it will not work.
If you have the Pixel phone rooted you can run these commands first
```
adb root
adb shell setenforce 0
```",DSP access is disabled so it will not work.
55621,OrazioLombardi,1100085411,2022-04-15 12:48:35,"@sushreebarsa the problem is that cuda 11.2 doesn't support compute capability 3.0.
I can tra to upgrade to 2.8 but I cannot use a tested build configuration as suggest.
I also try the tested build configuration of TF v2.4 but the result it's quite the same.",I can tra to upgrade to 2.8 but I cannot use a tested build configuration as suggest.
16768,Farazsh,1100076816,2022-04-15 12:28:50,"is this tensor size [2,512,512,512] too big to be trained on GPU on colab? I am getting the following error
OOM when allocating tensor with shape[2,512,512,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [[{{node Abs-0-0-TransposeNCHWToNHWC-LayoutOptimizer}}]]
I have image of size 512*512 and batch size of 2","I am getting the following error OOM when allocating tensor with shape[2,512,512,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [[node Abs-0-0-TransposeNCHWToNHWC-LayoutOptimizer]]"
16768,Aliktk,1100001851,2022-04-15 9:42:44,"opening this issue as @unwritten shows the same error. I tried the batch size and the different solutions online but none of them worked for me.
I trained the model yesterday on a single GPU which was found but when I increase my batch size by converting the model on both GPU then I am facing the same error.
Any kind help will be appriciated
Thank you",I tried the batch size and the different solutions online but none of them worked for me.
50121,Anant16,1099874226,2022-04-15 5:50:19,"> @pedro-abundio-wang As cuda/GPU works as expecred, please move this to closed status as the issue opened is addressed.
Hi, I don't think CUDA/GPU was the poster's issue anyway.
The issue is that inside the IDEA IDE, tensorlfow.compat.v1 gives ""No module named 'compat'"" error, which was not resolved in this thread.
I am facing the same issue right now.
Is there any update as to how it can be resolved?
Thank you","""No module named 'compat'"" error, which was not resolved in this thread."
55610,cheshire,1099596510,2022-04-14 20:32:19,"@wenscarl OK it might make sense to not compile tf.where in autoclustering environment altogether then. I can make this change.
> Say in a training loop, XLA will trigger compilation multiple times every time seeing a data with different size
Do you have a repro I could try? XLA compiles up to the upper bound of the `tf.where` output, not to the concrete bound seen at runtime, so it seems bizarre it would recompile.","""XLA compiles up to the upper bound of the tf.where output, not to the concrete bound seen at runtime, so it seems bizarre it would recompile."""
55610,wenscarl,1099523367,2022-04-14 18:46:30,"Thanks @cheshire. Making `tf.where` XLA compilable in autoclustering may results into a perf regression. Say in a training loop, XLA will trigger compilation multiple times every time seeing a data with different size. See the snapshot from profiler.
![recompile](https://user-images.githubusercontent.com/25590028/163456405-41e50e98-69b6-4d07-9410-9867cd067fd2.PNG)
Reverting this commit resolve this. I would prefer to have a switch flag to turn it off.","""I would prefer to have a switch flag to turn it off."""
54771,jayfurmanek,1099196977,2022-04-14 13:38:36,"Ugh, note to self - just use buildifier even on small little format things that are easy to fix manually.
Sanity *should* pass now.
Sorry for the churn.","Ugh, note to self - just use buildifier even on small little format things that are easy to fix manually."
55505,372046933,1099142065,2022-04-14 12:37:55,"@gadagashwini If I'm not mistaken, https://github.com/tensorflow/tensorflow/issues/21461#issuecomment-430449910 configures a local installed TF. Which is installed by pip. In my scenario, I was building TF wheel. Since TF is not installed, `cc_tf_configure` will fail.","""I'm not mistaken, https://github.com/tensorflow/tensorflow/issues/21461#issuecomment-430449910 configures a local installed TF. Which is installed by pip. In my scenario, I was building TF wheel. Since TF is not installed, cc_tf_configure will fail."""
20369,LucaCappelletti94,1098896600,2022-04-14 9:06:53,"This is still a very relevant and difficult issue, please STOP closing issues just because they are hard to solve.","""Please STOP closing issues just because they are hard to solve."""
55524,zhucan,1098790317,2022-04-14 7:22:19,https://github.com/tensorflow/tensorflow/blob/r2.8/configure.py#L1581 @gadagashwini no verbs config.,No verbs config.
55524,zhucan,1098787662,2022-04-14 7:18:39,@gadagashwini I mean tensorflow2.8 not 2.0,I mean tensorflow2.8 not 2.0.
54973,leondgarse,1098681739,2022-04-14 3:50:38,"Just verified still exists in `TF 2.9.0-rc0`. Testing results updated in [tf_280_xla_test.ipynb](https://colab.research.google.com/drive/1LTVJ7jRRzsODzMuPB-svocV4jcbx1SYY?usp=sharing). For other versions, `TF 2.7.1` works, and `TF 2.8.0-rc0` throws error.",Just verified still exists in TF 2.9.0-rc0.
55567,rthadur,1098507408,2022-04-13 21:31:29,"> Build is still failing. I don't think the fix is in Python, since the argument can be a list or a tensor or anything that can be converted to a tensor.
I agree ,any idea how we can fix this issue ?",Build is still failing.
55567,mihaimaruseac,1098466303,2022-04-13 20:39:47,"Build is still failing. I don't think the fix is in Python, since the argument can be a list or a tensor or anything that can be converted to a tensor.",Build is still failing.
55563,ayaka14732,1097575465,2022-04-13 5:30:16,"> @ayaka14732, Looks like Bazel version, can you try with Bazel 5.0.0 Version. Thanks!
I manually changed `.bazelversion` to 5.0.0 and built with Bazel 5.0.0, but there are even more build errors.","I manually changed .bazelversion to 5.0.0 and built with Bazel 5.0.0, but there are even more build errors."
52142,leandro-gracia-gil,1097561011,2022-04-13 5:02:30,"@MeghnaNatraj That limitation is absurd in the case where you have multiple signatures and some of them act as properties returning information about the model. It forces you to pass a tensor even if it's completely ignored inside.
Would it be possible to consider adding support for this?","""That limitation is absurd in the case where you have multiple signatures and some of them act as properties returning information about the model."""
55567,rthadur,1097280979,2022-04-12 22:14:06,"> Please use a proper commit message / PR title, not `Update <file>`. See https://cbea.ms/git-commit/
Sorry , updated description.","""Please use a proper commit message / PR title, not Update file>."""
55415,sshahrokhi,1097237814,2022-04-12 21:29:08,"@michaelbanfield could you please review this PR? It is changing where JAX is loading the libtpu, and removing executor file which was the light weight version of tpu_api_dlsym_initializer.","""It is changing where JAX is loading the libtpu, and removing executor file which was the light weight version of tpu_api_dlsym_initializer.."""
53047,ingura,1097092612,2022-04-12 18:49:29,"I have the same issue on tfLite 2.8.0, target sdk 32. So far the only workaround I have is to downgrade tensorflow to 2.5.0 if I would like to keep GPU support for yolo v4 , otherwise I can go with tf 2.8.0.
It would be great if someone would develop a more future ready solution","I have the same issue on tfLite 2.8.0, target sdk 32. So far the only workaround I have is to downgrade tensorflow to 2.5.0 if I would like to keep GPU support for yolo v4 , otherwise I can go with tf 2.5.0. It would be great if someone would develop a more future ready solution."
43790,Dimiftb,1096952647,2022-04-12 16:38:40,"@JessicaLopezEspejel It seems that you've got an authentication issue with your bucket rather than what is described here. My error message is:
```
(0) INTERNAL: {{function_node __inference_train_function_60782}} failed to connect to all addresses
```
Commenting out the lines below doesn't really solve the issue.
```
tf.config.experimental_connect_to_host(TPU_ADDRESS)
tensorflow_gcs_config.configure_gcs_from_colab_auth()
```
Appreciate your help anyway.",failed to connect to all addresses
55207,whatdhack,1096937356,2022-04-12 16:23:08,"@gadagashwini , thanks. I do not see the log, only see the output. Looks like there are issues with seeing logs in colab per - https://github.com/tensorflow/tensorflow/issues/31870 . You will need to use a cloud or on-premise instance to see the log.","I do not see the log, only see the output."
55495,lakshya-4gp,1096917349,2022-04-12 16:04:12,"@mohantym, that thread is unfortunately irrelevant. I'm already able to use GPU delegate TF-2.3. The issue is it give seg fault with TF-2.8",I'm already able to use GPU delegate TF-2.3. The issue is it give seg fault with TF-2.8.
55562,bhack,1096789258,2022-04-12 14:16:44,`feedback/copybara` is failing again.,feedback/copybara is failing again.
18829,iaverypadberg,1096712898,2022-04-12 13:14:36,"Anybody figure this one out? I'm facing the ""Relu6 is lacking min/max data"" error when converting my model to tflite. I need it to be quantized to UINT8, so the inference_type=FLOAT is not going to work for me. Specifying default ranges makes the accuracy terrible, so I'm not sure where to go from here.","""Relu6 is lacking min/max data"" error when converting my model to tflite."
55571,N1uY,1095940908,2022-04-12 3:42:55,"@tilakrayal ,thanks for your response! I have checked the relevant thread you mentioned, it seems that uncompatible cuda or cudnn version can be the cause. After running the bazel clean command, I rebuild the TensorFlow with(cuda 11.2 and Cudnn8.1 and GCC7.5.0 (so difficult to install 7.3.1 on ubuntu 20)).Unfortunately, it failed again.I guess other cuda installed on my machine cause this and i will try to uninstall them.","""Unfortunately, it failed again."""
55524,zhucan,1095864092,2022-04-12 2:38:31,"The ""--config=verbs"" is not suitable for tensorflow 2.8.0 . @gadagashwini","""--config=verbs"" is not suitable for tensorflow 2.8.0 ."
55505,372046933,1095835413,2022-04-12 2:13:48,"@gadagashwini Added `--symlink_prefix=/` to `bazel build`. But nothing changed. My build command is
```bash
bazel build --config=native_arch_linux --config=cuda --symlink_prefix=/ @org_tensorflow//tensorflow/tools/pip_package:build_pip_package
```
By the way, did you notice that I am building TF as an external dependency. i.e. `@org_tensorflow//tensorflow/tools/pip_package:build_pip_package` not `//tensorflow/tools/pip_package:build_pip_package`",--symlink_prefix=/
55582,mihaimaruseac,1095757707,2022-04-12 1:04:29,"TF 1.15 is too old. When it was released there was no python 3.8.
This is not a TF issue.",TF 1.15 is too old.
55332,DEKHTIARJonathan,1095494767,2022-04-11 19:48:03,"> @DEKHTIARJonathan I'm not sure that `_experimental_feature_scope` should be in `tf_trt_integration_test_base.py`. I need it in `trt_convert_test.py`, so it would make sense to put it in a separate `test_utils.py` or similar file.
Done
@bixia1: all comments have been addressed. Please review","""I need it in trt_convert_test.py, so it would make sense to put it in a separate test_utils.py or similar file."""
54481,AB5247,1095163302,2022-04-11 15:00:54,"@jvishnuvardhan @sushreebarsa Sorry to bother you again, but can this be assigned to someone else if @karimnosseir is not able to take a look? It has been almost 2 months since I opened this issue and no one has told me yet if you can even reproduce the same bug I experience. Thanks.","""It has been almost 2 months since I opened this issue and no one has told me yet if you can even reproduce the same bug I experience."""
50379,dcbarton,1095062499,2022-04-11 13:36:35,"THIS CODE ACCUMULATES MEMORY LEAK, EVEN WITH FORCED GC
class DataGenerator(Sequence):
def on_epoch_end(self):
self.indexes = np.arange(self.startIdx, self.endIdx, 1, dtype=np.int32)
np.random.shuffle(self.indexes)
THIS CODE WORKS AROUND BY REUSING THE BUFFER, BETTER CODE ANYWAY
class DataGenerator(Sequence):
def __init__(self):
self.indexes = np.arange(self.startIdx, self.endIdx, 1, dtype=np.int32)
def on_epoch_end(self): np.random.shuffle(self.indexes)","ACCUMULATES MEMORY LEAK, EVEN WITH FORCED GC"
55530,awf,1095023202,2022-04-11 13:03:04,"Still present in 55645c from 2 days ago.
I'll take a look, but I don't see anything more recent that might have fixed it.",Still present in 55645c from 2 days ago.
35506,xbwee1024,1095022499,2022-04-11 13:02:27,"@karimnosseir I have a 865_rb5 device and after flash the system (ubuntu_aarch64) has `/usr/lib/libhexagon_interface.so` and `/usr/lib/rfsa/adsp/libhexagon_nn_skel.so` existed already, and I can run tflite benchmark_model with hexagon delegate successfully, why? as you known hexagon_nn in tflite is different with which in Hexagon_SDK. I think the only way is that `/usr/lib/libhexagon_interface.so` on RB5 is built from tflite source code, is that right ?","""I have a 865_rb5 device and after flash the system (ubuntu_aarch64) has /usr/lib/libhexagon_interface.so and /usr/lib/rfsa/adsp/libhexagon_nn_skel.so existed already, and I can run tflite benchmark_model with hexagon delegate successfully, why?"""
32809,shaoxiang777,1094961505,2022-04-11 11:57:58,"> I implemented small lib to calculate FLOPs/MACs: https://github.com/evgps/flopco-keras
This repo doesn't work anymore.",This repo doesn't work anymore.
55476,BhaskarsarmaP,1094861066,2022-04-11 10:10:32,"I have run the benchmark tool for tensorflow 2.8.0 and still got less performance than python.
![Capture2](https://user-images.githubusercontent.com/37923435/162717140-c805cdaf-ebd8-43cf-a16d-4aa1d4a7d902.PNG)
![Capture1](https://user-images.githubusercontent.com/37923435/162717148-881f69cd-4cfb-4cba-8562-5de0c6679e83.PNG)",I have run the benchmark tool for tensorflow 2.8.0 and still got less performance than python.
55564,tilakrayal,1094654733,2022-04-11 7:36:04,"Hello @NathFarinha237 ,
On running the given code snippet, I am facing an error stating `OSError: File does not exist. Received: {filename}`. Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/fb61ab0f893d14efb33aa26335ed11ea/untitled297.ipynb).","""On running the given code snippet, I am facing an error stating OSError: File does not exist. Received: filename."""
55546,tilakrayal,1094574485,2022-04-11 5:41:19,"Hello @markub3327 ,
I tried to execute the given code snippet, I am facing different error stating. Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/773ef1a7f6105767925cd885691e4c21/untitled296.ipynb).",I am facing different error stating.
23728,rudSarkar,1094212498,2022-04-10 8:03:26,"> I had the same problem, and i solved it by :
> > from tensorflow.keras.optimizers import RMSprop
> > instead of :
> > from keras.optimizers import RMSprop
Tried this but not working either I use like `from tensorflow.keras.optimizers import Adam` it showing `Import ""tensorflow.keras.optimizers"" could not be resolved`
Current version of tensorflow is `2.8.0` should I roll back to 1.x.x ?","""Import ""tensorflow.keras.optimizers"" could not be resolved"""
55530,awf,1094062446,2022-04-09 15:05:37,"Update: test still fails on commit `55645ca964508507890529a71591f51a344a6356` April 9
test passes with ``--config=opt``","""test still fails on commit 55645ca964508507890529a71591f51a344a6356 April 9 test passes with --config=opt"""
55123,windmaple,1093506230,2022-04-09 0:10:11,I'm pretty you ran out of memory. You can confirm this by using benchmark tool to see how much memory it is using. Super resolution models are quite memory-hungry and you should look into model optimization techniques like distillation.,I'm pretty you ran out of memory.
53364,sumocodes,1093439212,2022-04-08 22:49:27,"I am seeing this issue as well, the `build_frameworks.sh` script runs successfully but does not provide a build output (TensorFlowLiteC_framework.zip).","I am seeing this issue as well, the build_frameworks.sh script runs successfully but does not provide a build output (TensorFlowLiteC_framework.zip)."
55549,TheBlackPlague,1093397875,2022-04-08 22:03:40,@sushreebarsa ~~I'm going to try and use the Tensorboard profiler for Tensorflow in order to find the root cause.~~ Seems like Tensorflow won't generate profiler data.,"""Seems like Tensorflow won't generate profiler data."""
55554,reedwm,1093353354,2022-04-08 20:50:36,"Calling `event_mgr->ThenExecute` effectively causes a GPU synchronization to occur before the op after SparseToDense can execute, which perhaps is causing some race condition to not occur, fixng the segfault. I don't think SparseToDense is the cause though.
If you give us an example to reproduce, we might be able to find where the segfault is coming from. But if the example is too large, there's a good chance we won't have time to debug it, unfortunately.","Calling event_mgr->ThenExecute effectively causes a GPU synchronization to occur before the op after SparseToDense can execute, which perhaps is causing some race condition to not occur, fixng the segfault."
55554,nluehr,1093336849,2022-04-08 20:36:15,Prior to this fix we observed intermittent CUDA segfaults during training when validate_indices was false. (this was in a TF1 model with identical SparseToDense GPU implementation),"""CUDA segfaults during training when validate_indices was false"""
16139,cericson,1093022248,2022-04-08 15:45:16,"FWIW I still can't replicate with tf2.8 on my cluster - it runs fine even with the shape (1, 41, 960, 1280, 1). I haven't used much colab before - could it just be running out of memory? I'm fine with closing this issue if that makes sense.
![image](https://user-images.githubusercontent.com/6691014/162476625-3d5df436-0394-4568-aa1a-6741228214ad.png)","I still can't replicate with tf2.8 on my cluster - it runs fine even with the shape (1, 41, 960, 1280, 1)."
55553,mihaimaruseac,1093015809,2022-04-08 15:38:33,Please fix more than one typo per file. There are several hours of CI that run due this one letter addition.,"""There are several hours of CI that run due this one letter addition."""
55123,douzaikongcheng,1092892106,2022-04-08 14:01:36,"@[windmaple](https://github.com/windmaple)Recently, I used tensorflow to train a super-resolution model, but this error still occurs, that is, an error will be reported when the image input size is too large, but there is no problem when the image is small","""Error still occurs"""
37706,amitmate,1092786246,2022-04-08 11:59:44,"@jdduke i am also trying to get batch processing to work in Android java, but faced the same issue as reported earlier in this thread. i tried to follow your instructions regarding conversion of the model and keeping batch_size as None, but i get the following error
ValueError: invalid literal for int() with base 10: 'None'
Pls note that my model graph was generated using 1.x , so for conversion i still have to use 1.x","""I am also trying to get batch processing to work in Android java, but faced the same issue as reported earlier in this thread."""
55505,372046933,1092476009,2022-04-08 6:11:43,"@tilakrayal No, it's not resolved. Cannot execute ```
./bazel-bin/external/org_tensorflow/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
```",Cannot execute  ./bazel-bin/external/org_tensorflow/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg .
34431,TramBao2000,1092381909,2022-04-08 2:28:19,"> > Replace tf.app with tf.compat.v1.app
> > It worked for me but later on I got tf.contrib error. Even I tried on Google Colab, tf.contrib error is same.
> > So finally on my local machine, I downgraded the tensorflow to 1.15.x using pip uninstall tensorflow pip install tensorflow==1.15.2
> > For Google Colab, I used
> > %tensorflow_version 1.x import tensorflow as tf
thanks somuch","""It worked for me but later on I got tf.contrib error."""
55349,DanBmh,1092226025,2022-04-07 21:39:31,"The smallest value working as zero-guard for `log` was `2 ** -5`, smaller values sometimes resulted in `-inf` results. The smallest value for `div` was `1/255`, smaller values sometimes resulted in `nan` results.
Batching might work here, but I think it won't work for my real representative dataset, because the audio inputs have quite different lengths.","The smallest value working as zero-guard for log was 2 ** -5, smaller values sometimes resulted in -inf results. The smallest value for div was 1/255, smaller values sometimes resulted in nan results. Batching might work here, but I think it won't work for my real representative dataset, because the audio inputs have quite different lengths."
54173,impjdi,1092156041,2022-04-07 20:09:33,"> Qualcomm's OpenCL can be more lax than NVidia's when it comes undefined behaviors at runtime
This is absolutely true. We do cut corners and rely on undefined behavior of the supported platforms, e.g. out of bound reads etc.",Qualcomm's OpenCL can be more lax than NVidia's when it comes undefined behaviors at runtime
43790,Dimiftb,1092000306,2022-04-07 17:17:52,@JessicaLopezEspejel Unfortunately not. I had to resort to using GPU. Please let me know if you find a solution.,I had to resort to using GPU.
55507,mihaimaruseac,1091964167,2022-04-07 16:35:41,"Yes, but these are just placeholders, they don't mean anything in the rich view","""these are just placeholders, they don't mean anything in the rich view"""
55507,mihaimaruseac,1091839228,2022-04-07 14:52:04,"This is just a snippet. The space doesn't bring anything.
When making typo fixes PRs please fix through the entire file/directory.",This is just a snippet. The space doesn't bring anything. When making typo fixes PRs please fix through the entire file/directory.
55363,bhack,1091809458,2022-04-07 14:27:07,`feedback/copybara` is failing again,feedback/copybara is failing again.
55200,bhack,1091764964,2022-04-07 13:50:09,"> Apparently that one is not triggered by the kokoro label but by gitHub buttons
Yes but I don't have the permission on this repo",Apparently that one is not triggered by the kokoro label but by gitHub buttons
55525,Soleimani64,1091716207,2022-04-07 13:09:26,"Hi @mohantym Thanks for the reply. but I used ""**-define tflite_with_xnnpack=true --define xnn_enable_qs8=true**"" to build tensorflow. I did **not** use --define xnn_enable_qu8=true","""I did **not** use --define xnn_enable_qu8=true"""
55425,bhack,1091643474,2022-04-07 11:54:21,@gadagashwini This is something else and it has no associated PR. Please check the mentioned @wangpengmit's comment at https://github.com/tensorflow/tensorflow/pull/55192#issuecomment-1081306349,This is something else and it has no associated PR.
55423,fedlucchetti,1091634071,2022-04-07 11:43:18,"I tried to find the relevant source file myself, and also modify and rebuild the whole TF package from source. No success so far.
Would this source file, at the heart of the matmul operation lie somewhere in `tensorflow/c` or even `tensorflow/core` ? Any help would be greatly appreciated. Thank you.",No success so far.
52845,hoangmt,1091629489,2022-04-07 11:37:52,"This works.
> FROM tf-devel-cpu-arm64v8-jupyter
> > Or use this version: RUN pip install tensorflow-aarch64 -f https://tf.kmtea.eu/whl/stable.html
> > RUN pip install tensorflow -f https://tf.kmtea.eu/whl/stable.html
> ```
However, tensorflow does not find gpus."," However, tensorflow does not find gpus.."
52845,dakl,1091595020,2022-04-07 11:01:17,"Are there any update here? Not being able to run tensorflow in docker on our new powerful M1-based machines is both a big downer and quite the annoyance to ""solve"". ATM our best workaround is having an intel-based VM for development, which feels quite unnecessary.",Not being able to run tensorflow in docker on our new powerful M1-based machines
55524,zhucan,1091560090,2022-04-07 10:43:19,"@gadagashwini Thanks for your response. it's ok for me. But there is no ""build doc"" for centos7.8. And there is no docs build tensorflow(v2.8.0) with verbs?","""And there is no docs build tensorflow(v2.8.0) with verbs?"""
55475,foxik,1091104181,2022-04-07 5:45:27,Closing as a duplicate of #46635 .,Closing as a duplicate of #46635 ..
55475,foxik,1091103965,2022-04-07 5:44:54,"Oh, I was just pointed to me (by djoshea) that this is a duplicate of 46635, so closing.","I was just pointed to me (by djoshea) that this is a duplicate of 46635, so closing."
53239,plopresti,1091049257,2022-04-07 3:48:20,"I encountered the same issue. I suspect it is the same problem the Chromium folks had:
https://codereview.qt-project.org/c/yocto/meta-qt6/+/364505/2/recipes-qt/qt6/chromium-gn.inc#38
Removing ""-g"" from the build options fixed it for me.",I encountered the same issue.
55517,mihaimaruseac,1090939714,2022-04-06 23:59:59,"Please use a better title and commit message: https://cbea.ms/git-commit/
The commas are instances of what is called an oxford comma, they are ok as they are.
When fixing typos, please fix all in a file/directory, instead of just a few. We need hours of CI to test each change, so let's try to minimize the CI hours/letters changed metrics :)
Thank you for your contribution, but we cannot accept this one.","""We need hours of CI to test each change, so let's try to minimize the CI hours/letters changed metrics :"")"
55466,pranavladkat,1090922869,2022-04-06 23:29:43,I believe this issue is somewhat related to https://github.com/tensorflow/tensorflow/issues/25513. I don't see any solution there as well.,I believe this issue is somewhat related to https://github.com/tensorflow/tensorflow/issues/25513. I don't see any solution there as well.
55365,psobot,1090431667,2022-04-06 15:53:32,+1 to @f90's comment - this patch would still result in misleading documentation and would not adequately resolve https://github.com/tensorflow/tensorflow/issues/55290.,+1 to @f90's comment - this patch would still result in misleading documentation and would not adequately resolve https://github.com/tensorflow/tensorflow/issues/55290.
55509,mihaimaruseac,1090423880,2022-04-06 15:45:57,Please don't spam with bad PRs. We never merge release branches back into main branch,Spam with bad PRs.
55503,w3sip,1090240193,2022-04-06 12:59:42,"- Unfortunately can't share the model, but it's a yolo v3 model.
- The leak is obvious, when running an inference under Instruments. It also seems to persist even after destroying the interpreter with TfLiteInterpreterDelete)
- Haven't checked in 2.8 (we'd have to upgrade CI to build it, hard dependency on Bazel makes things complex)
- did check in 2.6 (seems like the leak was already present there)","- Unfortunately can't share the model, but it's a yolo v3 model."
55435,szleb,1090082347,2022-04-06 9:57:40,"changing the method _set_output_names in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/functional.py to something like the following would solve my problem:
if type(self.output) is dict:
self.output_names = list(self.output.keys())
self.output_names.sort()
else:
<<current code>>",current code>
55471,zhazl,1090002468,2022-04-06 8:36:01,"![image](https://user-images.githubusercontent.com/20295041/161932162-e873634b-49a0-4971-bc75-e8b874d704f5.png)
Thanks for response. The code I used as follows. Inputs x and weights are all same, but I got diffenrent results.","The code I used as follows. Inputs x and weights are all same, but I got diffenrent results."
55455,David-Mao,1089721825,2022-04-06 3:05:27,> Sorry I didn't know that. Are you running on a M1 chip machine? I encounter this problem on the M1 arm chip Mac.,I encounter this problem on the M1 arm chip Mac.
55200,mihaimaruseac,1089434944,2022-04-05 22:17:40,Apparently that one is not triggered by the kokoro label but by gitHub buttons,Apparently that one is not triggered by the kokoro label but by gitHub buttons.
55281,jurahul,1089361506,2022-04-05 21:16:39,"Yes, I am looking into this. There are build failures from this change in internal code, which we need to fix on our side before this can be merged.",build failures from this change in internal code
55435,bhack,1089245338,2022-04-05 19:40:53,"I don't know if you can minimize your example a bit but the output names are
`print(my_model.output_names)`
`['model_1', 'model_2', 'model_2_1']`","I don't know if you can minimize your example a bit but the output names are print(my_model.output_names) ['model_1', 'model_2', 'model_2_1']."
55351,SandSnip3r,1089227166,2022-04-05 19:25:03,@philipphack It was again confirmed that this change caused drastically increased memory usage (out-of-memory errors). We are investigating.,"""This change caused drastically increased memory usage (out-of-memory errors)."""
53844,ttdd11,1089087947,2022-04-05 17:29:42,@Saduf2019 @gadagashwini @jvishnuvardhan I'm unsure what other options we have here. We are coming up on three months and the assignee has not yet responded. It effectively means that we can't run GCP experiments.,The assignee has not yet responded.
55228,ttdd11,1089084352,2022-04-05 17:26:48,@gadagashwini can we please re-open this ticket? The other ticket has never received a response from the assignee.,The other ticket has never received a response from the assignee.
54491,yaochengji,1089042941,2022-04-05 17:01:36,"Hi @joker-eph, The `AMD ROCm CI` complained about ```
ERROR: The project you're trying to build requires Bazel 5.1.0 (specified in /workspace/.bazelversion), but it wasn't found in /usr/local/lib/bazel/bin.
```
I don't think it is related to my fix. Could you help take a look at it?","""I don't think it is related to my fix."""
55200,bhack,1088985662,2022-04-05 16:29:32,"@mihaimaruseac Something is not working. The PyLint Action was executed ( failed <relative-time datetime=""2022-03-30T20:26:16Z"" class=""no-wrap"">Mar 30, 2022</relative-time> in 43s )",Something is not working.
55201,cantonios,1088879263,2022-04-05 15:35:18,You can submit a PR for that if you like. This is not unique to clip ops - all tensors are restricted by this limitation.,This is not unique to clip ops - all tensors are restricted by this limitation.
55379,penpornk,1088328221,2022-04-05 6:48:25,"@PatriceVignola We only run TensorFlow presubmit tests before merging. After merging, PRs could still be reverted if they broke TensorFlow nightly tests or other internal tests (e.g., unit / integration tests of internal applications that use TensorFlow). In this case, it broke an internal test.","""In this case, it broke an internal test."""
35932,ahmedalmindelawy93,1088224697,2022-04-05 3:16:34,I am facing the same error and I am using my laptop not the colab!,I am facing the same error and I am using my laptop not the colab!
55135,MeghnaNatraj,1088039952,2022-04-04 21:41:04,"> [tensorflow/model-optimization#775 (comment)](https://github.com/tensorflow/model-optimization/issues/775#issuecomment-894908619) this comment mentioned that TF2 converter does not support `uint8` quantization any more. Is this the reason why setting `converter.inference_type=uint8` does not help?
Yes, you're right. TF2 converter does not support `uint8` quantization.",TF2 converter does not support uint8 quantization any more.
55206,wangpengmit,1087977707,2022-04-04 20:22:07,Seems your Python paths are not set up properly. I'll try fixing the CI errors internally.,I'll try fixing the CI errors internally.
55317,bhack,1087873316,2022-04-04 18:23:48,"@mdanatg I suppose this was merged with b462db3b7482ff8e52b13ca686f2bca4508a6910. But I have totally lost my attribution on git:
`git show -s --format=""%ae"" b462db3` `gardener@tensorflow.org`","I have totally lost my attribution on git: git show -s --format=""%ae"" b462db3"
55306,npanpaliya,1087803472,2022-04-04 17:08:32,"@akuegel - Yes, LLVM fix can go directly into llvm-project too. But adding ppc64le support in LLVM will need more work and verification. For TF to compile, only this change is needed in llvm and hence I added a patch in TF itself (TF follows this approach anyway).
I'm working on llvm-project too but it will take time. Hence raised this PR to TF to fix the builds.","""Adding ppc64le support in LLVM will need more work and verification"""
55201,cantonios,1087650076,2022-04-04 14:46:12,"@DLFrameworkBug yes, the total number of elements in the tensor must be less than `int64` max. Here when trying to compute the new number of elements to add, it overflows to negative. We just don't support such large shapes.","""We just don't support such large shapes."""
55433,essandess,1087352257,2022-04-04 9:58:34,"> I could reproduce the issue with `Tensorflow-macos 2.8`. on macOS Monterey 12.3.
> > I resolved through.
This does not resolve the issue. It simply installs a different build of Tensorflow that does not support `tensorflow-metal` and macOS GPU support.",I could reproduce the issue with Tensorflow-macos 2.8. on macOS Monterey 12.3.
48213,mohantym,1087113615,2022-04-04 4:51:46,Yeah. It is replicating when saved without h5 extension.,It is replicating when saved without h5 extension.
50379,dcbarton,1086971962,2022-04-03 23:21:24,This memory leak is most definitely still not fixed. Keras remains unusable for any longer training time unless a custom raining loop is used.,"""Keras remains unusable for any longer training time unless a custom raining loop."""
53144,winthropharvey,1086731818,2022-04-02 22:03:05,"I can confirm that this issue still exits in latest PyCharm (2021.3.3 professional edition), python 3.10, TF 2.8. The workaround from cpuimage, then invalidating caches and restarting, did not fix the issue.","""The workaround from cpuimage, then invalidating caches and restarting, did not fix the issue."""
54714,gmernov,1086608134,2022-04-02 10:11:43,"Maybe it's not related exactly for Tensorflow, but for linker. IDK. But i suffer this for multiple versions of tensorflow code.","""I suffer this for multiple versions of tensorflow code."""
55470,dansuh17,1086495558,2022-04-02 2:19:31,Duplicate of #55464 .,Duplicate of #55464 ..
55206,philipphack,1086442491,2022-04-02 0:58:48,This fails with `ModuleNotFoundError: No module named 'tensorflow.core.kernels'`.,This fails with ModuleNotFoundError: No module named 'tensorflow.core.kernels'.
55453,bhack,1086335007,2022-04-01 21:20:47,I've just edited this file on github directly. Who have renamed the file? copybara?,"""Who have renamed the file?"""
55453,mihaimaruseac,1086040515,2022-04-01 15:28:24,"Oh, this accidentally renames the file :(",:(
54650,christopherbate,1085919792,2022-04-01 13:44:20,"This pr says changes were requested, but nothing has been requested as far as I can see. @bixia1 ?","""Nothing has been requested as far as I can see."""
55464,pshiko,1085900361,2022-04-01 13:28:15,"my pr is failed because this problem. https://github.com/tensorflow/tensorflow/pull/54455
@gbaned",my pr is failed because this problem.
55463,waseem-arshad94,1085805997,2022-04-01 11:54:05,"I am using ADRV9361Z7035 breakout board. Ubuntu version linaro 12.06 I want to install tensorflow lite on it. I have **python3 version 3.4.0 and pip3 version 1.5.4**
root@analog:~# pip3 install tensorflow
Downloading/unpacking tensorflow
Cannot fetch index base URL https://pypi.python.org/simple/
Could not find any downloads that satisfy the requirement tensorflow
Cleaning up...
No distributions at all found for tensorflow
Storing debug log for failure in /root/.pip/pip.log
Need guidance.","""Cannot fetch index base URL"""
55454,VihGrimm,1085581996,2022-04-01 8:16:15,"@tilakrayal For me, the problem is solved with the workaround.
However, it feels unsatisfactory to call the whole thing resolved, since the error still exists when loading models that use `tf.where`. And I would describe it as unexpected behavior.
Since a fix is not foreseeable for now: maybe it would be good to point out this behavior in the docs so that others don`t face the same problem?","""unsatisfactory to call the whole thing resolved, since the error still exists when loading models that use tf.where"""
55298,Agono0,1085563749,2022-04-01 8:04:34,"Properly not solved for me, any help.
I describe the issue . But I didn't solve it",I describe the issue . But I didn't solve it.
55375,gadagashwini,1085369917,2022-04-01 3:12:28,"@nirnayr, `tf.sysconfig.get_link_flags()`. returns ```
['-L/usr/local/lib/python3.7/dist-packages/tensorflow',
'-l:libtensorflow_framework.so.2'] ```
Here, `library directory` is `-L` and `library` is `-l`. Now replace the `-L` and `-l` with `library_dirs` and `libraries `arguments to your Extension object in `setup.py`.
Finally, set your `LD_LIBRARY_PATH` environment variable to point to the directory with the `libtensorflow_framework.so` library is located.","tf.sysconfig.get_link_flags(). returns ['-L/usr/local/lib/python3.7/dist-packages/tensorflow', '-l:libtensorflow_framework.so.2'] "
55206,wangpengmit,1085270878,2022-04-01 0:41:47,There are some `api_compatibility_test` CI failures. Please run `bazel run //tensorflow/tools/api/tests:api_compatibility_test -- --update_goldens True` to update the API golden files.,api_compatibility_test CI failures.
55379,PatriceVignola,1085081161,2022-03-31 20:37:34,"The build failed again without a ""Details"" link. Is there anything I can do to expedite this PR? I successfully built it and ran the tests locally but maybe I'm missing something. Is there a way to confirm whether it is an infra issue or not?","The build failed again without a ""Details"" link."
41632,mcourteaux,1085000153,2022-03-31 19:10:54,Tensorflow issue handling is a freaking joke. Nobody takes anything serious. All issues get closed by some stupid bot just because nobody takes time to look at them.,"""Nobody takes anything serious"""
31173,IanQS,1084996367,2022-03-31 19:06:32,> @omalleyt12 sorry to bother you did you manage to get a solution merged in? Looks like they're not working at Google anymore (check the commit history). @gowthamkpr ane @ymodak can we get some help here?,"""Looks like they're not working at Google anymore"""
46833,wangpengmit,1084960115,2022-03-31 18:31:58,"Hi @miguelusque, you've hit a long-running issue internally known as ""the int32 problem"". Basically TF carves out int32 as ""the shape dtype"", to workaround some design problems of the device-placement system. We're designing potential alternatives but it's hard to fix. At the moment just don't use int32 for your normal computation (use e.g. int64 or uint32 instead).","""the int32 problem"""
55435,szleb,1084810534,2022-03-31 16:21:20,In my case I have nested models with multiple outputs which I use to build larger models. Giving the correct names to the output layers of the inner model does not solve the issue because they are not visible any more. The output layers of the outer model have the name of the inner model not of the inner outptu layers.,"""The output layers of the outer model have the name of the inner model not of the inner outptu layers."""
48167,lululxvi,1084719031,2022-03-31 15:04:12,"@sushreebarsa Yes, it is still an issue. L-BFGS in tfp helps, but it is not convenient to use, and we have to add an interface, as I discussed above https://github.com/tensorflow/tensorflow/issues/48167#issuecomment-941741493
Ideally, a direct support would be better, something like L-BFGS in PyTorch https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html","L-BFGS in tfp helps, but it is not convenient to use, and we have to add an interface, as I discussed above."
54455,pshiko,1084631863,2022-03-31 14:03:30,"When I merged the latest master again and pushed it, it is error with lint.
The parts pointed out in this lint are different from my changes, should I fix them?","The parts pointed out in this lint are different from my changes, should I fix them?"
28840,EdoardoCarlesi,1084210876,2022-03-31 7:40:22,"I know this should not happen. Unfortunately the code is quite long and complex, my workaround was to pre-process the data with numpy before feeding it to tensorflow, so I couldn't really solve this issue.
On other forums I found solutions which involved changing NumPy version and/or TF version.
All in all it's very frustrating that such a simple operation required so much time and effort, on PyTorch this is something that never gave me headaches, not even close.","""I know this should not happen. Unfortunately the code is quite long and complex, my workaround was to pre-process the data with numpy before feeding it to tensorflow, so I couldn't really solve this issue."""
55441,reedwm,1084044136,2022-03-31 3:49:45,"A somewhat hacky way to get the device is to create an empty tensor and immediately query its device: ```
device = tf.constant([], dtype=tf.float32).device
```
@rohan100jain do you know of a better way?
I don't think the fact opening a graph scope clears the device scope is a bug, since each graph has a different device scope, although I admit this behavior is unintuitive. In any case, since this only affects the TF1 API (graphs), I doubt this behavior would ever get fixed.","""I don't think the fact opening a graph scope clears the device scope is a bug, since each graph has a different device scope"""
55381,penpornk,1084011388,2022-03-31 2:38:27,This PR got reverted because it broke internal tests. Will add this to the list of `DEVICE_DEFAULT` ops to revisit / investigate later.,"""PR got reverted because it broke internal tests"""
55390,zhaozheng09,1083991577,2022-03-31 2:10:33,"@sushreebarsa @chunduriv I change my example in issues context.
Please add TF_XLA_FLAGS=""--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit --tf_xla_min_cluster_size=0"", because there is only 1 node in my example xla cluster, 1 node does not meet the tf_xla_min_cluster_size(default:4) condition
![image](https://user-images.githubusercontent.com/33950866/160961857-bcf84532-eb56-4bbc-9f30-37e449a52af5.png)","""I change my example in issues context. Please add TF_XLA_FLAGS=""--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit --tf_xla_min_cluster_size=0"", because there is only 1 node in my example xla cluster, 1 node does not meet the tf_xla_min_cluster_size(default:4) condition"""
54714,gmernov,1083539349,2022-03-30 19:26:11,"> Linking large binaries is supposed to take a lot of time. Please also watch memory usage, in case it uses swap (which results in even more slowdown).
Yes, but the linking doesn't want to end. It produces no load. I don't think that it's gonna end while it does no load... I'm using 32 cores, and 40GB or RAM","Linking large binaries is supposed to take a lot of time. Please also watch memory usage, in case it uses swap (which results in even more slowdown). Yes, but the linking doesn't want to end. It produces no load. I don't think that it's gonna end while it does no load... I'm using 32 cores, and 40GB or RAM."
55379,PatriceVignola,1083528851,2022-03-30 19:16:26,"I'm not sure why the Windows Bazel GPU build is failing. I built it locally yesterday and it completed just fine, and the Linux GPU build completed successfully. Is it possible to look at the failures or is it unrelated to this PR?",I'm not sure why the Windows Bazel GPU build is failing.
55431,DEKHTIARJonathan,1083455019,2022-03-30 18:09:07,"Original Error:
```bash
2022-03-29 19:38:35.717118: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger 4: [network.cpp::validate::2647] Error Code 4: Internal Error (Repeated layer name: TRTEngineOp_000_002/StatefulPartitionedCall/mrcnn/multilevel_crop_and_resize/stack-slice_2:SLICE (layers must have distinct names))
```",bash 2022-03-29 19:38:35.717118: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40]
25939,ironcadiz,1083440886,2022-03-30 17:53:06,"Running that same command I got `3.8.10` for `tensorflow/tensorflow:2.6.1-gpu` and `3.6.9` for `tensorflow/tensorflow:2.5.1-gpu`. Which is consistent with the behavior we've seen in our builds which is we are forced to use python 3.6 when using tf<2.6.
I guess the issue is fixed but only for more recent versions",Running that same command I got 3.8.10 for tensorflow/tensorflow:2.6.1-gpu and 3.6.9 for tensorflow/tensorflow:2.5.1-gpu.
55428,drivanov,1083383690,2022-03-30 16:52:50,"> [fail1.log](https://github.com/tensorflow/tensorflow/files/8374305/fail1.log)
FIXED
Sorry, while resolving a merge conflict online, I didn't notice that the line `template <typename T>` disappeared from ```
template <typename T>
class OpConverter_BinaryTest
```",template typename T> disappeared from template typename T> class OpConverter_BinaryTest .
28840,EdoardoCarlesi,1083346753,2022-03-30 16:16:16,"My issue is that the code is running in eager mode (`tf.executing_eagerly()` returns True) but if I do `.numpy()` on my tensors I still get _AttributeError: 'Tensor' object has no attribute 'numpy'_ . So I cannot just do `.numpy()` on my EagerTensor, it always returns an error.
I am using TF 2.8.0 and NumPy 1.22.3",tf.executing_eagerly() returns True  but if I do .numpy() on my tensors I still get _AttributeError: 'Tensor' object has no attribute 'numpy'_
54650,bixia1,1083319422,2022-03-30 15:54:00,"I understand how this PR may speed up the BUILD of multiple models, but for inference, we actually want to get the best performance for the engines and sharing a single auto tuning cache can hurt performance (because except for one engine, the rest engines aren't using their true input for auto tune?).
What would you said about this?","""Because except for one engine, the rest engines aren't using their true input for auto tune?"""
25939,ironcadiz,1083248325,2022-03-30 14:59:18,"@mohantym It seems that images for 2.5 and below still have python 3.6.
Is the fix you mention only for 2.6 and above?","""Is the fix you mention only for 2.6 and above?"""
55432,tilakrayal,1083096366,2022-03-30 12:46:38,"@chunduriv ,
I was able to reproduce the issue in tf v2.8 and [nightly](https://colab.research.google.com/gist/tilakrayal/cb4fb69f9096a4a2260acae5fd5fb9e7/nigtlytf_issues_fails_to_load_mobilenet_weights_in_2_8_but_not_2_7.ipynb), whereas in [v2.7](https://colab.research.google.com/gist/tilakrayal/182544e20eeb1d103f8d2cea85bdd781/tf_issues_fails_to_load_mobilenet_weights_in_2_8_but_not_2_7.ipynb) i was able to execute without facing any issue.Please find the gist.","""I was able to reproduce the issue in tf v2.8 and [nightly](https://colab.research.google.com/gist/tilakrayal/cb4fb69f9096a4a2260acae5fd5fb9e7/nigtlytf_issues_fails_to_load_mobilenet_weights_in_2_8_but_not_2_7.ipynb), whereas in [v2.7](https://"
55332,Nyrio,1082868033,2022-03-30 9:47:53,"@DEKHTIARJonathan I'm not sure that `_experimental_feature_scope` should be in `tf_trt_integration_test_base.py`.
I need it in `trt_convert_test.py`, so it would make sense to put it in a separate `test_utils.py` or similar file.","I'm not sure that _experimental_feature_scope should be in tf_trt_integration_test_base.py. I need it in trt_convert_test.py, so it would make sense to put it in a separate test_utils.py or similar file."
55332,bixia1,1082455931,2022-03-29 23:09:52,@DEKHTIARJonathan there are still unaddressed comments.,unaddressed comments
55067,jinzzasol,1082340115,2022-03-29 20:25:11,"> WSL is slow in reading files. There are a lot of files that need to be loaded when `import tensorflow as tf` gets executed.
> > This is likely not a TF issue.
@mihaimaruseac You don't get the point. `import tensorflow as tf ` works okay and fast in WSL too. It only takes less than 0.1s. Run the code line by line then you will understand what I'm saying.",WSL is slow in reading files.
55067,mihaimaruseac,1082320476,2022-03-29 20:01:46,"WSL is slow in reading files. There are a lot of files that need to be loaded when `import tensorflow as tf` gets executed.
This is likely not a TF issue.",WSL is slow in reading files.
54714,mihaimaruseac,1082013875,2022-03-29 15:25:07,"Linking large binaries is supposed to take a lot of time. Please also watch memory usage, in case it uses swap (which results in even more slowdown).",Linking large binaries is supposed to take a lot of time.
54279,edwardyehuang,1081993755,2022-03-29 15:10:03,"> Is it just the gradient that's slow, or the entire convolution?
> > IIRC we simply extend the filter (filled with zeros) to match the dilation, so a large dilation would lead to large filters/gradients. How ""large"" are we talking about?
Slow gradient only",Slow gradient.
51765,mihaimaruseac,1081981909,2022-03-29 15:00:54,Still needs a unit test before we can take it,Still needs a unit test before we can take it.
54337,DanijelDomazet,1081839311,2022-03-29 12:58:31,I still have this issue with `v2.8.0`.,I still have this issue with v2.8.0.
42428,hadikoub,1081692330,2022-03-29 10:16:54,"Hi, As it turns out, TF2 GPU is not supported on older GPUs (I had Nvidia GTX 960m ). It only supports Cuda compute capability >=3.5
Refer: https://github.com/tensorflow/tensorflow/issues/41990#issuecomment-670626077","""TF2 GPU is not supported on older GPUs"""
50511,harshayelchuri,1081441849,2022-03-29 6:09:54,"@1148330040 I tried adding a bracket, but it's still not working","I tried adding a bracket, but it's still not working."
50511,1148330040,1081438583,2022-03-29 6:05:00,"> @1148330040 can you see #55403 and find out what exactly is causing the problem?
sorry, i cant open your code, i use tf2.2, My mistake is missing a bracket. You can try it","""Can you see #55403 and find out what exactly is causing the problem?"""
55220,gadagashwini,1081347001,2022-03-29 2:55:20,"@algoteam5,
Seems like issue is with custom loss function. Since evaluate method takes x_test and y_test
`results = a.evaluate([x_test_1, x_test_2], labels_test)`
Change this to `results = a.evaluate([x_test_1,x_test_2])`","""Seems like issue is with custom loss function."""
55317,bhack,1081295319,2022-03-29 1:03:31,Copybara again,"""Copybara again"""
55382,PatriceVignola,1081272514,2022-03-29 0:14:17,"@penpornk Since it doesn't look like the `TF_AssignRefVariable` PR will be able to make it in TF 2.9, is there a possibility to prioritize the review of this PR for early 2.10 nightly builds, maybe by the end of the week? We want to at least be able to point users to nightly builds if they want to run the benchmarks that still use those ops.","TF_AssignRefVariable PR will be able to make it in TF 2.9, is there a possibility to prioritize the review of this PR for early 2.10 nightly builds, maybe by the end of the week?"
55413,skye,1081269293,2022-03-29 0:10:29,"Marking ""ready to pull"" so I can run internal tests, not actually ready for submission.","""ready to pull"" so I can run internal tests, not actually ready for submission."
54591,Nyrio,1081017098,2022-03-28 18:50:55,Closing due to CLA issues.,Closing due to CLA issues.
54591,bixia1,1080855525,2022-03-28 16:22:59,"@Nyrio Could you please recreate this PR? I am trying to manually merge this, but got error "" WARN: Cannot migrate http://github.com/tensorflow/tensorflow/pull/54591 because the following check runs have not been passed: [cla/google]"".
Here is what I was told:
Mihai Maruseac, 2 min
Hi. CLA process changed since february
I think best is for author to recreate the PR
I cannot seem to be able to trigger the new CLA process on the PR","""I cannot seem to be able to trigger the new CLA process on the PR."""
55272,mihaimaruseac,1080772220,2022-03-28 15:07:02,"Currently blocked on ""import/copybara Pending — Waiting for internal safe review approval"". Needs another eye on review there.
Windows builds were broken last week due to LLVM updates, but now they should be ok.","Currently blocked on ""import/copybara Pending — Waiting for internal safe review approval"""
55206,mihaimaruseac,1080770849,2022-03-28 15:05:53,"Rerunning the pylint job, seems to have been using an old version?
Anyway, if still broken, please ignore.","""Rerunning the pylint job, seems to have been using an old version"""
55405,f90,1080607231,2022-03-28 12:47:36,"To add on to this, this is a serious issue because the sparse label version currently runs 50x faster on GPU than the dense label version, but due to the above issue, cannot be used","To add on to this, this is a serious issue because the sparse label version currently runs 50x faster on GPU than the dense label version, but due to the above issue, cannot be used."
55399,mohantym,1080494426,2022-03-28 10:48:54,"Hi Sachin! Could you please look at this issue? It is replicating in [2.7](https://colab.sandbox.google.com/gist/mohantym/b6d40ef800f27a5853c515fbf9104629/github_55399.ipynb#scrollTo=HBHgb24w3V3l) ,[ 2.8](https://colab.sandbox.google.com/gist/mohantym/69634a06b55281b56cb5f105e6152ab8/github_55399.ipynb#scrollTo=-DZ4o0WaCDIZ) and[ nightly](https://colab.sandbox.google.com/gist/mohantym/48c3236fb6c45083d7ebc6fb56de68df/github_55399.ipynb#scrollTo=sGje8x42Frfn).","""It is replicating in [2.7](https://colab.sandbox.google.com/gist/mohantym/b6d40ef800f27a5853c515fbf9104629/github_55399.ipynb#scrollTo=HBHgb24w3V3l) ,[ 2.8](https://colab.sandbox.google.com/gist/mohantym/69634a06b55"
55377,gadagashwini,1080361415,2022-03-28 8:41:02,"@klorinczi,
> RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd
This looks like numpy version 1.19.5 and TensorFlow 2.4 not playing well together. Use `numpy ~= 1.19.2`",RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd
52222,gadagashwini,1080125441,2022-03-28 2:53:15,"@mrnucleation, This issue can be due to several reasons, 1. out of memory: Limit the GPU memory growth
2. configuration Mismatch with CUDA/cuDNN
3. remove the cache folder ~/.nv/","""Can be due to several reasons, 1. out of memory: Limit the GPU memory growth 2. configuration Mismatch with CUDA/cuDNN 3. remove the cache folder /.nv/."""
55394,zhaozheng09,1080096130,2022-03-28 1:46:50,"I got the correct answer under float type by XLA, but my change maybe bring precision problem of double64 type, because the implementation of TANH in EIGEN3 is ```template<float>```.
Should we implement ```void tanh_float``` or ```void tanh_double``` separately?
And I don't find xla service runtime unittest.","""And I don't find xla service runtime unittest."""
40070,bill-connelly,1080049548,2022-03-28 0:02:43,"@lauraht Sorry, I'm honestly still kinda confused as to how my one worked [and it really did, I've got the confusion matrix to prove it]. I don't understand how this dictionary of ""labels"" got mapped to the actually training dataset, as it's a one-hot encoded vector. i.e. how does the model know that the key [1] in the class_weights_dict is related training dataset of [0 1 0 0 0 0 0]?","""I don't understand how this dictionary of ""labels"" got mapped to the actually training dataset, as it's a one-hot encoded vector."""
54379,penpornk,1080014024,2022-03-27 20:35:00,"It seems we still got the [Ubuntu CPU](https://source.cloud.google.com/results/invocations/87b01a5b-8b87-415d-bf05-864eeff01ac6) error
```
tensorflow/core/kernels/mkl/mkl_fused_ops_test.cc:794
Expected equality of these values:
::tensorflow::Status::OK()
Which is: OK
(RunOpKernel())
Which is: ABORTED: Operation received an exception:Status: 3, message: could not create a primitive descriptor iterator, in file tensorflow/core/kernels/mkl/mkl_conv_ops.cc:893
```",
55228,ttdd11,1079979472,2022-03-27 17:25:23,@gadagashwini This is why a new ticket was opened. If you look at the communication in the other ticket there isn't a single response from the assignee and it's just labelled awaiting tensorflower.,"""There isn't a single response from the assignee and it's just labelled awaiting tensorflower."""
55382,penpornk,1079962736,2022-03-27 16:14:34,@PatriceVignola I don't think `TF_AssignRefVariable` will make it. It's too short notice and it touches C API (but at least it's in experimental). Branch cut is this coming Monday (3/28) or Tuesday (3/29) if Monday's nightly isn't clean. I can check with @wangpengmit though.,"""It's too short notice and it touches C API (but at least it's in experimental). Branch cut is this coming Monday (3/28) or Tuesday (3/29) if Monday's nightly isn't clean."""
55272,Gelesh,1079742977,2022-03-26 17:52:16,"@mihaimaruseac , any thought on the internal CI fail Internal",CI fail
52989,Cortlandd,1079731148,2022-03-26 16:49:31,"I'm running into this same issue. Attempting to convert model to onnx.
` AttributeError: '_UserObject' object has no attribute 'add_slot'`",AttributeError: '_UserObject' object has no attribute 'add_slot'
36510,cattuna,1079529844,2022-03-26 0:16:47,"> > @Yannik1337 @sureshbhusare @crccw These options still lead to slowdowns since NCCL is not being used.
> > On another issue, I found it mentioned to try using a `MultiWorkerMirroredStrategy` on a single device with multiple GPUs (see here: [#41898 (comment)](https://github.com/tensorflow/tensorflow/issues/41898#issuecomment-668786507)). Have you tried that (though it's proposed in a different context).
Can confirm that `MultiWorkerMirroredStrategy` works but `MirroredStrategy` don't.",Can confirm that MultiWorkerMirroredStrategy works but MirroredStrategy don't.
55206,philipphack,1079524099,2022-03-26 0:00:32,"I can't reproduce the PyLint error, and I don't see any lines that are longer than 80 characters in the diff.","I can't reproduce the PyLint error, and I don't see any lines that are longer than 80 characters in the diff."
36510,cattuna,1079503691,2022-03-25 23:03:21,"Experiencing similar errors on ai platform with A100s. Has this not been resolved or am I nuts?
<img width=""755"" alt=""Screen Shot 2022-03-25 at 4 02 38 PM"" src=""https://user-images.githubusercontent.com/42384776/160212811-e36a6874-86dc-4b62-bc4e-5683286f8341.png"">",Experiencing similar errors on ai platform with A100s.
54714,gmernov,1079405681,2022-03-25 20:21:50,"I think it's not possible to upgrade CUDA version, because of i'm using latest Jetson JetPack. And it has it's certain version in. But i tried to build TF 2.7.1 using bazel =<3.99.0. The situation changed. Now it doesn't wait to the end to start linking process, but it does it on the run. Compiling,linking,compiling,linking. But it ""hangs"" somewhere in the middle of as well as TF 2.8.0 does.
So, still no go...","""still no go"""
51439,bhack,1079339288,2022-03-25 19:04:03,"Note: with the Codespace ""free tier"" available disk space (individual account) you will have a `no space left on device` cause with our manylinux/docker refactory effort we still have only a CUDA embedded image that is very large to run on the ""free tier"". See more at https://github.com/tensorflow/build/pull/47",No space left on device
55342,mihaimaruseac,1079170507,2022-03-25 16:01:41,"Can you try to fix multiple typos in the same file/directory please? As you see, the CI time is significant (and this is ~1/3rd of the total CI if we take into account the internal CI too). We're trying to minimize the hours_of_ci/delta_of_change metric",Can you try to fix multiple typos in the same file/directory please?
55132,mihaimaruseac,1079157621,2022-03-25 15:47:06,I already changed the affected versions on GitHub but it didn't seem to propagate.,I already changed the affected versions on GitHub but it didn't seem to propagate.
53144,vanakema,1079143109,2022-03-25 15:30:24,"> @vanakema I already did that but no luck... Any other idea? > > > > <img width=""567"" alt=""image"" src=""https://user-images.githubusercontent.com/32769401/160150987-a6edd25e-7450-4927-89cc-d8b2e3941be8.png"">
> > Oh I thought I saw someone else say that for whatever reason that style import doesn't work with the fix. Do you get autocompletes when you write something like `tf.keras.layers` in your actual code?",I already did that but no luck... Any other idea?
53144,nicolasperezdeo,1079140917,2022-03-25 15:28:02,"@vanakema I already did that but no luck... Any other idea? <img width=""567"" alt=""image"" src=""https://user-images.githubusercontent.com/32769401/160150987-a6edd25e-7450-4927-89cc-d8b2e3941be8.png"">",I already did that but no luck...
55363,bhack,1079114483,2022-03-25 15:00:13,"Sorry but my local `bazel` cache in the container was invalidated again. I could do some ""blind edit"" if you want but we need to abuse the CI in the meantime.","""I could do some ""blind edit"" if you want but we need to abuse the CI in the meantime."""
53144,vanakema,1079063228,2022-03-25 14:07:43,"> I am also having the same issue. After updating the `__init__.py` however, Pycharm does not pick up the changes... I am using `tensorflow-macos==2.8.0` in M1. You'll probably need to invalidate caches and restart. Your indexes are likely still from before the change. PyCharm index cache invalidations works differently with native interpreters it seems","""After updating the __init__.py however, Pycharm does not pick up the changes... I am using tensorflow-macos==2.8.0 in M1"""
53144,nicolasperezdeo,1078904462,2022-03-25 10:59:17,"I am also having the same issue. After updating the `__init__.py` however, Pycharm does not pick up the changes... I am using `tensorflow-macos==2.8.0` in M1.","I am also having the same issue. After updating the __init__.py however, Pycharm does not pick up the changes..."
41535,rickstaa,1078743759,2022-03-25 7:43:35,@google-ml-butler This is still an issue. @gustavo-dev can you please reopen.,This is still an issue.
55334,bhack,1078549950,2022-03-25 0:54:44,"@piEsposito This is a very well old know issue and `decode_image`, `decode_jpg` and `decode_gif` are ""unfied"" you need to use `expand_animations = False`.
As you can see, the thread is active since 2017 https://github.com/tensorflow/tensorflow/issues/9356#issuecomment-298469582","""unfied"""
55138,SeeForTwo,1078063227,2022-03-24 19:48:25,"> Provide an argument to choose the DCT method in tf.image.adjust_jpeg_quality
Do you have any interest in contributing a PR for this?
> at least mention this caveat in the method's documentation.
I can add something to the documentation and add an example without clipping.
> Use the accurate integer DCT by default.
Changing the default (which would change behavior for existing users) is unlikely to be accepted.","""Changing the default (which would change behavior for existing users) is unlikely to be accepted."""
34962,dhruvgoel99,1077885952,2022-03-24 17:50:43,"Hi, I got the same error, i tried some solution to resolve them:
just comment out line 2163 & 2164
i.e.,
self.keras_model._losses = []
self.keras_model._per_input_losses = {}","I got the same error, i tried some solution to resolve them: just comment out line 2163 & 2164 i.e., self.keras_model._losses = [] self.keras_model._per_input_losses = ."
54868,swapnilsayansaha,1077837456,2022-03-24 16:57:35,"No it still does not work. I am surprised so many tools exist to disable logging, yet TF cannot interface with these tools properly.",No it still does not work.
8369,uzi0espil,1077767732,2022-03-24 15:40:06,"I think this is a needed feature, stacking and transposing makes the code ugly and hard to read, where you can just say `concat(axis=1)`.","I think this is a needed feature, stacking and transposing makes the code ugly and hard to read, where you can just say concat(axis=1)."
40070,nviswanathan15,1077389673,2022-03-24 8:59:53,"> I am getting the same errors with version 2.8.0 and python 3.9. I need to use class_weight cause of my data is heavily imbalanced. If I run without the class_weight, no errors. Is there solutions or workarounds for this issue? Thanks
I used focal loss to handle imbalance as I was not able to use class_weight..
this link helped me:
https://www.dlology.com/blog/multi-class-classification-with-focal-loss-for-imbalanced-datasets/",I am getting the same errors with version 2.8.0 and python 3.9.
54325,jiayugedede,1077346387,2022-03-24 8:07:41,"I have the same problems, I have checked the driver(ok), cuda(ok), cudnn(ok), nccl(ok), all of these were normal operation. However, the distribution training was broken and frozen. It is so disappointing.","I have the same problems, I have checked the driver(ok), cuda(ok), cudnn(ok), nccl(ok), all of these were normal operation. However, the distribution training was broken and frozen."
55132,alexlang74,1077283847,2022-03-24 6:33:04,"Would it be possible to flag something like ""TensorFlow 2.7.9"" as affected release for these CVEs, if you still want to keep them around? It doesn't feel right either, but it would at least ensure that TF 2.7.1 users are no longer affected.","""It doesn't feel right either, but it would at least ensure that TF 2.7.1 users are no longer affected."""
55280,sapphire008,1076968494,2022-03-24 1:09:37,Found a problem. Close for now.,Found a problem. Close for now.
55353,DEKHTIARJonathan,1076968160,2022-03-24 1:08:57,"Currently TF-TRT may reject Slice OPs during the conversion phase, which obviously leads to:
```bash
2022-03-24 01:00:01.700857: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:1055] TF-TRT Warning: Engine creation for TRTEngineOp_0_2 failed. The native segment will be used instead. Reason: INTERNAL: tensorflow/compiler/tf2tensorrt/convert/ops/slice_ops.cc:116 TRT_ENSURE failure
```
CC: @tfeher","Currently TF-TRT may reject Slice OPs during the conversion phase, which obviously leads to: bash 2022-03-24 01:00:01.700857: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:1055] TF-TRT Warning: Engine creation for TRTEngineOp_0_2 failed. The native segment will be used instead. Reason: INTERNAL: tensorflow/compiler/t"
55341,AndryCU,1076837096,2022-03-23 21:27:02,"@mohantym sorry, can you explain better, i don't know what to do sorry, if you preffer you can text me on telegram @andryssd, of course i will post evething here, but i want more interchange only if you can",I don't know what to do.
55350,bhack,1076821487,2022-03-23 21:07:28,"@mdanatg I've tried to insert `f = autograph.tf_convert(f, autograph_ctx.control_status_ctx())` there but it is failing with the same error","I've tried to insert f = autograph.tf_convert(f, autograph_ctx.control_status_ctx()) there but it is failing with the same error."
55349,DanBmh,1076753749,2022-03-23 19:46:24,"Fails as well when I'm replacing the `abs` with `spectrogram = tf.math.real(stfts) ** 2 + tf.math.imag(stfts) ** 2`:
`RuntimeError: /workspace/tensorflow/lite/kernels/complex_support.cc:43 output->type != kTfLiteFloat32 (INT8 != FLOAT32)Node number 46 (IMAG) failed to prepare.`",RuntimeError: /workspace/tensorflow/lite/kernels/complex_support.cc:43 output->type != kTfLiteFloat32 (INT8 != FLOAT32)Node number 46 (IMAG) failed to prepare.
54669,RenuPatelGoogle,1076683960,2022-03-23 18:33:12,"> Please retitle the PR message (and in the future use a better commit message). See for example https://cbea.ms/git-commit/
I tried to mention meaningful information now. Please check.","""Please retitle the PR message (and in the future use a better commit message)"""
47400,snyuryev,1076452962,2022-03-23 14:38:28,"@yyoon Tried you suggestion and got this error:
```
ERROR: Config expansion has a cycle: config value ios_sim_arm64 expands to itself, see inheritance chain [ios_sim_arm64]
```
Any updates on this issue? What is the way to compile for arm64 simulator?",ERROR: Config expansion has a cycle:
55306,npanpaliya,1076373925,2022-03-23 13:26:23,I don't see an option to re-trigger the checks. Could someone please do it for me?,I don't see an option to re-trigger the checks.
31173,LiranPo,1076213863,2022-03-23 10:39:06,"Any updates on this? Still a very useful feature, and TF2.8 still doesn't have it :( @omalleyt12",:(
55250,ranvirdesai,1076176983,2022-03-23 10:02:48,"I was using a small dataset and it was giving problems, but when I used bigger dataset it worked.
Also changed python 3.8 to 3.7","I was using a small dataset and it was giving problems, but when I used bigger dataset it worked. Also changed python 3.8 to 3.7."
49832,fig666,1076146361,2022-03-23 9:30:47,Currently in the process of being in the conversion of getting help with better components and malware protection software for the issues currently being produced towards my keyboard system.,Currently in the process of being in the conversion of getting help with better components and malware protection software for the issues currently being produced towards my keyboard system.
55132,alexlang74,1076011270,2022-03-23 7:27:14,"Unfortunately, not. Our customers run 3rd party security scanners, like twistlock and aquascan. We can't make these tools add exemptions, and customers are typically wary of discussing ""you know, this issue is not really an issue"". I'd really hope we can address this in the CVE itself, by changing the affected versions to the correct value...","""We can't make these tools add exemptions, and customers are typically wary of discussing ""you know, this issue is not really an issue""."
50744,gadagashwini,1075857993,2022-03-23 2:44:36,"@MisRight,
> ERROR: G:/tensorflow-r2.4.0-gpu/tensorflow-r2.4/tensorflow/compiler/xla/BUILD:423:1: C++ compilation of rule '//tensorflow/compiler/xla:literal' failed (Exit 2): python.exe failed: error executing command
This issue is related to Python and Numpy version compatibility issue. Tensorflow v2.4.1 source incompatible with Numpy version >=1.20.1. Tensorflow 2.4 support numpy ~= 1.19.5.
`pip install numpy==1.19.5`",ERROR: G:/tensorflow-r2.4.0-gpu/tensorflow-r2.4/tensorflow/compiler/xla/BUILD:423:1: C++ compilation of rule '//tensorflow/compiler/xla:literal' failed (Exit 2): python.exe failed: error executing command
55305,mihaimaruseac,1075477045,2022-03-22 18:23:50,"Reproduced
```
In [6]: tf.strings.unsorted_segment_join(inputs=['123'],segment_ids=[0],num_segments=-1)
F0322 11:23:25.727334 1107260 tensor_shape.cc:396] Check failed: size >= 0 (-1 vs. 0) ```","In [6]: tf.strings.unsorted_segment_join(inputs=['123'],segment_ids=[0],num_segments=-1) F0322 11:23:25.727334 1107260 tensor_shape.cc:396] Check failed: size >= 0 (-1 vs. 0) "
55317,mdanatg,1075472850,2022-03-22 18:19:41,"Probably a tool failure, I'll trigger it again. But let's replace the line continuation from the string, just in case.","Probably a tool failure, I'll trigger it again."
55318,mihaimaruseac,1075436905,2022-03-22 17:43:56,Please stop spamming with github actions or we might need to ban you from contributing to this repository.,Spamming with github actions.
48787,jedichen121,1075213305,2022-03-22 13:57:06,"I'm having the same problem here. I have 3060Ti and running Ubuntu 18.04 with driver 470.103.01. I used conda to install tensorflow-gpu 2.4. Even for simple operation like `python -c ""import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))""` it can take a minute to finish execution. I tried tensorflow-gpu 2.1 and it's the same issue. But using CPU tensorflow is fast as it should be.
Did you manage to solve this problem?",I have 3060Ti and running Ubuntu 18.04 with driver 470.103.01.
35211,AndryCU,1075205053,2022-03-22 13:49:51,Why when i traid to use TransformToGrayscaleOp i see a: **Unresolved reference: TransformToGrayscaleOp** error?,Why when i traid to use TransformToGrayscaleOp i see a: **Unresolved reference: TransformToGrayscaleOp** error?
53065,mohantym,1075085088,2022-03-22 11:56:10,Hi @mihaimaruseac ! Shall we close this pull request? it has been going through a lot of loops for a long time now. I am afraid that it might disturb the peace of processes happening here.,"""it has been going through a lot of loops for a long time now."""
55206,wangpengmit,1074569394,2022-03-22 0:59:16,I don't like that `ReadVariableOp` does this COR-to-COW conversion as a side effect (even if it only happens when `no_copy` is true). How about adding a new op `TurnOffCopyOnRead` to do this job?,I don't like that ReadVariableOp does this COR-to-COW conversion as a side effect (even if it only happens when no_copy is true). How about adding a new op TurnOffCopyOnRead to do this job?
55132,mihaimaruseac,1074458792,2022-03-21 21:54:13,"I was OOO for the entire period this issue existed
Seems there was a typo when the GH advisories were created",I was OOO for the entire period this issue existed
55309,caliari-italo,1074314231,2022-03-21 19:13:20,Unfortunately it didn't worked. The error is still the same even changing Cuda and CuDNN.,"""The error is still the same even changing Cuda and CuDNN."""
55303,API92,1074178717,2022-03-21 17:15:19,"@tilakrayal , you should enter path to your bucket in Google Cloud Storage, because tensorflow doesn't support storing checkpoints locally on TPU. This folder may be empty. For example, I used this gs://dc795ab9-90a2-4cb9-b1dd-badcf556350b/init_scope_replica_id , but it deleted now. There is no other code dependencies for notebooks.","""should enter path to your bucket in Google Cloud Storage, because tensorflow doesn't support storing checkpoints locally on TPU"""
54481,AB5247,1074162133,2022-03-21 17:00:35,"@jvishnuvardhan Sorry, but this is not related to anything reported before and answered by @karimnosseir. TFlite with the Hexagon delegate is simply giving completely wrong results for the model I am working with, someone from your team will have to look at it and tell me what is going on. https://github.com/tensorflow/tensorflow/issues/36804 was a similar problem and it was a bug that you eventually fixed.","""TFlite with the Hexagon delegate is simply giving completely wrong results for the model I am working with, someone from your team will have to look at it and tell me what is going on."""
55278,bhack,1074026370,2022-03-21 15:15:30,"> I admit it's frustrating :( Most PRs rely on the internal build system, making it hard to get traction to improve the external one.
It is one of the ""chicken or the egg"" problems we have for the community growing.
I will ping you when the compilation is done.","""It is one of the ""chicken or the egg"" problems we have for the community growing."""
55259,aflah02,1073988808,2022-03-21 14:44:53,Hey I no longer have that code and i used a different process from scratch. I guess it's best to close the issue as your point is correct but I don't have the code as i overwrote it,I don't have the code as i overwrote it.
55228,ttdd11,1073935508,2022-03-21 14:01:48,@gadagashwini it's a duplicate for sure. I opened a new ticket after basically no update for months. Will this get escalated in any way?,I opened a new ticket after basically no update for months.
36219,expeon07,1073906017,2022-03-21 13:35:44,"Hi, anybody else experiencing the same problem? I have the following error when trying to run my TFLite micro model on an ESP32
`Only 1 subgraph is currently supported.
AllocateTensors() failed`
My model is a 3-layer LSTM.","""Only 1 subgraph is currently supported. AllocateTensors() failed"""
55262,tilakrayal,1073884908,2022-03-21 13:16:50,"@DagonArises ,
I do not have access to the link you have provided. Could you please provide the required permissions to view the files.",I do not have access to the link you have provided.
55278,mdanatg,1073876735,2022-03-21 13:10:37,"Oops, edited.
Yes, improving the error message makes sense too; one challenge is that it comes from outside of autograph, and can be triggered from other places as well.","Oops, edited. Yes, improving the error message makes sense too; one challenge is that it comes from outside of autograph, and can be triggered from other places as well."
55278,bhack,1073853525,2022-03-21 12:49:13,"> Yes. When evaluating `a < b < c` Python calls something in the lines of `(a < b).__bool__()`, which only works in eager mode.
Yes honestly it is so hard to explain this kind of underline machinery to our users/developers. I needed to point another user to the `Cpython` source for a [""similar"" case](https://discuss.tensorflow.org/t/you-cannot-build-your-model-by-calling-build-if-your-layers-do-not-support-float-type-inputs/8356/6)","""When evaluating a  b  c Python calls something in the lines of (a  b).__bool__(), which only works in eager mode."""
55292,kanghj,1073555035,2022-03-21 7:33:27,"Thanks for checking. Yes, indeed it crashes, which I am reporting as a bug. The expected, correct behavior is that`tf.experimental.numpy.array` should throw an exception (perhaps a `ValueError`, same as numpy.array), and not crash. Crashing is an unexpected, wrong behavior.","""Crashing is an unexpected, wrong behavior."""
55292,tilakrayal,1073529368,2022-03-21 6:45:48,"@kanghj ,
I have tried in colab with v 2.8 version and noticed that session is being crashed. Please, find the gist [here](https://colab.research.google.com/gist/tilakrayal/39587d8c4222dbbb35d876cb316973be/untitled255.ipynb). Also please find the errorlog of the crashed colab which stated as below.
`Check failed: ndims_byte() < MaxDimensions() (unsigned char value 254 vs. 254)Too many dimensions in tensor`","""Check failed: ndims_byte()  MaxDimensions() (unsigned char value 254 vs. 254)Too many dimensions in tensor"""
41639,jhetherly,1073359163,2022-03-20 22:12:46,Running the [gist](https://colab.research.google.com/gist/amahendrakar/697836288a6d02f048d11e08b7044079/41639-tf-nightly.ipynb) linked above with the latest nightly (`2.9.0-dev20220320`) fails with the exact same error.,Running the [gist](https://colab.research.google.com/gist/amahendrakar/697836288a6d02f048d11e08b7044079/41639-tf-nightly.ipynb) linked above with the latest nightly (2.9.0-dev20220320) fails with the exact same error.
55207,whatdhack,1073171167,2022-03-20 5:20:33,"Hi, I think it is coming from the following. Looks like this is not guarded by any check for whether XLA is asked for or not. https://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850e/tensorflow/compiler/jit/jit_compilation_pass_registration.cc#L35
What I am trying to answer if this pollutes non-XLA executions with XLA kernels.","""Looks like this is not guarded by any check for whether XLA is asked for or not."""
45994,scdub,1073153070,2022-03-20 2:43:49,"This issue is still present, and can be identified by searching for `sys.argv[0]` instances in the codebase, and is still present in the nightly builds.","This issue is still present, and can be identified by searching for sys.argv[0] instances in the codebase, and is still present in the nightly builds."
36465,DylanHsu,1073029438,2022-03-19 15:26:47,"@FirefoxMetzger This is probably the worst instance in the steady parade of “TF apologists” over the past two years, of various individuals who do not understand the issue claiming it is not a problem. As explained above, it is a real issue in production. At this point, it has become amusing to receive a new email from another apologist every few months, but what is not amusing is that this repeatedly derails the conversation and the problem never gets solved.","""TF apologists"""
47554,imander,1072938889,2022-03-19 4:37:56,This was also affecting me but I just noticed that if I save the model as `hdf5` I don't get the error :man_shrugging:,I just noticed that if I save the model as hdf5 I don't get the error.
12843,sachinprasadhs,1072887155,2022-03-18 23:33:44,"scatter_update has been changed to tensor_scatter_nd_update in Tensorflow 2 and the error message has been modified as well. Below is the code and the output.
```
testVar = tf.Variable(tf.zeros([5,1]))
ind = tf.constant([0,3])
data = tf.constant([5,7], dtype=tf.float32)
up = tf.tensor_scatter_nd_update(testVar, ind, data )
InvalidArgumentError: Inner dimensions of output shape must match inner dimensions of updates shape. Output: [5,1] updates: [2] [Op:TensorScatterUpdate]
```","[5,1]"
53767,lgeiger,1072474624,2022-03-18 14:39:19,"This is still an issue in `2.9.0-dev20220318`. Are there any updates on this?
Being able to trigger a converter segfault seems to be quite problematic.",still an issue in 2.9.0-dev20220318. Are there any updates on this? Being able to trigger a converter segfault seems to be quite problematic.
55232,rsanthanam-amd,1072307348,2022-03-18 11:09:27,@gbaned An import/copybara error occurred. please let me know how i can help fix this.,An import/copybara error occurred.
55262,DagonArises,1071987486,2022-03-18 3:15:56,@tilakrayal Sorry I forgot to specify the imports. Here is the [notebook](https://colab.research.google.com/drive/1CQyLVcz3VX6eEi1d5UqwU68HevkmHKg6#scrollTo=z8tmyVsRCwhb) Please note that ```tf.gradients``` is an expensive computation. Is it possible to carry out my computations in Gradient Tape? I had this None gradient issue before when I tried to watch c0.,I had this None gradient issue before when I tried to watch c0.
53214,tomzx,1071943608,2022-03-18 1:53:15,"I do not have a need for this image, I was only letting you know it's unavailable.
Feel free to close if there's no plan to release 2.4.4 on docker hub.","I do not have a need for this image, I was only letting you know it's unavailable."
54474,ArrowIntoTheSky,1071938241,2022-03-18 1:40:57,"> I think the issue has been fixed in #54441
Thanks for the PR! However, I checked on tf-nightly, and the following code still passed:
```
tf.math.reduce_mean(tf.random.uniform([1,2,2]), axis=[1,], keepdims=-1)
```
here `keepdims` should also be a `bool`, but no exception is raised.","I checked on tf-nightly, and the following code still passed:  tf.math.reduce_mean(tf.random.uniform([1,2,2], axis=[1,], keepdims=-1)  here keepdims should also be a bool, but no exception is raised."
55240,Gelesh,1071161228,2022-03-17 18:01:02,"This issue is from the URL , noting is being fetched from URL, even when we hit the url from browser. I am sorry, I dont understand how is it even related to Visual Studio C++. However, I do have that in my pc","I am sorry, I dont understand how is it even related to Visual Studio C++. However, I do have that in my pc."
55245,drivanov,1071118208,2022-03-17 17:22:26,"> @drivanov I edited the PR description, would you please check?
To make it simpler, my intention was to was to submit two separate PRs: - first: for refactoring of converter for Binary operations
- second: for `LogicalAnd` and `LogicalOr`
I will remove the code related `LogicalAnd` and `LogicalOr` from `ops/binary_ops.cc` and I will change PR description.","I edited the PR description, would you please check?"
45940,sseung0703,1070951243,2022-03-17 13:53:00,"@caandewiel As I said above, it is impossible to update mirrored variables in the jit compiled graph.
So I had to remove all the update operations in jit compiled code and then update each variable outside.
You can find my work [here.](https://github.com/sseung0703/EKG/blob/feb7c3ccb1244aea3f42663c2f426a692dd6e65a/op_utils.py#L41-L87)","""It is impossible to update mirrored variables in the jit compiled graph."""
38800,ghost,1070849601,2022-03-17 12:13:59,"Hi
I am using:
python 3.6
tensorflow==1.5
i am getting this error :
File ""C:\Users\User\miniconda3\envs\bot\lib\site-packages\tflearn\__init__.py"", line 4, in <module>
import tensorflow.compat.v1 as tf
ModuleNotFoundError: No module named 'tensorflow.compat
Can anyone please suggest some work around or solution?","""Can anyone please suggest some work around or solution?"""
48482,TimbusCalin,1070819125,2022-03-17 11:34:07,"Invocation also fails in my case, 3DConvolution + 3DMaxPool, I was able eventually to convert them by specifying `converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]` prior to conversion; nevertheless invocation fails, with exit code ""random integer 192391234313""","Invocation also fails in my case, 3DConvolution + 3DMaxPool, I was able eventually to convert them by specifying converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS] prior to conversion; nevertheless invocation fails, with exit code ""random integer 192391234313""."
55250,ranvirdesai,1070637728,2022-03-17 9:35:18,"@tilakrayal I face issue at the end of training, after completion of epochs and I am not using tensorboard profiling.
Mirroredstrategy and TPUStrategy works fine.
Just the Multiworkermirroredstrategy fails giving error - ""terminate called without an active exception aborted""","I face issue at the end of training, after completion of epochs and I am not using tensorboard profiling."
55177,drivanov,1069793740,2022-03-17 0:58:54,"> @drivanov this test fail, see [log](https://github.com/tensorflow/tensorflow/files/8265934/boolpr.log)
Fixed.","""this test fail, see [log](https://github.com/tensorflow/tensorflow/files/8265934/boolpr.log) Fixed."")"
54923,wangpengmit,1069717867,2022-03-16 22:48:32,"Looks like if https://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850e/tensorflow/python/eager/context.py#L497 fails, https://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850e/tensorflow/python/eager/context.py#L489 should also be reverted.","Looks like if https://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850e/tensorflow/python/eager/context.py#L497 fails, https://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850"
54681,piEsposito,1069551589,2022-03-16 19:49:02,Same problem here.,Same problem here.
55177,bixia1,1069333532,2022-03-16 16:38:31,"@drivanov this test fail, see
[log](https://github.com/tensorflow/tensorflow/files/8265934/boolpr.log)","""this test fail, see [log](https://github.com/tensorflow/tensorflow/files/8265934/boolpr.log)"""
55211,Gelesh,1069130133,2022-03-16 13:28:45,"Is the plot, talking about training loss or validation loss. ?
Early stopping is required when, validation loss is increasing to answer 'what is the time Early stopping is required to prevent overfitting ? '.
Im sorry I dont understand the bug/issue being raised over here","Is the plot, talking about training loss or validation loss. ?"
55240,Gelesh,1069123831,2022-03-16 13:22:52,"this is not duplicate of #55218, this issue is about error 404, file not found. Where as 55218 is about , bazelisk is not attempted to build when bazel is not installed. Some are even attempting to set the path of bazel == path of bazelisk as a work arround","This is not duplicate of #55218, this issue is about error 404, file not found. Where as 55218 is about , bazelisk is not attempted to build when bazel is not installed. Some are even attempting to set the path of bazel == path of bazelisk as a work arround."
54434,akuegel,1068860669,2022-03-16 8:28:16,"Unfortunately I am not the one who can accept this change.
@rohan100jain, can you please decide whether this change is ok? Or find someone else to approve?",I am not the one who can accept this change.
55194,smit-hinsu,1068826527,2022-03-16 7:42:50,We don't have a plan to support ImageProjectiveTransform at the moment so for now the best option will be to put this op outside of the XLA cluster.,"""We don't have a plan to support ImageProjectiveTransform at the moment so for now the best option will be to put this op outside of the XLA cluster."""
55248,thaink,1068704020,2022-03-16 3:53:49,I think you need to change MismatchBiasSizeTest in the test as well.,I think you need to change MismatchBiasSizeTest in the test as well.
55192,bhack,1068514622,2022-03-15 22:05:54,"Can we give a better self-explainable string to the ""avg"" developer?
I think that the current strings are a little bit cryptic and we don't have any hints on what the user need to check/change to workaround the fallback.","Can we give a better self-explainable string to the ""avg"" developer?"
55244,Gelesh,1068399191,2022-03-15 19:46:16,"@mihaimaruseac , The CICD build failed, on this patch seems to be from some other issue. Because this patch is just to fix the code alignment and spacing. Nothing functionally different.","The CICD build failed, on this patch seems to be from some other issue."
33303,chao-camect,1068201819,2022-03-15 16:36:27,"I downgraded to tf2.6.1. It works. So it's a regression.
The bug is only fixed in 2.5 and 2.6.
@zldrobit @swarmidentity @NAM-hj",I downgraded to tf2.6.1. It works. So it's a regression. The bug is only fixed in 2.5 and 2.6.
55174,mattiasu96,1068023965,2022-03-15 14:01:59,"Oh sorry, should I copy the issue to Keras then?","""should I copy the issue to Keras then?"""
50414,gadagashwini,1067870895,2022-03-15 11:21:21,"@ctrouillefou,
Try changing Python file path
`
python_path=C:/Program Files/Python3/python.exe -> this to C:/Python3/python.exe`
Check msys64 is `C:\msys64\usr\bin` mast be present in your PATH variable.",python_path=C:/Program Files/Python3/python.exe -> this
55189,dwightnw,1067865031,2022-03-15 11:18:08,"I included code in my original post, but the formatting is not correct. Only part of the code is gray, and the rest is in text format. I can't fix the formatting even though I try","I included code in my original post, but the formatting is not correct. Only part of the code is gray, and the rest is in text format. I can't fix the formatting even though I try."
55189,dwightnw,1067849170,2022-03-15 11:01:41,"I have an update. I ran the code on tensorflow 2.4, and I get the same issue as I did on 2.1.","I ran the code on tensorflow 2.4, and I get the same issue as I did on 2.1."
55038,gadagashwini,1067840401,2022-03-15 10:52:17,"@adam1brownell,
You can suppress all debugging logs using below code snippet. Try setting log level before importing tf
```
import os
os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""3""
import tensorflow as tf
```
OR
```
import logging
logging.getLogger('tensorflow').disabled = True
```","""Can suppress all debugging logs using below code snippet."""
54714,gmernov,1067714351,2022-03-15 8:47:54,"Tensorflow 2.7.1, bazel 4.2.2 the same thing: I also tried to install clang6.0 and clang10.0, but clang compilation provides an errors.
![image](https://user-images.githubusercontent.com/4686690/158339107-d719f6b8-e9b7-46b6-a1c7-99f5156881cc.png)","I also tried to install clang6.0 and clang10.0, but clang compilation provides an errors."
55190,gadagashwini,1067617376,2022-03-15 6:34:56,"@HimGautam,
There are the Deprecation Warnings, which we can suppress using one of the below code snippets ```
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' import tensorflow as tf
```
OR
```
import warnings
warnings.filterwarnings(""ignore"")
import tensorflow as tf
```",Deprecation Warnings
54506,anyaconda,1067333708,2022-03-14 21:51:50,"@[jvishnuvardhan](https://github.com/jvishnuvardhan)
thank you for the explanation. i will try to remember that the behavior is intended to be different between `numpy` and `tensorflow`. i wish it was consistent - imho `numpy` is a standard for many developers and data scientists.","""i wish it was consistent"""
47174,duncanriach,1067322592,2022-03-14 21:39:08,"@edwardyehuang, to trigger the determinism-unimplemented exception in `tf.nn.depthwise_conv2d` (in TensorFlow version 2.8) your model must use the back-prop paths through the GPU implementation of the op. It's possible to run GPU inference only or train on CPU using the op without the exception being thrown. Please confirm that you're definitely _training_ your models on _GPU_.","""to trigger the determinism-unimplemented exception in tf.nn.depthwise_conv2d (in TensorFlow version 2.8) your model must use the back-prop paths through the GPU implementation of the op"""
54868,swapnilsayansaha,1067241408,2022-03-14 20:12:19,I added the lines. It still does not work.,I added the lines. It still does not work.
51759,penpornk,1067064532,2022-03-14 17:00:03,I'm sorry I get to this PR so late. Is this required for TF-DirectML for TF 2.9? Asking because we are having an API freeze starting tomorrow.,I'm sorry I get to this PR so late.
53367,bhack,1067049384,2022-03-14 16:44:53,"> This has landed but GitHub fails to notice that.
If it fails the detection do we need to manually close the linked issue https://github.com/tensorflow/tensorflow/issues/53300 ?",GitHub fails to notice that.
47265,rafikg,1066745350,2022-03-14 12:47:55,@mastergao57 No really,No really.
22623,ns96,1066690557,2022-03-14 11:49:20,"Seeing same issue on my end. I have a 4GB GTX 3050Ti with only about 2GB being reported by TensorFlow. Strange that this issue hasn't been resolved, or at least an explanation provided for at this point?",Seeing same issue on my end.
47265,mastergao57,1066328227,2022-03-14 4:26:55,"Sir, I met the same problem when I ran a similar yolov3 project. `model.predict_on_batch()` returns Nan after the first iteration. My code is in the picture. My gpu gtx1070, tensorflow==2.7.0. Have you solved the problem?
![image](https://user-images.githubusercontent.com/62379808/158105049-b9f36339-40a2-4d9d-bfc9-cce29ff02ccf.png)",I met the same problem when I ran a similar yolov3 project.
55208,tilakrayal,1066310827,2022-03-14 3:47:59,"@GodsNightmare ,
We see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]","""We see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]"""
55202,tilakrayal,1066310140,2022-03-14 3:45:54,"@shubhambagwari ,
We see that you are using tf version 1.15, 1.x is not actively supported, please update to latest tensorflow v2.8 and let us know if you are facing same issue.","""We see that you are using tf version 1.15, 1.x is not actively supported, please update to latest tensorflow v2.8 and let us know if you are facing same issue."""
54868,gadagashwini,1066283781,2022-03-14 2:42:41,"@swapnilsayansaha,
```
import logging
logging.getLogger('tensorflow').disabled = True
```
Take a look at comment on similar issue [#8340](https://github.com/tensorflow/tensorflow/issues/8340#issuecomment-294618310) (`sys.flags.interactive = False`)",import logging logging.getLogger('tensorflow').disabled = True
44266,jvishnuvardhan,1065698279,2022-03-12 0:11:29,@daniel-pp @dakshvar22 Sorry for late response. Can you please open this issue with the standalone code in Keras-team/keras repository? I am not able to move this to keras repo as it is outside of TF repository. Is it possible for one of you to open it in keras-team/keras repo? Keras team is focussed on resolving issues in Keras repo. Sorry for the inconvenience. Thanks,"""I am not able to move this to keras repo as it is outside of TF repository."""
55190,HimGautam,1065373152,2022-03-11 18:27:49,"I tried but my machine when I install these versions of cuda and cudnn, tensorflow is not able to access gpu on my system.","I tried but my machine when I install these versions of cuda and cudnn, tensorflow is not able to access gpu on my system."
54788,bixia1,1065262403,2022-03-11 16:13:46,@DEKHTIARJonathan would you please rebase?,would you please rebase?
54714,gmernov,1065049127,2022-03-11 12:00:50,"As for now, it doesn't want to build even with 5.0.0 bazel.
![image](https://user-images.githubusercontent.com/4686690/157863092-ef7832a0-e462-4687-80aa-fa4fed9581de.png)
/root/.cache/bazel/_bazel_root/2b5c4e51f9e1005b4128ae187b9d110f/server/jvm.out - has nothing in","""It doesn't want to build even with 5.0.0 bazel."""
55178,ranvirdesai,1064999765,2022-03-11 10:53:03,"@chunduriv @andre-bauer Thanks for the response.
I tried running vertex ai custom job using suggested method.
I initialized strategy right after importing tf, it did not give error but training was also not done.
training was for only one epoch and takes 2min in my local, but I think it went in never ending loop on vertex ai.","I initialized strategy right after importing tf, it did not give error but training was also not done."
44266,daniel-pp,1064911398,2022-03-11 9:03:35,"@dakshvar22 I could not find a workaround, and I am also wondering if there has been any work on the issue.","I could not find a workaround, and I am also wondering if there has been any work on the issue."
33255,henaff,1064897026,2022-03-11 8:44:10,What about tensorflow-cpu ? There is no options to lower memory usage like tensorflow for gpu. I've 32GB RAM with a 5900x and it just kill when memory is full like a bad c program,No options to lower memory usage like tensorflow for gpu.
54899,taegeonum,1064866313,2022-03-11 8:00:36,"@gadagashwini I also added github.com cert to java keystore, but still got the same error. Is there any way to change `https` to `http`?","I also added github.com cert to java keystore, but still got the same error."
54104,bschnurr,1064599814,2022-03-10 23:09:11,"tf.data doens't have an issue
![image](https://user-images.githubusercontent.com/1946977/157770196-0556c2b9-3fd3-45f2-8843-0c7dec089907.png)
i'm just looking at anything that is loaded with `_LazyLoader`
and adding a `if _typing.TYPE_CHECKING:` only normal import
```
losses = _LazyLoader(""losses"", globals(), _keras_package + ""losses"")
metrics = _LazyLoader(""metrics"", globals(), _keras_package + ""metrics"")
```",tf.data doens't have an issue ![image](https://user-images.githubusercontent.com/1946977/157770196-0556c2b9-3fd3-45f2-8843-0c7dec089907)
54482,mihaimaruseac,1064400098,2022-03-10 19:10:42,"Yes, probably broken by c6531b8bcb83d1314acc96360156b549afecb377 LLVM/MLIR","""probably broken by c6531b8bcb83d1314acc96360156b549afecb377 LLVM/MLIR"""
54482,yimeisun123,1064366282,2022-03-10 18:28:49,"I checked the logs for ""Ubuntu CPU - Internal CI build failed"" and ""Intel® oneDNN -- Community CI Build"", they have the same two test case failures and the failures are not related to this PR change.","I checked the logs for ""Ubuntu CPU - Internal CI build failed"" and ""Intel® oneDNN -- Community CI Build"", they have the same two test case failures and the failures are not related to this PR change."
23803,mohantym,1064321272,2022-03-10 17:35:03,Hi @ClaudioCimarelli ! You are using older versions(1.x versions) of Tensorflow which is not supported any more. Have you checked this [thread ](https://its.tntech.edu/display/MON/Installing+TensorFlow+in+Your+HPC+Account)on using Tensorflow on HPC cluster though?,"""You are using older versions(1.x versions) of Tensorflow which is not supported any more."""
55190,HimGautam,1064127965,2022-03-10 14:33:52,"No, v2.8 is not supported by CUDA 10.1. So that's why I have not checked it.","No, v2.8 is not supported by CUDA 10.1."
55189,dwightnw,1063964932,2022-03-10 11:39:28,"For some reason I cannot properly add my code to this log. Some of it is correctly recognized and put in gray, but a lot of it is not",I cannot properly add my code to this log.
55123,sheepmaster,1063883429,2022-03-10 10:09:33,"I don't have the model that you've been using, but you will definitely also have to change the size in native code (https://github.com/tensorflow/examples/blob/master/lite/examples/super_resolution/android/app/src/main/cc/SuperResolution.h#L35).","I don't have the model that you've been using, but you will definitely also have to change the size in native code (https://github.com/tensorflow/examples/blob/master/lite/examples/super_resolution/android/app/src/main/cc/SuperResolution.h#L35)."
30746,pemoi1982jpm,1063602166,2022-03-10 2:59:18,"Is there a solution for this?
I am trying to use tf2onnx tool but I get an error:
`python3.7 -m tf2onnx.convert --saved-model . --output model.onnx`
```
File ""/home/jupyter/.local/lib/python3.7/site-packages/tf2onnx/tfonnx.py"", line 20, in <module>
from tensorflow.tools.graph_transforms import TransformGraph
ModuleNotFoundError: No module named 'tensorflow.tools.graph_transforms'
```
Using tensorflow 2.8.0","""I am trying to use tf2onnx tool but I get an error: python3.7 -m tf2onnx.convert --saved-model . --output model.onnx"""
54818,ArrowIntoTheSky,1063590037,2022-03-10 2:30:02,"Thanks @rthadur .
Here are more `tf.experimental.numpy` APIs that also lack `bfloat16` support:
```
tf.experimental.numpy.cos
tf.experimental.numpy.exp
tf.experimental.numpy.log1p
tf.experimental.numpy.expm1
tf.experimental.numpy.cosh
tf.experimental.numpy.tanh
tf.experimental.numpy.exp2
tf.experimental.numpy.log2
```",tf.experimental.numpy APIs that also lack bfloat16 support
54476,sachinprasadhs,1063427827,2022-03-09 22:17:04,"All the OPS in TFLite is not having support for `int8`, you can use the flag for OPS which supports TFLite quint8 in their input and output [here](https://www.tensorflow.org/mlir/tfl_ops).
To avoid error, like you have tried you can mention like below `converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]`
Which would apply `TFLITE_BUILTINS_INT8` if OP is supported, else it will apply `SELECT_TF_OPS`","""All the OPS in TFLite is not having support for int8, you can use the flag for OPS which supports TFLite quint8 in their input and output [here](https://www.tensorflow.org/mlir/tfl_ops). To avoid error, like you have tried you can mention like below converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.O"
55178,andre-bauer,1063131241,2022-03-09 16:47:15,"This is not the first line of code... Did you check the GPU memory at this line? you could have ```
def do_something(var_name=tf.constant(0)):
pass
```
somewhere above main or in any imported module and it would already fail",This is not the first line of code...
53812,deven-amd,1063125163,2022-03-09 16:41:22,"@cheshire, please re-review....I have updated this branch with a subset of the changes you reviewed for the PR to introduce `RocComputeCapability` in the rocm fork.
Also I see that the `Code Check - Changed Files` is failing, but am unable to determine what exactly is causing the failure...please let me know if that is something I need to look into","I have updated this branch with a subset of the changes you reviewed for the PR to introduce RocComputeCapability in the rocm fork. Also I see that the Code Check - Changed Files is failing, but am unable to determine what exactly is causing the failure...please let me know if that is something I need to look into."
53760,bixia1,1063048013,2022-03-09 15:35:10,TensorRT7_2 still fails with the same message.,"""TensorRT7_2 still fails with the same message."""
44159,xucian,1062972503,2022-03-09 14:23:31,"any updates? it's been 2 years after all. proposed solutions are better than nothing, but their hackiness level is not on par with the prestige TF+NVIDIA have in the ML world","""it's been 2 years after all"""
55123,sheepmaster,1062916491,2022-03-09 13:22:44,"Hi @douzaikongcheng, if you change the size of the input image, you will also need to retrain/re-convert the model (see https://github.com/tensorflow/examples/blob/master/lite/examples/super_resolution/ml/super_resolution.ipynb) and change the corresponding values in the native code. I suspect the mismatch between these is what is causing the crash.","""I suspect the mismatch between these is what is causing the crash."""
55148,elfringham,1062857226,2022-03-09 12:10:44,"This can be avoided by using ""--build_tests_only"" on the command line.","""-- build_tests_only"" on the command line."
55178,ranvirdesai,1062829351,2022-03-09 11:34:39,"@chunduriv Its just this code raises error while initializing strategy.
if __name__==""__main__"":
mirrored_strategy = tf.distribute.MultiWorkerMirroredStrategy()",This code raises error while initializing strategy.
55178,andre-bauer,1062695596,2022-03-09 8:58:50,Can you put a breakpoint before this line `mirrored_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()` and check if any gpu memory is allocated already by TF? if so you accidentally created a tf.Var or Tensor which you are not allowed to before creating MultiWorkerMirroredStrategy,"""Can you put a breakpoint before this line mirrored_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() and check if any gpu memory is allocated already by TF?"""
53260,youchangkim,1062459750,2022-03-09 1:25:46,"@penpornk In the code below, it works fine when I comment out `@tf.function`. So it seems to be a problem with graph execution.
```python
import numpy as np
import tensorflow as tf
np.random.seed(0)
@tf.function
def my_func():
print('tracing')
x = np.ones([1, 1, 384, 1, 1], dtype=np.float16)
y = tf.constant(x, name='97f6762f328ca2b7cceaf2851e0c01a6')
z = y * 3
return z, y
z, y = my_func()
print(y.numpy().squeeze())
assert np.array_equiv(y, np.ones(y.shape))
```",@tf.function
28601,bhack,1062240389,2022-03-08 21:41:38,@seanpmorgan I think that as we cannot have `keras_tensor` exposed this could be closed.,I think that as we cannot have keras_tensor exposed
54657,rmlarsen,1062220146,2022-03-08 21:16:37,@mihaimaruseac Why was this closed?,Why was this closed?
41460,StanislawAntol,1062081806,2022-03-08 18:32:46,"I also ran into this issue on TF 2.5, which is using an outdated version of keras-preprocessing (i.e., v1.1.2). This issue was fixed via this [PR](https://github.com/keras-team/keras-preprocessing/pull/318), but had not made its way into the current TF versions. Though it seems like it will arrive soon based on [this commit](https://github.com/keras-team/keras/commit/373ad97c72ed1ac4b6898e85b2cfd7b016e4b469) from a month ago.","I also ran into this issue on TF 2.5, which is using an outdated version of keras-preprocessing (i.e., v1.1.2)."
55128,deeplearner2022,1062009135,2022-03-08 17:11:55,"I have the latest updates and, its still the same error @chunduriv <img width=""470"" alt=""Screen Shot 2022-03-08 at 6 14 16 PM"" src=""https://user-images.githubusercontent.com/101177094/157290129-92b2b3f4-ea28-4192-b825-6d22f8a21bce.png"">","I have the latest updates and, its still the same error"
55133,elfringham,1061894297,2022-03-08 15:23:01,Made moot by https://github.com/tensorflow/tensorflow/commit/214d2ed3ff4228e92f246873d3ff535bdb7de35a,"""Made moot by"""
54382,rsanthanam-amd,1061704026,2022-03-08 12:01:00,Closing this and opening a new PR due to git rebase error.,Closing this and opening a new PR due to git rebase error.
9968,mohantym,1061666594,2022-03-08 11:12:57,Hi @singlasahil14 ! It seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. Attaching relevant [thread](https://stackoverflow.com/questions/45193238/why-out-channels-must-be-greater-then-channel-multiplier-in-channels-in-pointw?noredirect=1&lq=1) for reference though.,"""It seems you are using older versions(1.x versions) of Tensorflow which is not supported any more."""
54764,chsigg,1061590952,2022-03-08 9:46:21,"No, this PR now touches 200 files.","No, this PR now touches 200 files."
54382,chsigg,1061590558,2022-03-08 9:45:56,"No, this PR now touches 484 files.","No, this PR now touches 484 files."
55067,jinzzasol,1061396289,2022-03-08 4:32:21,"@mohantym Now it is fixed and Tensorflow recognises GPU. However, nothing has changed. The original issues are still there. I also launched jupyter notebook in docker and run the examples, but nothing is different.","""I also launched jupyter notebook in docker and run the examples, but nothing is different."""
54832,rameshkunasi,1061391650,2022-03-08 4:22:40,"@sushreebarsa , Can you please add this as a serious issue?",Can you please add this as a serious issue?
55067,jinzzasol,1061344790,2022-03-08 2:34:18,"> Hi @jinzzasol ! Did you check instructions for [WSL ubuntu](https://docs.nvidia.com/cuda/wsl-user-guide/index.html) on Nvidia site too?
@mohantym Yes, and I tried. But I don't think it helped.","""But I don't think it helped."""
54463,njzjz,1061343477,2022-03-08 2:31:17,"It may be a bug of cublas. [cublas 11.4](https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cublas-11.4.0) resolved an issue:
> Some gemv cases were producing incorrect results if the matrix dimension (n or m) was large, for example 2^20.
In your case, m=1638400>2^20. As cublas is not open-source, it's unclear what versions of cublas have this issue.","""It may be a bug of cublas."""
52526,victorconan,1061262623,2022-03-07 23:56:20,Having the same issue. Can we reopen this issue?,Having the same issue.
55082,drivanov,1061074134,2022-03-07 19:53:46,"@bixia1 : This PR is a replacement for [PR#54230](https://github.com/tensorflow/tensorflow/pull/54230).
I created it because there are a lot of merge conflicts with some current tensorflow files.",I created it because there are a lot of merge conflicts with some current tensorflow files.
42738,JuanVargas,1060668259,2022-03-07 13:08:05,same problem. Ubuntu 21.10. Hey TF guys: Wake up. CUDA/GPUs work just fine with Julia on same machine.,"""CUDA/GPUs work just fine with Julia on same machine."""
55023,tomandjake,1060320679,2022-03-07 8:25:29,"Sorry I has already wirite things according to that template and I do not think this ""little and understandable bug"" need other information such as My environment and etc. I am not convenient to check those information :(",I am not convenient to check those information :(
45753,baixueangela,1060193186,2022-03-07 4:53:26,"Still have this issue, the above bazel build command somehow does not solve the issue.
Here is the error:
```
The thread 0x2ebc has exited with code 0 (0x0).
Exception thrown at 0x00007FF7A7EB6DE3 in FYDP_Demo.exe: 0xC0000005: Access violation reading location 0xFFFFFFFFFFFFFFFF.
Unhandled exception thrown: read access violation.
**_My_data** was 0xFFFFFFFFFFFFFFF7.
```","Still have this issue, the above bazel build command somehow does not solve the issue."
54868,swapnilsayansaha,1060118908,2022-03-07 2:12:23,"I tried this also before posting the problem here. This also doesn't work. When you are not using eager execution (tf.disable_eager_execution()), whenever you compile (using model.compile()) or call a model (e.g. using model.predict(data) or model(data)), the logging information I showed above appears.","""This also doesn't work."""
54868,gadagashwini,1060114539,2022-03-07 2:03:41,"@swapnilsayansaha, Try this ```
import tensorflow as tf
tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)
```",import tensorflow as tf
54900,Bidski,1060045859,2022-03-06 21:51:28,"@tilakrayal the code is closed source so I cant provide that and constructing a MWE that show-cases the problem is difficult since I dont know where the problem lies. Are you able to provide any insight at all into what might be going on so that I may try and create a MWE for you?
I tried building tensorflow from source (from current master) to see if the commit I referenced solved the problem and it didnt seem to help.",I tried building tensorflow from source (from current master) to see if the commit I referenced solved the problem and it didnt seem to help.
55027,callmekofi,1059986674,2022-03-06 15:47:35,I am using pycharm version 2021.3 on apple macbook. I turned off my vpn once i sign in my google account but the result is the same.,I am using pycharm version 2021.3 on apple macbook. I turned off my vpn once i sign in my google account but the result is the same.
54435,BlueskyFR,1059952212,2022-03-06 12:16:37,"> Thanks for the links. They are very helpful. My apologies for rehashing an issue that is clearly already recognized.
Don't apologize, the issue has been around since TF 2.0 release and nothing was ever done about it, it is unacceptable that every new release breaks PyLance.
I am not saying that this is either TF or PyLance's fault, but that the larger part of the community probably using VSCode, the support for it should be taken more seriously IMO.","""Don't apologize, the issue has been around since TF 2.0 release and nothing was ever done about it, it is unacceptable that every new release breaks PyLance."""
33303,NAM-hj,1059901652,2022-03-06 6:03:22,"I use Tensorflow 2.8.0. I have the same problem.
With or Without assigning name of each inputs and outputs of my model, TFLite converter change orders",I have the same problem.
47268,mihaimaruseac,1059656519,2022-03-05 2:34:50,"These are deprecated and will be removed in TF 2.9.
There is no owner for the code, which is why this PR stalled for so long. Apologies.","No owner for the code, which is why this PR stalled for so long."
54868,swapnilsayansaha,1059415075,2022-03-04 18:30:35,No that doesn't work. I actually have it set to level 3. It does not suppress non-eager execution information.,No that doesn't work.
34679,Noltibus,1059142187,2022-03-04 13:00:56,"According to the documentation here: https://www.tensorflow.org/lite/performance/gpu_advanced none of the operations I listed above are supported, yet, so this is an ongoing issue for me.","None of the operations I listed above are supported, yet, so this is an ongoing issue for me."
54973,leondgarse,1059091785,2022-03-04 11:45:37,"@mohantym Uh, right, we can run scripts in colab... Try this [tf_280_xla_test.ipynb](https://colab.research.google.com/drive/1LTVJ7jRRzsODzMuPB-svocV4jcbx1SYY?usp=sharing). Just setting `CUDA_VISIBLE_DEVICES='1'` leaves it no GPU to use in yours, my bad. Updated commands.","""Just setting CUDA_VISIBLE_DEVICES='1' leaves it no GPU to use in yours, my bad."""
54502,yiyaz,1059085418,2022-03-04 11:35:08,@sampathweb I did install tensorflow (i.e. tensorflow-macos and tensorflow-metal) as you suggested but it is not possible for me to install other libraries which have tensorflow as a dependency. Is this that a bug?,I did install tensorflow (i.e. tensorflow-macos and tensorflow-metal) as you suggested but it is not possible for me to install other libraries which have tensorflow as a dependency.
54868,gadagashwini,1059014332,2022-03-04 9:59:10,"@swapnilsayansaha, You can suppress all debugging logs using below code snippet. Try setting log level before importing tf ```
import os
os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""2""
import tensorflow as tf
```","""Can suppress all debugging logs using below code snippet."""
54231,yongtang,1058938692,2022-03-04 8:19:04,The issue is actually in PyObject conversion side I believe. It should have been fixed in: https://github.com/tensorflow/tensorflow/pull/54441,The issue is actually in PyObject conversion side I believe. It should have been fixed in: https://github.com/tensorflow/tensorflow/pull/54441.
17417,mohantym,1058817232,2022-03-04 4:30:32,"It is still replicating in[ 2.8](https://colab.sandbox.google.com/gist/mohantym/a6746a2aa7a9988ed10c8cde68c8f996/github_17417.ipynb), [2.9](https://colab.sandbox.google.com/gist/mohantym/c936dafa04dd9e4cad6dbd7644dbf90b/github_17417.ipynb#scrollTo=E4Z4rqTEK1G5) and [nightly](https://colab.sandbox.google.com/gist/mohantym/32e8e81e1700fc16a1bcd21b5d83f65d/github_17417.ipynb#scrollTo=E4Z4rqTEK1G5) (2.10.0dev) version.","It is still replicating in[ 2.8](https://colab.sandbox.google.com/gist/mohantym/a6746a2aa7a9988ed10c8cde68c8f996/github_17417.ipynb), [2.9](https://colab.sandbox.google.com/gist/mohantym/c936dafa04dd9e4cad6dbd7644db"
54899,taegeonum,1058803881,2022-03-04 3:59:50,"@gadagashwini I got the following error
```
ERROR: /home/user/tensorflow/WORKSPACE:25:1: name 'http_archive' is not defined
ERROR: error loading package 'external': Package 'external' contains errors
```",I got the following error  ERROR: /home/user/tensorflow/WORKSPACE:25:1: name 'http_archive' is not defined ERROR: error loading package 'external': Package 'external' contains errors .
53437,SandSnip3r,1058671022,2022-03-03 23:39:18,I am currently working to resolve internal conflicts,I am currently working to resolve internal conflicts.
54749,patlevin,1058582623,2022-03-03 22:41:10,"Try to [Activate Developer Mode](https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development) - this did the trick for me.
The error occurs if Windows couldn't create a symbolic link. The `mklink`-command requires administrator privileges for some reason unless dev mode is enabled.",The error occurs if Windows couldn't create a symbolic link.
54104,bschnurr,1058383268,2022-03-03 18:57:57,@qlzh727 @gbaned I think the ios build was missing python3.. not sure how to retrigger,I think the ios build was missing python3.. not sure how to retrigger.
54856,rthadur,1058339668,2022-03-03 18:11:49,"I am not able to reproduce the same if i run as below ```
import tensorflow as tf
print(tf.__version__)
import numpy as np
print(tf.experimental.numpy.floor_divide(0,0 )) # InvalidArgumentError: Integer division by zero [Op:FloorDiv]
```
can you please check","""InvalidArgumentError: Integer division by zero"""
48081,CoffeRobot,1058073792,2022-03-03 14:04:45,I have the same issue. I have been trying to run few quantized models on a Motorola moto G200 and a One Plus 9 pro and the model is very slow on both phones. I guess I cannot really use the DSP of the phones.,I have been trying to run few quantized models on a Motorola moto G200 and a One Plus 9 pro and the model is very slow on both phones.
41990,daveabiy,1058022064,2022-03-03 13:05:06,"This could sometimes happen because of different codes accessing the gpu one after another. I suggest that you use, > `sudo fuser -v /dev/nvidia*`
and kill the python or code PID using
> `sudo kill -9 ""PID code here""`.
This will free the gpu memory.","""could sometimes happen because of different codes accessing the gpu one after another"""
54747,peter-jp,1057676392,2022-03-03 5:19:27,"There is no TF2.8 version available for IBM power 9 PPCLE64 architecture. I use the following channel for installing packages
conda config --prepend channels \
https://public.dhe.ibm.com/ibmdl/export/pub/software/server/ibm-ai/conda/",No TF2.8 version available for IBM power 9 PPCLE64 architecture.
54687,gadagashwini,1057615637,2022-03-03 2:56:36,"@SohaK, When i tried to save the Fine_tune_bert model with same configuration as you mentioned, it has only one .pb file `saved_model.pb`. Size of the model ~3.4MB. Use keras load model to load your model.
```
from tensorflow import keras
new_model = keras.models.load_model(""save_model_keras"")
new_model.summary()
```","""It has only one .pb file saved_model.pb. Size of the model 3.4MB"""
54274,andmis,1057432039,2022-03-02 21:54:45,"- It's not an operation, it's a type annotation.
- The error message says, `Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, 'export AUTOGRAPH_VERBOSITY=10') and attach the full output.`
- Type annotations like `n: int = 3` work fine, with no error. Why is `n: int = 3` ok but `n: int` not ok?
- This is obviously a bug in the parser code.","- It's not an operation, it's a type annotation."
54423,BptGrm,1057030026,2022-03-02 15:01:41,"I have been able to install TF like this previously. My issue was that those packages used to be wrapped in a single metapackage that, if I remember correctly, would install the correct verions of everything needed. It was, in my opinion, an easier and safer way to install TF, and I was wondering why it isn't available anymore.",I have been able to install TF like this previously.
54249,akuegel,1056802519,2022-03-02 11:10:02,"I think we have seen segfaults also in other cases when using the gcc 8.3 compiler. Can you please try a newer version? gcc-9.3.1 should work for example:
https://github.com/tensorflow/build/blob/nitin/manylinux2014/tf_sig_build_dockerfiles/builder.devtoolset/build_devtoolset.sh#L115",I think we have seen segfaults also in other cases when using the gcc 8.3 compiler.
54747,peter-jp,1056371243,2022-03-02 6:37:57,"I can reduce the batch size but cannot reduce the input size (as it must be equal to the number of columns in the dataset). In IBM conda, the latest version available for Tensorflow-gpu is 2.1.3, no other versions like TF 2.8 is available for ppcle64 arch.
I will use tf.device and will update the result.
FYI: Meanwhile, I used Tensorflow Mirrored strategy within one GPU, but that also resulted in segmentation fault after some more epochs than normal implementation.","""In IBM conda, the latest version available for Tensorflow-gpu is 2.1.3, no other versions like TF 2.8 is available for ppcle64 arch."""
54507,jimch3n,1056074677,2022-03-02 2:31:20,"I found that probably my misunderstanding, I am able to match the keras model.predict, with tflite interpreter output in python, but some how unable to match gru written in c , sorry to interrupt. Please close the issue. Thanks.","I am able to match the keras model.predict, with tflite interpreter output in python, but some how unable to match gru written in c , sorry to interrupt. Please close the issue. Thanks."
54249,yaochengji,1056049533,2022-03-02 1:47:00,"I'm using gcc 8.3.0.
```
gcc (Debian 8.3.0-6) 8.3.0
Copyright (C) 2018 Free Software Foundation, Inc.
This is free software; see the source for copying conditions. There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
```",gcc (Debian 8.3.0-6) 8.3.0
53938,awpr,1055739316,2022-03-01 18:36:08,Still trying to figure out how to get it to retry.,Still trying to figure out how to get it to retry.
53385,IAL32,1055665422,2022-03-01 17:10:36,"@mihaimaruseac I think I have a mismatch of `clang-format` version, as executing `clang-format --style=Google -i tensorflow/python/lib/io/record_io_wrapper.cc` outputs a different result from the one in the Code Check step.
I am using Ubuntu 18.04 with `clang-format --version`:
```
clang-format version 6.0.0-1ubuntu2 (tags/RELEASE_600/final)
``` Any way of knowing which clang-format version it's used in the CI step?","""I think I have a mismatch of clang-format version, as executing clang-format --style=Google -i tensorflow/python/lib/io/record_io_wrapper.cc outputs a different result from the one in the Code Check step."""
54301,christopherbate,1055611455,2022-03-01 16:15:07,"rebased, squashed fixes for the failing test fix. Issue was the graph_def being modified by the MaybeRewriteCast() function was a copy of the input graph def, when really it needed to be a reference.","rebased, squashed fixes for the failing test fix."
54406,ToonTalk,1055603588,2022-03-01 16:07:42,"The typo is in https://blog.tensorflow.org/2022/02/boost-your-models-accuracy.html?linkId=8033010
...layer with a on top...",The typo is in https://blog.tensorflow.org/2022/02/boost-your-models-accuracy.html?linkId=8033010 ...layer with a on top.
54578,Leemy9612,1055351278,2022-03-01 11:54:42,">I want to build tf from source on windows, so I follow the [https://github.com/tensorflow/docs/blob/cbaa96238c823b39e4433c843823895cef903343/site/en/install/source_windows.md](url), but now, the bazel 4.2.1 is too out-of-date to build tf. and it report that `Please upgrade your bazel installation to version 4.2.2 or higher to build TensorFlow!`. So I tried the version 5.0.0 which is also useless.","""I want to build tf from source on windows, so I follow the [https://github.com/tensorflow/docs/blob/cbaa96238c823b39e4433c843823895cef903343/site/en/install/source_windows.md](url), but now, the bazel 4.2.1 is too out-of-date to build tf. and it report that Please upgrade your bazel installation to version 4.2.2 or higher to build TensorFlow"
26546,CL500Coupe,1055269719,2022-03-01 10:21:09,I'm getting error with: import tensorflow.compat.v2 as tf,I'm getting error with: import tensorflow.compat.v2 as tf.
36465,andreped,1055260411,2022-03-01 10:12:06,"I agree with @3a2l. This has **not** been resolved. Need a proper, stable solution for freeing GPU memory through the TF API. This is relevant for all programming languages that use TF. Strange that this does not exist in 2022.","""This has **not** been resolved"""
36465,3a2l,1055245816,2022-03-01 9:59:00,"No, this does not resolve the issue. What we are asking for is a clear and clean TensorFlow API function (some of us need it in the C API as well) to clear the memory TF allocated.","No, this does not resolve the issue. What we are asking for is a clear and clean TensorFlow API function (some of us need it in the C API as well) to clear the memory TF allocated."
54719,mpcallanan,1054832030,2022-03-01 0:04:23,"@bhack Hey, sorry, I just joined TF and I'm still learning this github integration (and am mortified that my placeholder text was apparently public!). I did not see either comment internally. I only saw your second comment, only from a Github email.","I did not see either comment internally. I only saw your second comment, only from a Github email."
53659,christopherbate,1054617601,2022-02-28 20:07:02,"I've been pulling individual commits out as separate PRs. The last one, which is still awaiting merge, is #54301","I've been pulling individual commits out as separate PRs. The last one, which is still awaiting merge, is #54301."
54301,bixia1,1054514515,2022-02-28 17:55:25,"I got this test failur
[casttest.log](https://github.com/tensorflow/tensorflow/files/8155552/casttest.log)
e",I got this test failur
30256,eyildiz-ugoe,1054365996,2022-02-28 15:22:15,"Same story. I installed it through Anaconda and it takes forever to toad TF.
Python 3.9.7
TF 2.4.1
CUDA 10.1
Cudnn 7.6.5 It's unbelievable that this was not solved for over a year. Time to switch to Pytorch, I guess.",I installed it through Anaconda and it takes forever to toad TF. Python 3.9.7 TF 2.4.1 CUDA 10.1 Cudnn 7.6.5 It's unbelievable that this was not solved for over a year.
53713,heeh,1054279510,2022-02-28 13:53:45,"Hi @nutsiepully It has been 26 days since this issue was assigned to you and I understand that this issue might not be your priority.
However, I need to get your response for moving forward. I don't mind even if this issue is not going to be fixed anytime soon.","I understand that this issue might not be your priority. However, I need to get your response for moving forward. I don't mind even if this issue is not going to be fixed anytime soon."
54648,wihobbs,1054252734,2022-02-28 13:23:05,"```python
sample_image = tf.io.read_file(str('/content/drive/MyDrive/1737/1737-01.tif'))
sample_image = tf.io.decode_jpeg(sample_image)
print(sample_image.shape)
```
There is no `decode_tiff` or equivalent AFAIK and the error message from this is displayed in the original issue.",python sample_image = tf.io.read_file(str('/content/drive/MyDrive/1737/1737-01.tif')) sample_image = tf.io.decode_jpeg(sample_image) print(sample_image.shape)
54572,akuegel,1054174522,2022-02-28 11:45:25,"> I got below error message.
> > ```
> The specified bucket does not exist.
> ```
I guess it is an infrastructure problem. I have notified our infrastructure team.",I got below error message.
54572,lipracer,1054165182,2022-02-28 11:33:32,"I got below error message.
```
The specified bucket does not exist.
````",I got below error message.
50575,neqkir,1053904728,2022-02-28 5:27:35,"Hi, same error/warning on AMD GPUs ![image](https://user-images.githubusercontent.com/89974426/155928912-e4ab8830-f0cf-47f8-b1b0-29fe808c1303.png)
AFAIK batch size is fixed to 64 in my code.",AFAIK batch size is fixed to 64 in my code.
54514,sampathweb,1053886858,2022-02-28 4:49:44,"@Mather10 - You may want to use ```
from tensorflow.keras.layers import LayerNormalization
```
Seems to be duplicate of #54637",Seems to be duplicate of #54637
40065,swghosh,1053667253,2022-02-27 20:17:13,"Gentle ping @jhseu @mihaimaruseac This problem is still around atleast on Fedora 35 with latest version of TensorFlow. (expecting the same to occur on other RPM based distros)
I could use this workaround for the time being:
```sh
sudo ln -s /etc/ssl/certs/ca-bundle.crt /etc/ssl/certs/ca-certificates.crt
```",This problem is still around atleast on Fedora 35 with latest version of TensorFlow. (expecting the same to occur on other RPM based distros)
54401,mohantym,1053545227,2022-02-27 12:46:27,"Just replace this command. `import keras `
with
`from tensorflow import keras ` Can you please update the steps too in the template?",import keras
54582,alanpurple,1053487364,2022-02-27 10:48:00,"@mohantym I think I did have some misconcept, as soon as the program load cuda context, it takes almost all of gpu's memory, it's normal I guess....
sorry for misunderstanding( of cudnn )","I think I did have some misconcept, as soon as the program load cuda context, it takes almost all of gpu's memory, it's normal I guess..."
39240,arashnh11,1053103939,2022-02-27 4:29:50,"Looking at the [Keras-FasterRCNN repo it uses ""keras"" libraries and if one mixes that up with tf.keras equivalent libraries, this error would occur. Getting all modules from tensorflow.keras stack may be the fix",Getting all modules from tensorflow.keras stack may be the fix.
52160,Mblakey,1052304888,2022-02-26 16:51:30,@juancrescente why are you installing bazel via conda? Surely easier to have brew install bazelisk and let that take care of bazel versions? You need full Xcode with the command tools,Why are you installing bazel via conda? Surely easier to have brew install bazelisk and let that take care of bazel versions? You need full Xcode with the command tools.
54502,juancresc,1051297673,2022-02-25 21:50:48,"@sampathweb I'm able to do this inside conda, but then it [looks like](https://tensorflow-diy.readthedocs.io/en/latest/tensorflow/go/README.html) I need to install tf from source in order to make golang tf work","I'm able to do this inside conda, but then it [looks like](https://tensorflow-diy.readthedocs.io/en/latest/tensorflow/go/README.html) I need to install tf from source in order to make golang tf work."
54612,sampathweb,1051293201,2022-02-25 21:43:29,"@serdarakyol - Did you downgrade the CUDA to 11.2? Looking at Nvidia docs it looks like the display driver and cuda driver do not match - https://docs.nvidia.com/deploy/cuda-compatibility/#check-for-compatibility-support
I think TensorFlow should work for 11.2+ so you may not need to downgrade the driver or Cuda version. I have not verified this, but believe 11.2+ are compatible with 11.2.","""Looking at Nvidia docs it looks like the display driver and cuda driver do not match"""
52160,lmiklosko,1051185180,2022-02-25 20:01:56,"@juancrescente I had exactly the same issue, until I reinstalled bazel using brew to 4.2.1 and fresh installing XCode through AppStore. Try to do `bazel clean` before reinstalling bazel though.And I re-did the configure step as well!","I had exactly the same issue, until I reinstalled bazel using brew to 4.2.1 and fresh installing XCode through AppStore."
54508,DEKHTIARJonathan,1051137821,2022-02-25 19:35:27,@bixia1 : sorry you're gonna need to re-approve. PyLint was failing I had to fix a few indentations,"""I had to fix a few indentations"""
54249,yaochengji,1051081983,2022-02-25 18:13:00,"> Is this still happening? I just tried it, and for me it seems to work now.
@akuegel Thanks. But this bug still appears on my side. I'm using the latest commit `3362b358bbad2e6d`.","""But this bug still appears on my side."""
53710,penpornk,1051080984,2022-02-25 18:11:38,"Cc'ing @jsimsa for the answer. IIUC, `tf.data` is meant to be run only on TF native devices be design.","IIUC, tf.data is meant to be run only on TF native devices be design."
53144,vanakema,1051040484,2022-02-25 17:17:07,Any update on this? It's been a minute and neither this issue nor #53271 have received any updates since Dec,It's been a minute and neither this issue nor #53271 have received any updates since Dec.
54582,alanpurple,1050916201,2022-02-25 14:46:57,"@mohantym so almost all 24GB of GPU memory being taken right after load pretrained BERT-LARGE is normal?
In that case, this is should be closed","almost all 24GB of GPU memory being taken right after load pretrained BERT-LARGE is normal? In that case, this is should be closed."
32052,arianmaghsoudnia,1050913937,2022-02-25 14:44:07,I have the same problem in TF 2.8.0,I have the same problem in TF 2.8.0.
54540,ae20cg,1050843767,2022-02-25 13:14:50,"@mohantym Since the argument that is causing the code to fail is 'optional', I removed it and the code runs is able to run train. And the tensor that is causing an error does not come from a list.
I have updated the code to get rid of the empty lists, still the same error.","""And the tensor that is causing an error does not come from a list"""
54502,juancresc,1050790756,2022-02-25 11:57:24,"@tilakrayal Just to add more information, I'm trying to run a golang app that uses tensorflow. I was able to install it using conda but it's not working for go via that approach. I'm also not able to run TF even in docker (see [this](https://stackoverflow.com/questions/68105073/tensorflow-error-when-used-as-docker-baseimage) and [this](https://github.com/docker/for-mac/issues/5342)), so I'm a little bit lost.","I'm also not able to run TF even in docker (see [this](https://stackoverflow.com/questions/68105073/tensorflow-error-when-used-as-docker-baseimage) and [this](https://github.com/docker/for-mac/issues/5342)), so I'm a little bit lost."
54505,bixia1,1050180388,2022-02-24 19:17:04,"Remove this from PR description to here
@bixia1 : for review
@tfeher @Nyrio: CC","""Remove this from PR description to here"""
54499,sampathweb,1050103601,2022-02-24 17:45:45,"@ebonat - `pip install tensorflow` will install a version thats compatible with GPU and CPU. So it gives you that warning messages. If you don't want to see warning messages and want to install CPU only version, you could - `pip install tensorflow-cpu` that's a smaller wheel file for CPU only version","""So it gives you that warning messages"""
54498,sampathweb,1050101952,2022-02-24 17:43:35,"CUDA version. needs to be 11.2+, from your config it looks like its 10.1. So, please see if you can upgrade your local CUDA version to 11.x. 11.2 builds should be compatible with anything 11.x","CUDA version. needs to be 11.2+, from your config it looks like its 10.1. So, please see if you can upgrade your local CUDA version to 11.x. 11.2 builds should be compatible with anything 11.x."
54516,Leemy9612,1049870103,2022-02-24 13:38:26,"I've tried change the version of bazel , but it not works.","I've tried change the version of bazel , but it not works."
52160,Mblakey,1049833361,2022-02-24 12:58:26,"@janhartman yes, tried with both standalone command line tools and full Xcode install, error persists","""Error persists"""
52160,Mblakey,1049828278,2022-02-24 12:51:45,"> > ```
> > bazel build --config opt --cpu=darwin_arm64 --host-cpu=darwin_arm64 //tensorflow/tools/lib_package:libtensorflow
> > ```
> > > ERROR: --host-cpu=darwin_arm64 :: Unrecognized option: --host-cpu=darwin_arm64
Same result",ERROR: --host-cpu=darwin_arm64 :: Unrecognized option: --host-cpu=darwin_arm64 Same result.
52160,juancresc,1049806530,2022-02-24 12:23:48,"> ```
> bazel build --config opt --cpu=darwin_arm64 --host-cpu=darwin_arm64 //tensorflow/tools/lib_package:libtensorflow
> ```
ERROR: --host-cpu=darwin_arm64 :: Unrecognized option: --host-cpu=darwin_arm64",ERROR: --host-cpu=darwin_arm64 :: Unrecognized option: --host-cpu=darwin_arm64
33688,stefan-falk,1049655038,2022-02-24 9:27:36,"@WingsOfPanda No, I wasn't able to make this work properly. There is [tensorflow/mesh](https://github.com/tensorflow/mesh) (unfortunately it's for TF 1) which seems to be supporting model parallelism. Idk if there's already something we can use for TF2 but my focus has shifted by now.",I wasn't able to make this work properly.
44177,N-damo,1049634553,2022-02-24 9:03:01,"miss the same problem,exspecially in blank label=0","miss the same problem,exspecially in blank"
30448,deadsoul44,1049618938,2022-02-24 8:43:28,I still have this problem with TF 2.0.,I still have this problem with TF 2.0.
54498,JueonPark,1049542650,2022-02-24 6:41:19,"@sampathweb I tried with Python 3.9.7, yet it fails with the same error.
@mohantym I'll try with the tested configuration and let you know.","I tried with Python 3.9.7, yet it fails with the same error."
54155,MeghnaNatraj,1049126404,2022-02-23 19:17:17,"Hello @raminmohammadi, this will take a longer to debug, please give us a few days to take a look and get back to you.","""this will take a longer to debug, please give us a few days to take a look and get back to you."""
53367,mihaimaruseac,1048977002,2022-02-23 16:37:52,This has landed but GitHub fails to notice that.,GitHub fails to notice that.
43367,dsuedholt,1048753859,2022-02-23 12:55:55,"@terryheo I followed steps 1-3 of the workaround, added both `tensorflowlite.dll.if.lib` and `tensorflowlite_flex.dll.if.lib` as a Linker dependency and placed both `tensorflowlite.dll` and `tensorflowlite_flex.dll` in the application folder. I still receive the error message > ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.
Am I missing anything I should be doing?","I followed steps 1-3 of the workaround, added both tensorflowlite.dll.if.lib and tensorflowlite_flex.dll.if.lib as a Linker dependency and placed both tensorflowlite.dll and tensorflowlite_flex.dll in the application folder. I still receive the error message > ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply"
53025,creotiv,1048604114,2022-02-23 9:45:51,Have the same issue(,Have the same issue(.
53834,tilakrayal,1048560468,2022-02-23 8:50:53,"@chunduriv ,
While executing the given code in colab, i was facing different error and noticed session is being crashed.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/16f3ccd62f8b78bcc2e57746e847a423/untitled233.ipynb).","""I was facing different error and noticed session is being crashed."""
34025,davidycliao,1048464842,2022-02-23 5:41:14,"> Am using only 1 gpu and still am facing this retracing issue.
> After trying out verious fixes to this problem by searching over the internet, nothing worked. Including tf-nightly-gpu since its first of all not detecting my gpu.
> **So I have just downgraded from tf 2.3.0 to tf 2.2.1 and it has fixed this issue for now.**
I got the same issue. now downgrade to tf 2.2.1 working. Thanks","""After trying out verious fixes to this problem by searching over the internet, nothing worked."""
54298,karimnosseir,1048458321,2022-02-23 5:21:40,"@peyer np.
Sadly no, Snapdragon 888 doesn't leverage Hexagon NN API. They had a new APIs for executing on the DSP, and hexagon delegate doesn't work with it.","Sadly no, Snapdragon 888 doesn't leverage Hexagon NN API."
53698,andrei5055,1048315799,2022-02-22 23:41:57,Closing this one because the latest version is in [PR#54427](https://github.com/tensorflow/tensorflow/pull/54427),Closing this one because the latest version is in [PR#54427](https://github.com/tensorflow/tensorflow/pull/54427)
14798,bhack,1048082485,2022-02-22 18:20:07,Yes when we don't have or we want to have the orchestration/environment with public visibility we need to have exrta docs on how to test this locally if we want to collect community contribution. As also the TFlite markdown is on hold since 2020 we could ping also the TFlite team,"""When we don't have or we want to have the orchestration/environment with public visibility we need to have exrta docs on how to test this locally if we want to collect community contribution."""
14798,bhack,1048067267,2022-02-22 18:01:50,I meant could it be tested locally when we have no visibility of the CI logs?,I meant could it be tested locally when we have no visibility of the CI logs?
53938,kaixih,1048057446,2022-02-22 17:49:56,"Same with this one. I see ""Windows Bazel GPU"" fails but the details are not accessible. @awpr @jurahul","""Windows Bazel GPU"" fails but the details are not accessible."
14798,bhack,1048055756,2022-02-22 17:48:00,"> t's an internal tool that runs those.\nThey're run from the target version's github branch, with bazel available, so just calling that bazel command and Thanks so probably It Is a little bit hard to contribute a PR with only the OSS/Github visibilty.","t's an internal tool that runs those.nThey're run from the target version's github branch, with bazel available, so just calling that bazel command and Thanks so probably It Is a little bit hard to contribute a PR with only the OSS/Github visibilty."
53843,kaixih,1048055710,2022-02-22 17:47:57,It seems the PR has been hanging for a while. I see some tests fail but they seem unrelated to the PR. Can you help check? @awpr @jurahul,I see some tests fail but they seem unrelated to the PR.
53710,mihaimaruseac,1048012705,2022-02-22 17:01:57,"See https://github.com/tensorflow/tensorflow/pull/50605#issuecomment-919453092
It seems `tf.data` kernels should not use `DEVICE_DEFAULT`",tf.data kernels should not use DEVICE_DEFAULT
45081,jvishnuvardhan,1047967886,2022-02-22 16:21:30,"@eaedk Sorry for the late response. Can you please share a simple standalone code to reproduce the issue or share the data? When you run a model in a `for` loop, keras will take more memory to keep those models in the memory. Did you try releasing the memory https://www.tensorflow.org/api_docs/python/tf/keras/backend/clear_session. Thanks","""When you run a model in a for loop, keras will take more memory to keep those models in the memory. Did you try releasing the memory https://www.tensorflow.org/api_docs/python/tf/keras/backend/clear_session. Thanks."""
31312,UntotaufUrlaub,1047961829,2022-02-22 16:15:52,"@wissambenhaddad I used `FROM tensorflow/tensorflow:latest-gpu`.
So i guess the latest version available back in May. Sadly I got no clue how to tell the exact tag of it today.",Sadly I got no clue how to tell the exact tag of it today.
54480,mpettinger,1047960831,2022-02-22 16:14:54,I'm having the exact same issue on different browsers and machines.,I'm having the exact same issue on different browsers and machines.
54478,aliencaocao,1047893952,2022-02-22 15:09:39,Many pages especially keras related pages all have this issue. Changing browsers does not work.,Many pages especially keras related pages all have this issue. Changing browsers does not work.
14798,bhack,1047872509,2022-02-22 14:48:20,At least can we reopen this ticket adding also the XLA label?,can we reopen this ticket adding also the XLA label?>
54463,gadagashwini,1047646586,2022-02-22 10:26:12,"@arvindrajan92, Given configurations [Tested build configuration](https://www.tensorflow.org/install/source#gpu) were tested on different platforms. This error is due to - OOM error -GPU is running out of memory
- Doesn't have enough compute capacity
- There's a driver issue.
Can you verify the memory usage with nvidia-smi? If you have any other
processes using the GPU. And also check CUDA compute capability for the given nvidia drivers.",OOM error - GPU is running out of memory - Doesn't have enough compute capacity - There's a driver issue.
54197,unrealwill,1047626117,2022-02-22 10:04:43,"@chunduriv I tried to use tf.device(""/GPU:0"") to force the computation to happen on the GPU, (I don't care at all about the CPU performance) but it has no effect whatsoever.","I tried to use tf.device(""/GPU:0"") to force the computation to happen on the GPU, (I don't care at all about the CPU performance) but it has no effect whatsoever."
54435,Arkaikus,1047389591,2022-02-22 3:25:03,Hope the pr gets merged soon that linter error it's driving me nuts,That linter error it's driving me nuts.
54466,lipracer,1047378902,2022-02-22 3:00:18,"It seem like infra tool error.
```
[31m[1mERROR: [0mAn error occurred during the fetch of repository 'local_config_rocm':
```",Error: [0mAn error occurred during the fetch of repository 'local_config_rocm':
54407,itmo153277,1047377486,2022-02-22 2:56:45,"I think you forgot to put MSYS2 to your `%PATH%`. Check if this command prints a path to `realpath.exe`:
```cmd
for /F %a in (""realpath.exe"") do @echo.%~f$PATH:a
```
If it prints an empty string, your `%PATH%` is not configured correctly","I think you forgot to put MSYS2 to your %PATH%. Check if this command prints a path to realpath.exe: cmd for /F %a in (""realpath.exe"") do @echo.%f$PATH:a  If it prints an empty string, your %PATH% is not configured correctly."
54423,BptGrm,1047109614,2022-02-21 17:42:48,"The GPU still isn't recognized, and cuDNN still isn't installed either through conda with cudatoolkit or pip with tensorflow.","The GPU still isn't recognized, and cuDNN still isn't installed either through conda with cudatoolkit or pip with tensorflow."
54447,firatsarlar,1046858505,2022-02-21 13:06:34,"currently, I can do it for myself to get rid of every model load extra code for additional acc. object
thank you
by the way is not there an internal issue forwarding, splitting, editing system , like this team
what an English,
i'm not a native speekaer, please do try to understanda what i wanted to say",I can do it for myself to get rid of every model load extra code for additional acc. object
54465,pnuzyf,1046804377,2022-02-21 12:03:36,"you can clone the source code from https://github.com/pnuzyf/tf-model.git, the test code locates in test/msmt/test_memory.py if I set ""tf.compat.v1.disable_eager_execution()"", anthor error comes up.","""tf.compat.v1.disable_eager_execution()"", ""anthor error comes up""."
54336,tilakrayal,1046793350,2022-02-21 11:49:05,"@markub3327 ,
Looks like code is incomplete. Request you to provide tensorflow version you are using and complete code to reproduce the issue in our environment. It helps us in localizing the issue faster.",Looks like code is incomplete.
54386,tilakrayal,1046762263,2022-02-21 11:12:25,"@charlielam0615 ,
The code provided is not complete hence it would be difficult for us to pinpoint the issue in the tensorboard. Please share complete stand alone code to replicate the issue or a colab gist with the error reported.?",The code provided is not complete.
54437,fengyuentau,1046690044,2022-02-21 10:06:10,"@Maratyszcza The last compilation I tried was 3 days ago, but somehow CMake scripts downloaded XNNPACK at https://github.com/google/XNNPACK/commit/d605aa76904aa1c5ea98bb7a0a4bafc4886b8a0a, which was 6 days ago (The fix you mention was 5 days ago). Do you know how to force using latest XNNPACK?","""How to force using latest XNNPACK?"""
53632,elfringham,1046665936,2022-02-21 9:41:43,This seems to have been caused by the use of BAZEL_LINKLIBS to add '-l%:stdc++' to the build. This was introduced in our initial CI scripts and has since just been carried forward. I am not sure of the initial need for this usage but it is not needed now.,This seems to have been caused by the use of BAZEL_LINKLIBS to add '-l%:stdc++' to the build.
48040,gadagashwini,1046656136,2022-02-21 9:32:02,"@ZoeZhang91,
Tensorflow v2.4.1 source incompatible with Numpy version 1.20.1. Downgrade numpy ~= 1.19.5 will resolve the issues.
`pip install numpy==1.19.5`","""Tensorflow v2.4.1 source incompatible with Numpy version 1.20.1"""
52014,thaink,1046623190,2022-02-21 9:01:30,I saw the tflite-micro PR is closed. Can this PR be submitted at current state?,I saw the tflite-micro PR is closed. Can this PR be submitted at current state?
45794,vinhngx,1046519588,2022-02-21 6:33:52,+1 I've noticed the same that the axis parameter doesn't seem to be effective and always points to dimension 0.,I've noticed the same that the axis parameter doesn't seem to be effective and always points to dimension 0
53491,ruyanyinian,1046436278,2022-02-21 3:25:59,@woojinn8 Did you finally figure this issues out? I got in the same trouble.,I got in the same trouble.
54360,mihaimaruseac,1046298002,2022-02-20 18:46:47,The WSL2 usage should be handled by Microsoft or other documentation. There is nothing special about TF for this.,WSL2 usage should be handled by Microsoft or other documentation.
54383,mihaimaruseac,1046297855,2022-02-20 18:45:49,"No template filled. Rant issues are not productive, they just waste the time of everyone.",No template filled.
45256,Nickjgniklu,1046259110,2022-02-20 15:13:10,"I was looking at the source code for tensorflow micro and believe a fix may have been implemented. However, in order to receive the fix you would have build the library from source instead of downloading it through the arduino package manager.","I was looking at the source code for tensorflow micro and believe a fix may have been implemented. However, in order to receive the fix you would have build the library from source instead of downloading it through the arduino package manager."
54438,mihaimaruseac,1046159935,2022-02-20 4:19:00,"Hi @gadagashwini , please don't use a ""Update <file>"" commit message. Instead, please try to follow [proper commit etiquette](https://cbea.ms/git-commit/)","""Update file>"" commit message."
45068,fabiooshiro,1046157553,2022-02-20 3:57:09,workaround: I changed my batch size from 100 to 50 :-/,I changed my batch size from 100 to 50.
47554,chanwcom,1046131967,2022-02-20 0:31:12,I am also experiencing the same kind of issues. I am currently using tf 2.7.0,I am also experiencing the same kind of issues.
54423,mohantym,1046020340,2022-02-19 13:36:32,"Please try with ""conda install -c anaconda cudatoolkit "" command . If that does not resolve the issue please check with instructions from [here ](https://www.tensorflow.org/install/pip) with TF 2.8 after activating Conda environment?","""conda install -c anaconda cudatoolkit "" command . If that does not resolve the issue please check with instructions from [here ](https://www.tensorflow.org/install/pip) with TF 2.8 after activating Conda environment?"""
31870,SergioG-M,1045981198,2022-02-19 9:58:02,"I am getting warnings ""WARNING:tensorflow:The following Variables were used in a Lambda layer's call"" even if I set the env var. Using tf.get_logger().setLevel('ERROR') works though. Anyone else with the same issue?","""WARNING:tensorflow:The following Variables were used in a Lambda layer's call"""
43344,dgoldenberg-audiomack,1045887582,2022-02-19 6:38:47,Have what? I'm not a contributor. Gave up on [the ticket](https://github.com/tensorflow/tensorflow/issues/1252.) that this one was apparently blocking,Gave up on [the ticket](https://github.com/tensorflow/tensorflow/issues/1252.) that this one was apparently blocking.
43344,sheromon,1045706361,2022-02-19 4:07:53,"@sachinprasadhs, I'm not sure who your comment is addressing. I've given up on this functionality. m(-_-)m Perhaps @dgoldenberg-audiomack has what they needed now?",I'm not sure who your comment is addressing.
38167,Gift-py,1045321338,2022-02-18 23:04:57,"I have this problem too
I'm new to machine learning and this problem is pretty frustrating
I'm using my feature column in an estimator.DNNClassifier",I have this problem too
43648,milmor,1045284532,2022-02-18 22:34:33,Still I am getting this error. Tensorflow 2.8.,Still I am getting this error.
54329,hironaka,1044809802,2022-02-18 16:40:54,"I just sent a similar pull request: https://github.com/tensorflow/tensorflow/pull/54409
One comment, I don't think --std=c++1z is supported by nvcc unfortunately: https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#options-for-altering-compiler-linker-behavior-std","I just sent a similar pull request: https://github.com/tensorflow/tensorflow/pull/54409 One comment, I don't think --std=c++1z is supported by nvcc unfortunately: https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#options-for-altering-compiler-linker-behavior-std."
54337,distlibs,1044756131,2022-02-18 16:03:01,"@chunduriv
I able to compile TensorFlow Lite 2.7 with XNNPACK=ON. Problems with 2.8.",I able to compile TensorFlow Lite 2.7 with XNNPACK=ON. Problems with 2.8.
54237,surak,1044665024,2022-02-18 15:02:50,"Seems nobody cares, or get it so I will close it.","Nobody cares, or get it so I will close it."
54237,surak,1044664551,2022-02-18 15:02:31,"@gadagashwini thanks, but only shows the most recent version created. If an older version has a bug fix release issued, that one will come up on top.... And this is my beef here. The page the way you show seems correct now, it wasn't when I opened this issue - look at the comment above: https://github.com/tensorflow/tensorflow/issues/54237#issuecomment-1028034032","""The page the way you show seems correct now, it wasn't when I opened this issue"""
33627,Koussailakadi,1044582122,2022-02-18 14:06:49,"Hi guys, please, did anyone manage to solve this problem, I still have the problem of multi-threads running every time, and I can't find a solution. ??","I still have the problem of multi-threads running every time, and I can't find a solution."
54439,tilakrayal,1044387647,2022-02-18 11:43:38,"@lhy2749 ,
We see that you are using tf version 1.15, 1.x is not actively supported, please update to latest stable version 2.8 and let us know if you are facing same issue.","""We see that you are using tf version 1.15, 1.x is not actively supported, please update to latest stable version 2.8 and let us know if you are facing same issue."""
54437,mjp9527,1044368091,2022-02-18 11:24:20,"> @mjp9527 Did you have undefined reference related to `ruy` when linking TFLite static library to your programe?
@fengyuentau ......I was wrong, this error appeared again when linking TFLite static library.
![image](https://user-images.githubusercontent.com/54735487/154673663-238ab5bc-a780-47b4-97e2-f5f68b0d43e9.png)","""I was wrong, this error appeared again when linking TFLite static library."""
54436,Cheril311,1044282060,2022-02-18 10:21:49,"@jacoblubecki I guess you cannot add layers using the '+' operator, as I think it would be confusing if that was allowed","I guess you cannot add layers using the '+' operator, as I think it would be confusing if that was allowed."
54437,mjp9527,1044204998,2022-02-18 9:25:58,"> Hi @fengyuentau ! Can you check below command after declaring [ARM 64 SDK tool chain](https://www.tensorflow.org/lite/guide/build_cmake#cross-compilation) in path and removing -DBUILD_SHARED_LIBS=ON flag ? Thanks!
> > `cmake -DCMAKE_TOOLCHAIN_FILE=<CMakeToolchainFileLoc> ../tensorflow/lite/`
I met the same problem and I can compile static library successfully, but failed with ""-DBUILD_SHARED_LIBS=ON"" on Android64. ""error: undefined reference to `xnn_f16_ibilinear_ukernel__neonfp16arith_c8'""","""Error: undefined reference to xnn_f16_ibilinear_ukernel__neonfp16arith_c8'"""
54388,elfringham,1044197213,2022-02-18 9:18:20,@gadagashwini I disagree. Cleaning the cache is no help. That issue does not seem to relate in any way.,I disagree. Cleaning the cache is no help. That issue does not seem to relate in any way.
51201,zaleslaw,1044142851,2022-02-18 8:37:23,The same error on the 2.7.0 @ymodak,The same error on the 2.7.0 @ymodak.
54442,edwardyehuang,1044101120,2022-02-18 8:02:05,@mohantym I think it is a bug,I think it is a bug.
50919,crischeng,1043832172,2022-02-18 3:42:22,"![image](https://user-images.githubusercontent.com/27774893/154613430-d9c61388-5554-4b33-8c38-d007957b9044.png)
I have the same problem when i use GCC 5.4.0. Maybe use GCC 7.3.1 can solve this problem.",I have the same problem when i use GCC 5.4.0.
26642,npuichigo,1043826395,2022-02-18 3:34:08,"@coleary-hyperscience I always want a LSTM implementation which is a fused kernel on CPU and can seamlessly switch to CuDNN on GPU. The recommendations here don't meet my needs, but PyTorch LSTM does.
As for the document, I think it's out of date since there's no more tf.contrib anymore. It was the original motive of this issue, to find an alternative in tensorflow 2.0","The recommendations here don't meet my needs, but PyTorch LSTM does."
54418,qjia7,1043739211,2022-02-18 2:10:27,"> Hi @qjia7 ! Can you try the instruction in this [thread](https://stackoverflow.com/a/43401929/11530462)?
These two files `random_standard_normal_test.cc` and `random_uniform_test.cc` even don't exist in current source code. I don't think it can work.","""these two files random_standard_normal_test.cc and random_uniform_test.cc even don't exist in current source code"""
53437,cheshire,1043553697,2022-02-17 22:37:19,Now it seems like GemmRewriteTest is failing?,"""It seems like GemmRewriteTest is failing"""
54416,ngam,1043541583,2022-02-17 22:25:11,"Still not working?
Could you please try to install all from conda-forge to see if that helps? Print `conda info` and `conda list` please if that doesn't work. Also try something like the below command:
`mamba create -n tf -c conda-forge tensorflow==2.7.0=cuda112*`","""still not working"""
54416,Cheeseboy8020,1043539673,2022-02-17 22:23:14,I fixed by trying all the solutions that were provided. That didn't work so I deleted all my nvidia drivers and cuda drivers. Then I installed the cuda driver first with the runfile and I didn't include the nvidia driver in the install. Then I installed the nvidia driver. I used cuda 11.6 and nvidia driver 510. After that it worked.,That didn't work so I deleted all my nvidia drivers and cuda drivers. Then I installed the cuda driver first with the runfile and I didn't include the nvidia driver in the install. Then I installed the nvidia driver. I used cuda 11.6 and nvidia driver 510. After that it worked.
52988,ngam,1043349215,2022-02-17 19:41:39,"Not defending this change, since it is obviously inconvenient and can cause serious issues for people on HPC (e.g. OpenSSL conflicts), but this LD_LIBRARY_PATH stuff seems to have been documented here: https://www.tensorflow.org/install/gpu#linux_setup (not sure when it was added)","Not defending this change, since it is obviously inconvenient and can cause serious issues for people on HPC (e.g. OpenSSL conflicts), but this LD_LIBRARY_PATH stuff seems to have been documented here: https://www.tensorflow.org/install/gpu#linux_setup (not sure when it was added)"
52988,ngam,1043346328,2022-02-17 19:39:19,"fwiw when modifying LD_LIBRARY_PATH to $CONDA_PREFIX/lib, you would risk conflicts in OpenSSL (thereby making it impossible to use git or ssh); this generally impacts fedora-like systems (CentOS and the like). Of course, if you have the cuda libraries elsewhere (e.g. /usr/local/cuda) that would not be an issue; and generally pointing LD_LIBRARY_PATH to these local libraries (non-conda) will work","modifying LD_LIBRARY_PATH to $CONDA_PREFIX/lib, you would risk conflicts in OpenSSL (thereby making it impossible to use git or ssh); this generally impacts fedora-like systems (CentOS and the like)."
54413,ArrowIntoTheSky,1043179198,2022-02-17 16:51:56,"@tilakrayal Yes, they are different APIs. I just use `tf.compat.as_bytes` to show the correct error handling of a **wrong** encoding string. It is obvious that `encoding` cannot be `valid` or `hi` as in the following example:
```
import tensorflow as tf
bytes_or_text = ""hello""
encoding = ""hi""
t1 = tf.compat.as_text(bytes_or_text, encoding=encoding) # This pass! But it should not.
```","""wrong"" encoding string."
54416,Cheeseboy8020,1042941681,2022-02-17 13:19:21,"I installed it with the instructions on the website and I also installed it with the conda instructions [here](https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/). echo $LD_LIBRARY_PATH is blank and echo $PATH gives me this:
```
/home/aneesh/miniconda3/bin:/home/aneesh/miniconda3/condabin:/home/aneesh/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
```",I installed it with the instructions on the website and I also installed it with the conda instructions [here](https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/). echo $LD_LIBRARY_PATH is blank and echo $PATH gives me this:  /home/aneesh/miniconda3/bin:/home/aneesh/miniconda3/condabin:/home/aneesh/.
54407,OpDaSo,1042899299,2022-02-17 12:28:24,@sushreebarsa With TF version 2.8.0 it is possible to build tensorflow. But with TF version 2.8.0 there is no possibiliy to choose TensorRT support.,"""With TF version 2.8.0 it is possible to build tensorflow. But with TF version 2.8.0 there is no possibiliy to choose TensorRT support."""
54405,mohantym,1042849228,2022-02-17 11:25:21,"Hi @chunduriv! Could you please look at this issue? It is replicating in 2.7 ,[2.8 ](https://colab.sandbox.google.com/gist/mohantym/0e54856c2c952ab3d5c9763384fc6131/github_54405.ipynb#scrollTo=iQvBGhKm-p7v)and nightly. Size of tflite file came 400 mb after conversion as mentioned in template.","""It is replicating in 2.7 ,[2.8 ](https://colab.sandbox.google.com/gist/mohantym/0e54856c2c952ab3d5c9763384fc6131/github_54405.ipynb#scrollTo=iQvBGhKm-p7v)and nightly"""
54388,elfringham,1042814696,2022-02-17 10:47:40,"No, that does not help at all.","No, that does not help at all."
54384,minwang-ai,1042788067,2022-02-17 10:17:22,"Hi @sushreebarsa according the compatibility table, all the tensorflow doesnot support cuda11.3 since the latest supported version is cuda 11.2.","""doesnot support cuda11.3 since the latest supported version is cuda 11.2.."""
54406,ToonTalk,1042630289,2022-02-17 6:55:17,I only read the blog post and noticed a typo. Why fill in a template for that?,I only read the blog post and noticed a typo. Why fill in a template for that?
45770,mohantym,1042619971,2022-02-17 6:34:11,It is still replicating in CPU mode for [TF 2.8](https://colab.sandbox.google.com/gist/mohantym/0f41a7f797bf930813d8383dc1d955a3/github_45770.ipynb#scrollTo=F8ttmtF-X2zd).,It is still replicating in CPU mode for [TF 2.8]
54342,itmo153277,1042496298,2022-02-17 2:08:59,"I believe the issue itself has not been resolved, If the tensorflow team has decided that the issue is not worth fixing, feel free to close it.","I believe the issue itself has not been resolved, If the tensorflow team has decided that the issue is not worth fixing, feel free to close it."
54170,Kh4L,1042450813,2022-02-17 0:44:16,"> Some tests are failing because of error message mismatch, convert_nodes_test.cc: Value of: status Expected: has a status code that is equal to UNIMPLEMENTED, and has an error message that has substring ""Conversion for Fill is not implemented inimplicit batch mode"" Actual: UNIMPLEMENTED: Op type Fill is not supported. (of type tensorflow::Status), whose error message is wrong
I didn't add the version check in the tests. I just pushed the fix.","""Error message mismatch"""
52053,jshu12,1042018331,2022-02-16 18:46:09,I verified on Tensorflow 2.8 and the bug is still there.,I verified on Tensorflow 2.8 and the bug is still there.
54170,bixia1,1041832572,2022-02-16 16:13:34,"Some tests are failing because of error message mismatch, convert_nodes_test.cc:
Value of: status
Expected: has a status code that is equal to UNIMPLEMENTED, and has an error message that has substring ""Conversion for Fill is not implemented inimplicit batch mode""
Actual: UNIMPLEMENTED: Op type Fill is not supported. (of type tensorflow::Status), whose error message is wrong","""Error message mismatch"""
54403,sanamlimbu1010,1041577781,2022-02-16 14:56:22,"I made a typo by passing lowercase 'gpu' instead of 'GPU' as the device_type, but it went unnoticed as there was no error. Fixing that solved the issue.
```
gpus = tf.config.experimental.list_physical_devices(device_type='gpu')
for gpu in gpus:
tf.config.experimental.set_memory_growth(gpu, True)
```","I made a typo by passing lowercase 'gpu' instead of 'GPU' as the device_type, but it went unnoticed as there was no error."
54378,SteveArias,1041521646,2022-02-16 14:01:16,@sushreebarsa I have updated the Colab notebook (https://colab.research.google.com/drive/1F-KqnVYFq4UEEcwKSt2hxWb161QNS668?usp=sharing) to use tensorflow v2.8.0. I still get the same error.,I still get the same error.
14356,shellrazer,1041462295,2022-02-16 12:53:53,"I use train_and_evaluate() and meet the same error. @damienpontifex since this issue is continuously referenced by similar errors, could you kindly upload the fixed code please?",I use train_and_evaluate() and meet the same error.
52973,bhack,1041411302,2022-02-16 11:54:47,"Just a side note also having a Linux aarch64 image on M1 we are not going to be able to use the AMX2 instruction set:
https://github.com/tensorflow/tensorflow/issues/52845#issuecomment-974559363
https://nod.ai/comparing-apple-m1-with-amx2-m1-with-neon/",Having a Linux aarch64 image on M1 we are not going to be able to use the AMX2 instruction set.
53718,MarkDaoust,1040980677,2022-02-16 1:12:13,It might have been a wrong-way merge or something like that. At this point it's usually easier to just close it and make a new PR from scratch.,"""Wrong-way merge"""
53414,wraveane,1040721554,2022-02-15 19:45:46,@bixia1 I had to resolve another merge conflict that came up in the last few days. Does the PR need to be re-approved?,I had to resolve another merge conflict that came up in the last few days.
53718,drivanov,1040612173,2022-02-15 18:11:46,"@bixia1 : I rebased before the squash, therefore I see **Conflicting files**. Is it OK?
OR, perhaps I need to create a new PR.","I rebased before the squash, therefore I see **Conflicting files**."
54329,smuzaffar,1040336575,2022-02-15 14:26:23,I have signed CLA but no idea why it still shows missing CLA.,I have signed CLA but no idea why it still shows missing CLA.
22294,donutloop,1039990464,2022-02-15 8:25:17,@zmajew version mismatch between client library and Tensorflow engine. Check that both version are compatible with each other.,"""Version mismatch between client library and Tensorflow engine"""
47171,lingster,1039965301,2022-02-15 7:55:51,"I have similar issues when I try to load a parquet file using:
```
ds = tfio.IODataset.from_parquet(...)
iter = ds.as_numpy_iterator()
```
In this case, is it better to use spark to manipulate the data before conversion to a tf dataset for training?",I have similar issues when I try to load a parquet file using:  ds = tfio.IODataset.from_parquet(...) iter = ds.as_numpy_iterator()
22294,zmajew,1039870611,2022-02-15 5:20:51,"> princesegzy01 did not ask you what was the problem, but hot to fix it.","did not ask you what was the problem, but hot to fix it."
54304,ScholliYT,1039727792,2022-02-15 0:47:30,"Hi @abattery, thanks for the suggestion. Unfortunately, his workaround doesn't seem to work either. It throws the following error: `RuntimeError: Quantization not yet supported for op: 'IF'.`. I added it to a new cell in the colab notebook above.",RuntimeError: Quantization not yet supported for op: 'IF'.
53718,drivanov,1039408081,2022-02-14 18:21:18,"@bixia1: Sorry, I have no idea how this happened. Some of these files are not even from Tensorflow but from my MXNET projects. I just fixed it.
> I see a bunch a unrelated files were uploaded, and an existing file op_convert.h is a copied to a different directory and shown as ""new""? Can you please fix those?",I have no idea how this happened.
54369,sijia-w,1039067911,2022-02-14 13:10:06,"Hi @mohantym Thanks a lot for your quick response! Unfortunately, still the same error.
<img width=""908"" alt=""image"" src=""https://user-images.githubusercontent.com/53718687/153872757-8a6adea8-5155-4276-9747-08a266b5da1e.png"">","""Unfortunately, still the same error"""
21742,fathysr,1038930902,2022-02-14 10:43:53,"I have the same problem: usage: untitled1.py [-h] video
untitled1.py: error: the following arguments are required: video
An exception has occurred, use %tb to see the full traceback.
could you please tell me how can I use app.run(debug=True, use_reloader=False) in my code","""I have the same problem: usage: untitled1.py [-h] video untitled1.py: error: the following arguments are required: video An exception has occurred, use %tb to see the full traceback. could you please tell me how can I use app.run(debug=True, use_reloader=False) in my code."""
52605,tilakrayal,948522091,2021-10-21 11:33:37,"@sanatmpa1 ,
I was able to reproduce the issue in tf v2.6 and nightly.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/a699c96140c7e85086d89e4bee47306b/untitled100.ipynb).
[EDIT] This is still an issue with `tf-nightly`(2.9.0-dev20220309)",I was able to reproduce the issue in tf v2.6 and nightly. Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/a699c96140c7e85086d89e4bee47306b/untitled100.ipynb). [EDIT] This is still an issue with tf-nightly(2.9.0-dev20220309).
37245,Saduf2019,922485171,2021-09-19 14:44:51,This issue is still reproducible on tf nightly-2.13.0-dev20230305 [gist](https://colab.sandbox.google.com/gist/tilakrayal/161482a5a0ee8e079bd1ac19ec1a3c87/untitled52.ipynb) version.,This issue is still reproducible on tf nightly-2.13.0-dev20230305 [gist](https://colab.sandbox.google.com/gist/tilakrayal/161482a5a0ee8e079bd1ac19ec1a3c87/untitled52.ipynb) version..
50765,tilakrayal,880039748,2021-07-14 16:33:14,"@jvishnuvardhan ,
I was able to reproduce the issue in tensorflow [2.11.0-dev20220829](https://colab.research.google.com/gist/tilakrayal/39ee242dcd8f745fe98f4d46cacf3caf/2_4memory_leak-1.ipynb) it is giving error.Please find the gist here.",I was able to reproduce the issue in tensorflow [2.11.0-dev20220829](https://colab.research.google.com/gist/tilakrayal/39ee242dcd8f745fe98f4d46cacf3caf/2_4memory_leak-1.ipynb) it is giving error.Please find the gist here.
42680,ymodak,850017495,2021-05-28 0:00:35,"TF 2.11 raises `ValueError: num_tokens must be set to use this layer. If the number of tokens is not known beforehand, use the IntegerLookup layer instead.` Switching to `IntegerLookup layer` results in same reported behavior. See [gist](https://colab.research.google.com/gist/pjpratik/91214b91019d90518d530e52671d7b3b/42680.ipynb)","TF 2.11 raises ValueError: num_tokens must be set to use this layer. If the number of tokens is not known beforehand, use the IntegerLookup layer instead."
43043,bhack,734288390,2020-11-26 13:07:02,"Mine was not a solution it was just to check that when it is not constant anymore the dtype is working correctly.
Your problem is related that in the default case `unconnect_gradients='null'` we are not handling any specific `dtype` on the source.
https://github.com/tensorflow/tensorflow/blob/31f0b2159751642a146d70561ac5095a3c4722ff/tensorflow/python/eager/pywrap_tfe_src.cc#L2804-L2814
/cc @edloper","""mine was not a solution it was just to check that when it is not constant anymore the dtype is working correctly."""