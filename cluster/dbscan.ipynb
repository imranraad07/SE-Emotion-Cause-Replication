{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "%pip install nltk\n",
    "%pip install pandas\n",
    "%pip install scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def text_cleaning(text):\n",
    "    text = str (text)\n",
    "\n",
    "    printable = set(string.printable)\n",
    "    text = ''.join(filter(lambda x: x in printable, text))\n",
    "    text = text.replace('\\x00', ' ')  # remove nulls\n",
    "    text = text.replace('\\r', ' ')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n",
    "    # text = re.sub(\"(<.*?>)\", \"\", text)  # remove html markup\n",
    "    # text = re.sub(\"(\\W|\\d)\", \" \", text)  # remove non-ascii and digits\n",
    "    text = text.lower()  # Lowercasing\n",
    "\n",
    "    # def remove_sw(word_list):\n",
    "    #     keep = []\n",
    "    #     for word in word_list:\n",
    "    #         if not word in sw:\n",
    "    #             keep.append(word)\n",
    "    #     return keep\n",
    "\n",
    "    # words = text.split()\n",
    "    # removed_stop_words = remove_sw(words)\n",
    "    # text = ' '.join([str(elem) for elem in removed_stop_words])\n",
    "\n",
    "    words = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    wn_tags = {'N': wordnet.NOUN, 'V': wordnet.VERB, 'R': wordnet.ADV, 'J': wordnet.ADJ}\n",
    "    # lemmatize each word with its POS tag\n",
    "    lemmatized_words = []\n",
    "    for word, pos in pos_tags:\n",
    "        if pos[0] in wn_tags:\n",
    "            wn_tag = wn_tags[pos[0]]\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word, wn_tag))\n",
    "        else:\n",
    "            lemmatized_words.append(word)\n",
    "    text = ' '.join([str(elem) for elem in lemmatized_words])\n",
    "\n",
    "    words= text.split()\n",
    "    stemmed_words=[porter_stemmer.stem(word=word) for word in words] # Stemming\n",
    "    text = ' '.join([str(elem) for elem in stemmed_words])\n",
    "\n",
    "    text = text.strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaning(\"I'm getting the same error, on 2.9.0 but I reproduced it in 2.8.0 and 2.9.1 too.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"anger_causes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = df[\"Comment\"]\n",
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df[\"Emotion Causes\"]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_corpus = corpus.apply(text_cleaning)\n",
    "processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\",ngram_range=(1,1))\n",
    "X = vectorizer.fit_transform(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering (DBSCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(metric='cosine', min_samples=3, eps=0.45).fit(X)\n",
    "labels = db.labels_\n",
    "\n",
    "no_clusters = len(np.unique(labels) )\n",
    "no_noise = np.sum(np.array(labels) == -1, axis=0)\n",
    "\n",
    "print('Estimated num of clusters: %d' % no_clusters)\n",
    "print('Estimated num of noise points: %d' % no_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db.labels_)\n",
    "unique, counts = np.unique(db.labels_, return_counts = True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_freq = dict(zip(unique, counts))\n",
    "print(type(cluster_freq))\n",
    "print(sorted(dict(zip(unique, counts)).items(), key=lambda x:x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain text points for each cluster\n",
    "text_points = []\n",
    "unique_labels = set(db.labels_)\n",
    "cluster_ids = []\n",
    "\n",
    "for label in unique_labels:\n",
    "    if cluster_freq[label] < 10 or label == -1:\n",
    "        continue\n",
    "    cluster_indices = np.where(db.labels_ == label)[0]\n",
    "    cluster_docs = [corpus[j] for j in cluster_indices]\n",
    "    cluster_vecs = X[cluster_indices]\n",
    "    centroid_vec = np.mean(cluster_vecs.toarray(), axis=0)\n",
    "    similarity_scores = cosine_similarity(cluster_vecs, [centroid_vec])\n",
    "    # print(cluster_indices)\n",
    "    text_point_index = cluster_indices[np.argmax(similarity_scores)]\n",
    "    text_points.append(corpus[text_point_index])\n",
    "    print(f\"Cluster #{label} text point: {corpus[text_point_index]}\")\n",
    "\n",
    "    for idx in cluster_indices:\n",
    "        cluster_ids.append([label, comments[idx], corpus[idx]])\n",
    "        # print(idx, corpus[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "arr = np.asarray(cluster_ids)\n",
    "pd.DataFrame(arr).to_csv('clusters.csv', index_label = \"Index\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def remove_sw(word_list):\n",
    "    keep = []\n",
    "    for word in word_list:\n",
    "        if not word in sw:\n",
    "            keep.append(word)\n",
    "    return keep\n",
    "\n",
    "\n",
    "# Get the top features for each cluster\n",
    "unique_labels = set(db.labels_)\n",
    "top_n = 5 # Number of top features to retrieve for each cluster\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "for label in unique_labels:\n",
    "    if cluster_freq[label] < 10 or label == -1:\n",
    "        continue\n",
    "    indices = np.where(db.labels_ == label)[0]\n",
    "    indices = indices.tolist()\n",
    "    sentences = []\n",
    "    for idx in indices:\n",
    "        sentences.append(processed_corpus[idx])\n",
    "    words = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Convert the sentence to lowercase and split it into words\n",
    "        words += sentence.lower().split()\n",
    "    # words = remove_sw(words)\n",
    "    # Count the frequency of each word using the Counter class\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    # Get the top 5 most common words\n",
    "    top_words = word_counts.most_common(15)\n",
    "\n",
    "    print(top_words)\n",
    "\n",
    "    # cluster_vecs = X[indices].toarray()\n",
    "    # centroid_vec = np.mean(cluster_vecs, axis=0)\n",
    "    # # print(np.max(centroid_vec))\n",
    "    # # print(np.sort(centroid_vec))\n",
    "    # centroid_vec = np.argsort(centroid_vec)\n",
    "    # # print(centroid_vec)\n",
    "    # top_features_indices = centroid_vec[::-1][:top_n]\n",
    "    # top_features = feature_names[top_features_indices]\n",
    "    # # print(top_features)\n",
    "    # print(f\"Cluster #{label} top features: {', '.join(top_features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # # Apply t-SNE to reduce the dimensionality of the data to 2D\n",
    "# # tsne = TSNE(n_components=2, random_state=42)\n",
    "# # X_tsne = tsne.fit_transform(X_norm)\n",
    "\n",
    "# # # Plot the clusters\n",
    "# # plt.scatter(X_tsne[:,0], X_tsne[:,1], c=db.labels_)\n",
    "# # plt.title(\"DBSCAN Clustering of Emotional Cause Dataset\")\n",
    "# # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_norm_filtered = []\n",
    "# labels_filtered = []\n",
    "# idx = 0\n",
    "# for idx in range(len(X_norm)):\n",
    "#     if db.labels_[idx] < 2:\n",
    "#         continue\n",
    "#     # print (db.labels_[idx], X_norm[idx])\n",
    "#     X_norm_filtered.append(X_norm[idx])\n",
    "#     labels_filtered.append(db.labels_[idx])\n",
    "\n",
    "# X_norm_filtered = np.array(X_norm_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply t-SNE to reduce the dimensionality of the data to 2D\n",
    "# tsne = TSNE(n_components=2, random_state=42)\n",
    "# X_tsne = tsne.fit_transform(X_norm_filtered)\n",
    "\n",
    "# # # Plot the clusters\n",
    "# # plt.scatter(X_tsne[:,0], X_tsne[:,1], c=labels_filtered)\n",
    "# # plt.title(\"DBSCAN Clustering of Emotional Cause Dataset\")\n",
    "# # plt.show()\n",
    "\n",
    "# # Define a list of colors with one color per scatter plot\n",
    "# num_scatter_plots = len(set(labels_filtered))\n",
    "# colors = [\n",
    "#     '#FF0000', '#00FF00', '#0000FF', '#FFFF00', '#00FFFF', \n",
    "#     '#FF00FF', '#800000', '#008000', '#000080', '#808000', \n",
    "#     '#800080', '#008080', '#C00000', '#00C000', '#0000C0', \n",
    "#     '#C0C000', '#C000C0', '#00C0C0', '#400000', '#004000', \n",
    "#     '#000040', '#404000', '#400040', '#004040', '#200000', \n",
    "#     '#002000', '#000020', '#202000', '#200020', '#002020', \n",
    "#     '#600000', '#006000', '#000060', '#606000', '#600060', \n",
    "#     '#006060', '#A00000', '#00A000', '#0000A0', '#A0A000', \n",
    "#     '#A000A0', '#00A0A0', '#E00000', '#00E000', '#0000E0', \n",
    "#     '#E0E000', '#E000E0', '#00E0E0', '#100000', '#001000', \n",
    "#     '#000010', '#101000', '#100010', '#001010', '#500000', \n",
    "#     '#005000', '#000050', '#505000', '#500050', '#005050', \n",
    "#     '#900000', '#009000', '#000090', '#909000', '#900090', \n",
    "#     '#009090', '#D00000', '#00D000', '#0000D0', '#D0D000', \n",
    "#     '#D000D0', '#00D0D0', '#300000', '#003000', '#000030', \n",
    "#     '#303000', '#300030', '#003030', '#700000', '#007000', \n",
    "#     '#000070', '#707000', '#700070', '#007070', '#B00000', \n",
    "#     '#00B000', '#0000B0', '#B0B000', '#B000B0', '#00B0B0', \n",
    "#     '#F00000', '#00F000', '#0000F0', '#F0F000', '#F000F0', \n",
    "#     '#00F0F0'\n",
    "# ]\n",
    "# print(colors)\n",
    "\n",
    "# # Plot the scatter plots with different colors based on their index\n",
    "# fig, ax = plt.subplots()\n",
    "# for i, label in enumerate(set(labels_filtered)):\n",
    "#     X_plot = X_tsne[labels_filtered == label]\n",
    "#     ax.scatter(X_plot[:, 0], X_plot[:, 1], label=label, c=colors[i])\n",
    "\n",
    "# # Add a legend and title to the plot\n",
    "# ax.set_title(\"DBSCAN Clustering of Emotional Cause Dataset\")\n",
    "# # ax.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "# # Perform topic modeling with LDA\n",
    "# lda = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "# lda.fit(X)\n",
    "\n",
    "# # Print the top words for each topic\n",
    "# feature_names = vectorizer.get_feature_names_out()\n",
    "# for topic_idx, topic in enumerate(lda.components_):\n",
    "#     top_words_indices = topic.argsort()[:-6:-1]\n",
    "#     top_words = [feature_names[i] for i in top_words_indices]\n",
    "#     print(f\"Topic #{topic_idx}: {' '.join(top_words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyLDAvis\n",
    "# import pyLDAvis.gensim_models as gensimvis\n",
    "# import gensim\n",
    "# from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_clean=[]\n",
    "\n",
    "# for text in processed_corpus:\n",
    "#     text_clean.append(text.split())\n",
    "\n",
    "\n",
    "# dictionary = corpora.Dictionary(text_clean)\n",
    "# text_term_matrix = [dictionary.doc2bow(text) for text in text_clean]\n",
    "\n",
    "# number_of_topics = 73\n",
    "\n",
    "# LDA = gensim.models.ldamodel.LdaModel\n",
    "# ldamodel = LDA(text_term_matrix, num_topics=number_of_topics, id2word = dictionary, passes=10)\n",
    "\n",
    "# # Show Topics\n",
    "# display(ldamodel.show_topics(formatted=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyLDAvis.enable_notebook()\n",
    "# vis = gensimvis.prepare(ldamodel, text_term_matrix, dictionary)\n",
    "# vis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
