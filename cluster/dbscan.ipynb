{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./.venv/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.10/site-packages (from nltk) (2023.3.23)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./.venv/lib/python3.10/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/imranm3/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/imranm3/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/imranm3/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/imranm3/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/imranm3/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "%pip install nltk\n",
    "%pip install pandas\n",
    "%pip install scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def text_cleaning(text):\n",
    "    text = str (text)\n",
    "\n",
    "    printable = set(string.printable)\n",
    "    text = ''.join(filter(lambda x: x in printable, text))\n",
    "    text = text.replace('\\x00', ' ')  # remove nulls\n",
    "    text = text.replace('\\r', ' ')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n",
    "    text = text.lower()  # Lowercasing\n",
    "\n",
    "    words = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    wn_tags = {'N': wordnet.NOUN, 'V': wordnet.VERB, 'R': wordnet.ADV, 'J': wordnet.ADJ}\n",
    "    # lemmatize each word with its POS tag\n",
    "    lemmatized_words = []\n",
    "    for word, pos in pos_tags:\n",
    "        if pos[0] in wn_tags:\n",
    "            wn_tag = wn_tags[pos[0]]\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word, wn_tag))\n",
    "        else:\n",
    "            lemmatized_words.append(word)\n",
    "    text = ' '.join([str(elem) for elem in lemmatized_words])\n",
    "\n",
    "    words= text.split()\n",
    "    stemmed_words=[porter_stemmer.stem(word=word) for word in words] # Stemming\n",
    "    text = ' '.join([str(elem) for elem in stemmed_words])\n",
    "\n",
    "    text = text.strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i m get the same error on 2 9 0 but i reproduc it in 2 8 0 and 2 9 1 too'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cleaning(\"I'm getting the same error, on 2.9.0 but I reproduced it in 2.8.0 and 2.9.1 too.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"anger_causes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       @synandi Tensorflow 2.12.0 Can't use GPU on Wi...\n",
       "1       Sorry, even with the addition of `prefetch` I ...\n",
       "2       @zyxkad Your problem is not related to the one...\n",
       "3       I am using Teachable Machine for my audio mode...\n",
       "4       @tiruk007 Having the same issue. CUDA 11.8 and...\n",
       "                              ...                        \n",
       "2606    @sanatmpa1 ,\\nI was able to reproduce the issu...\n",
       "2607    This issue is still reproducible on tf nightly...\n",
       "2608    @jvishnuvardhan ,\\nI was able to reproduce the...\n",
       "2609    TF 2.11 raises `ValueError: num_tokens must be...\n",
       "2610    Mine was not a solution it was just to check t...\n",
       "Name: Comment, Length: 2611, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = df[\"Comment\"]\n",
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            Can't use GPU on Windows 11?\n",
       "1                     I am still seeing this log message.\n",
       "2       \"Your problem is not related to the one that t...\n",
       "3       I am using Teachable Machine for my audio mode...\n",
       "4                                  Having the same issue.\n",
       "                              ...                        \n",
       "2606    I was able to reproduce the issue in tf v2.6 a...\n",
       "2607    This issue is still reproducible on tf nightly...\n",
       "2608    I was able to reproduce the issue in tensorflo...\n",
       "2609    TF 2.11 raises ValueError: num_tokens must be ...\n",
       "2610    \"mine was not a solution it was just to check ...\n",
       "Name: Emotion Causes, Length: 2611, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = df[\"Emotion Causes\"]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              can t use gpu on window 11\n",
       "1                           i be still see thi log messag\n",
       "2       your problem be not relat to the one that thi ...\n",
       "3       i be use teachabl machin for my audio model i ...\n",
       "4                                      have the same issu\n",
       "                              ...                        \n",
       "2606    i be abl to reproduc the issu in tf v2 6 and n...\n",
       "2607    thi issu be still reproduc on tf nightli 2 13 ...\n",
       "2608    i be abl to reproduc the issu in tensorflow 2 ...\n",
       "2609    tf 2 11 rais valueerror num token must be set ...\n",
       "2610    mine be not a solut it be just to check that w...\n",
       "Name: Emotion Causes, Length: 2611, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_corpus = corpus.apply(text_cleaning)\n",
    "processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\",ngram_range=(1,1))\n",
    "X = vectorizer.fit_transform(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2611"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering (DBSCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated num of clusters: 63\n",
      "Estimated num of noise points: 2147\n"
     ]
    }
   ],
   "source": [
    "db = DBSCAN(metric='cosine', min_samples=3, eps=0.45).fit(X)\n",
    "labels = db.labels_\n",
    "\n",
    "no_clusters = len(np.unique(labels) )\n",
    "no_noise = np.sum(np.array(labels) == -1, axis=0)\n",
    "\n",
    "print('Estimated num of clusters: %d' % no_clusters)\n",
    "print('Estimated num of noise points: %d' % no_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33  0 -1 ... 22 -1 -1]\n",
      "{-1: 2147, 0: 7, 1: 142, 2: 3, 3: 3, 4: 4, 5: 12, 6: 14, 7: 7, 8: 12, 9: 4, 10: 4, 11: 4, 12: 3, 13: 5, 14: 18, 15: 6, 16: 3, 17: 3, 18: 3, 19: 4, 20: 3, 21: 3, 22: 9, 23: 6, 24: 5, 25: 8, 26: 6, 27: 4, 28: 17, 29: 4, 30: 9, 31: 7, 32: 3, 33: 16, 34: 6, 35: 3, 36: 3, 37: 3, 38: 3, 39: 3, 40: 3, 41: 4, 42: 4, 43: 4, 44: 5, 45: 5, 46: 3, 47: 7, 48: 3, 49: 3, 50: 3, 51: 3, 52: 3, 53: 5, 54: 5, 55: 3, 56: 3, 57: 3, 58: 3, 59: 3, 60: 4, 61: 3}\n"
     ]
    }
   ],
   "source": [
    "print(db.labels_)\n",
    "unique, counts = np.unique(db.labels_, return_counts = True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "[(-1, 2147), (1, 142), (14, 18), (28, 17), (33, 16), (6, 14), (5, 12), (8, 12), (22, 9), (30, 9), (25, 8), (0, 7), (7, 7), (31, 7), (47, 7), (15, 6), (23, 6), (26, 6), (34, 6), (13, 5), (24, 5), (44, 5), (45, 5), (53, 5), (54, 5), (4, 4), (9, 4), (10, 4), (11, 4), (19, 4), (27, 4), (29, 4), (41, 4), (42, 4), (43, 4), (60, 4), (2, 3), (3, 3), (12, 3), (16, 3), (17, 3), (18, 3), (20, 3), (21, 3), (32, 3), (35, 3), (36, 3), (37, 3), (38, 3), (39, 3), (40, 3), (46, 3), (48, 3), (49, 3), (50, 3), (51, 3), (52, 3), (55, 3), (56, 3), (57, 3), (58, 3), (59, 3), (61, 3)]\n"
     ]
    }
   ],
   "source": [
    "cluster_freq = dict(zip(unique, counts))\n",
    "print(type(cluster_freq))\n",
    "print(sorted(dict(zip(unique, counts)).items(), key=lambda x:x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster #1 text point: I am also having this issue.\n",
      "Cluster #5 text point: \"Update file>\" commit message.\n",
      "Cluster #6 text point: No response.\n",
      "Cluster #8 text point: \"Resolve merge conflict?\"\n",
      "Cluster #14 text point: \"still not working\"\n",
      "Cluster #28 text point: \"In this case, it broke an internal test.\"\n",
      "Cluster #33 text point: The windows build was still failed.\n"
     ]
    }
   ],
   "source": [
    "# Obtain text points for each cluster\n",
    "text_points = []\n",
    "unique_labels = set(db.labels_)\n",
    "cluster_ids = []\n",
    "\n",
    "for label in unique_labels:\n",
    "    if cluster_freq[label] < 10 or label == -1:\n",
    "        continue\n",
    "    cluster_indices = np.where(db.labels_ == label)[0]\n",
    "    cluster_docs = [corpus[j] for j in cluster_indices]\n",
    "    cluster_vecs = X[cluster_indices]\n",
    "    centroid_vec = np.mean(cluster_vecs.toarray(), axis=0)\n",
    "    similarity_scores = cosine_similarity(cluster_vecs, [centroid_vec])\n",
    "    # print(cluster_indices)\n",
    "    text_point_index = cluster_indices[np.argmax(similarity_scores)]\n",
    "    text_points.append(corpus[text_point_index])\n",
    "    print(f\"Cluster #{label} text point: {corpus[text_point_index]}\")\n",
    "\n",
    "    for idx in cluster_indices:\n",
    "        cluster_ids.append([label, comments[idx], corpus[idx]])\n",
    "        # print(idx, corpus[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "arr = np.asarray(cluster_ids)\n",
    "pd.DataFrame(arr).to_csv('clusters.csv', index_label = \"Index\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 80), ('the', 77), ('be', 53), ('issu', 52), ('same', 51), ('have', 36), ('thi', 33), ('2', 29), ('with', 26), ('it', 25), ('still', 25), ('problem', 25), ('0', 25), ('to', 24), ('error', 22)]\n",
      "[('updat', 9), ('commit', 8), ('the', 6), ('messag', 6), ('file', 5), ('at', 4), ('a', 4), ('it', 3), ('pleas', 3), ('use', 3), ('and', 3), ('pr', 3), ('still', 3), ('these', 2), ('make', 2)]\n",
      "[('respons', 8), ('merg', 7), ('pr', 4), ('no', 4), ('the', 4), ('await', 3), ('be', 3), ('i', 3), ('thi', 2), ('delay', 2), ('my', 2), ('review', 2), ('and', 2), ('there', 2), ('it', 2)]\n",
      "[('resolv', 10), ('conflict', 9), ('merg', 5), ('have', 4), ('i', 4), ('be', 4), ('to', 4), ('you', 3), ('intern', 3), ('not', 3), ('can', 2), ('pleas', 2), ('work', 2), ('issu', 2), ('thi', 2)]\n",
      "[('work', 13), ('still', 7), ('not', 6), ('doesn', 6), ('t', 6), ('look', 5), ('it', 4), ('be', 3), ('like', 3), ('thi', 3), ('issu', 2), ('i', 2), ('test', 2), ('tf', 2), ('to', 2)]\n",
      "[('be', 10), ('test', 9), ('fail', 8), ('break', 8), ('thi', 7), ('the', 7), ('pr', 5), ('intern', 5), ('i', 5), ('unrel', 4), ('to', 4), ('failur', 3), ('seem', 3), ('it', 3), ('check', 2)]\n",
      "[('the', 12), ('be', 11), ('window', 8), ('i', 8), ('build', 7), ('fail', 7), ('experi', 6), ('still', 5), ('2', 5), ('issu', 5), ('same', 5), ('gpu', 4), ('on', 4), ('tensorflow', 3), ('not', 3)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def remove_sw(word_list):\n",
    "    keep = []\n",
    "    for word in word_list:\n",
    "        if not word in sw:\n",
    "            keep.append(word)\n",
    "    return keep\n",
    "\n",
    "\n",
    "# Get the top features for each cluster\n",
    "unique_labels = set(db.labels_)\n",
    "top_n = 5 # Number of top features to retrieve for each cluster\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "for label in unique_labels:\n",
    "    if cluster_freq[label] < 10 or label == -1:\n",
    "        continue\n",
    "    indices = np.where(db.labels_ == label)[0]\n",
    "    indices = indices.tolist()\n",
    "    sentences = []\n",
    "    for idx in indices:\n",
    "        sentences.append(processed_corpus[idx])\n",
    "    words = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Convert the sentence to lowercase and split it into words\n",
    "        words += sentence.lower().split()\n",
    "    # words = remove_sw(words)\n",
    "    # Count the frequency of each word using the Counter class\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    # Get the top 5 most common words\n",
    "    top_words = word_counts.most_common(15)\n",
    "\n",
    "    print(top_words)\n",
    "\n",
    "    # cluster_vecs = X[indices].toarray()\n",
    "    # centroid_vec = np.mean(cluster_vecs, axis=0)\n",
    "    # # print(np.max(centroid_vec))\n",
    "    # # print(np.sort(centroid_vec))\n",
    "    # centroid_vec = np.argsort(centroid_vec)\n",
    "    # # print(centroid_vec)\n",
    "    # top_features_indices = centroid_vec[::-1][:top_n]\n",
    "    # top_features = feature_names[top_features_indices]\n",
    "    # # print(top_features)\n",
    "    # print(f\"Cluster #{label} top features: {', '.join(top_features)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
