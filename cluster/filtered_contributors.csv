Issue_ID,User,Comment_ID,Date,Comment,Emotion Causes,UserType
59984,elfringham,1490303833,2023-03-30 13:26:47,@zyxkad Your problem is not related to the one that this issue refers to. I think you should open a new issue to cover your Mac specific build problem.,"""Your problem is not related to the one that this issue refers to.""",CONTRIBUTOR
60124,SuryanarayanaY,1488462459,2023-03-29 11:55:07,"The check fail is happening due to different ` input_min=-1`, `input_max=[-1,1]`. As per [API](https://www.tensorflow.org/api_docs/python/tf/raw_ops/QuantizeAndDequantizeV4) both `input_min` and `input_max` should be of same type and size if `range_given=True`.
Please refer to attached [gist](https://colab.research.google.com/gist/SuryanarayanaY/f1ccb35630bfa6b9109bff3071da2815/untitled173.ipynb).","The check fail is happening due to different  input_min=-1,  input_max=[-1,1]. As per [API](https://www.tensorflow.org/api_docs/python/tf/raw_ops/QuantizeAndDequantizeV4) both  input_min and  input_max should be of same type and size if  range_given=True. Please refer to attached [gist](https://colab.research.",COLLABORATOR
59642,reedwm,1487781445,2023-03-29 0:33:58,"@qlzh727, the ClipByValue gradient is commented out and there is a TODO assigned to you from 2018: https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/python/ops/clip_ops.py;l=126;drc=ec69e86e01436b58d53f49b12782a0452bd26c8c. Do you remember why it's commented out?","""The ClipByValue gradient is commented out and there is a TODO assigned to you from 2018""",MEMBER
60109,sachinprasadhs,1487445646,2023-03-28 18:54:51,"@TortoiseHam , Tested build configuration for Tensorflow 2.12 is cuDNN 8.6 and CUDA 11.8. Make sure while installing torch also this should not conflict.
Also. could you please do one more check by creating two different environments and installing `torch` and `Tensorflow` separately and check the internal package dependency versions using `pip list`.
My hunch is that it might be because of version conflicts on package dependencies.","""Version conflicts on package dependencies""",CONTRIBUTOR
59984,elfringham,1486903787,2023-03-28 13:36:35,@sdeoras Actually your build error looks more like there was a stale object in the bazel cache that was not rebuilt when it should have been. If this still needs resolving please try again but run 'bazel clean --expunge' first. If you can try 2.11.1 or even better 2.12.0 then that would be good.,stale object in the bazel cache that was not rebuilt when it should have been.,CONTRIBUTOR
60139,AyanmoI,1486783915,2023-03-28 12:20:31,"> I attached comments to the places I had to adjust because of ClangTidy and Linter. It might be easier for you if you revert the revert PR, then you have all those fixes already and can add additional fixes for the build time issues on top.
The reopened PR seemed to have just the last commit and I don't think I can revert the reverted PR. Are u able to do that on your end? Otherwise, I'll manually make the changes.",I attached comments to the places I had to adjust because of ClangTidy and Linter.,CONTRIBUTOR
59984,elfringham,1486727281,2023-03-28 11:53:53,"@tiruk007 Sorry, I see that the r2.11 branch had a temporary build failure for a while which is what caused the error you found. This build error was not present in 2.11.0 nor is it present in 2.11.1 but was introduced and then fixed in between those two releases.","""This build error was not present in 2.11.0 nor is it present in 2.11.1 but was introduced and then fixed in between those two releases.""",CONTRIBUTOR
59984,elfringham,1486521062,2023-03-28 9:30:37,@tiruk007 _ComputeOutputShape is not in the 2.11.x source code so I think maybe you were not building 2.11?,_ComputeOutputShape is not in the 2.11.x source code,CONTRIBUTOR
59984,elfringham,1486519501,2023-03-28 9:29:29,@sdeoras Your original issue looks like you have a bad version of tensorflow_io_gcs_filesystem installed. For building TensorFlow 2.11.x please use tensorflow_io_gcs_filesystem==0.31.0.,"""Your original issue looks like you have a bad version of tensorflow_io_gcs_filesystem installed.""",CONTRIBUTOR
60047,akuegel,1486297931,2023-03-28 6:40:14,"> Actually all of it was reverted:
https://github.com/tensorflow/tensorflow/commit/7a4d2e8b1a503a45ff86d40b0387fc832302379f
Why it doesn't show properly merged in this PR, I don't know :-(
Maybe it is easier to create a new PR?","""Why it doesn't show properly merged in this PR, I don't know""",MEMBER
60047,AyanmoI,1486279647,2023-03-28 6:19:43,It looks like only the last commit got reverted. I don't think the issue (of build failing with CUDA 11.4 and CuDNN 8.2) will have resolved with reverting the last commit only.,I don't think the issue (of build failing with CUDA 11.4 and CuDNN 8.2) will have resolved with reverting the last commit only.,CONTRIBUTOR
58206,yyoon,1486039254,2023-03-28 0:27:46,"Apologies for the delayed response. Before we jump in to the details of quantization debugger, wanted to do a quick sanity check. Are you sure you're correctly quantizing your inputs before feeding them to the model?
In https://github.com/tensorflow/tensorflow/issues/58206#issuecomment-1416118721 I don't see any code for getting the quantization parameters for the input tensor, which would be causing issues.","""I don't see any code for getting the quantization parameters for the input tensor, which would be causing issues.""",MEMBER
59762,MarkDaoust,1486017154,2023-03-28 0:01:44,"Yeah, it's cool that there's a workaround, but it would be much better if we could find the script that makes these, and fix it.","""It would be much better if we could find the script that makes these, and fix it.""",MEMBER
60062,mraunak,1485812476,2023-03-27 20:20:46,"Hi @Eva-An, please try MSVC 2019 (set BAZEL_VC=C:\Program Files(x86)\Microsoft Visual Studio\2019\Enterprise\VC). I think MSVC 2022 is not supported for the TF build.",I think MSVC 2022 is not supported for the TF build.,CONTRIBUTOR
59725,sachinprasadhs,1485585386,2023-03-27 17:58:39,"Currently it falls under the limitations of `tf.function()`, you try try wrapping `tf.py_function ` https://www.tensorflow.org/api_docs/python/tf/py_function inside tf.function. If it still fails, you need to execute it outside tf.function which uses eager mode instead of graph mode.","Currently it falls under the limitations of tf.function(), you try try wrapping tf.py_function  https://www.tensorflow.org/api_docs/python/tf/py_function inside tf.function. If it still fails, you need to execute it outside tf.function which uses eager mode instead of graph mode.",CONTRIBUTOR
60047,AyanmoI,1485442660,2023-03-27 16:22:13,Can we re-open this PR to work on fixing the issues? I don't see a way to re-open this.,I don't see a way to re-open this.,CONTRIBUTOR
60047,AyanmoI,1485432593,2023-03-27 16:17:48,"> Unfortunately this change needs to be rolled back, it seems it breaks JAX build under CUDA 11.4 and CuDNN 8.2
@akuegel what are the issues? We can work on fixing them and re-merge.","""it seems it breaks JAX build under CUDA 11.4 and CuDNN 8.2""",CONTRIBUTOR
60047,akuegel,1485425131,2023-03-27 16:12:51,"Unfortunately this change needs to be rolled back, it seems it breaks JAX build under CUDA 11.4 and CuDNN 8.2","""it seems it breaks JAX build under CUDA 11.4 and CuDNN 8.2""",MEMBER
52217,bhack,1485365563,2023-03-27 15:41:08,Yes probably it hard to maintain availability on the same PR since Oct 1 2021.,It hard to maintain availability on the same PR since Oct 1 2021.,CONTRIBUTOR
59534,mattbahr,1485324526,2023-03-27 15:19:40,"ROCm CI build failure appears to be due to the removal of this constructor from `status.h` in commit: https://github.com/tensorflow/tensorflow/commit/667081b1c3d84c8fee4b5965d5e151158c74d4f8#diff-2497c155e2f97993a2cea18969d80f2d414e2f1b346cf006d8a8b655a8316b1d
Looks like other pull request builds are failing with the same errors.",ROCm CI build failure appears to be due to the removal of this constructor from status.h in commit: https://github.com/tensorflow/tensorflow/commit/667081b1c3d84c8fee4b5965d5e151158c74d4f8#diff-2497c155e2f97993a2cea18969d80f2d414e2f1b346cf006d8a8b655a8316b1d,CONTRIBUTOR
57060,kun-lu20,1485052699,2023-03-27 12:34:16,Closing this PR since we'll raise another one to fix this issue.,Closing this PR since we'll raise another one to fix this issue.,CONTRIBUTOR
52217,shkarupa-alex,1484600192,2023-03-27 6:56:21,Can anybody tell the reason why this PR was rejected?,Can anybody tell the reason why this PR was rejected?,CONTRIBUTOR
60104,mihaimaruseac,1483843707,2023-03-25 14:53:36,"Please always makes PRs from master branch against master branch.
The release branches are only updated when we do patch releases and those are limited. Currently, only the latest release is considered for patch release. This means that r2.8 won't be updated anymore.",The release branches are only updated when we do patch releases and those are limited.,COLLABORATOR
60092,tilakrayal,1482660304,2023-03-24 11:33:29,"@sachinprasadhs,
I tried to execute the mentioned code on tensoflow v2.9 and nightly & it was failing due to `No OpKernel was registered` error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/22ac50ef65e150184ddce505c2bccaba/invalidargumenterror_unsupported_data_type_for_tpu_string.ipynb).",No OpKernel was registered,CONTRIBUTOR
59719,reedwm,1482014900,2023-03-23 22:52:54,"`numpy` also measures the device transfer time, which sometimes you want, but you're right this example does not want to measure this, since it's just comparing the performance of `tf.function`.
Perhaps the example with `timeit` should be removed. I don't think the example adds much value since it doesn't show `tf.function` being faster, even if `sync_devices` is added.","numpy also measures the device transfer time, which sometimes you want, but you're right this example does not want to measure this, since it's just comparing the performance of tf.function.",MEMBER
59728,trevor-m,1482009697,2023-03-23 22:45:08,"Hi @reedwm, I spent some time on this but the cast code is a bit confusing.
Using the macros, I keep getting duplicate registration errors or missing instantiations.
I also tried to explicitly only add int32, which compiles, but when running the script it still always uses the CPU. Any idea what I'm missing here: https://github.com/trevor-m/tensorflow/commit/ac9a6b134ac022dd520cf0c3a73a222f0824418d ?",I spent some time on this but the cast code is a bit confusing.,CONTRIBUTOR
59719,DEKHTIARJonathan,1481947865,2023-03-23 21:38:49,"@reedwm I disagree. We implemented `tf.test.experimental.sync_devices()` precisely for this usecase. Let's not introduce poor/bad practices in the official documentation.
`.numpy()` is NOT a good idea to measure / assess performance. Hence we had to come up with `sync_devices()`",I disagree. We implemented tf.test.experimental.sync_devices() precisely for this usecase. Let's not introduce poor/bad practices in the official documentation. .numpy() is NOT a good idea to measure / assess performance. Hence we had to come up with sync_devices().,CONTRIBUTOR
59270,mihaimaruseac,1481697596,2023-03-23 18:26:33,"Please don't use ""update "" commit messages. These make it harder to understand at a glance at commit history what the changes are.
https://cbea.ms/git-commit/","""these make it harder to understand at a glance at commit history what the changes are.""",COLLABORATOR
59975,mihaimaruseac,1481695404,2023-03-23 18:24:47,"Please don't use ""update <file>"" commit messages. These make it harder to understand at a glance at commit history what the changes are.
https://cbea.ms/git-commit/","""update file>"" commit messages. These make it harder to understand at a glance at commit history what the changes are. https://cbea.ms/git-commit/.",COLLABORATOR
59853,yishuangP,1481642687,2023-03-23 17:56:29,"Hi guanyu, this seems a tooling issue on tensorflow side. Could you help file a separate issue for this?","""This seems a tooling issue on tensorflow side.""",CONTRIBUTOR
58067,poulsbo,1481564522,2023-03-23 17:07:42,@gbaned I should not review in Github right? NVIDIA should review here.,I should not review in Github right? NVIDIA should review here.,COLLABORATOR
58368,kulinseth,1481492160,2023-03-23 16:19:43,"> Would anyone have a workable version of ld to share? I can't install XCode 13.x once 14.x is in place with MacOS 13.x is installed..
Hi @feranick this is fixed in https://developer.apple.com/services-account/download?path=/Developer_Tools/Xcode_14.3_beta_2/Xcode_14.3_beta_2.xip.
Can you please check and verify?",I can't install XCode 13.x once 14.x is in place with MacOS 13.x is installed.,CONTRIBUTOR
59853,yishuangP,1481453187,2023-03-23 15:58:16,"`This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)`
Hmm what's the python version you have? We have an internal job that runs this script and the job is working fine. It uses python 3.9.9. I think this error is related to some configuration issue.","This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)",CONTRIBUTOR
60080,npanpaliya,1480621772,2023-03-23 5:25:35,Looks like TF 2.12.0 tag is created from r2.11 branch instead of r2.12. Because r2.12 branch shows correct version in setup.py.,"""Looks like TF 2.12.0 tag is created from r2.11 branch instead of r2.12""",CONTRIBUTOR
58994,edwardyehuang,1480590817,2023-03-23 4:36:20,"@mihaimaruseac @mohantym Could you remove the label ""awaiting PR merge"" please?","""awaiting PR merge""",CONTRIBUTOR
59938,mraunak,1480357019,2023-03-22 22:52:28,"@AndrewVallette try the command: ""bazel clean --expunge"" to clean the bazel cache and then re-rerun the build. I observed you are using --config=mkl, https://github.com/tensorflow/tensorflow/blob/master/.bazelrc#L222 as per the documentation it is yet to be supported on windows. you may need to remove --config=mkl.","""I observed you are using --config=mkl, https://github.com/tensorflow/tensorflow/blob/master/.bazelrc#L222 as per the documentation it is yet to be supported on windows.""",CONTRIBUTOR
58014,milpuz01,1480244989,2023-03-22 20:52:47,"> Hi @milpuz01 Any update on this PR? Please. Thank you!
HI @gbaned, very sorry for very long delay in addressing review comments. I believe they are addressed with [069cb25](https://github.com/tensorflow/tensorflow/pull/58014/commits/069cb256f1555ec2b1db14da86ee2d8898c7a760)","""very sorry for very long delay in addressing review comments""",CONTRIBUTOR
59607,jpienaar,1480060681,2023-03-22 18:24:20,(I don't know why AMD build failed here),I don't know why AMD build failed here.,MEMBER
56352,vufg,1480051069,2023-03-22 18:19:14,@mahmoud-abuzaina i think you need to rebase this PR to the head to trigger the tests,i think you need to rebase this PR to the head to trigger the tests.,MEMBER
59853,yishuangP,1479931590,2023-03-22 16:56:04,Hi @heguanyu sorry I just realized you'll need to use `tflite_custom_cc_library` instead of `tflite_custom_c_library`. I'm sending out another PR for review.,I'm sending out another PR for review.,CONTRIBUTOR
59504,georgthegreat,1479376086,2023-03-22 11:16:41,"Well, I can, but I am not sure if this will be of any help.
This is not the first PR which gets stalled and remains unmerged for months.",This is not the first PR which gets stalled and remains unmerged for months.,CONTRIBUTOR
59916,yijie-yang,1478627217,2023-03-21 21:45:24,"Hi @gbaned, I'm not authorized to merge this PR. Could you request it from Hexagon team?",I'm not authorized to merge this PR. Could you request it from Hexagon team?,CONTRIBUTOR
58067,poulsbo,1478486475,2023-03-21 19:45:20,"I'm seeing a failure here:
```
[.../tf2tensorrt/trt_convert_api_test.cc:255]
Expected equality of these values:
::tsl::OkStatus()
Which is: OK
(result.status())
Which is: NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_000_000)
```
Followed by a string of similar failures.
@drivanov, can you repro these problems on your side and address them?",[.../tf2tensorrt/trt_convert_api_test.cc:255] Expected equality of these values: ::tsl::OkStatus() Which is: OK (result.status()) Which is: NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_000_000),COLLABORATOR
55566,bhack,1478417531,2023-03-21 18:47:03,This is 11 months old. I will invest time again if there are reviewers resources,I will invest time again if there are reviewers resources.,CONTRIBUTOR
59900,pjpratik,1477695235,2023-03-21 11:46:09,"Hi @TheHellTower, sorry for the inconvenience. We are working on the C example as well. Please feel free to raise a PR if you have the updated code implementation available.","""We are working on the C example as well. Please feel free to raise a PR if you have the updated code implementation.""",CONTRIBUTOR
59930,elfringham,1475966065,2023-03-20 10:20:10,"@SuryanarayanaY Yes, that matches the failure I see.","""That matches the failure I see.""",CONTRIBUTOR
59815,pjpratik,1475945112,2023-03-20 10:07:44,"Hi @milincheta, sorry for the inconvenience caused.
Can I know which Android API and SDK version you are using and did you following the [#TODO](https://codelabs.developers.google.com/codelabs/recognize-flowers-with-tensorflow-on-android#6) steps for recognition?
The issue is not reproducible at my end. Can you post the errors you have encountered during the process?
Thanks.","""The issue is not reproducible at my end.""",CONTRIBUTOR
59914,tatwaichong,1474220723,2023-03-17 18:07:52,"move the new tests back, and delete the new test file.","move the new tests back, and delete the new test file.",CONTRIBUTOR
59534,swachhandl,1472718064,2023-03-16 20:47:45,"Apologies for the late response. I'm not quite sure that it could be happening due to switching out `port::InternalError` with `tsl::errors::Internal`. Are you able to check why `a.opaque()` is a nullptr? And how does it (or whether it even does) map to the input arguments to the op? If we can, we might be able to put a check in the op.",I'm not quite sure that it could be happening due to switching out port::InternalError with tsl::errors::Internal.,CONTRIBUTOR
59675,SandSnip3r,1472534269,2023-03-16 18:32:55,@pgpetrak I guess you're still the reviewer for this? I'm going to remove myself as reviewer if you can take care of this.,I guess you're still the reviewer for this?,CONTRIBUTOR
59743,trevor-m,1472408337,2023-03-16 17:31:45,@reedwm Looks like CudnnRNN doesn't support BF16 yet. Should I add a bf16 registration for the op and just cast to float always? I guess the alternative would be to exclude CudnnRNN from the auto mixed precision allowlist for bf16 only - I don't see any place where there is currently a way to have a different list for bf16 vs fp16.,"""I don't see any place where there is currently a way to have a different list for bf16 vs fp16.""",CONTRIBUTOR
60015,terryheo,1472357484,2023-03-16 17:00:19,"> Is https://github.com/tensorflow/tensorflow/issues/59631 the right issue?
Yes, this is a cherry-pick so I didn't change commit message. It's a breakage on CMake build.","""It's a breakage on CMake build.""",MEMBER
60008,mihaimaruseac,1472328733,2023-03-16 16:43:24,"No template filled in, new user, looks spam","No template filled in, new user, looks spam.",COLLABORATOR
59932,elfringham,1472103749,2023-03-16 14:37:11,"[api_compat_test_fail.zip](https://github.com/tensorflow/tensorflow/files/10991956/api_compat_test_fail.zip)
Attached log of fail with Python pip etc commands at beginning to show that protobuf 4.22.1 was indeed installed into the place that TensorFlow used in its build.
I think the gist is not going to happen, sorry, I only have a free account and its going to hit the runtime limit well before it finishes.","I think the gist is not going to happen, sorry, I only have a free account and its going to hit the runtime limit well before it finishes.",CONTRIBUTOR
59971,reedwm,1471198520,2023-03-16 2:42:33,Can you resolve conflicts? a695fffc660994e892583391d69e4c1b9ce89320 has some minor conflicts.,Can you resolve conflicts? a695fffc660994e892583391d69e4c1b9ce89320 has some minor conflicts.,MEMBER
59853,yishuangP,1470346435,2023-03-15 16:21:53,"We need to update tensorflow/lite/ios/build_frameworks.sh, it is outdated. In the meantime, can you try building each framework manually? For example, the c framework https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/ios/BUILD.apple#L102, and the selected ops framework:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/ios/BUILD.apple#L153","""It is outdated""",CONTRIBUTOR
59767,tilakrayal,1469930202,2023-03-15 12:38:48,Closing this as stale. Please reopen if this is still a valid request.,Closing this as stale. Please reopen if this is still a valid request.,CONTRIBUTOR
52576,fergushenderson,1469838640,2023-03-15 11:32:06,"It looks like Alan Kelly already wrote an update for the RELEASE.md:
* Add int16x8 support for the built-in ops `space_to_batch_nd` and
`batch_to_space_nd`
However, it looks like some tests are failing with this change, in particular //tensorflow/lite/testing:zip_test_batch_to_space_nd","""It looks like some tests are failing with this change, in particular //tensorflow/lite/testing:zip_test_batch_to_space_nd.""",CONTRIBUTOR
59943,mraunak,1469523083,2023-03-15 7:58:43,@mikcla @SuryanarayanaY I was able to run the bazel build --config=opt --copt=-march=native //tensorflow/tools/pip_package:build_pip_package successfully for TF 2.11.0 with Bazel 5.3.0 on Windows 10 and for Python 3.10. I think this error is related to the path/environmental variable setup,"""I think this error is related to the path/environmental variable setup.""",CONTRIBUTOR
59861,terryheo,1468932567,2023-03-14 22:32:17,"I also couldn't find a way to get API level in Bazel BUILD rules.
That's why TF's Android builds requires API level 26 or higher.
https://github.com/tensorflow/tensorflow/blob/master/configure.py#L760
If you want to use lower level, you need to modify the file manually.","""I also couldn't find a way to get API level in Bazel BUILD rules.""",MEMBER
59932,elfringham,1467952915,2023-03-14 11:49:04,@SuryanarayanaY You are missing an install of keras and tensorflow-estimator. Or alternatively keras-nightly and tf-estimator-nightly.,"""You are missing an install of keras and tensorflow-estimator. Or alternatively keras-nightly and tf-estimator-nightly.""",CONTRIBUTOR
59932,SuryanarayanaY,1467946084,2023-03-14 11:43:33,"@elfringham ,
Actually earlier I did not downloaded any protobuf version explicitly. I tried to test the command by installing latest protobuf version (4.22.1) now and the test is failing.The log is attached below for reference.
[#59932.log](https://github.com/tensorflow/tensorflow/files/10967721/59932.log)",I did not downloaded any protobuf version explicitly.,COLLABORATOR
58749,cantonios,1467086661,2023-03-13 22:55:55,"The issue here is that `uint32.max` is not actually representable in `float32` - and rounds up when converting, from 4294967295 to 4294967300.0. This eventually leads to a cast overflow and undefined behavior - which is why we see different values between CPU and GPU.","The issue here is that uint32.max is not actually representable in float32 - and rounds up when converting, from 4294967295 to 4294967300.0. This eventually leads to a cast overflow and undefined behavior - which is why we see different values between CPU and GPU.",CONTRIBUTOR
59950,SuryanarayanaY,1465722800,2023-03-13 8:41:47,"@Zaoyee ,
For this case it seems `tf.io.parse_single_sequence_example` not working when `context_features =FixedLenSequenceFeature()` with Sparse input. Workaround seems to use` tf.io.VarLenFeature()` instead of `FixedLenSequenceFeature()`. Could you confirm whether this works for your case or do you have your case to use only with` FixedLenSequenceFeature()` ?","""For this case it seems tf.io.parse_single_sequence_example not working when context_features = FixedLenSequenceFeature() with Sparse input""",COLLABORATOR
58762,SuryanarayanaY,1465647510,2023-03-13 7:38:41,"Hi @SajjadAemmi ,
It seems the version compatibility between tensorflow and tensorflow-io causing the problem.
Can you try installing tensorflow-io alongwith its compatible tensorflow using the below code.
`!pip install tensorflow-io[tensorflow]`
For me the reported Import error gone with above installation code.Please refer to attached [gist](https://colab.research.google.com/gist/SuryanarayanaY/ad1c1c1af38ca6c5a6a80805751426cb/58762_r2.ipynb).","""It seems the version compatibility between tensorflow and tensorflow-io causing the problem.""",COLLABORATOR
59534,mattbahr,1465087968,2023-03-12 4:12:24,"It looks like `a.opaque()` is returning a null pointer. I tried adding the following lines of code before the call to `DoBlasInternalImpl`, and my test error was triggered.
```
const void *test = a.opaque();
if (!test) {
return tsl::errors::Internal(""TEST ERROR TRIGGERED"");
}
```
@swachhandl do you have any ideas on how we might handle this better?",a.opaque() is returning a null pointer.,CONTRIBUTOR
58700,cantonios,1462584225,2023-03-09 18:42:41,"We no longer support Windows GPU builds. 2.10 was the last supported release.
https://www.tensorflow.org/install/pip#windows-native",We no longer support Windows GPU builds. 2.10 was the last supported release.,CONTRIBUTOR
59915,nouiz,1460834406,2023-03-08 20:27:34,"The CI failed, but it doesn't seem related.","The CI failed, but it doesn't seem related.",CONTRIBUTOR
59724,albertz,1460582082,2023-03-08 17:48:50,"Maybe you misunderstood what I wrote. I did not say that the behavior is unexpected or not documented. I know that it is exactly as expected. But I'm not really asking about `tf.gather` here at all.
I'm asking for a **separate** new `tf.gather_with_fallback` or whatever you want to name it. Or maybe a new **option** for `tf.gather`.
I don't want to change any existing behavior.","""Maybe you misunderstood what I wrote.""",CONTRIBUTOR
59859,jpienaar,1460566153,2023-03-08 17:40:31,Still see failure tosa/tests/tfl-to-tosa-pipeline.mlir:390:45 and :358,Still see failure tosa/tests/tfl-to-tosa-pipeline.mlir:390:45 and :358.,MEMBER
59724,SuryanarayanaY,1460512659,2023-03-08 17:05:55,"Hi @ albertz,
Unfortunately this can't be considered as Feature now.With invalid data the behaviour is expected and also documented. Please refer the Developer [comment-1439267105](https://github.com/tensorflow/tensorflow/issues/59750#issuecomment-1439267105) regarding similar issue with same Op where it was clearly mentioned that it won't be considered for fix and closed the issue well.","""Unfortunately this can't be considered as Feature now.""",COLLABORATOR
59094,johnnkp,1459233542,2023-03-08 2:56:43,"My last comment is, kotlin gradle plugin 1.8+ required Java 11 as target. Sometimes need to freeze its version to 1.7 series.","""kotlin gradle plugin 1.8+ required Java 11 as target""",CONTRIBUTOR
59859,jpienaar,1458916760,2023-03-07 21:46:41,"I'm seeing
```
mlir/tosa/tests/tfl-to-tosa-pipeline.mlir:358:45: error: undefined variable: VAL_0
// CHECK: %[[VAL_3:.*]] = ""tosa.rescale""(%[[VAL_0]]) {double_round = false, input_zp = 128 : i32, multiplier = array<i32: 1073741824>, output_zp = 0 : i32, per_channel = false, scale32 = true, shift = array<i32: 30>}
^
```
on internal result.",mlir/tosa/tests/tfl-to-tosa-pipeline.mlir:358:45: error: undefined variable: VAL_0,MEMBER
59896,mihaimaruseac,1456733783,2023-03-06 18:34:29,"Oh they are, the PR is a clone of an old change. Solving the conflicts shows that there's nothing new.","Oh they are, the PR is a clone of an old change. Solving the conflicts shows that there's nothing new.",COLLABORATOR
59857,DEKHTIARJonathan,1456627997,2023-03-06 17:47:10,"@matthiaskramm could we work at making this not SavedModel specific maybe ?
https://github.com/tensorflow/tensorflow/blob/b737ee48f1de346151b1e32e4f2d8351071e64d7/tensorflow/compiler/mlir/tensorflow/transforms/tf_savedmodel_passes.td#L93-L113","""could we work at making this not SavedModel specific maybe ?""",CONTRIBUTOR
59887,mihaimaruseac,1456408291,2023-03-06 16:01:25,This seems like a recent regression,This seems like a recent regression.,COLLABORATOR
45446,PureTryOut,1456259351,2023-03-06 14:38:02,"Note that since then libexecinfo has been removed from Alpine Linux. They had technical reasons but I don't recall them exactly, it's been a while since I looked at this.",libexecinfo has been removed from Alpine Linux.,CONTRIBUTOR
38658,tilakrayal,1455923528,2023-03-06 11:04:03,"@leeyeetonn,
As per the documentation, whenever we are trying to use dtype=tf.int32 , it throws a **TypeError** if you try to clip an int to a float value ([tf.cast](https://www.tensorflow.org/api_docs/python/tf/cast) the input to float first) which was intended.
![image](https://user-images.githubusercontent.com/81610181/223092890-98ccf5a5-339b-4f38-add6-390d5ba267ce.png)
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/clip_ops.py","""TypeError""",CONTRIBUTOR
59534,mattbahr,1454356200,2023-03-04 3:19:50,"> Did you try investigating where the error is originating from? I'm not sure why it isn't being propagated with your changes in place.
I was on the wrong track comparing those two functions in `lstm_ops.cc` and `lstm_ops_gpu.cu.cc`. The internal error is originating here:
https://github.com/tensorflow/tensorflow/blob/4139a2ada21479f923a850bf2708dfe077a22254/tensorflow/core/kernels/rnn/blas_gemm.cc#L52-L56",I was on the wrong track comparing those two functions in lstm_ops.cc and lstm_ops_gpu.cu.cc. The internal error is originating here: https://github.com/tensorflow/tensorflow/blob/4139a2ada21479f923a850bf2708dfe077a22254/tensorflow/core/kernels/rnn/blas_gemm.cc#L52-L56.,CONTRIBUTOR
59772,shkarupa-alex,1453053957,2023-03-03 6:41:38,"> Till then could you please test it against the published tested configurations as below.
@sachinprasadhs , my test with stable release and cuda 11.2 is here https://github.com/tensorflow/tensorflow/issues/59772#issuecomment-1440047780
// ptxas warnings and DWConv is 2x slower","""ptxas warnings and DWConv is 2x slower""",CONTRIBUTOR
59828,adamcrume,1452860970,2023-03-03 2:26:11,"https://github.com/tensorflow/tensorflow/blob/90fda24ac68fffb9d6c3ca8db0268d47b6404735/README.md#official-builds just says ""Status Temporarily Unavailable"", nothing about it being permanently removed. Dropping GPU support on Windows wasn't widely announced. I see https://github.com/tensorflow/tensorflow/releases/tag/v2.11.0 says ""For using TensorFlow GPU on Windows, you will need to install TensorFlow in WSL2"", but that's not even listed under Breaking Changes (which this definitely is!).","""Dropping GPU support on Windows wasn't widely announced""",CONTRIBUTOR
59837,gzmkl,1452595395,2023-03-02 21:49:59,"I am not sure if my recent two commits (unblock 2 unit tests) caused the ""ARM CI / build (3.10) (pull_request)"" failure.
I could not find much useful info from ""Details""","I am not sure if my recent two commits (unblock 2 unit tests) caused the ""ARM CI / build (3.10) (pull_request) failure""",CONTRIBUTOR
59838,SuryanarayanaY,1452158414,2023-03-02 16:27:35,"Hi @trickiwoo , Thanks for your time for reporting this issue.
Basically the issue is coming due to tf.cast which is called by this API. With CPU `tf.cast(np.nan, tf.int32)` returns the min value of `tf.int32` i.e `-2147483648`. But with GPU `tf.cast(np.nan, tf.int32)` casting to `'0'`. This may be due to inconsistent behaviour with `nan` which is an undefined value. The result is coming from `pywrap_tfe.TFE_Py_FastPathExecute`. This type of behaviour expected with garbage data.","""This type of behaviour expected with garbage data.""",COLLABORATOR
59861,shelper,1452002258,2023-03-02 14:53:51,"i am not a bazel guy, played with it a little bit, but i didnt figure out how to access this api_level variable in this bazel file. I do have api_level defined in my root WORKSPACE file as below, but just dont know how can i get it ```
android_ndk_repository(
name=""androidndk"",
path=""/path/to/android-ndk-r21"",
api_level=21,
)
```",api_level = 21,CONTRIBUTOR
58493,JW1992,1451023470,2023-03-01 23:28:11,Closing it as the underlying cause should have been fixed in [#58369](https://github.com/tensorflow/tensorflow/issues/58369#issuecomment-1373394109). As a verification I cannot reproduce it using the latest version,Closing it as the underlying cause should have been fixed in [#58369](https://github.com/tensorflow/tensorflow/issues/58369#issuecomment-1373394109) as a verification I cannot reproduce it using the latest version.,CONTRIBUTOR
59675,kaixih,1450966441,2023-03-01 22:44:50,"Basically, there are two changes of this PR: (1) update the download url and (2) update the patch file.
Then, I checked the above automatically created PR https://github.com/openxla/xla/pull/1589, and saw one test ""XLA Linux GPU"" fails.
It seems that the above PR only fetches the change (2) and I don't know how to make it to apply the change (1) as well (I guess it uses a different way to download the zip rather than `workspace2.bzl`.).
@reedwm Can you help or redirect?","""I guess it uses a different way to download the zip rather than workspace2.bzl.""",CONTRIBUTOR
59794,shelper,1449354982,2023-03-01 5:05:08,"I had a different but similar issue that builds for tflite but failed for GPU delegates
It is the final linkage step that raise an error stating `can't find -lnativewindow`..
I remember it worked before but then I pulled the latest commits and forgot where I was before. ...",I had a different but similar issue that builds for tflite but failed for GPU delegates,CONTRIBUTOR
59084,tilakrayal,1449344508,2023-03-01 4:56:21,"@dmc1778,
I tried to execute the mentioned code on tf-nightly(2.13.0-dev20230228), the crash did not happen when invalid input was provided to ragged_tensor_to_variant. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/a39039cba5adacc7a775d5cf0bd1b1a3/untitled1000.ipynb) and also please find the reference of the ubuntu22.04.
![Screenshot 2023-03-01 10 16 02 AM](https://user-images.githubusercontent.com/81610181/222048309-5e9896b0-deb0-489e-b384-e59a8ec4f617.png)","""The crash did not happen when invalid input was provided to ragged_tensor_to_variant""",CONTRIBUTOR
59534,swachhandl,1449155553,2023-03-01 1:01:30,"> I get the internal error when I run the on the GPU, but no error is produced when I run the same code on the CPU.
Did you try investigating where the error is originating from? I'm not sure why it isn't being propagated with your changes in place.","I get the internal error when I run the on the GPU, but no error is produced when I run the same code on the CPU.",CONTRIBUTOR
58665,alankelly,1447997661,2023-02-28 11:14:02,"Sorry but as I said, the error messages in most cases but this specific case would serve to only confuse users.",The error messages in most cases but this specific case would serve to only confuse users.,CONTRIBUTOR
59804,reedwm,1446962323,2023-02-27 19:44:00,@philipphack Can you please fix merge conflicts?,Can you please fix merge conflicts?,MEMBER
39479,csuter,1446760930,2023-02-27 17:39:44,"TFP is maintained by a separate group from TF proper. The TFP Layers are not under active development, so we're not planning on implementing this. We welcome PRs on the [TFP github repo](https://github.com/tensorflow/probability); these layers can be finicky to work with, though.",TFP is maintained by a separate group from TF proper.,MEMBER
58400,tucan9389,1446152236,2023-02-27 11:18:31,"Hi @SaoirseARM
We found several failure tests. Could you check it with the following commands and fix those?
```shell
bazel test //tensorflow/compiler/mlir/lite/quantization/lite:quantize_model_test
```
```shell
bazel test //tensorflow/lite/kernels:transpose_conv_test
```","""We found several failure tests. Could you check it with the following commands and fix those?""",MEMBER
59117,SuryanarayanaY,1444998558,2023-02-25 5:22:36,Reopening the issue as Bot couldn't remove stall label.,Bot couldn't remove stall label.,COLLABORATOR
59685,matthiaskramm,1444496622,2023-02-24 21:10:26,"I can't find a pass that would already do this (take the exported_names and create a wrapper function for each). The one that comes closest is `tensorflow/compiler/mlir/tfrt/transforms/lower_saved_model.cc`, but it duplicates the function(s) in question which I don't think we should.
Will write a new pass and hook up to `tf-lower-to-mlprogram-and-hlo`.","I can't find a pass that would already do this (take the exported_names and create a wrapper function for each). The one that comes closest is tensorflow/compiler/mlir/tfrt/transforms/lower_saved_model.cc, but it duplicates the function(s) in question which I don't think we should. Will write a new pass and hook up to tf-lower-to-mlprogram-and-hlo.",CONTRIBUTOR
59606,jpienaar,1443869494,2023-02-24 15:42:21,I'll check later but the error was around a non-integer/float type being queried from wrong attribute. Could you check with assertions enabled? (I think for this tool you could still build the debug version).,"""I'll check later but the error was around a non-integer/float type being queried from wrong attribute.""",MEMBER
58758,cantonios,1442900252,2023-02-24 7:10:22,"This is behaving as expected. There isn't enough input data to satisfy any ""valid"" pooling region without padding, so there's nothing that can be done. If you want padding, it needs to be specific in the input parameters by setting `padding=SAME'.","""This is behaving as expected.""",CONTRIBUTOR
59708,Jerry-Ge,1442624435,2023-02-24 0:36:53,"There is actually no implementation of the `dtype.isUnsigned()` [Reference](https://mlir.llvm.org/doxygen/classmlir_1_1quant_1_1QuantizedType.html) . Simply using getStorageTypeMin/Max for much cleaner implementation.
Jerry","""There is actually no implementation of the dtype.isUnsigned()""",CONTRIBUTOR
59276,grantjensen,1442548263,2023-02-23 23:05:58,"Hi all, I just ran through the instructions of https://github.com/Bahar-BM/test_openCL with cmake version=3.25.1 & NDK version=22.0.7026061. I ran into the following error when I ran `make`:
[openCL_inf_error_output.txt](https://github.com/tensorflow/tensorflow/files/10819243/openCL_inf_error_output.txt)
Please advise. Perhaps a different NDK version needs to be used?","""I ran into the following error when I ran make:""",CONTRIBUTOR
59606,jpienaar,1442478106,2023-02-23 21:52:08,"I see a failure on tensorflow/compiler/mlir/tosa/tests:tfl-to-tosa-pipeline.mlir.test , could you check locally?","I see a failure on tensorflow/compiler/mlir/tosa/tests:tfl-to-tosa-pipeline.mlir.test , could you check locally?",MEMBER
56937,rposts,1442411853,2023-02-23 20:49:30,Closing - no response.,No response.,CONTRIBUTOR
59787,wecing,1442369365,2023-02-23 20:10:43,"A better solution might be to clamp to `[-1, 1]` (assuming `clamp(NaN, [-1, 1])` is still `NaN`), then round (away from zero).","""A better solution might be to clamp to [-1, 1] (assuming clamp(NaN, [-1, 1]) is still NaN), then round (away from zero)""",MEMBER
59678,AndreasMadsen,1442344533,2023-02-23 19:51:20,@google-ml-butler The stalling is not my responsibility and I don't see a reason why this issue should be closed. Maybe you can remove the `stat:awaiting response` label to avoid confusion.,"""stalling""",CONTRIBUTOR
59777,MarkDaoust,1440666325,2023-02-22 19:22:48,"I see the TODO(numpy 1.24) in the setup.py file.
Darn, your other packages won't work with np 1.23?","I see the TODO(numpy 1.24) in the setup.py file. Darn, your other packages won't work with np 1.23?",MEMBER
59774,elfringham,1439905742,2023-02-22 12:01:38,"I believe that this may be masking some unit test failures as well.
//tensorflow/python/kernel_tests/proto:decode_proto_op_test
//tensorflow/python/util/protobuf:protobuf_compare_test
//tensorflow/tools/api/tests:api_compatibility_test
These fail with protobuf > 3.20.3",I believe that this may be masking some unit test failures as well.,CONTRIBUTOR
59773,elfringham,1439788615,2023-02-22 10:35:24,This does not happen with protobuf==3.20.3 installed. However the default install currently will be protobuf==4.22.0,This does not happen with protobuf==3.20.3 installed. However the default install currently will be protobuf==4.22.0.,CONTRIBUTOR
59743,SuryanarayanaY,1439775669,2023-02-22 10:26:29,"Hi @yufang67 ,
`bfloat16` data type specifically designed for TPUs only and on GPUs this wont work and hence throwing user error as `ValueError:Value passed to parameter 'input' has DataType bfloat16 not in list of allowed values: float16, float32, float64` which is intended behaviour.
Please use `mixed_bfloat16` for TPUs environment only. For GPUs you should use `mixed_float16` .
Thanks!","""ValueError:Value passed to parameter 'input' has DataType bfloat16 not in list of allowed values: float16, float32, float64""",COLLABORATOR
59633,mraunak,1439223860,2023-02-21 23:27:22,"Hi @gbaned, this file ( tensorflow/python/keras/engine/training.py) is getting called when the user is executing his code in the issue mentioned which resides inside the TensorFlow. I am not sure how fixing in Keras repo would fix this issue.","""I am not sure how fixing in Keras repo would fix this issue.""",CONTRIBUTOR
59765,elfringham,1439182976,2023-02-21 22:38:13,"Changing the installed version of the Python protobuf package did not make a difference, 3.20.1, 3.20.3, 4.21.9, 4.21.12 all the same failure. This is perhaps not surprising as the tests are c++ based.","Changing the installed version of the Python protobuf package did not make a difference, 3.20.1, 3.20.3, 4.21.9, 4.21.12 all the same failure.",CONTRIBUTOR
59765,elfringham,1439181698,2023-02-21 22:36:45,The start of these failures was bisected to https://github.com/tensorflow/tensorflow/commit/84f40925e929d05e72ab9234e53c729224e3af38,The start of these failures was bisected to https://github.com/tensorflow/tensorflow/commit/84f40925e929d05e72ab9234e53c729224e3af38.,CONTRIBUTOR
59390,zichuan-wei,1439077241,2023-02-21 20:53:18,"1 is correct, for 2, there are some complexities, the original model in keras is taking float32 as input & output, as such ""my_weight"" has to be float32 to enable the correct execution in keras. as such, when we go to 3, it has to be treated as float even in the quantized execution, because how the variable tensor is initialized. My understanding is that if you enforce ""my_weight"" as a int8, then it's no longer a valid graph in keras.","""my_weight"" has to be float32 to enable the correct execution in keras.",CONTRIBUTOR
58749,cantonios,1438884877,2023-02-21 17:54:40,"We make no guarantees that CPU and GPU results are identical, especially for garbage data. The input doesn't crash, so it's not a security issue. Error checking is expensive.
The GPU result is flushing all results to the max value (essentially saturating the input). We could potentially do the same on CPU. I wouldn't say it's a requirement though.","""The GPU result is flushing all results to the max value (essentially saturating the input)""",CONTRIBUTOR
59595,RoboTux,1436952077,2023-02-20 12:42:22,"1. Delete bazel cache
2. Run `bazel build --config=dbg -c opt tensorflow/compiler/mlir:tf-opt`
3. ls bazel-out/*-opt/bin/external/llvm-project/llvm/lib/Target
Which for me shows AArch64, AMDGPU, ARM and X86. I have a WIP patch that allow me to select the targets I want enabled and have for instance only AArch64.","""I have a WIP patch that allow me to select the targets I want enabled and have for instance only AArch64..""",CONTRIBUTOR
59722,albertz,1436614003,2023-02-20 9:23:39,"Thanks for creating the gist.
However, as I said, it is crucial to run it on Apple M1 hardware to reproduce it. In your gist, I see that you use an Nvidia GPU.
Also, you did not exactly use the commits I specified, although this probably should not matter.","""You did not exactly use the commits I specified, although this probably should not matter.""",CONTRIBUTOR
59721,pjpratik,1436545681,2023-02-20 8:33:35,"@vishnukvmd I have tried to build in MacOS with the command provided and I have received different error. Please refer the below screenshot. Thanks!
<img width=""1037"" alt=""Screenshot 2023-02-20 at 1 23 32 PM"" src=""https://user-images.githubusercontent.com/118897289/220045388-e4f9f5cf-2dda-437d-86fe-5d0e4434af27.png"">",I have tried to build in MacOS with the command provided and I have received different error.,CONTRIBUTOR
58881,svenstaro,1435993940,2023-02-19 13:47:20,This isn't upstreamed. Why is this closed? Current master branch still doesn't compile with gcc 12.,"""Why is this closed?""",CONTRIBUTOR
59300,mihaimaruseac,1434882969,2023-02-17 16:30:11,This seems to be an issue in TF Probability.,This seems to be an issue in TF Probability.,COLLABORATOR
58762,SuryanarayanaY,1434497058,2023-02-17 11:17:08,"Hi @SajjadAemmi ,
The Error coming from this piece of [code](https://github.com/keras-team/keras/blob/r2.11/keras/utils/audio_dataset.py#L171-176).
It is expecting `tensorflow_io` imported as `tfio`.
Could you try adding the import statement like below and confirm whether it works for you ?
`import tensorflow_io as tfio
`",Error coming from this piece of code.,COLLABORATOR
59506,nyadla-sys,1433973434,2023-02-17 1:35:01,"I have yet to find support for rsqrt int16 on TFLM (TensorFlow Lite micro).
Refer the below link for more details
https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/kernels/elementwise.cc#L61
bool IsRsqrtSupportedType(const TfLiteType type) {
return type == kTfLiteFloat32 || type == kTfLiteInt8;
}",I have yet to find support for rsqrt int16 on TFLM (TensorFlow Lite micro),MEMBER
59511,jszaday,1433938736,2023-02-17 0:49:33,"This program exhibits two behaviors presently unsupported by TPUs:
1) Variables with unknown shapes.
a) ""TPUs do not have registered OpKernel support for [tf.raw_ops.]Shape.""
2) Variables whose shapes change over time.
This error is appearing due to the first behavior, but resolving that will induce the second.
My guidance would be to rewrite this program to avoid these behaviors, if possible. For example, you could place these variables on the host CPU.","""TPUs do not have registered OpKernel support for [tf.raw_ops.]Shape.""",CONTRIBUTOR
59472,sachinprasadhs,1433871769,2023-02-16 23:19:01,"I don't think subclassing a `tf.Tensor` is doable in Tensorflow with the existing design, since Tensorflow tensors are immutable.
I would go with the suggestion mentioned here https://github.com/tensorflow/tensorflow/issues/59472#issuecomment-1408120513","I don't think subclassing a tf.Tensor is doable in Tensorflow with the existing design, since Tensorflow tensors are immutable. I would go with the suggestion mentioned here https://github.com/tensorflow/tensorflow/issues/59472#issuecomment-1408120513.",CONTRIBUTOR
59390,zichuan-wei,1433759918,2023-02-16 21:52:03,"Hi @MalekItani Looking at your code, I think this is expected behaviour. The input and weight to the graph are both float32, and since every time after the execution, you're writing the results back to ""My_weight"" we have to do the dequant and quant to ensure the datatype and value are still valid. If you didn't need to write back to the weight, I would recommend you use tf.const instead for weight. Please let us know this is a valid fix for you","""Looking at your code, I think this is expected behaviour.""",CONTRIBUTOR
59117,DEKHTIARJonathan,1433726712,2023-02-16 21:18:03,Do not close the issue. @qqfish didn't reply !,"""@qqfish didn't reply""",CONTRIBUTOR
59704,nrwahl2,1433707160,2023-02-16 20:59:34,"Would this be undefined by the specification anyway? The [tf.image.convert_image_dtype](https://www.tensorflow.org/api_docs/python/tf/image/convert_image_dtype) documentation says floating-point values are expected to be in the range `[0, 1)`. If so, then I'd expect inconsistent behavior and perhaps a warning message for out-of-range values.","""Would this be undefined by the specification anyway?""",CONTRIBUTOR
58998,mihaimaruseac,1433605811,2023-02-16 19:30:17,This has been suprseeded by internal code changes and also #59703,This has been suprseeded by internal code changes and also #59703,COLLABORATOR
59715,DEKHTIARJonathan,1433604177,2023-02-16 19:28:46,@rjpower seems like your change in 8a216d2c29881745c9851dd3e22b11eca56b96c3 introduced a bug,"""Your change in 8a216d2c29881745c9851dd3e22b11eca56b96c3 introduced a bug""",CONTRIBUTOR
59703,mihaimaruseac,1433465031,2023-02-16 17:38:03,"Internally the imports get rewritten based on where the code is situated in the Google monorepo and the type of the code (OSS -> third_party). Plus, code linters want the imports sorted.","""Internally the imports get rewritten based on where the code is situated in the Google monorepo and the type of the code (OSS -> third_party). Plus, code linters want the imports sorted.""",COLLABORATOR
55418,mihaimaruseac,1433321146,2023-02-16 16:01:23,The docker containers will be moved in a different direction. I think we should instead close this.,The docker containers will be moved in a different direction.,COLLABORATOR
59534,mattbahr,1432504728,2023-02-16 4:44:33,"@swachhandl I confirmed that when run on the CPU, no error is thrown, but I'm having a hard time finding a way to get the GPU code to behave in same way. Would it be valid to mark this test case to run only on GPU?",I'm having a hard time finding a way to get the GPU code to behave in same way.,CONTRIBUTOR
59619,DEKHTIARJonathan,1432444248,2023-02-16 3:26:00,@k-w-w we tried to register an OP that can define or not an optional `func`.,"""We tried to register an OP that can define or not an optional func..""",CONTRIBUTOR
59407,mihaimaruseac,1431551222,2023-02-15 15:29:31,Still waiting on file name changes,Still waiting on file name changes.,COLLABORATOR
59675,pgpetrak,1430291135,2023-02-14 19:49:22,v0.8 is not available at https://github.com/NVIDIA/cudnn-frontend at the main branch. I cannot proceed until it has been updated.,v0.8 is not available at https://github.com/NVIDIA/cudnn-frontend at the main branch.,MEMBER
59617,mihaimaruseac,1430241208,2023-02-14 19:04:56,Need to be removed from CI too. This PR is not sufficient.,Need to be removed from CI too. This PR is not sufficient.,COLLABORATOR
59685,jpienaar,1430204771,2023-02-14 18:38:28,@matthiaskramm for visibility (can't seem to assign you),Can't seem to assign you.,MEMBER
59355,kenfranko,1430188800,2023-02-14 18:27:27,In this case there is no security issue. What occurs is that -67794891775896 overflows to 1166991464. The output shape of unsorted_segment_sum is then (1166991464) which for a float64 is approximately 8GB of memory. This leads to OOM if there is less than 8GB of memory available.,-67794891775896 overflows to 1166991464.,MEMBER
59355,sachinprasadhs,1430188782,2023-02-14 18:27:26,"@dmc1778 , This is the intended behavior, as per the discussion this behavior is because of `-67794891775896` overflows to `1166991464`. The output shape of `unsorted_segment_sum` is then (`1166991464`) which for a `float64` is approximately 8GB of memory. This leads to OOM if there is less than 8GB of memory available.
In some colab runtimes this succeeds and others it fails due to different amounts of memory available.",67794891775896 overflows to 1166991464,CONTRIBUTOR
59671,AndreasMadsen,1430067671,2023-02-14 16:54:18,@pjpratik I think you should add the `comp:xla` label too. As this bug only occurs when XLA is enabled.,comp:xla label too. As this bug only occurs when XLA is enabled.,CONTRIBUTOR
59671,AndreasMadsen,1430065672,2023-02-14 16:52:45,"In this gist, I have removed the dependency on `transformers` and a large amount of irrelevant code:
V1: https://gist.github.com/AndreasMadsen/dc5785b5a55bf740c555b2fb5cdab1db (~600 LOC)
V2: https://gist.github.com/AndreasMadsen/5fdaa8431929e25cf3a990f234f88a8c (update, more code removed. ~450 LOC)
V3: https://gist.github.com/AndreasMadsen/2590423c055bc47a932a449a5161bac7 (update, more code removed. 209 LOC)
I have tried to reduce the code further, but this causes the bug to disappear.","I have tried to reduce the code further, but this causes the bug to disappear.",CONTRIBUTOR
59636,mihaimaruseac,1428874146,2023-02-13 23:50:22,We cannot accept PRs on versions that are out of life.,We cannot accept PRs on versions that are out of life.,COLLABORATOR
59247,vufg,1428804274,2023-02-13 22:44:03,There isn't any updates for 3 weeks on fixing the failed test case (added from this PR). So will close this PR for now. Feel free to reopen when the test is fixed. Thanks,"""There isn't any updates for 3 weeks on fixing the failed test case (added from this PR). So will close this PR for now.""",MEMBER
59273,terryheo,1428766170,2023-02-13 22:14:29,Updating build toolchain makes a regression. I've reverted the change and will revisit this.,Updating build toolchain makes a regression.,MEMBER
58270,mihaimaruseac,1428479396,2023-02-13 18:52:31,"2.9 is out of life, no longer updated. Always try to fix on master branch, test if the issue reproduces with nightly.","2.9 is out of life, no longer updated.",COLLABORATOR
57956,JXRiver,1426255313,2023-02-10 19:40:56,"Looks like the pywrap_gradient_exclusions.cc test still fails with
```
tensorflow/python/eager/pywrap_gradient_exclusions.cc:420:4: error: too many initializers for 'std::__array_traits<{anonymous}::OpIndexInfo, 365>::_Type' {aka '{anonymous}::OpIndexInfo [365]'}
420 | }};
| ^
```
It is unclear to me what is the cause. Maybe sync to master and rerun the script?",looks like the pywrap_gradient_exclusions.cc test still fails,CONTRIBUTOR
59440,mihaimaruseac,1426001781,2023-02-10 15:49:57,(that's the reason I closed this as not actionable),That's the reason I closed this as not actionable.,COLLABORATOR
57956,philipphack,1425036794,2023-02-10 1:04:39,I updated goldens and the gradient exclusions. I still see a problem with the doctest which doesn't seem to be caused by this change.,I updated goldens and the gradient exclusions. I still see a problem with the doctest which doesn't seem to be caused by this change.,CONTRIBUTOR
57936,hhb,1423705391,2023-02-09 6:33:26,For anyone came here: this is a duplicate of https://github.com/tensorflow/tensorflow/issues/58368. Probably an issue in `ld`.,ld.,CONTRIBUTOR
59615,nfelt,1423182354,2023-02-08 20:13:18,"This can't be merged until we actually have published a 2.12 release of TensorBoard. We're working on it right now and it should be ready later today, but please do not merge this until we let you know it's ready.","""We're working on it right now and it should be ready later today, but please do not merge this until we let you know it's ready.""",CONTRIBUTOR
59514,mihaimaruseac,1423013948,2023-02-08 17:50:55,Commit needs to be cherrypicked to 2.12 branch first,Commit needs to be cherrypicked to 2.12 branch first.,COLLABORATOR
59327,joeyearsley,1422797303,2023-02-08 15:31:49,"Tried in v2.11 and allowing cuda to see the 2nd GPU, but neither worked.","Tried in v2.11 and allowing cuda to see the 2nd GPU, but neither worked.",CONTRIBUTOR
59327,joeyearsley,1422731102,2023-02-08 15:00:13,"I've tried setting numerous `XLA_PYTHON_CLIENT_MEM_FRACTION` values from 2.0 to 16.0, the issue still persists.","I've tried setting numerous XLA_PYTHON_CLIENT_MEM_FRACTION values from 2.0 to 16.0, the issue still persists.",CONTRIBUTOR
59163,mihaimaruseac,1421352085,2023-02-07 19:48:07,"Looks like an issue on cleanup side.
@learning-to-play",Looks like an issue on cleanup side.,COLLABORATOR
59179,mihaimaruseac,1421351506,2023-02-07 19:47:34,Typical OOM,Typical OOM,COLLABORATOR
59325,mihaimaruseac,1421349509,2023-02-07 19:45:44,not actionable.,not actionable.,COLLABORATOR
59341,mihaimaruseac,1421349284,2023-02-07 19:45:31,"Typical OOM, not a vuln","Typical OOM, not a vuln.",COLLABORATOR
59359,mihaimaruseac,1421345345,2023-02-07 19:41:49,"Clear OOM, not a vuln","Clear OOM, not a vuln.",COLLABORATOR
59365,mihaimaruseac,1421344623,2023-02-07 19:41:11,Closing as fixed. No credit,No credit.,COLLABORATOR
59401,mihaimaruseac,1421338691,2023-02-07 19:35:48,"Fixed, no credit.","""No credit""",COLLABORATOR
59415,mihaimaruseac,1421337406,2023-02-07 19:34:37,"@tilakrayal this is a vulnerability report (which should not be on GitHub, anyway). @learning-to-play","""this is a vulnerability report""",COLLABORATOR
59439,mihaimaruseac,1421336429,2023-02-07 19:33:40,"Not actionable, closing. Please report using the right channels.","Not actionable, closing. Please report using the right channels.",COLLABORATOR
59506,nyadla-sys,1421287059,2023-02-07 18:53:53,"@mohantym I used PTQ(post training quantization )with representative data set and generated int8 model,however it is producing wrong results,Soon I will share colab notebook here","I used PTQ(post training quantization )with representative data set and generated int8 model,however it is producing wrong results.",MEMBER
59583,mihaimaruseac,1421265968,2023-02-07 18:35:24,"Internal error is a valid error message if arguments are invalid.
In this case, you are trying to run a kernel on a device that does not support it with the given types","""Internal error""",COLLABORATOR
59594,mihaimaruseac,1421261726,2023-02-07 18:31:42,"Please use better commit messages and PR titles.
https://cbea.ms/git-commit/","""Please use better commit messages and PR titles.""",COLLABORATOR
57591,poulsbo,1420978255,2023-02-07 15:36:48,"@pjannaty
Hi Pooya, could you please review (or mark as reviewed by NVIDIA) this PR? If I review it here in Github, then I will need to a find a separate reviewer on the internal Google side, which will slow things down. It's best if I do the reviews on the Google side, and NVIDIA staff (yourself or Nathan representing NVIDIA) handle the Github side.","If I review it here in Github, then I will need to a find a separate reviewer on the internal Google side, which will slow things down.",COLLABORATOR
59450,mihaimaruseac,1419973834,2023-02-07 0:11:40,Closing as reported wrongly. Not a vuln.,Closing as reported wrongly.,COLLABORATOR
59574,tatwaichong,1418660871,2023-02-06 8:03:12,need to work with a counterpart change in mlir: https://reviews.llvm.org/D143312,need to work with a counterpart change in mlir: https://reviews.llvm.org/D143312,CONTRIBUTOR
57956,philipphack,1416326861,2023-02-03 19:45:54,NNTest and TPUEmbeddingForServingTest were still calling `embedding_lookup_ragged`. The other test failures in the log don't seem to be related to this change.,The other test failures in the log don't seem to be related to this change.,CONTRIBUTOR
59442,mihaimaruseac,1416265982,2023-02-03 18:50:15,I just went over these stale reports as the team responsible with security (my former team) has failed to react to them in time.,"""my former team"" has failed to react to them in time.",COLLABORATOR
59442,mihaimaruseac,1416260809,2023-02-03 18:44:52,"This is not a real vulnerability. Closing.
@dmc1778 please stop posting vulns on GitHub. Please consult SECURITY.md for how to properly and ethically disclose them.",This is not a real vulnerability. Closing.,COLLABORATOR
59349,mihaimaruseac,1416259096,2023-02-03 18:42:58,"This is a vulnerability that got fixed. Since it was not reported on the proper channels, no credit is given. @dmc1778 please stop posting vulns on GitHub. Please consult SECURITY.md for how to properly and ethically disclose them.","""since it was not reported on the proper channels, no credit is given.""",COLLABORATOR
59497,mihaimaruseac,1416258446,2023-02-03 18:42:14,"3 commits for a single line change? Can you please merge the commits in just one?
In general, we don't want to insert all and every links to the README. There are too many and some don't have the same quality as others. It takes too much time to evaluate the materials, so it is better to only include links that Google has vetted. As such, I don't think this PR is worthwhile.",3 commits for a single line change?,COLLABORATOR
59441,mihaimaruseac,1416256704,2023-02-03 18:40:31,"This is a vulnerability that needs fixing.
@sushreebarsa Given that this is a vulnerability, don't be too eager to close it (by adding the label).
@dmc1778 please stop posting vulns on GitHub. Please consult SECURITY.md for how to properly and ethically disclose them.","""Don't be too eager to close it (by adding the label)""",COLLABORATOR
59124,mihaimaruseac,1416255644,2023-02-03 18:39:26,"This is a vulnerability that needs fixing. Please make sure the stalleness labels are not added as that will result in the issue getting closed before team finds time to look at the vulns (which should not be reported here)
@dmc1778 please stop posting vulns on GitHub. Please consult SECURITY.md for how to properly and ethically disclose them.","""Please make sure the stalleness labels are not added as that will result in the issue getting closed before team finds time to look at the vulns (which should not be reported here"")""",COLLABORATOR
59404,mihaimaruseac,1416254327,2023-02-03 18:38:02,"This is a vulnerability that needs fixing.
@sushreebarsa Colab doesn't always report crashes/vulnerability issues.
@dmc1778 please stop posting vulns on GitHub. Please consult SECURITY.md for how to properly and ethically disclose them.",Colab doesn't always report crashes/vulnerability issues.,COLLABORATOR
59177,mihaimaruseac,1416253417,2023-02-03 18:37:00,"Closing as fixed vuln.
@dmc1778 please stop posting vulns on GitHub. Please consult SECURITY.md for how to properly and ethically disclose them.",Closing as fixed vuln.,COLLABORATOR
59444,mihaimaruseac,1416252742,2023-02-03 18:36:14,"Closing as not a vuln
@dmc1778 please stop posting vulns on GitHub. Please consult SECURITY.md for how to properly and ethically disclose them.",Closing as not a vuln,COLLABORATOR
59368,mihaimaruseac,1416251473,2023-02-03 18:34:52,"This is a vulnerability that needs fixing.
@tilakrayal Given that this is a vulnerability, don't be too eager to close it (by adding the label).
@dmc1778 please stop posting vulns on GitHub. Please consult SECURITY.md for how to properly and ethically disclose them.","""Don't be too eager to close it (by adding the label)""",COLLABORATOR
59410,mihaimaruseac,1416251248,2023-02-03 18:34:36,"Closing as not a vuln.
@dmc1778 please stop posting vulns on GitHub. Please consult SECURITY.md for how to properly and ethically disclose them.",Closing as not a vuln.,COLLABORATOR
59084,mihaimaruseac,1416249122,2023-02-03 18:32:22,"@tilakrayal this is potentially a vulnerability. Don't add the awaiting response tag as that closes the issue before it gets fixed given team does not notice these.
Please test with nightly, not just last release.
@dmc1778 Please stop posting vulenrabilities on GitHub page. It is not the usual procedure for reporting these.","""Don't add the awaiting response tag as that closes the issue before it gets fixed given team does not notice these.""",COLLABORATOR
59302,mdfaijul,1414531719,2023-02-03 0:05:23,There seem to be no way to set loading tf_type dialect before mlir-pdll executes. Closing this PR.,No way to set loading tf_type dialect before mlir-pdll executes.,CONTRIBUTOR
59398,aaudiber,1414381492,2023-02-02 21:14:23,"`from_generator` datasets are not checkpointable due to their dependency on a Python runtime. That being said, it would be good to improve the error message to something clearer.","from_generator datasets are not checkpointable due to their dependency on a Python runtime. That being said, it would be good to improve the error message to something clearer.",CONTRIBUTOR
59117,DEKHTIARJonathan,1413873938,2023-02-02 14:55:44,"@SuryanarayanaY if it's a ""DEBUG INFO"" can you move this message to a DEBUG log. This message has nothing to do at INFO level as you clearly states in the new message (`DEBUG INFO ...`)?
CC: @reedwm","""if it's a ""DEBUG INFO"" can you move this message to a DEBUG log""",CONTRIBUTOR
59520,DonghakPark,1413622883,2023-02-02 11:58:15,"@gbaned there was failed ci.
Is there anything to do?",there was failed ci.,CONTRIBUTOR
59122,tilakrayal,1413622634,2023-02-02 11:58:02,"@sachinprasadhs,
I was able to reproduce the issue on tensorflow [v2.11](https://colab.research.google.com/gist/tilakrayal/4212b2c3d4ede31976079649f401eef1/untitled841_gpu.ipynb) and nightly and the error was Python process crashing. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/8f3c41bc045d1a3d65f5f692b764f9ca/untitled914.ipynb).",I was able to reproduce the issue on tensorflow [v2.11](https://colab.research.google.com/gist/tilakrayal/4212b2c3d4ede31976079649f401eef1/untitled841_gpu.ipynb) and nightly and the error was Python process crashing.,CONTRIBUTOR
59491,mihaimaruseac,1413040496,2023-02-02 1:59:13,"The issue is that you are mixing different versions of TF between TF and other packages installed in your ecosystem.
This is not really a TF issue, you need to make sure your Python environment (local, in colab, in Jupyter, does not really matter) contains a consistent set of dependencies. Look at the output of `pip list` for example.
In any case, since this is not a TF issue, this should be closed",mixing different versions of TF between TF and other packages installed in your ecosystem.,COLLABORATOR
59342,mihaimaruseac,1413029496,2023-02-02 1:40:46,"@pjpratik vulnerabilities occur when invalid arguments are being sent to APIs. So your reply here is wrong, this is an actual issue that must be handled to make sure it is not a vulnerability with negative effects.","""vulnerabilities occur when invalid arguments are being sent to APIs.""",COLLABORATOR
59366,mihaimaruseac,1413027179,2023-02-02 1:37:16,"@pjpratik vulnerabilities occur when invalid arguments are being sent to APIs. So your reply here is wrong.
@nimashiri this is not a vulnerability, you're receiving a user error. Please consult SECURITY.md before sending new reports as very few of them are really actionable (and they should have been submitted via private channels)","""vulnerabilities occur when invalid arguments are being sent to APIs.""",COLLABORATOR
59362,mihaimaruseac,1413025783,2023-02-02 1:35:06,@pjpratik vulnerabilities occur when invalid arguments are being sent to APIs. So your reply here is wrong. This is an actual issue. Please don't be too eager to get issues closed for wrong reasons.,"""vulnerabilities occur when invalid arguments are being sent to APIs.""",COLLABORATOR
59340,mihaimaruseac,1413025318,2023-02-02 1:34:25,"@synandi vulnerabilities occur when invalid arguments are being sent to APIs. So your reply here is wrong.
@nimashiri this is not a vulnerability, it looks like the code is hanging based on your output. Please consult SECURITY.md before sending new reports as very few of them are really actionable (and they should have been submitted via private channels)","""vulnerabilities occur when invalid arguments are being sent to APIs.""",COLLABORATOR
59325,mihaimaruseac,1413024697,2023-02-02 1:33:26,"@pjpratik this is a segfault, needs to be further investigated, not necessarily caused by OOM. Don't be over eager in pushing issues towards closure","This is a segfault, needs to be further investigated, not necessarily caused by OOM. Don't be over eager in pushing issues towards closure.",COLLABORATOR
59400,mihaimaruseac,1413022898,2023-02-02 1:30:54,"@synandi vulnerabilities occur when invalid arguments are being sent to APIs. So your reply here is wrong.
@nimashiri this is not a vulnerability, you're receiving a user error. Please consult SECURITY.md before sending new reports as very few of them are really actionable (and they should have been submitted via private channels)","""vulnerabilities occur when invalid arguments are being sent to APIs.""",COLLABORATOR
59351,mihaimaruseac,1413019915,2023-02-02 1:29:10,"@synandi : wrong answer. The termination of the process could have other causes, this needs further investigation to make sure it is not a vulnerability",wrong answer,COLLABORATOR
59355,mihaimaruseac,1413016499,2023-02-02 1:27:20,@synandi this is the wrong answer.This is a vulnerability that needs investigating and fixing. Your wrong reply just made the issue go into the stalling pipeline and then it would have been closed without an actual fix.,"""Your wrong reply just made the issue go into the stalling pipeline and then it would have been closed without an actual fix.""",COLLABORATOR
59412,mihaimaruseac,1413012774,2023-02-02 1:24:24,Closing as duplicate of already fixed issue.,Closing as duplicate of already fixed issue.,COLLABORATOR
59059,mihaimaruseac,1413009384,2023-02-02 1:18:47,"Nope, the overflow is an issue but @SuryanarayanaY gave a wrong answer.","Nope, the overflow is an issue but @SuryanarayanaY gave a wrong answer.",COLLABORATOR
59453,mihaimaruseac,1413006910,2023-02-02 1:14:55,"Closing as duplicate report.
@nimashiri please see SECURITY.md regarding how to report vulnerabilities.",Closing as duplicate report.,COLLABORATOR
59411,mihaimaruseac,1413005106,2023-02-02 1:12:21,"Closing as duplicate report.
@nimashiri please make sure to always test against latest release and nightly. Please don't automate opening issues, instead use private reporting media. See SECURITY.md.",Closing as duplicate report.,COLLABORATOR
59447,mihaimaruseac,1413003938,2023-02-02 1:10:48,Closing as duplicate report,Closing as duplicate report.,COLLABORATOR
58738,sachinprasadhs,1412930585,2023-02-02 0:05:07,"@LIONEFAN , as per the commit mentioned here https://github.com/tensorflow/tensorflow/commit/52992fc29f00fc743e07e16067f6418af6c489ff, I don't think there is any workaround to generate trace.json.gz file other than the solution mentioned above. Even the Test cases has been moved out in the above change.
![image](https://user-images.githubusercontent.com/73069040/216196724-d631275c-b7e5-4fd3-988e-5ea74a3c4df3.png)",I don't think there is any workaround to generate trace.json.gz file other than the solution mentioned above. Even the Test cases has been moved out in the above change.,CONTRIBUTOR
59506,nyadla-sys,1412539485,2023-02-01 18:38:33,"I attempted to convert to a full int8 encoder model, but it produced inaccurate results. That's why I want to try using int16 activations and int8 weights instead","I attempted to convert to a full int8 encoder model, but it produced inaccurate results.",MEMBER
58451,nyadla-sys,1412525703,2023-02-01 18:26:54,I apologize for bothering you frequently. I do not have any other options for generating the full int8 model of Whisper apart from relying on the TFLite Converter. Do you have an ETA for this feature to be available?,I apologize for bothering you frequently.,MEMBER
52853,mihaimaruseac,1412221285,2023-02-01 15:10:50,Closing due to https://github.com/tensorflow/tensorflow/commit/84f40925e929d05e72ab9234e53c729224e3af38,Closing due to https://github.com/tensorflow/tensorflow/commit/84f40925e929d05e72ab9234e53c729224e3af38.,COLLABORATOR
59438,chenmoneygithub,1411360338,2023-02-01 2:23:42,"@kulinseth Sorry I forgot to include the context for it. Currently we are seeing an issue that the new Keras optimizer runs very slowly on M1 mac, so we need the profiling tool to do a deep dive. Your help will be very much appreciated! We are struggling now without the visual tool.","""We are struggling now without the visual tool.""",CONTRIBUTOR
58806,mihaimaruseac,1411229330,2023-01-31 23:50:21,Closing since PRs should be against master branch,Closing since PRs should be against master branch.,COLLABORATOR
59334,mihaimaruseac,1411229062,2023-01-31 23:49:57,"Since 2.10 is no longer getting patched, closing this one.","Since 2.10 is no longer getting patched, closing this one.",COLLABORATOR
58022,ymwangg,1410992264,2023-01-31 20:05:46,I suspect there's a race condition between the main stream and BFC allocator asynchronous deallocation as well while debugging the ViT model. More details can be found here https://github.com/pytorch/xla/issues/4541.,I suspect there's a race condition between the main stream and BFC allocator asynchronous deallocation as well while debugging the ViT model.,CONTRIBUTOR
59153,cheshire,1410735613,2023-01-31 16:54:07,"> Attempting to fetch value instead of handling error NOT_FOUND: no CUDA devices found
You probably need to add gpu_plugin as a dependency.",Attempting to fetch value instead of handling error NOT_FOUND: no CUDA devices found,MEMBER
59381,mihaimaruseac,1409493676,2023-01-30 23:05:44,"This is valid behavior, you are requesting a large computation.
@nimashiri please report vulnerabilities outside of GitHub. Either via OSS VRP or via the security form. It seems the vulenrabilities reported on GitHub have a tendency of being mistreated by the first responders to these issues. Plus, it's not really good to report vulnerabilities in public directly.","""It seems the vulenrabilities reported on GitHub have a tendency of being mistreated by the first responders to these issues.""",COLLABORATOR
59439,mihaimaruseac,1409492665,2023-01-30 23:04:50,"@tiruk007 vulnerabilities arise when invalid arguments are passed. So replying with ""you are holding it wrong"" is not valid.
@nimashiri please report vulnerabilities outside of GitHub. Either via OSS VRP or via the security form. It seems the vulenrabilities reported on GitHub have a tendency of being mistreated by the first responders to these issues. Plus, it's not really good to report vulnerabilities in public directly.","""vulenrabilities reported on GitHub have a tendency of being mistreated by the first responders to these issues.""",COLLABORATOR
59353,mihaimaruseac,1409491897,2023-01-30 23:04:13,"This is valid behavior, user sees an error from Python, not a crash/vulnerability.
@nimashiri please report vulnerabilities outside of GitHub. Either via OSS VRP or via the security form. It seems the vulenrabilities reported on GitHub have a tendency of being mistreated by the first responders to these issues. Plus, it's not really good to report vulnerabilities in public directly.","""It seems the vulenrabilities reported on GitHub have a tendency of being mistreated by the first responders to these issues.""",COLLABORATOR
59397,mihaimaruseac,1409490856,2023-01-30 23:03:18,"@gaikwadrahul8 vulnerabilities arise when invalid arguments are passed. So replying with ""you are holding it wrong"" is not valid.
@nimashiri please report vulnerabilities outside of GitHub. Either via OSS VRP or via the security form. It seems the vulenrabilities reported on GitHub have a tendency of being mistreated by the first responders to these issues. Plus, it's not really good to report vulnerabilities in public directly.","""vulenrabilities reported on GitHub have a tendency of being mistreated by the first responders to these issues.""",COLLABORATOR
59443,mihaimaruseac,1409488203,2023-01-30 23:00:49,"(finally, please always test against latest TF release, preferably also against latest nightly; this way duplicate reports won't be made)","""finally, please always test against latest TF release, preferably also against latest nightly; this way duplicate reports won't be made""",COLLABORATOR
59443,mihaimaruseac,1409487579,2023-01-30 23:00:06,"@nimashiri please report vulnerabilities outside of GitHub.
Also, please post code and error directly on Github issue if using this form.","Please report vulnerabilities outside of GitHub. Also, please post code and error directly on Github issue if using this form.",COLLABORATOR
59416,mihaimaruseac,1409114633,2023-01-30 18:28:21,"@synandi Vulnerabilities are exploited by sending in invalid arguments.
This issue should not be autoclosed. The vulnerability in it should be resolved","""Vulnerabilities are exploited by sending in invalid arguments.""",COLLABORATOR
59448,mihaimaruseac,1409110953,2023-01-30 18:25:33,"@nimashiri Please post code snippet and error report in the issue itself, not hidden behind a link. Links can go stale, the contents behind the link are not accessible on a search in the github interface.
Also, I think it is better to use other media to report vulnerabilities to TF, not public issues.","""Links can go stale, the contents behind the link are not accessible on a search in the github interface.""",COLLABORATOR
59122,mihaimaruseac,1409107738,2023-01-30 18:23:19,"@sushreebarsa https://github.com/tensorflow/tensorflow/issues/59350#issuecomment-1399378528 does not apply. That comment was about user receiving a valid Python error, whereas here (and in several places where the same canned response was used) the user sees the Python process crashing.
CC @learning-to-play","""The Python process crashing""",COLLABORATOR
50147,mihaimaruseac,1409096390,2023-01-30 18:16:08,Closing due to https://github.com/tensorflow/tensorflow/commit/84f40925e929d05e72ab9234e53c729224e3af38 Please reopen if that does not fix.,Closing due to https://github.com/tensorflow/tensorflow/commit/84f40925e929d05e72ab9234e53c729224e3af38,COLLABORATOR
59494,mihaimaruseac,1409091940,2023-01-30 18:13:16,"CC @learning-to-play @pak-laura Is the advisory wrong?
In any case, @yili731 TF 2.9 is out of SLO.","Is the advisory wrong? In any case, @yili731 TF 2.9 is out of SLO.",COLLABORATOR
59489,vam-google,1409031094,2023-01-30 17:30:16,"These nightly releases are indeed broken:
2.12.0.dev20230129
2.12.0.dev20230128
The lates one (2.12.0.dev20230130) seems healthier. Although it still misses py311 and py310 wheels.","""It still misses py311 and py310 wheels.""",COLLABORATOR
59169,SuryanarayanaY,1408966601,2023-01-30 16:46:40,"The issue got resolved in tf-nightly (2.12.0-dev20230130).Executed the code multiple times and got expected error.Please refer to attached snapshot below.Hence the issue not reopening.
<img width=""1503"" alt=""Screenshot 2023-01-30 at 10 05 44 PM"" src=""https://user-images.githubusercontent.com/116063290/215539424-6d658630-7292-4857-a347-5327345792f9.png"">
@nimashiri FYIP. If anybody still observed check fail please confirm and the issue can be reopened.",Executed the code multiple times and got expected error.,COLLABORATOR
58768,Flamefire,1408301591,2023-01-30 9:52:15,"> I was also getting this error for ppc64le build and then `--define=tflite_with_xnnpack=false` allowed me to avoid it but now I am getting build errror [a]. Any idea what might be wrong here? Note I am trying to build TF 2.11.0
> Yes this is a bug in Eigen. I was able to solve it by including https://gitlab.com/libeigen/eigen/-/commit/886aad136111eeeb7604e1d17f62efcc4d824568 as a patch for the `eigen_archive`-`tf_http_archive`","""I was also getting this error for ppc64le build and then --define=tflite_with_xnnpack=false allowed me to avoid it but now I am getting build errror [a]. Any idea what might be wrong here?""",CONTRIBUTOR
59468,mihaimaruseac,1407422785,2023-01-28 15:29:55,"Closing spam issue (second time it happens, same person)","Closing spam issue (second time it happens, same person)",COLLABORATOR
57956,JXRiver,1407156056,2023-01-27 22:38:14,"Saw the following error in macos presubmit:
```
In file included from tensorflow/core/kernels/fill_empty_rows_op.cc:16:
In file included from ./tensorflow/core/kernels/fill_empty_rows_op.h:19:
./tensorflow/core/framework/op_kernel.h:24:10: fatal error: 'absl/time/time.h' file not found
#include ""absl/time/time.h""
^~~~~~~~~~~~~~~~~~
```
Trying to figure out why.
Meanwhile, could you sync this PR to the head? Thanks!","Trying to figure out why. Meanwhile, could you sync this PR to the head? Thanks!",CONTRIBUTOR
58769,cantonios,1406913218,2023-01-27 18:20:40,"> Hi @cantonios ,
> > I found the `Windows Bazel` test showed `Internal CI infrastructure error`. I think it's not related to this code change, right?
> > Thank you!
I don't think so - it looks like it was manually aborted. We're still waiting for other reviewers internally to approve.","""We're still waiting for other reviewers internally to approve.""",CONTRIBUTOR
58384,mihaimaruseac,1406780248,2023-01-27 16:54:01,Closing as this fixes just one letter and keeps failing CI.,Closing as this fixes just one letter and keeps failing CI.,COLLABORATOR
59463,smuzaffar,1405600924,2023-01-26 20:19:10,"According to https://github.com/google/XNNPACK/issues/4207, `ppc64le` is not supported by XNNPack and they have no plans to add the support so does this mean we can not build Tensorflow for ppc64le? Is there any way to disabled it for TF build?",ppc64le is not supported by XNNPack and they have no plans to add the support so does this mean we can not build Tensorflow for ppc64le? Is there any way to disabled it for TF build?,CONTRIBUTOR
59463,smuzaffar,1404740130,2023-01-26 9:26:29,just to add that I am able to build `tensorflow 2.11.0` for `aarch64 and x86_64` with `gcc 11.2` on `AlmaLinux 8` it only fails for ppc64le,I am able to build tensorflow 2.11.0 for aarch64 and x86_64 with gcc 11.2 on AlmaLinux 8 it only fails for ppc64le.,CONTRIBUTOR
59437,penpornk,1404142764,2023-01-25 19:46:04,"[MacOS CPU Python3](https://source.cloud.google.com/results/invocations/709f8779-1cf0-4c10-a8cd-fc06ffcddd52/targets) failures seem unrelated to this PR:
```
clang++: error: no such file or directory: 'tensorflow/compiler/mlir/lite/sparsity/sparsify_model.cc'
clang++: error: no input files
```","""Failures seem unrelated to this PR""",MEMBER
59283,sachinprasadhs,1404105657,2023-01-25 19:11:03,"`num_classes` can be anywhere between 1 to n, including the first example given in the document which shows num_classes as 3. Below is the example for `num_classes=2 `.
```
metric = tfa.metrics.F1Score(num_classes=2, threshold=0.5)
y_true = np.array([[1,1],
[0,1],
[0,1]], np.int32)
y_pred = np.array([[1,0.6],
[0.2,0.7],
[0.6,0.2]], np.float32)
metric.update_state(y_true, y_pred)
result = metric.result()
result.numpy()
array([0.6666667, 0.8 ], dtype=float32)
```",num_classes=2,CONTRIBUTOR
59117,reedwm,1402908928,2023-01-25 1:09:58,"b63d9a4ec0887cee07665b2bbfba5abf0cdd5891 makes it more clear the log is not an error. It does not remove it, however. I think removing this is hard, since it sometimes provides useful context for other errors, but I don't know the context behind it.",b63d9a4ec0887cee07665b2bbfba5abf0cdd5891,MEMBER
58769,cantonios,1402743450,2023-01-24 21:56:31,"The macos one looks like a failing test: `math_ops:approx_topk_test_cpu`. Looks like this is currently failing at head, so not related to this change. The arm CI looks like a bad pip setup. That's not blocking though, so it's okay.
Update: we see your change internally now, but it spans three different teams needing three different approvals (me for TF, someone from MLIR, and someone from SavedModel). I'll try to get it in before branch cut today, but no guarantees.",The macos one looks like a failing test: math_ops:approx_topk_test_cpu.,CONTRIBUTOR
59343,synandi,1402469248,2023-01-24 19:21:18,"@sushreebarsa, I was able to replicate the issue in [TF v2.10](https://colab.sandbox.google.com/gist/synandi/c853a7c8d51e9412152c988f70c3475a/59343_2-10.ipynb) and [TF v2.11](https://colab.sandbox.google.com/gist/synandi/eaf3157cad81ea56a6c7751492618aca/59343_2-11.ipynb) in colab. Could you please check this issue?",I was able to replicate the issue in [TF v2.10](https://colab.sandbox.google.com/gist/synandi/c853a7c8d51e9412152c988f70c3475a/59343_2-10.ipynb) and [TF v2.11](https://colab.sandbox.google.com/gist/synandi/eaf3157cad81ea56a6c7751,CONTRIBUTOR
58393,penpornk,1402465279,2023-01-24 19:18:19,"The added test failed internally, so I don't think this PR will make it into TF 2.12. But I'll follow up on this with the tfg team soon. Sorry again for the long delay!","The added test failed internally, so I don't think this PR will make it into TF 2.12.",MEMBER
59367,synandi,1402219067,2023-01-24 16:19:55,"Hi @nimashiri , the input tensor must be either 1D or 2D. You are trying to pass uint64 tensor to the API which is not supported. It supports int32 and int64. Also, the size attribute should be a non-negative integer. Kindly refer to [DenseBincount](https://www.tensorflow.org/api_docs/python/tf/raw_ops/DenseBincount) API. Please find the gist of working code [here](https://colab.sandbox.google.com/gist/synandi/d3c6e6dc94846bb09c5fa3f92681a0c7/59367.ipynb). Thank you!","""Trying to pass uint64 tensor to the API which is not supported""",CONTRIBUTOR
59328,penpornk,1401811276,2023-01-24 11:50:21,Closing this PR since the same change already went in through https://github.com/tensorflow/tensorflow/pull/59315,Closing this PR since the same change already went in through https://github.com/tensorflow/tensorflow/pull/59315.,MEMBER
57214,janpfeifer,1401568296,2023-01-24 8:51:14,"hi @mohantym , I haven't had the opportunity to use it again so I haven't tried it.
But from what I was told the Go API is no longer maintained and a friend who is actually trying to use it had to patch the current API, and is planning to wrap their TensorFlow needs in a small C code, and create their own Go API for that.
It's quite sad actually, that TensorFlow is dropping the support for various languages outside python.","""It's quite sad actually, that TensorFlow is dropping the support for various languages outside python.""",CONTRIBUTOR
59029,jprabhas,1401105719,2023-01-23 22:39:53,"@cheshire , the tests on this PR (CodeCheck and Py+Cpp) seem to be stuck. They have been running since Friday, 01/20. Could you please check what's going on with these tests ?",tests on this PR (CodeCheck and Py+Cpp) seem to be stuck.,CONTRIBUTOR
59213,annop-w,1400998820,2023-01-23 21:22:31,@penpornk Looks like the pipeline is broken. All red for other PRs with the same error.,Looks like the pipeline is broken.,CONTRIBUTOR
59101,nluehr,1400768477,2023-01-23 18:10:32,@rainwoodman is the merge of this PR blocked by something?,is the merge of this PR blocked by something?,CONTRIBUTOR
59213,annop-w,1400675198,2023-01-23 17:00:44,The ARM CI pipeline failed even before running the tests https://github.com/tensorflow/tensorflow/actions/runs/3988056012/jobs/6839402805#step:5:16929,The ARM CI pipeline failed even before running the tests.,CONTRIBUTOR
59213,annop-w,1400600145,2023-01-23 16:07:36,"@penpornk I have been trying to reproduce the failure locally, but without success so far. Could we try running the CI pipeline once more ?","I have been trying to reproduce the failure locally, but without success so far.",CONTRIBUTOR
59416,synandi,1399969635,2023-01-23 8:32:46,"Hi @nimashiri,
You are sending invalid inputs to the `sparse_concat`. Please note that
- The first argument of `sparse_concat` should be a list of at least two 2-D Tensor objects with type `int64`. - The third argument(shapes): The shape of a tensor must be a tuple of non-negative integers, but you are passing negative integers to it. Please find the gist of working code [here](https://colab.sandbox.google.com/gist/synandi/74792a6ec1c2b1f460a9773087ca7f14/59416.ipynb). Thank you!","""You are sending invalid inputs to the sparse_concat.""",CONTRIBUTOR
59350,mihaimaruseac,1399378528,2023-01-22 1:28:43,This is not an issue. You are sending invalid inputs to the API and the API returns a custom error back,"""You are sending invalid inputs to the API and the API returns a custom error back.""",COLLABORATOR
58206,LukeBoyer,1399166794,2023-01-21 3:37:17,"Hi @gsirocco, I can confirm I can reproduce this by running the supplied code. Just to confirm, are you receiving this same warning?
`tensorflow/lite/python/convert.py:789: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.`","""I can confirm I can reproduce this by running the supplied code. Just to confirm, are you receiving this same warning?""",CONTRIBUTOR
59318,trevor-m,1397735420,2023-01-19 23:20:13,"> Eek, that's a lot of changes. Just double-checking: do we have a sense of whether these all work with rocm as well?
Hi @cantonios, there is no actual change happening in this PR. This is just moving the call to `TF_CALL_bfloat16` from each individual op to the `TF_CALL_GPU_NUMBER_TYPES` macro.
If there was no problems with rocm before there should be no effect.","""That's a lot of changes. Just double-checking: do we have a sense of whether these all work with rocm as well?""",CONTRIBUTOR
59029,jprabhas,1397409293,2023-01-19 18:13:33,"> Where is this previous comment?
I couldnt find that comment either. Could you add it again, @cheshire ?","I couldnt find that comment either. Could you add it again, @cheshire ?",CONTRIBUTOR
58806,mihaimaruseac,1397378844,2023-01-19 17:49:41,Can you please make this against master branch?,Can you please make this against master branch?,COLLABORATOR
59310,cantonios,1397214976,2023-01-19 16:03:54,"Rather than add a warning in the documentation, it would be better to fix the underlying issue. Can you file an issue instead?","Rather than add a warning in the documentation, it would be better to fix the underlying issue. Can you file an issue instead?",CONTRIBUTOR
59278,penpornk,1396577757,2023-01-19 8:01:37,PR https://github.com/tensorflow/tensorflow/pull/59253 already went in. Could you please help resolve merge conflict?,"""Resolve merge conflict?""",MEMBER
58451,nyadla-sys,1396323619,2023-01-19 1:25:53,"I still see below issue with tf-nighty
0x00007fffcb614553 in mlir::quant::QuantizedType::getExpressedType() const () from ~whisper-tflite/venv/lib/python3.10/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so",I still see below issue with tf-nighty 0x00007fffcb614553 in mlir::quant::QuantizedType::getExpressedType() const () from whisper-tflite/venv/lib/python3.10/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so.,MEMBER
58966,sachinprasadhs,1396191299,2023-01-18 22:40:23,"When `validation_steps' > len(Dataset)` as per the existing implementation where it runs validation for first epoch is proper. Otherwise, it would increase the evaluation time when the validation is performed on each epoch for the above scenario.","""validation_steps"" > len(Dataset)",CONTRIBUTOR
59292,sachinmuradi,1387444050,2023-01-18 17:22:17,"> Could you please help fix the [PyLint error](https://github.com/tensorflow/tensorflow/actions/runs/3949051506/jobs/6759788729)?
> > ```
> tensorflow/dtensor/python/tests/test_util.py:278:0: C0301: Line too long (81/80) (line-too-long)
> ```
@penpornk Pushed the Fix for pylint error.","""PyLint error""",CONTRIBUTOR
59123,sushreebarsa,1387394184,2023-01-18 17:00:03,"@nimashiri Sorry for the late response!
I tried to execute the provided code and colab is crashing during execution.
Please check this [gist ](https://colab.research.google.com/gist/sushreebarsa/3e44c83f10a0e242e6c88958b0386e7d/59123.ipynb)and confirm the same?
Thank you!","""I tried to execute the provided code and colab is crashing during execution.""",CONTRIBUTOR
59293,mihaimaruseac,1387267143,2023-01-18 15:36:32,Please send PRs with size difference too.,"""Please send PRs with size difference too.""",COLLABORATOR
59029,cheshire,1387227908,2023-01-18 15:10:16,@reedwm Seems the previous comment about a request to change the variable name is not addressed?,Seems the previous comment about a request to change the variable name is not addressed?,MEMBER
59292,penpornk,1387064793,2023-01-18 13:21:02,"Could you please help fix the [PyLint error](https://github.com/tensorflow/tensorflow/actions/runs/3949051506/jobs/6759788729)?
```
tensorflow/dtensor/python/tests/test_util.py:278:0: C0301: Line too long (81/80) (line-too-long)
```",tensorflow/dtensor/python/tests/test_util.py:278:0: C0301: Line too long (81/80) (line-too-long),MEMBER
59282,cheshire,1386896599,2023-01-18 11:22:34,Could you add a test which crashes it?,Could you add a test which crashes it?,MEMBER
59037,arfaian,1386272230,2023-01-18 0:15:57,"Building the whl for armv6 should be fixed with 63c028fe7ba4d82c0ad9156533acaddaad377049 and was broken for some time due to an outdated toolchain. @Athuliva you can sync to HEAD and run the following command:
```sh
make -C tensorflow/lite/tools/pip_package docker-build TENSORFLOW_TARGET=rpi0
```",Building the whl for armv6 should be fixed with 63c028fe7ba4d82c0ad9156533acaddaad377049 and was broken for some time due to an outdated toolchain.,MEMBER
58925,aaudiber,1386163935,2023-01-17 22:23:22,"The issue with the original repro is that it doesn't set the `reshuffle_each_iteration` argument to [shuffle](https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly#shuffle). By default `shuffle` will reshuffle, so you get a different order each iteration.
You can [save dataset iterators into checkpoints](https://www.tensorflow.org/guide/checkpoint#create_the_checkpoint_objects), so that they can be restored to the exact state.",The issue with the original repro is that it doesn't set the reshuffle_each_iteration argument to shuffle.,CONTRIBUTOR
59277,snadampal,1386016333,2023-01-17 20:34:02,"Not ready for merge yet.
ARM CI build passed but it's not testing with TF_ONEDNN_ASSUME_FROZEN_WEIGHTS=1
I see few unit test failures when I tested locally with the above flag. Will fix and update the PR.",ARM CI build passed but it's not testing with TF_ONEDNN_ASSUME_FROZEN_WEIGHTS=1,CONTRIBUTOR
56852,k-w-w,1385834571,2023-01-17 18:14:12,"Sorry for extremely late reply. Use `strategy.run` to call the model, or use `model.fit`. Here is the guide: https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_custom_training_loops
Only model creation should be done inside of the distribution scope.",Sorry for extremely late reply.,MEMBER
58014,cantonios,1385731582,2023-01-17 16:57:55,"@milpuz01 is this still active? If not, we should close it.","""If not, we should close it.""",CONTRIBUTOR
59229,SuryanarayanaY,1385084305,2023-01-17 9:26:06,"@kamil5b ,
The CUDA Driver version (528.02) may not compatible with the CUDA library i.e CUDA-11.2 here. You may try to install Driver version compatible with CUDA 11.2 library from [here](https://www.nvidia.com/Download/index.aspx). For example Driver Version 470.161.03 may be suitable for CUDA 11.2-11.4. Please try this and let us know if it works.",The CUDA Driver version (528.02) may not compatible with the CUDA library i.e CUDA-11.2 here.,COLLABORATOR
59263,pjpratik,1383962053,2023-01-16 12:11:07,Closing this as spam. Will be reopened once the template is updated.,Closing this as spam.,CONTRIBUTOR
59048,akuegel,1383738443,2023-01-16 9:29:27,@cheshire I think my review comment was not addressed yet. The PR as-is will fail when we try to merge it.,I think my review comment was not addressed yet.,MEMBER
58763,reedwm,1382331275,2023-01-13 20:03:25,Sorry for the delays in merging. This is failing internal tests and we are still trying to figure out why. We hope to have it merged soon.,delays in merging,MEMBER
58022,ymwangg,1382218433,2023-01-13 18:17:47,Can someone from Google take this over? I tried several test cases but non of them can capture this bug in tensorflow.,I tried several test cases but non of them can capture this bug in tensorflow.,CONTRIBUTOR
56647,adis300,1381858755,2023-01-13 13:35:42,"> I have signed the CLA multiple times. I am pretty sure the recheck feature doesn't work for me. It show's me ""400. Thats an error.
Thats all we know."" with a Google logo.
I noticed as a Googler you could help me to rescan this pull request from the page.
""Googlers: Go [here](https://goto.google.com/prinfo/github.com%2Ftensorflow%2Ftensorflow%2Fpull%2F56647) to view more details and manage scans for this pull request.""","I have signed the CLA multiple times. I am pretty sure the recheck feature doesn't work for me. It show's me ""400. Thats an error. Thats all we know."" with a Google logo. I noticed as a Googler you could help me to rescan this pull request from the page. ""Googlers: Go [here](https://goto.google.com/prinfo/github.com%2Ftensorflow%2Ftensorflow%2Fpull%2",CONTRIBUTOR
59242,pjpratik,1381540963,2023-01-13 9:35:28,"@clime When attempting to remove a function, a segmentation fault may occur if the garbage collector in the child process is unable to properly manage the TensorFlow objects inherited from the parent process. Further, `gc.disable()`may cause memory leak issue. Can you please provide any reproducible code to further expedite the trouble shooting process?","attempting to remove a function, a segmentation fault may occur if the garbage collector in the child process is unable to properly manage the TensorFlow objects inherited from the parent process.",CONTRIBUTOR
58973,chenmoneygithub,1380866070,2023-01-12 18:53:33,"`deepcopy` is not supported due to the restriction on distribution strategy. you can actually bypass the issue by nullifying the `self._distribution_strategy`, i.e.,
```
strategy = old_optimizer._distribution_strategy
old_optimizer._distribution_strategy = None
new_optimizer = deepcopy(old_optimizer)
new_optimizer._distribution_strategy = strategy
```
Also we will not deprecate the old optimizer, but not adding new features to it.",deepcopy is not supported due to the restriction on distribution strategy.,CONTRIBUTOR
53765,Flamefire,1380204771,2023-01-12 11:42:53,This seems to be currently broken. Testing with TF 2.9.1 and Abseil 20220623.1 I get undefined references due to missing `cordz_functions` library e.g. `cordz_should_profile_slow` and even fails before due Bazel files referencing `hash_testing` which isn't included either.,I get undefined references due to missing cordz_functions library e.g. cordz_should_profile_slow and even fails before due Bazel files referencing hash_testing which isn't included either.,CONTRIBUTOR
59035,SuryanarayanaY,1379996232,2023-01-12 8:52:40,"Hi @maifeeulasad ,
The requested feature may not be possible inside the Model.However your intention is to use GPU you can use the context of tf.device('GPU') like below where the code within this context runs under GPU.
```
with tf.device('GPU'):
model = tf.keras.models.Sequential()
model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
model.add(StringLayer())
```",The requested feature may not be possible inside the Model.However your intention is to use GPU you can use the context of tf.device('GPU') like below where the code within this context runs under GPU.,COLLABORATOR
59235,mihaimaruseac,1379782280,2023-01-12 4:02:35,"nothing filled in, closing as spam","nothing filled in, closing as spam",COLLABORATOR
53146,zichuan-wei,1379510017,2023-01-11 21:31:25,"still can't reproduce the error on the latest tip, running XNNPACK on 1 - 4 threads results in: 39.441ms, 19.834ms, 14.066ms and 11.099ms respectively, matching expected behaviour",still can't reproduce the error on the latest tip,CONTRIBUTOR
59224,mohantym,1378561846,2023-01-11 10:48:43,Sorry! Closing as duplicate to PR #58753 .,Closing as duplicate to PR #58753 ..,CONTRIBUTOR
56947,penpornk,1377625938,2023-01-10 17:43:15,[ARM CI failures](https://github.com/tensorflow/tensorflow/actions/runs/3883911445/jobs/6626246458) seem to be already failing tests (7 tests; no new failures).,"""ARM CI failures""",MEMBER
58071,penpornk,1377625335,2023-01-10 17:42:47,[ARM CI failures](https://github.com/tensorflow/tensorflow/actions/runs/3856922764/jobs/6623077289) seem to be already failing tests (7 tests; no new failures).,[ARM CI failures](https://github.com/tensorflow/tensorflow/actions/runs/3856922764/jobs/6623077289) seem to be already failing tests (7 tests; no new failures).,MEMBER
55411,penpornk,1377624767,2023-01-10 17:42:23,[ARM CI failures](https://github.com/tensorflow/tensorflow/actions/runs/3883926311/jobs/6626240719) seem to be already failing tests (7 tests; no new failures).,[ARM CI failures](https://github.com/tensorflow/tensorflow/actions/runs/3883926311/jobs/6626240719) seem to be already failing tests (7 tests; no new failures).,MEMBER
59192,mihaimaruseac,1377487535,2023-01-10 15:59:26,@tilakrayal Security issues sometimes manifest by running the code multiple times. If you run the same cell multiple times you will get the session crashing.,"""Security issues sometimes manifest by running the code multiple times.""",COLLABORATOR
58640,rohan100jain,1377347791,2023-01-10 14:15:42,"To be safe one option would be to hollow out the kernel implementation and just throw an error. That could also lead to a fair amount of code deletion?
We leave it like that for one release and next release if no one complains, we can then do the full scale deletion",To be safe one option would be to hollow out the kernel implementation and just throw an error. That could also lead to a fair amount of code deletion?,MEMBER
58640,rohan100jain,1377346911,2023-01-10 14:15:02,"Do we expect these ops to ever show up in a SavedModel? If not, I think its fine to remove them.","Do we expect these ops to ever show up in a SavedModel? If not, I think its fine to remove them.",MEMBER
59203,joker-eph,1376572865,2023-01-10 1:02:31,Seems like the check should just be relaxed to not be so picky about the precision,The check should just be relaxed to not be so picky about the precision.,CONTRIBUTOR
58851,nouiz,1376397021,2023-01-09 22:09:40,"Any update? I tried locally, now it build, but without the CUDA backend included:
```
2023-01-09 22:03:33.475237: I tensorflow/compiler/xla/service/platform_util.cc:72] platform Host present but no XLA compiler available:
could not find registered compiler for platform Host -- was support for that platform linked in?
2023-01-09 22:03:33.475295: F tensorflow/compiler/xla/client/client_library.cc:127] Non-OK-status: client_status.status() status: NOT_F
OUND: no platforms found
```","I tried locally, now it build, but without the CUDA backend included: ",CONTRIBUTOR
58248,apivovarov,1376328450,2023-01-09 21:16:26,"@sushreebarsa @jkr26 I do not think the comment gives exact solution to TF-TRT users on how to return fast TF-TRT inference back.
It seems that the only working solution which we have now is to revert commit 612a531",I do not think the comment gives exact solution to TF-TRT users on how to return fast TF-TRT inference back. It seems that the only working solution which we have now is to revert commit 612a531.,CONTRIBUTOR
56783,gaurides,1376151022,2023-01-09 19:12:46,"Adding @ezhulenev back to the review, not sure how he was removed","Adding @ezhulenev back to the review, not sure how he was removed.",CONTRIBUTOR
59117,pavanimajety,1375998444,2023-01-09 17:32:46,"@mohantym I am able to consistently reproduce with the nightly build or any commit from almost first week of December. I believe it doesn't matter that the issue is not reproducible in 2.11, since I am on the latest releases for TF, Cuda and Cudnn.",I am able to consistently reproduce with the nightly build or any commit from almost first week of December.,CONTRIBUTOR
59192,mihaimaruseac,1375946498,2023-01-09 16:56:23,This is serious and will need fixing,This is serious and will need fixing.,COLLABORATOR
59081,dev0x13,1375778839,2023-01-09 15:17:09,"I confirm the issue. The exact same auxiliary script wrapping Bazel calls (with `--config=monolithic`) producing different libtensorflow_cc.so artifacts for 2.9.3 and 2.10.1: the former is actually monolithic and the latter depends on libtensorflow_framework.so.
Also with `--config=monolithic` I experience the same errors OP does.
I can provide more details on my setup if needed.",The exact same auxiliary script wrapping Bazel calls (with --config=monolithic) producing different libtensorflow_cc.so artifacts for 2.9.3 and 2.10.1: the former is actually monolithic and the latter depends on libtensorflow_framework.so. Also with --config=monolithic I experience the same errors OP does.,CONTRIBUTOR
59086,SuryanarayanaY,1375097678,2023-01-09 4:32:36,"@nimashiri ,
I tried to execute the same code few times again and i can't get the illegal memory error.Please refer to attached snapshot.
<img width=""1512"" alt=""Screenshot 2023-01-08 at 12 38 27 PM"" src=""https://user-images.githubusercontent.com/116063290/211241317-fb853e62-eae9-4e76-a57f-19077b3e4a9a.png"">
As the behaviour is not observed now shall we close the issue now ? Please feel free to report if you find the behaviour again.Awaiting your confirmation.Thank you!",I tried to execute the same code few times again and i can't get the illegal memory error.,COLLABORATOR
58657,rsanthanam-amd,1374843475,2023-01-08 14:00:54,"When I make this change, I get some other error so I am currently debugging that.","""I get some other error so I am currently debugging that.""",CONTRIBUTOR
58995,SuryanarayanaY,1374389403,2023-01-07 6:10:38,"@Neizvestnyj ,
Sorry,I couldn't notice earlier you are using DirectML plugin.For windows native Tensorflow GPU supported for TF 2.10 Versions.Anything TF>=2.11v will not support GPU on windows native.
Can you try this setting `tf.config.set_soft_device_placement(True)` and let us know if it works.","""I couldn't notice earlier you are using DirectML plugin.For windows native Tensorflow GPU supported for TF 2.10 Versions.Anything TF>=2.11v will not support GPU on windows native.""",COLLABORATOR
59094,johnnkp,1373236553,2023-01-06 7:14:45,Downgrade AGP doesn't help. Errors still occur.,Downgrade AGP doesn't help. Errors still occur.,CONTRIBUTOR
58813,akuegel,1373191858,2023-01-06 6:18:58,"I am back from vacation, but not the person who needs to approve it internally.","I am back from vacation, but not the person who needs to approve it internally.",MEMBER
58678,jameshilliard,1372921257,2023-01-05 23:23:16,Anything holding this up from being merged? I see a `Google internal checks FAILED for runs with create time 2022-12-30T16:36:16.327881750Z.` test failure but there aren't any failure details visible.,I see a Google internal checks Failed for runs with create time 2022-12-30T16:36:16.327881750Z. test failure but there aren't any failure details visible.,CONTRIBUTOR
58677,jameshilliard,1372920645,2023-01-05 23:22:22,Anything holding this up from being merged? I see a `Google internal checks FAILED for runs with create time 2022-12-30T01:04:31.307953960Z.` test failure but there aren't any failure details visible.,I see a Google internal checks Failed for runs with create time 2022-12-30T01:04:31.307953960Z. test failure but there aren't any failure details visible.,CONTRIBUTOR
59111,nluehr,1372651997,2023-01-05 19:40:54,This fix doesn't seem to be sufficient with the latest commits in the master branch. I need to investigate the test failures I'm seeing on A100 more closely. Closing this PR for now.,I need to investigate the test failures I'm seeing on A100 more closely.,CONTRIBUTOR
59078,cantonios,1372642352,2023-01-05 19:30:41,"Visible to who? I'm pushing through a fix for this on our end. The XLA error message was slightly different, causing the `*_xla_gpu` test to fail.","The XLA error message was slightly different, causing the *_xla_gpu test to fail.",CONTRIBUTOR
47285,sachinprasadhs,1372591229,2023-01-05 18:37:36,"`tf.keras.backend` API is not suggested for public use, most of the functions which are not listed fall under legacy code.
Users are encouraged to use alternative Tensorflow APIs as the direct usage.","API is not suggested for public use, most of the functions which are not listed fall under legacy code. Users are encouraged to use alternative Tensorflow APIs as the direct usage.",CONTRIBUTOR
59072,SuryanarayanaY,1371817401,2023-01-05 6:02:33,Closing the issue as it is duplicate of #58809,Closing the issue as it is duplicate of #58809,COLLABORATOR
58539,tatwaichong,1371276460,2023-01-04 18:29:05,"For PReLU, this change performs rescale on the tensor of input and alpha before and after the multiplication. My colleague @Tai78641 found out my previous patch didn't consider zero point in the operation, so the incorrect result happens in certain numeric ranges.","""My colleague @Tai78641 found out my previous patch didn't consider zero point in the operation, so the incorrect result happens in certain numeric ranges.""",CONTRIBUTOR
58248,sushreebarsa,1371178136,2023-01-04 16:54:41,"@apivovarov I tried to replicate the issue on colab using tf-nightly, and faced `RuntimeError: Tensorflow has not been built with TensorRT support`. Please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/c6be14b1375c204f48de99e30c521d04/58248.ipynb) and confirm the same?
Thank you!","""RuntimeError: Tensorflow has not been built with TensorRT support""",CONTRIBUTOR
58665,mihaimaruseac,1370387156,2023-01-04 1:19:24,"It got pulled in the Google system (see the 2 copybara checks). However, it seems internal build fails, probably some system is looking for the same error message (Hyrum's Law).
Someone will need to look at it internally and try to fix.","""It seems internal build fails, probably some system is looking for the same error message (Hyrum's Law)""",COLLABORATOR
56647,mihaimaruseac,1370386507,2023-01-04 1:17:56,"> > Please sign CLA. We shouldn't have reviewed before that
> > I signed the CLA a few months ago. Clicked ""update check"" and google showed me 404 error.
> > I tried again today, and still 404. You can help me on rechecking the commit as a Googler?
If you are not currently covered under a CLA, please visit https://cla.developers.google.com/. Once you've signed, follow the ""New Contributors"" link at the bottom of this page to update this check.","""I tried again today, and still 404.""",COLLABORATOR
58867,cantonios,1369969377,2023-01-03 16:26:30,"> Hi @cantonios Can you please review this PR ? Thank you!
I added my review, and there was no response.","I added my review, and there was no response.",CONTRIBUTOR
58769,joker-eph,1369882788,2023-01-03 15:13:04,"It's waiting on an internal reviewer to come back from vacations, should get done ""soon"".","waiting on an internal reviewer to come back from vacations, should get done ""soon"".",CONTRIBUTOR
57128,tilakrayal,1369598215,2023-01-03 10:19:10,"@sachinprasadhs,
I was able to reproduce the issue on tensorflow [v2.9](https://colab.research.google.com/gist/tilakrayal/fc3bb093faa07f69c6e82f49cb1efdd9/untitled831.ipynb), [v2.11](https://colab.research.google.com/gist/tilakrayal/01952d120576ddb7108abad1625fbadb/untitled832.ipynb) and [nightly](https://colab.research.google.com/gist/tilakrayal/2075154ea5c2a250e9399e97ea0ba2f3/untitled833.ipynb). Kindly find the gist.",I was able to reproduce the issue on tensorflow [v2.9](https://colab.research.google.com/gist/tilakrayal/fc3bb093faa07f69c6e82f49cb1efdd9/untitled831.ipynb),CONTRIBUTOR
56647,adis300,1369339151,2023-01-03 2:39:28,"> Please sign CLA. We shouldn't have reviewed before that
I signed the CLA a few months ago. Clicked ""update check"" and google showed me 404 error.
I tried again today, and still 404. You can help me on rechecking the commit as a Googler?","""update check"" and ""google showed me 404 error""",CONTRIBUTOR
58995,SuryanarayanaY,1368670476,2023-01-02 5:37:56,"@Neizvestnyj ,
I observed you installed tensorflow-cpu==2.10 and it seems you are trying to use GPU also. Since Windows native can support GPU for TF <=2.10 v could you uninstall tensorflow and then try removing cpu tag with `pip install tensorflow==2.10` and let us know if problem still persists.","""I observed you installed tensorflow-cpu==2.10 and it seems you are trying to use GPU also.""",COLLABORATOR
58996,edwardyehuang,1368099803,2022-12-30 21:45:59,"> I think the overflow fixes have to be in C++, not in Python
Agree. The example below is also dangerous.
```
import tensorflow as tf
a = 2147483647
a = tf.convert_to_tensor(a, tf.int32)
print(a) # tf.Tensor(2147483647, shape=(), dtype=int32)
b = a + tf.constant(1, tf.int32)
print(b) # tf.Tensor(-2147483648, shape=(), dtype=int32)
```","I think the overflow fixes have to be in C++, not in Python",CONTRIBUTOR
58982,mihaimaruseac,1368005020,2022-12-30 16:41:34,"TF 2.3 was released before Python 3.9. Dependencies it contains are also not able to work with Python 3.9
On the other hand, you are compiling from source and if you look at the error message closely this is what is happening to `scipy` too, pip tries to compile it from source. You could succeed if you ensure you have the needed environment to do that, but this is no longer a TF issue.
Hence, closing.",TF 2.3 was released before Python 3.9. Dependencies it contains are also not able to work with Python 3.9,COLLABORATOR
58221,mihaimaruseac,1367720168,2022-12-30 4:35:49,"There are no changes when this gets imported. This is because internal autoformatter is different.
Closing PR and associated issue.",No changes when this gets imported. This is because internal autoformatter is different. Closing PR and associated issue.,COLLABORATOR
53469,mihaimaruseac,1367452458,2022-12-29 16:37:17,This keeps failing internal tests. Let me see if I can quickly generate a fix,This keeps failing internal tests.,COLLABORATOR
52464,SamuelMarks,1367451049,2022-12-29 16:34:47,"@mihaimaruseac It's not stale, looking at master your latest commit to that file still has the wrong type: https://github.com/tensorflow/tensorflow/blob/dd79e90/tensorflow/compiler/jit/increase_dynamism_for_auto_jit_pass.cc#L80","""It's not stale, looking at master your latest commit to that file still has the wrong type""",CONTRIBUTOR
58285,bhack,1367385871,2022-12-29 14:53:45,"Here we have multiple values that we have not set (-1).
The PR is editable on your side so we could differentiate the number of days x labels. We need to set something to replace `-1`
E.g. `contribution welcome` could have a different threshold from `awaiting response` etc..",awaiting response,CONTRIBUTOR
58978,freedomtan,1367295121,2022-12-29 12:45:57,"> Should this also check that the ranks are equal?
@mihaimaruseac I guess NO, because NNAPI does support broadcast (rank=1), see [documentation of NNAPI's ADD op](https://developer.android.com/ndk/reference/group/neural-networks#group___neural_networks_1ggaabbe492c60331b13038e39d4207940e0ad681988001e5f8ab73230a311f4ab034)","""I guess NO, because NNAPI does support broadcast (rank=1), see [documentation of NNAPI's ADD op](https://developer.android.com/ndk/reference/group/neural-networks#group___neural_networks_1ggaabbe492c60331b13038e39d4207940e0ad681988001e5f8ab73230a311f4ab034)""",CONTRIBUTOR
33312,mihaimaruseac,1367078981,2022-12-29 5:18:43,Closing stale PR. Should be reopened when modular filesystem is fully implemented.,Closing stale PR.,COLLABORATOR
42903,mihaimaruseac,1367078577,2022-12-29 5:17:10,Closing stale PR. Please reopen if still needed and wanting to work on it.,Closing stale PR. Please reopen if still needed and wanting to work on it.,COLLABORATOR
54352,mihaimaruseac,1367076575,2022-12-29 5:10:32,"Closing as blocked by protobuf update. When protobuf updates this will also be updated, so the PR will be obsolete",Closing as blocked by protobuf update.,COLLABORATOR
54519,mihaimaruseac,1367075775,2022-12-29 5:08:18,Closing as it stalled. Please open a new one when coming back to this,Closing as it stalled.,COLLABORATOR
52217,mihaimaruseac,1366842044,2022-12-28 18:29:24,@gbaned another one which had the old CLA and failed to import. Needed to force trigger new CLA scan and then it works,"""had the old CLA and failed to import""",COLLABORATOR
53005,mihaimaruseac,1366837185,2022-12-28 18:19:42,@gbaned This fails because CLA check did not pass before. I triggered a force scan now,"""CLA check did not pass before""",COLLABORATOR
58817,seanshpark,1365602062,2022-12-27 4:21:31,"@joker-eph , @nutsiepully doesn't seem to have time so can someone else take the review?","""doesn't seem to have time""",CONTRIBUTOR
59015,mihaimaruseac,1365275972,2022-12-26 16:28:39,"Please only report vulnerabilities for __the latest__ version of TF. Old versions cannot be patched, and in this case, it looks like someone already patched these, so your report on old versions can only result in duplicated work.
Also check your report for duplicates against previous versions at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/README.md","""It looks like someone already patched these, so your report on old versions can only result in duplicated work.""",COLLABORATOR
59005,mihaimaruseac,1365275832,2022-12-26 16:28:07,"Using all available RAM is not an issue that we should spend time on, since in this case this is similar to user calling `malloc(a_very_huge_number)` and then expecting this to work.
There's a difference if a small input can generate a large memory allocation, but this doesn't seem to be case","Using all available RAM is not an issue that we should spend time on, since in this case this is similar to user calling malloc(a_very_huge_number) and then expecting this to work.",COLLABORATOR
59012,mihaimaruseac,1365275322,2022-12-26 16:26:26,"Please only report vulnerabilities for __the latest__ version of TF. Old versions cannot be patched, and in this case, it looks like someone already patched these, so your report on old versions can only result in duplicated work.
Also check your report for duplicates against previous versions at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/README.md
Please reopen if this occurs on 2.11, preemptively closing now as all other reports on 2.4 seem to have been resolved.","""Please only report vulnerabilities for __the latest__ version of TF.""",COLLABORATOR
59010,mihaimaruseac,1365274867,2022-12-26 16:25:06,"Please only report vulnerabilities for __the latest__ version of TF. Old versions cannot be patched, and in this case, it looks like someone already patched these, so your report on old versions can only result in duplicated work.
Also check your report for duplicates against previous versions at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/README.md
Please reopen if still present in 2.11, closing as all other similar issues so far have been false positives.","""Please only report vulnerabilities for __the latest__ version of TF.""",COLLABORATOR
59000,mihaimaruseac,1365273892,2022-12-26 16:22:09,"No. Please only report vulnerabilities for __the latest__ version of TF. Old versions cannot be patched, and in this case, it looks like someone already patched these, so your report on old versions can only result in duplicated work.
Also check your report for duplicates against previous versions at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/README.md","No. Please only report vulnerabilities for __the latest__ version of TF. Old versions cannot be patched, and in this case, it looks like someone already patched these, so your report on old versions can only result in duplicated work. Also check your report for duplicates against previous versions at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/README.md.",COLLABORATOR
59004,mihaimaruseac,1365272936,2022-12-26 16:19:31,"This seems invalid. Affected version is old, out of support.","This seems invalid. Affected version is old, out of support.",COLLABORATOR
58999,mihaimaruseac,1365272147,2022-12-26 16:17:25,This seems invalid. Log message does not show claimed crash. Affected version is old.,This seems invalid. Log message does not show claimed crash. Affected version is old.,COLLABORATOR
53750,mihaimaruseac,1365268729,2022-12-26 16:10:58,Someone needs to manually shepherd this inside as it touches code managed by copybara (inside it looks different),"""Manually shepherd this inside as it touches code managed by copybara (inside it looks different"")""",COLLABORATOR
53750,georgthegreat,1365107876,2022-12-26 11:28:05,"@gbaned, is there a chance for this PR to be accepted and merged?
I have rebased this PR twice and it is almost one year old as of today.
If it is not going to be merged, I suggest closing this PR.","I have rebased this PR twice and it is almost one year old as of today. If it is not going to be merged, I suggest closing this PR.",CONTRIBUTOR
58368,freedomtan,1364678435,2022-12-25 12:59:50,@nitins17 FYR. Setting `DEVELOPER_DIR=/Applications/Xcode.app/Contents/Developer` doesn't work for me on macOS 13.1 + Xcode 14.2 (either M1 or x86_64).,Setting DEVELOPER_DIR=/Applications/Xcode.app/Contents/Developer doesn't work for me on macOS 13.1 + Xcode 14.2 (either M1 or x86/64).,CONTRIBUTOR
58970,cheshire,1363206067,2022-12-22 18:19:58,"> there are a number of tests in tensorflow that have one or more sanitizers disabled, when they shouldn't, i.e. noasan or nomsan tags
Sure, but then I don't think this patch would solve it given that otherwise all XLA tests would have to have noasan?
We've discussed it a bit more, why not instead write
```
static Thread* t = tsl::Env::Default()->StartThread(....)
```
I think this should be enough to silence asan, and the resource usage would be the same?",noasan or nomsan tags,MEMBER
58752,SuryanarayanaY,1361044398,2022-12-21 9:09:45,"Hi @YaoJiayi ,
Sorry for delayed response.
The API `tf.random.stateless_uniform` for `float` dtypes seems working fine with the default range [0,1) only. Please refer attached [gist](https://colab.sandbox.google.com/gist/SuryanarayanaY/6784c0a6b2ae5936faea3e1106d99e2a/58752-gpu.ipynb).","""The API tf.random.stateless_uniform for float dtypes seems working fine with the default range [0,1] only""",COLLABORATOR
58967,nitins17,1360619142,2022-12-21 0:59:00,Closed as I still need to add configs for Python 3.11 in `tensorflow/tools/toolchains/remote_config/configs.bzl`,I still need to add configs for Python 3.11 in tensorflow/tools/toolchains/remote_config/configs.bzl.,MEMBER
58857,impjdi,1360437506,2022-12-20 23:11:59,"I don't have an AMD GPU to test this out. It could be OpenCL driver bug, or our shaders may be relying on undefined behavior of mobile GPUs that don't translate to your AMD GPU. The only thing I would try out is using MAX PRECISION as top inference priority to make sure it's not the FP16 getting in the way. Regardless of the outcome, this is beyond our level of support.","I don't have an AMD GPU to test this out. It could be OpenCL driver bug, or our shaders may be relying on undefined behavior of mobile GPUs that don't translate to your AMD GPU. The only thing I would try out is using MAX PRECISION as top inference priority to make sure it's not the FP16 getting in the way. Regardless of the outcome, this is beyond our level of support.",CONTRIBUTOR
58851,nouiz,1360151782,2022-12-20 20:32:16,"`//tensorflow/compiler/xla/tools:run_hlo_module` is also broken.
Can the build of one of those 2 tools (or both) be included in the CI?
It isn't great to have our basic tools broken for over a week now.",//tensorflow/compiler/xla/tools:run_hlo_module is also broken.,CONTRIBUTOR
58843,mihaimaruseac,1359831997,2022-12-20 17:20:05,There is no TF 2.13 released yet.,No TF 2.13 released yet.,COLLABORATOR
40614,mihaimaruseac,1359692423,2022-12-20 16:35:52,The issue here is not about Windows.,The issue here is not about Windows.,COLLABORATOR
58768,Flamefire,1359412840,2022-12-20 14:03:06,"Further investigation showed that this (`__subpackages__`) wasn't the exact issue. But it is this: https://github.com/tensorflow/tensorflow/blob/8a254397ff86a5a2efc0b51ef2eaee1059299095/tensorflow/lite/delegates/xnnpack/BUILD#L128
As ""PowerPC is not supported in XNNPACK"" this means now TF is not supported on PowerPC as the build will fail. A fix might be to use a better default for `tflite_with_xnnpack` which depends on the architecture.","Further investigation showed that this (__subpackages__) wasn't the exact issue. But it is this: https://github.com/tensorflow/tensorflow/blob/8a254397ff86a5a2efc0b51ef2eaee1059299095/tensorflow/lite/delegates/xnnpack/BUILD#L128 As ""PowerPC is not supported in XNNPACK"" this means now TF is not supported on PowerPC as the build will",CONTRIBUTOR
58768,Flamefire,1359047186,2022-12-20 9:12:25,Any update here? The referenced commit doesn't seem to made it into any branch of XNNPACK so this needs to be fixed from this repo,The referenced commit doesn't seem to made it into any branch of XNNPACK so this needs to be fixed from this repo.,CONTRIBUTOR
57104,carlthome,1357441338,2022-12-19 10:39:35,"Any update on this from Googlers? This is a pretty tricky annoyance in our CI/CD workflows on GitHub Actions to Vertex and Dataflow, since `gcloud` just works while TensorFlow Datasets (`tfds` CLI and `tfds.load`) does not without jumping through hurdles.","pretty tricky annoyance in our CI/CD workflows on GitHub Actions to Vertex and Dataflow, since gcloud just works while TensorFlow Datasets (tfds CLI and tfds.load) does not without jumping through hurdles.",CONTRIBUTOR
58665,mihaimaruseac,1356591612,2022-12-18 0:45:33,"Can you make one single commit with the change in the first file? 7 commits for just one single file change are a little bit too much.
You can squash all commits to a single one too if you don't want to open a separate PR.",7 commits for just one single file change are a little bit too much.,COLLABORATOR
58738,tilakrayal,1354345761,2022-12-16 7:53:12,"@LIONEFAN,
Sorry for the delay. I tried on tensorflow v2.10 to reproduce the issue but I was not able to fetch the files which are mentioned. Kindly find the [gist](https://colab.research.google.com/gist/tilakrayal/a78bb5afe17b093e69cb489d2281cf12/2-1000000.ipynb) and the image for the reference.
![image](https://user-images.githubusercontent.com/81610181/208050074-5c7115bc-fdaf-47f1-b923-e989b9bb1877.png)",I tried on tensorflow v2.10 to reproduce the issue but I was not able to fetch the files which are mentioned.,CONTRIBUTOR
58675,akuegel,1354319162,2022-12-16 7:18:27,"> @trisolaran @akuegel Can one of you approve again this PR? It was already approved. I only fixed some builds issues after the approval.
I approved it 3 days ago, and I don't see another commit by you after that. So I guess we pulled in the latest state? I had to do a few more fixes, but should be merged soon.","I had to do a few more fixes, but should be merged soon.",MEMBER
58675,nouiz,1353874075,2022-12-15 23:44:21,"@trisolaran @akuegel Can one of you approve again this PR?
It was already approved. I only fixed some builds issues after the approval.",Can one of you approve again this PR? It was already approved. I only fixed some builds issues after the approval.,CONTRIBUTOR
58720,reedwm,1353659679,2022-12-15 20:18:48,"Also note, I submitted 5d27ce5a27e5123266f5872b3a25514f0c9453fe which causes even more merge conflicts. I will address. We made some similar changes to various helper functions, but since I was aware of internal tests I ensured they all passed. It was easier to submit my change first then revert the equivalent changes you made.","""It was easier to submit my change first then revert the equivalent changes you made.""",MEMBER
58720,reedwm,1353598411,2022-12-15 19:23:44,There are merge conflicts and internal test failures. I will fix and merge. Please don't update the PR to address merge conflicts since it will revert the internal test failure fixes I am doing.,merge conflicts and internal test failures.,MEMBER
44369,impjdi,1353473279,2022-12-15 17:43:18,"No, our OpenGL delegate doesn't accommodate per-op profiling (IIUC it's about per-shader profiling not being a part of the GL ES API, and only available as an extension which can be buggy / inaccurate for each vendor; Metal or OpenCL on the other hand have per-shader profiling in their API) and therefore we don't have an equivalent profiler for OpenGL.","""IIUC it's about per-shader profiling not being a part of the GL ES API, and only available as an extension which can be buggy / inaccurate for each vendor; Metal or OpenCL on the other hand have per-shader profiling in their API"")",CONTRIBUTOR
57630,bhack,1353336265,2022-12-15 16:11:57,@MichaelHudgins Let me know when you are ready so that I can reopen this old contribution. Currently the build time/required computing resources is one of the biggest bottleneck to the non enterprise OSS contribution in the repo.,"""Currently the build time/required computing resources is one of the biggest bottleneck to the non enterprise OSS contribution in the repo.""",CONTRIBUTOR
54352,mihaimaruseac,1353315364,2022-12-15 15:59:49,This should happen at the same time as upgrading protobuf. That one is blocked.,This should happen at the same time as upgrading protobuf. That one is blocked.,COLLABORATOR
58851,chr1sj0nes,1353072494,2022-12-15 13:32:33,"> @chr1sj0nes Could you take look at this issue? seems that the broken is related to your commits to `tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc`
Apologies for the slow reply. I can't see how my commit might have caused this error. @d0k 's theory looks a lot more likely to me.",I can't see how my commit might have caused this error.,CONTRIBUTOR
58851,nouiz,1352100692,2022-12-14 20:15:33,"> @mohantym I tried to reproduce the issue on Colab but got a different output. Please find the gist [here](https://colab.research.google.com/gist/tiruk007/9ffa05e9d87cd8f19d51065ec8ecad6f/untitled41.ipynb) for reference. Could you please look into this.
Note, the colab checkout TF 2.11. The issue reported here is about the master branch, not the last release.",I tried to reproduce the issue on Colab but got a different output.,CONTRIBUTOR
42138,mihaimaruseac,1351961713,2022-12-14 18:46:21,"Until modular TF is brought back, these updates are just treading water. We cannot take the PR in until modular TF work is being done and until then there will be conflicts and staleness.
It might be better to close it and revisit later, given there is no way to mark this work as paused. Or, maybe add a ""paused"" label?","Until modular TF is brought back, these updates are just treading water.",COLLABORATOR
58522,JXRiver,1351955487,2022-12-14 18:44:50,"This may be related to `py_function`. py_function requires GIL and is generally not recommended to be used in tf.data except for experimental purpose. See the limitation part of https://www.tensorflow.org/api_docs/python/tf/py_function.
Did you try use `tf.funciton` instead of `py_function`?",py_function. py_function requires GIL and is generally not recommended to be used in tf.data except for experimental purpose. See the limitation part of https://www.tensorflow.org/api_docs/python/tf/py_function. Did you try use tf.funciton instead of py_function?.,CONTRIBUTOR
55498,mihaimaruseac,1351549482,2022-12-14 14:46:43,"Can you rebase this on master again, please?","Can you rebase this on master again, please?",COLLABORATOR
58763,sergeykozub,1351476256,2022-12-14 14:18:55,"> Can you run Google internal performance tests first?
I've run the internal benchmarks, and there are no regressions (and no improvements though, probably the new code path is not triggered in the existing benchmarks).",Can you run Google internal performance tests first?,CONTRIBUTOR
58090,akuegel,1350693733,2022-12-14 9:14:48,"> @rsanthanam-amd Can you please change your fix so that instead of setting GPU count to 0, you change the MaybeCreateNcclCommunicator() method to also not create a NCCL manager if the device_filter doesn't include any GPU?","Can you please change your fix so that instead of setting GPU count to 0, you change the MaybeCreateNcclCommunicator() method to also not create a NCCL manager if the device_filter doesn't include any GPU?",MEMBER
55303,API92,1350048895,2022-12-13 23:32:45,"I am using TPUStrategy. There is no TPUEstimator in my code. The word ""TPUEstimator"" only appears in the exception message thrown by TensorFlow in what I consider to be a bug.","I am using TPUStrategy. There is no TPUEstimator in my code. The word ""TPUEstimator"" only appears in the exception message thrown by TensorFlow in what I consider to be a bug.",CONTRIBUTOR
58870,shawnwang18,1349203366,2022-12-13 17:31:52,"@cheshire I did not find a create new discussion option in https://github.com/openxla/xla/discussions, is there some permission issue?","I did not find a create new discussion option in https://github.com/openxla/xla/discussions, is there some permission issue?",CONTRIBUTOR
44918,pjpratik,1348186974,2022-12-13 10:58:54,"@edgarbc !
When original h5 model is used on TF 2.11 using [this](https://github.com/tensorflow/tensorflow/issues/44918#issuecomment-730525525), It throws shape mismatch error. Could you please refer this [gist](https://colab.research.google.com/gist/pjpratik/77f19ce813bd3371185064cb32787ab6/44918.ipynb). The original h5 model is taken from [here](https://www.dropbox.com/s/gb78ssrnv94puk9/mymodel.h5?dl=0) and tested on a random image with (640,480,3) shape to run the gist.","""It throws shape mismatch error""",CONTRIBUTOR
58675,nouiz,1346963460,2022-12-12 17:59:59,"I updated this PR with another commit to fix the build on some platform.
I mixed NCCL and NVTX macro.",I updated this PR with another commit to fix the build on some platform. I mixed NCCL and NVTX macro.,CONTRIBUTOR
58261,mihaimaruseac,1346931003,2022-12-12 17:33:15,"https://github.com/tensorflow/tensorflow/blob/de8c87d351456d5f0d6fcf6b6d3c7d5e63c2b701/tensorflow/core/kernels/rnn/gru_ops.cc#L32-L35
`OP_REQUIRES_OK` has a similar semantic, if the status is no `Ok` the macro finishes execution of the kernel and returns the invalid `Status` back to the user.",OP_REQUIRES_OK,COLLABORATOR
58441,cheshire,1346455254,2022-12-12 13:02:40,"Adding a huge number of ROCM-specific tests is not great, could we generalize existing ones instead?","Adding a huge number of ROCM-specific tests is not great, could we generalize existing ones instead?",MEMBER
58746,roserg,1346111959,2022-12-12 8:55:48,"Conv2DTranspose supported on gpu.
Not supported Pack and Shape that you have in the first case.",Not supported,CONTRIBUTOR
58851,shawnwang18,1345819296,2022-12-12 3:28:06,Seems that recent commits to `tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc` has broken the build? bazel build --verbose_failures //tensorflow/compiler/xla/tools:replay_computation_gpu,"""Recent commits to tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc has broken the build""",CONTRIBUTOR
58832,meteorcloudy,1344268731,2022-12-09 12:46:46,Looks like tf_runtime needs to be fixed first.,Looks like tf_runtime needs to be fixed first.,MEMBER
44306,pjpratik,1343932616,2022-12-09 7:04:22,@sayakpaul I was trying to reproduce this in TF 2.11. I got data access forbidden error. Can you please check [this](https://colab.research.google.com/gist/pjpratik/49028e7ef71fb197678dcb4d7013b677/albert_keras.ipynb) gist and help me with this. Thank you.,I was trying to reproduce this in TF 2.11,CONTRIBUTOR
58816,shkarupa-alex,1342172957,2022-12-08 7:03:12,+1 waiting for actual cuda version support,waiting for actual cuda version support,CONTRIBUTOR
57914,freedomtan,1340930107,2022-12-07 12:59:12,"> @freedomtan I suspect this is the problem now, thanks for reminding me.
Yes, I am pretty sure that's the ld problem (or maybe compiler+ld problem). It's not an M1 only issue. I ran into same problems on MacBook Pro 2018 + Ventura + Xcode 14.1",I am pretty sure that's the ld problem (or maybe compiler+ld problem). It's not an M1 only issue. I ran into same problems on MacBook Pro 2018 + Ventura + Xcode 14.1.,CONTRIBUTOR
56464,mihaimaruseac,1340070105,2022-12-06 22:04:42,Unfortunately nothing could be done until #53234 or `cc_shared_library` migration is fully done,cc_shared_library migration is fully done.,COLLABORATOR
57880,sachinprasadhs,1340058832,2022-12-06 21:53:28,"We intentionally don't check on GPU, and this is usually documented. On GPU, the check is not worth the extra kernel launch required. Garbage in, garbage out. The calculation has no meaning regardless.
Specifically, quoting from the docs for `scatter_nd`:
> Note that on CPU, if an out of bound index is found, an error is returned. On GPU, if an out of bound index is found, the index is ignored.","""The calculation has no meaning regardless.""",CONTRIBUTOR
58675,nouiz,1340052288,2022-12-06 21:46:22,"Sorry for the churn, this need a new approval as I updated again the last commit.
I missed one line in the last commit, so if_nccl wasn't available.
This changed between the 2 branches I'm working on and I didn't expected that.","I missed one line in the last commit, so if_nccl wasn't available.",CONTRIBUTOR
58676,kanvi-nervana,1339935468,2022-12-06 20:00:17,"@DrChrisLevy Can you please share the TF-version you tried?
I used TF-2.10 and did not see the issue with the env var set to false. However, I see the issue in TF-2.11 again and on further investigation found that the env var was removed from TF-2.11+ with this [commit](https://github.com/tensorflow/tensorflow/commit/0bbe98a93de2b19dcd4fb0f67fcdea9995c1e1db)
Can you please test TF-2.10?
cc: @sagunb","""Can you please test TF-2.10?""",CONTRIBUTOR
57679,markub3327,1339772160,2022-12-06 18:07:40,"I'm waiting for the stable release of 2.12.0, there will be TensorRT 8 working.
Until that, I'm using TF 2.9.
TF 2.10 and 2.11 are terrible releases.","Until that, I'm using TF 2.9. TF 2.10 and 2.11 are terrible releases.",CONTRIBUTOR
56868,rahulbatra85,1339695155,2022-12-06 17:08:57,"> Is it possible to do the check in a cleaner way? E.g. SkipTest based on platform?
@cheshire I am doing it based on platform, but this is the only way I was able to get platform info. Some other fields don't differentiate between GPU platforms like CUDA or ROCm","I am doing it based on platform, but this is the only way I was able to get platform info. Some other fields don't differentiate between GPU platforms like CUDA or ROCm.",CONTRIBUTOR
58676,penpornk,1338659876,2022-12-06 2:54:03,"> @DrChrisLevy I am an engineer from Intel and was able to reproduce this issue on TF2.10+. This [commit](https://github.com/tensorflow/tensorflow/commit/c1286fe88e14146a47) which changes the default value of the env var TF_RUN_EAGER_OP_AS_FUNCTION to true seems to be the causing the leak. As a work around, could you please try setting the env var to false at runtime. e.g. TF_RUN_EAGER_OP_AS_FUNCTION=false python test_script.py
Cc'ing @sagunb, the author of the commit.","""This [commit](https://github.com/tensorflow/tensorflow/commit/c1286fe88e14146a47) which changes the default value of the env var TF_RUN_EAGER_OP_AS_FUNCTION to true seems to be the causing the leak""",MEMBER
58742,angerson,1338350760,2022-12-05 23:28:58,"While I appreciate the work you've done here, I prefer not to accept this change because:
- These are ""officially supported"" TensorFlow docker containers, and it's not our goal to include community / third-party support as well
- The complicated ""partials"" system is a nightmare to maintain, and we're discussing deprecation of these containers in favor of a different Docker recommendation (but no concrete plans yet).","- The complicated ""partials"" system is a nightmare to maintain",MEMBER
58676,kanvi-nervana,1338276396,2022-12-05 22:38:17,"@DrChrisLevy I am an engineer from Intel and was able to reproduce this issue on TF2.10+. This [commit](https://github.com/tensorflow/tensorflow/commit/c1286fe88e14146a47) which changes the default value of the env var TF_RUN_EAGER_OP_AS_FUNCTION to true seems to be the causing the leak. As a work around, could you please try setting the env var to false at runtime. e.g. TF_RUN_EAGER_OP_AS_FUNCTION=false python test_script.py","""This [commit](https://github.com/tensorflow/tensorflow/commit/c1286fe88e14146a47) which changes the default value of the env var TF_RUN_EAGER_OP_AS_FUNCTION to true seems to be the causing the leak""",CONTRIBUTOR
58734,trevor-m,1338200092,2022-12-05 21:31:22,"> Hi @trevor-m, Google internal checks failed. I fixed it. Could you please sync? Sorry for the inconvenience.
Thanks @learning-to-play! I synced the PR but it looks like the internal checks are failing again. Let me know if I can help at all.","""It looks like the internal checks are failing again.""",CONTRIBUTOR
58675,nouiz,1338033552,2022-12-05 19:32:18,This PR broke the build on CPU. I'll fix it locally and update it. Not ready to be approved.,This PR broke the build on CPU.,CONTRIBUTOR
58658,nouiz,1338007042,2022-12-05 19:16:44,"I do not think this is a regression. I already saw such case where older GPUs where vectorized. So I suppose this isn't a regression.
Vectorization wasn't as important as it is right now, so adding new vectorization on old GPU the impact won't be high on them.
Those are older GPUs. Maxwell was released February 2014. Kepler even before that and is deprecated.
So everything make me think this is a very low priority new feature.","I already saw such case where older GPUs where vectorized. So I suppose this isn't a regression. Vectorization wasn't as important as it is right now, so adding new vectorization on old GPU the impact won't be high on them. Those are older GPUs. Maxwell was released February 2014. Kepler even before that and is deprecated. So everything make me think this is a very low priority new feature.",CONTRIBUTOR
58658,Artem-B,1337904980,2022-12-05 18:27:04,"This is certainly not expected. `.v2.f32` instructions should've been generated for Maxwell and even Kepler. Disablingthe test may be just papering over a real issue.
If you manage to extract LLVM IR from the failing case, I can take a look.",.v2.f32 instructions should've been generated for Maxwell and even Kepler.,MEMBER
58734,learning-to-play,1336648541,2022-12-05 2:29:36,"Hi @trevor-m, Google internal checks failed. I fixed it. Could you please sync? Sorry for the inconvenience.","""Google internal checks failed""",COLLABORATOR
33312,sagunb,1335590218,2022-12-02 17:49:20,"Could you try syncing to head please, it looks like there are some conflicts.","Could you try syncing to head please, it looks like there are some conflicts.",MEMBER
57477,talyz,1335200330,2022-12-02 13:04:49,"@gbaned I was waiting for a reponse from @terryheo, see above.","I was waiting for a reponse from @terryheo, see above.",CONTRIBUTOR
58556,sachinprasadhs,1334211813,2022-12-01 18:59:38,"@angerson , Could you please look into this, it seems like the python versions 3.9 and 3.10 are not included in the published docker images, unlike the version which we publish for the `tf-nightly-gpu` where python version is included from 3.7 - 3.10 here https://pypi.org/project/tf-nightly-gpu/2.12.0.dev20221201/#files.","python versions 3.9 and 3.10 are not included in the published docker images, unlike the version which we publish for the tf-nightly-gpu where python version is included from 3.7 - 3.10 here https://pypi.org/project/tf-nightly-gpu/2.12.0.dev20221201/#files..>",CONTRIBUTOR
58739,cantonios,1332492564,2022-11-30 17:17:49,"> @cantonios one of these days it could be nice if we could directly generate these from the ops def GT.
Yes, I know. They are generated for some ops - just not ones that have their own python docstrings. Otherwise, a check to detect when the docs are stale would be useful.","""It could be nice if we could directly generate these from the ops def GT.""",CONTRIBUTOR
58734,learning-to-play,1331663048,2022-11-30 5:29:18,"Hi @trevor-m, I see the following two warnings. Could you please fix?
- There is already a load from ""//third_party/tensorflow/core/platform:build_config_root.bzl"" on line 25. Please merge all loads from the same origin into a single one.
- Symbol ""if_static"" has already been loaded on line 27. Please remove it.","""I see the following two warnings. Could you please fix?""",COLLABORATOR
58206,terryheo,1331512774,2022-11-30 1:04:15,@gsirocco you've changed output type to int8 so it has int8 outputs [-128 127 127 ... -48 -48 127] instead of float outputs [0. 1. 0. ... 1. 0. 1.]. Could you share what wrong with this?,"""changed output type to int8""",MEMBER
57873,mihaimaruseac,1331481784,2022-11-30 0:12:18,You can attach the docstring to a new symbol in the Python file and remove it from the proto,can attach the docstring to a new symbol in the Python file and remove it from the proto>,COLLABORATOR
57645,sampathweb,1331460421,2022-11-29 23:41:32,"I was not suggesting that. I was suggesting that reduce the batch size to 1/2 of what's used in training, but keep the function as `model.predict(large_dataset, batch_size=0.5*train_batch_size)`.
Alternatively, you could chunk the dataset into smaller dataset, but still much larger than what fits in memory. For example, if you have 1M files, create a dataset with 200K files and call `model.predict` on it.",I was not suggesting that.,CONTRIBUTOR
58731,mihaimaruseac,1331231556,2022-11-29 20:04:25,"This is not true, has not been true since TF 2.0.
If you look on PyPI, `tensorflow` and `tensorflow-gpu` packages are identical, except the name","This is not true, has not been true since TF 2.0. If you look on PyPI, tensorflow and tensorflow-gpu packages are identical, except the name.",COLLABORATOR
58377,jurahul,1331013867,2022-11-29 17:23:51,"Overall it seems this is attempting to add tuple support for all-gather and reduce-scatter as well as add a optional dummy token input to the all-gather, the purpose of which is unclear.
I think we should split this into 2 PRs, one for tuple support and discuss support for token types in all-gather separately before adding it.","attempting to add tuple support for all-gather and reduce-scatter as well as add a optional dummy token input to the all-gather, the purpose of which is unclear.",CONTRIBUTOR
57679,markub3327,1330796052,2022-11-29 15:08:16,Probably yes. I have the same issue with TF 2.11.0 and TensorRT 8.5.1-1+cuda11.8.,I have the same issue with TF 2.11.0 and TensorRT 8.5.1-1+cuda11.8.,CONTRIBUTOR
58718,mihaimaruseac,1330766766,2022-11-29 14:49:53,I think the plan is for users to manually install the TF IO wheel. We don't want `pip install tensorflow` to install all and every package that exists in TF ecosystem.,I think the plan is for users to manually install the TF IO wheel. We don't want pip install tensorflow to install all and every package that exists in TF ecosystem.,COLLABORATOR
47170,bhack,1330429433,2022-11-29 10:41:59,"If it is not planned for the ""old"" bridge with the new bridge `tf.config.experimental.enable_mlir_bridge()` is probably https://github.com/tensorflow/tensorflow/issues/52030
As I see
```python
error: 'tf.TensorListReserve' op unknown tensor list element shape
```","""old"" bridge with the new bridge tf.config.experimental.enable_mlir_bridge() is probably https://github.com/tensorflow/tensorflow/issues/52030 As I see python error: 'tf.TensorListReserve' op unknown tensor list element shape .",CONTRIBUTOR
58705,mihaimaruseac,1329366151,2022-11-28 16:15:01,"This is somehow too late for TF 2.11, the final release happened ~2 weeks ago.","This is somehow too late for TF 2.11, the final release happened 2 weeks ago.",COLLABORATOR
58658,nouiz,1329267061,2022-11-28 15:09:16,"I updated the PR. It was passing here, but failed CI compilation. It should be fixed now.",Failed CI compilation.,CONTRIBUTOR
58621,svobora,1327463101,2022-11-25 13:11:54,The same warning/error happens to my nets using Dropout in Ubuntu 22.04 with TF 2.11.0 when using GPU. Did not happen with 2.10.0. Cannot confirm on Windows since GPU support was dropped in latest version of TF. Right now it seems it does not affect performance.,The same warning/error happens to my nets using Dropout in Ubuntu 22.04 with TF 2.11.0 when using GPU.,CONTRIBUTOR
58658,cheshire,1325191428,2022-11-23 14:50:02,Seems like vectorization is broken? I'm fine to skip.,Seems like vectorization is broken.,MEMBER
58648,agramesh1,1324380396,2022-11-22 23:57:51,CC @kaixih this failure seems to be due to this PR https://github.com/tensorflow/tensorflow/pull/58159,CC @kaixih this failure seems to be due to this PR,CONTRIBUTOR
51728,mohantym,1323019834,2022-11-22 4:10:59,"@benoitdescamps !
It still has not been addressed in[ 2.10 version](https://github.com/tensorflow/tensorflow/blob/r2.10/tensorflow/python/framework/sparse_tensor.py).",It still has not been addressed in[ 2.10 version](https://github.com/tensorflow/tensorflow/blob/r2.10/tensorflow/python/framework/sparse_tensor.py).,CONTRIBUTOR
58465,SuryanarayanaY,1323015642,2022-11-22 4:06:31,"Hi @Urkchar ,
I think we are still missing some thing here.Please find the error in the attached [gist](https://colab.research.google.com/gist/SuryanarayanaY/f76d1667758f46bf7d9f5a7ad0698f13/58465-r1.ipynb).
`NameError: name 'get_output_sequence_length' is not defined`.","""I think we are still missing some thing here.""",COLLABORATOR
58093,elfringham,1322158219,2022-11-21 14:38:15,My latest findings on this issue is that it was initially reproduced using gcc 9.3.1. I have now updated to gcc 10.2.1 and can no longer reproduce it.,I have now updated to gcc 10.2.1 and can no longer reproduce it.,CONTRIBUTOR
58544,SuryanarayanaY,1322002412,2022-11-21 12:37:51,"Hi @rohitreddy21122000,
As you are using Tf2.4 with Latest versions of Ubuntu there might arise incompatibility issues.Could you please Upgrade TF to new version like 2.10 and let us know if problem still persists.Request you go to similar [issue-54286](https://github.com/tensorflow/tensorflow/issues/54286) where issue solved by upgrading TF to newer versions.","""Incompatibility issues""",COLLABORATOR
57914,freedomtan,1321751470,2022-11-21 9:25:11,"Xcode 14.x come with ""problematic"" linker (ld), cf. https://github.com/tensorflow/tensorflow/issues/58368#issuecomment-1321729076","""problematic"" linker (ld)",CONTRIBUTOR
58606,SuryanarayanaY,1321643828,2022-11-21 8:10:55,I could able to replicate the issue in google colab but colab stopped at epoch 858 due to longer training time taken.But still the Expected behaviour captured in logs.Please find attached gist [here](https://colab.research.google.com/gist/SuryanarayanaY/f69d51ac0fd2c3b7a53833ac8b06a736/58606.ipynb#scrollTo=LWN099X70JnO).,I could able to replicate the issue in google colab but colab stopped at epoch 858 due to longer training time taken.But still the Expected behaviour captured in logs.Please find attached gist [here](https://colab.research.google.com/gist/SuryanarayanaY/f69d51ac0fd2c3b7a53833ac8b06a736/58606.ipynb#scrollTo=LWN099X70Jn,COLLABORATOR
26278,ppwwyyxx,1320550292,2022-11-18 21:35:06,"For more context, I understand that the results would match if I resize `[[6, 7], [11, 12]]` into 4x4 using TFv1's `resize_images(align_corners=True)`. However, `align_corners=True` was removed for good after #6720 and don't exist in TFv2. That's why this issue is a follow up of #6720 that we're still using the weird (and bad for many models) alignment behavior.",align_corners=True was removed for good after #6720 and don't exist in TFv2,CONTRIBUTOR
58416,reedwm,1320386924,2022-11-18 18:29:59,"BF16 L2Loss is only supported on the GPU, but the BiasAdd BF16 tests in this PR run with both CPU and GPU. So it's still failing.
I would either use cast the L2 Loss input to FP32 or only run these tests with BF16 if there is a GPU.","BF16 L2Loss is only supported on the GPU, but the BiasAdd BF16 tests in this PR run with both CPU and GPU. So it's still failing. I would either use cast the L2 Loss input to FP32 or only run these tests with BF16 if there is a GPU.",MEMBER
57883,cheshire,1319925254,2022-11-18 12:19:07,"This is implementation-defined behavior, so unlikely to have a fix (at least on the XLA side).","This is implementation-defined behavior, so unlikely to have a fix (at least on the XLA side).",MEMBER
58416,reedwm,1319148902,2022-11-17 20:14:18,"No need to rebase. But this is failing with the environment variable `TF2_BEHAVIOR=1` (you can run with `--test_env=TF2_BEHAVIOR=1` to reproduce the failure). The issue is `bias_op_base.py` calls l2_loss, which doesn't yet support BF16. Probably the easiest way to fix is to cast the input of l2_loss to fp32 when bf16 is used.","""The issue is bias_op_base.py calls l2_loss, which doesn't yet support BF16""",MEMBER
46602,MichaelHudgins,1317859729,2022-11-17 0:16:56,Seems like one of the first things we do is check why we use the net.cc PickUnusedPortOrDie in some places and directly use python_port_picker in others. After a quick look it seems like at least in the failure above it was in one of the instances where python_port_picker was used directly.,"""After a quick look it seems like at least in the failure above it was in one of the instances where python_port_picker was used directly.""",COLLABORATOR
58402,roserg,1317529380,2022-11-16 19:04:37,Should be fixed after this commit https://github.com/tensorflow/tensorflow/commit/78cbae184cb2cd9541fb21074c3420b930c682ad,"""should be fixed after this commit""",CONTRIBUTOR
55743,stewartmiles,1317490892,2022-11-16 18:32:12,"Hi everyone, thanks for approving but it appears merge is still blocked. I can't see the copybara failure - of course - and the AMD ROCm build - which has been failing forever - is still broken.",I can't see the copybara failure - of course - and the AMD ROCm build - which has been failing forever - is still broken.,CONTRIBUTOR
58523,cantonios,1317430984,2022-11-16 17:59:01,"Looks like the CUDA solver does actually raise an internal error. Not sure why it's not surfacing in colab.
@reedwm internally I get an `InvalidArgumentError`, notifying that the solver received a bad info status here: https://github.com/tensorflow/tensorflow/blob/0ba21962fe3e1f78063812ae9cc475ab928163bf/tensorflow/core/util/cuda_solvers.cc#L239
There's been an effort to return `NaN`s instead of crashing on these linalg failures. So maybe there's a general solution that can be implemented.",CUDA solver does actually raise an internal error.,CONTRIBUTOR
49520,mihaimaruseac,1316317927,2022-11-16 4:26:55,"This is too old, might need a new refactor if it is still an issue.","This is too old, might need a new refactor if it is still an issue.",COLLABORATOR
57549,sachinprasadhs,1316045762,2022-11-15 23:54:26,"In colab, with CPU runtime the code is getting executed without any error, attached gist [here](https://colab.sandbox.google.com/gist/sachinprasadhs/6c28fd2530d18727ad72d5af4c0f0f5c/57549.ipynb) for reference. In M1 mac, I'm getting different error. Note: I have installed tensorflow-metal to use GPU in my instance.
<img width=""1549"" alt=""image"" src=""https://user-images.githubusercontent.com/73069040/202049142-4fa25ca3-9377-4ad6-81dd-ec79ccd880fa.png"">","""In M1 mac, I'm getting different error""",CONTRIBUTOR
37729,wookayin,1314910216,2022-11-15 7:43:50,"Can we have this reopened and tracked somehow? I understand why the stalebot was added for such a large repository, but personally I don't like stale-bot closing and killing a long-wanted job suddenly... :| And the ml-butler bot doesn't distinguish the 'completed' and 'not planned' modes in Github issues.","I understand why the stalebot was added for such a large repository, but personally I don't like stale-bot closing and killing a long-wanted job suddenly... :| And the ml-butler bot doesn't distinguish the 'completed' and 'not planned' modes in Github issues.",CONTRIBUTOR
58538,bhack,1314651616,2022-11-15 1:58:26,"> But I could not find proper documentation on above interactive_graphviz command.
Yes no doc currently. It is only in the usage and in the source code.",I could not find proper documentation on above interactive_graphviz command.,CONTRIBUTOR
56696,cantonios,1314119666,2022-11-14 17:25:33,"@KingsleyLiu-NV no I think that's a Keras issue. Keras is a layer on top of TF, so TF's internals shouldn't need to know anything about keras. TF only deals directly with Tf tensor types (`Tensor`, `SparseTensor`, `RaggedTensor`, etc...)",I think that's a Keras issue.,CONTRIBUTOR
55394,cheshire,1314075752,2022-11-14 16:56:22,"> I don't know much about XLA, I don't think my single test is suitable for embedding Exhaustive32BitOrLessUnaryTest.
> input of Exhaustive32BitOrLessUnaryTest must be int type, but I need a float or double input .
Saw this in email, don't see it in GH UI somehow. It looks float to me, why does it need to be an int type?","""I don't know much about XLA, I don't think my single test is suitable for embedding Exhaustive32BitOrLessUnaryTest.""",MEMBER
58522,bhack,1313994846,2022-11-14 16:07:21,I've tried some runs but I cannot reproduce it. Probably you could `set_shape` in you py_function. As py_function has some limits (see Doc) you could explore also what I've suggested in https://github.com/tensorflow/tensorflow/issues/58448#issuecomment-1313695458,I've tried some runs but I cannot reproduce it. Probably you could set_shape in you py_function. As py_function has some limits (see Doc) you could explore also what I've suggested in https://github.com/tensorflow/tensorflow/issues/58448#issuecomment-1313695458.,CONTRIBUTOR
58534,bhack,1312762716,2022-11-13 15:56:48,"> As I said, it is dead slow while running in colab also.
I was just replying to your claim:
> Besides, it is running in Kaggle env. It shouldn't be env/installation issue
Actually I cannot reproduce your original error on a Colab env how you are going to exclusive that it is not an env issue if we cannot reproduce it on Colab?","Besides, it is running in Kaggle env. It shouldn't be env/ installation issue Actually I cannot reproduce your original error on a Colab env how you are going to exclusive that it is not an env issue if we cannot reproduce it on Colab?",CONTRIBUTOR
58515,cantonios,1312072903,2022-11-11 18:56:29,"nm, I double-checked the Eigen source, and it's because we implement negate as (0 - x), so we get this behavior even in no-fast-math mode. (+0 - -0) is +0. I'll change that on the Eigen end. Will take time to propagate.","nm, I double-checked the Eigen source, and it's because we implement negate as (0 - x), so we get this behavior even in no-fast-math mode. (+0 - -0) is +0. I'll change that on the Eigen end. Will take time to propagate..",CONTRIBUTOR
57224,sachinprasadhs,1312067608,2022-11-11 18:48:59,This merge would conflict with the commit https://github.com/tensorflow/tensorflow/commit/5984ea373e8804386fa60cf57eb6a18005c02b56. I'm closing the issue.,This merge would conflict with the commit https://github.com/tensorflow/tensorflow/commit/5984ea373e8804386fa60cf57eb6a18005c02b56. I'm closing the issue.,CONTRIBUTOR
58538,bhack,1311819302,2022-11-11 15:24:08,"Isn't `--hlo_text`?
https://github.com/tensorflow/tensorflow/blob/3e3f1ba6dbefcb7643a22409e242b0f0e32e71c5/tensorflow/compiler/xla/tools/interactive_graphviz.cc#L106",Isn't --hlo_text?,CONTRIBUTOR
58448,bhack,1311662328,2022-11-11 12:50:22,"> I tried to download the dataset and code from the mentioned Kaggle link and was facing a different issue while execution.
Cause you missed to copy the first cell from the user's Kaggle Colab:
`!pip install natsort`",Cause you missed to copy the first cell from the user's Kaggle Colab: !pip install natsort.,CONTRIBUTOR
58453,bhack,1310864678,2022-11-10 20:36:50,"Your env varriable is correctly set to pass the condition
https://github.com/tensorflow/tensorflow/blob/1d34295f45965c7090039773dea44c25ac00ec00/third_party/clang_toolchain/cc_configure_clang.bzl#L9-L19
But I suppose that as we are not testing this in the CI the download part is quite unmaintained. Last commit and clang updated hash -> Sept. 2019
https://github.com/tensorflow/tensorflow/blob/master/third_party/clang_toolchain/download_clang.bzl","""But I suppose that as we are not testing this in the CI the download part is quite unmaintained.""",CONTRIBUTOR
58453,bhack,1310854616,2022-11-10 20:26:39,I see that `ERROR: Config value 'download_clang' is not defined in any .rc file` disappeared now but still not using the new one.,I see that ERROR: Config value 'download_clang' is not defined in any .rc file disappeared now but still not using the new one.,CONTRIBUTOR
58422,cantonios,1310663142,2022-11-10 17:46:10,"> * can you tell me the steps or point me to the right documentation from where I can know how to compile my changes in my local system?
https://www.tensorflow.org/install/source
> * I have tried to update my signature on the CLA form but it still showing failed. please let me know where might be the issue?
https://github.com/tensorflow/tensorflow/pull/58422/checks?check_run_id=9261642567","""I have tried to update my signature on the CLA form but it still showing failed. please let me know where might be the issue""",CONTRIBUTOR
58445,cantonios,1310571626,2022-11-10 16:36:13,"Yes, we should remove the `(and between CPU and GPU)` claim.",(and between CPU and GPU),CONTRIBUTOR
58521,mohantym,1310422424,2022-11-10 15:00:07,Closing due to conflict.,Closing due to conflict.,CONTRIBUTOR
58398,yishuangP,1309742398,2022-11-10 4:09:29,"Hi, sorry I just tried switching branch to v2.10.0, it also failed with the same error, but it succeeds on master. We already drop support for 32 bits since Apple drops support for it since iOS 11. To build for 64bits devices, you can run the following command:
```
bazel build --ios_multi_cpus=arm64 -c opt --config=ios --cxxopt=-std=c++17 //tensorflow/lite/ios:TensorFlowLiteC_framework
```","""I just tried switching branch to v2.10.0, it also failed with the same error, but it succeeds on master.""",CONTRIBUTOR
58445,wangpengmit,1309686512,2022-11-10 2:33:46,BTW I suspect the reason it's not reproduced in graph mode is because soft placement placed both ops on CPU.,BTW I suspect the reason it's not reproduced in graph mode is because soft placement placed both ops on CPU.,MEMBER
57873,sachinprasadhs,1309577496,2022-11-10 0:21:07,"I guess the raw_ops does not have python file, and the example mentioned here https://www.tensorflow.org/api_docs/python/tf/raw_ops/UniqueV2 in the document throws an exception.","I guess the raw_ops does not have python file, and the example mentioned here https://www.tensorflow.org/api_docs/python/tf/raw_ops/UniqueV2 in the document throws an exception.",CONTRIBUTOR
57779,cantonios,1309437017,2022-11-09 22:03:31,"@jxy we know the cause, and we know the proper solution is to use `tf.convert_to_tensor` rather than `tf.cast` to convert numpy arrays to tensors. We're blocked from changing `tf.cast` to address this directly, since doing so changes the graph representation which breaks existing models and workflows. So I think our hands are tied. You should be using `tf.convert_to_tensor` to avoid precision loss.","""We're blocked from changing tf.cast to address this directly, since doing so changes the graph representation which breaks existing models and workflows.""",CONTRIBUTOR
58144,DEKHTIARJonathan,1309189161,2022-11-09 18:26:04,@bixia1 no more comments. Can you re-approve,No more comments.,CONTRIBUTOR
58140,SuryanarayanaY,1309109988,2022-11-09 17:40:11,"Hi @rrajkumar1990 ,
From the error log as we can see `MLIR V1 optimization pass is not enabled` as the API `tf.config.experimental.enable_mlir_graph_optimization` is under development. Could you please spare some time to refer the attached [TF Forum](https://discuss.tensorflow.org/t/none-of-the-mlir-optimization-passes-are-enabled/2247/18) link regarding similar issue and please let us know if they works for your case.
Thankyou!","""MLIR V1 optimization pass is not enabled""",COLLABORATOR
58290,bhack,1308917164,2022-11-09 15:15:48,"I've tested the gist again with the last nightly and the code path of this example it seems to be different from the one fixed with the PR https://github.com/tensorflow/tensorflow/pull/58328:
https://colab.research.google.com/gist/bhack/9dc16546d360291eb943bbeb36a01c57/untitled192.ipynb
But I don't have enough cloud resources to recompile TF again a prepare a new PR /cc @mihaimaruseac",I've tested the gist again with the last nightly and the code path of this example it seems to be different from the one fixed with the PR,CONTRIBUTOR
58469,bhack,1308646945,2022-11-09 12:02:37,"> I also noticed the restriction mentioned in the high level API doc, which means that an increasing sequence with negative values is also invalid
@Kristoff-starling I don't know if it is really a cosntrain it was more an open question as if you try it is also not validated by the high level API op `tf.math.segment_max`.","I also noticed the restriction mentioned in the high level API doc, which means that an increasing sequence with negative values is also invalid",CONTRIBUTOR
58481,bhack,1308009281,2022-11-09 0:11:31,Yes the root cause is the same as the ticket but I think that the Cmake definition was introduced almost at the same time with https://github.com/tensorflow/tensorflow/commit/888dd68c774ba5d48dbb5181077ee454936822ed that it is forward to your 21b567d26ca4cfa5c02260acd77b8b5f2fd57d84,I think that the Cmake definition was introduced almost at the same time with https://github.com/tensorflow/tensorflow/commit/888dd68c774ba5d48dbb5181077ee454936822ed that it is forward to your 21b567d26ca4cfa5c02260acd77b8b5f2fd57d84.,CONTRIBUTOR
58328,mihaimaruseac,1307410177,2022-11-08 15:34:39,I did a manual import.,I did a manual import.,COLLABORATOR
58354,bhack,1307268410,2022-11-08 14:02:51,"> Unlikely to backport if it doesn't turn out to be a security issue
So as it is solved in 2.11rc I think it could go stale/closed.",Unlikely to backport if it doesn't turn out to be a security issue,CONTRIBUTOR
58442,mraunak,1306697860,2022-11-08 6:25:19,"Hey @bhack and @MarkDaoust, I have attached the logs, but I didn't find much information in them. I re-run it but got the same output log. Below is the log when there is a TIMEOUT error
[error_log.docx](https://github.com/tensorflow/tensorflow/files/9958245/error_log.docx).","""I have attached the logs, but I didn't find much information in them.""",CONTRIBUTOR
58354,mihaimaruseac,1306551552,2022-11-08 2:54:07,Unlikely to backport if it doesn't turn out to be a security issue,Unlikely to backport if it doesn't turn out to be a security issue.,COLLABORATOR
58472,elfringham,1306042666,2022-11-07 18:51:12,The size of the elements needed seems to have recently increased causing this test to begin failing again.,The size of the elements needed seems to have recently increased causing this test to begin failing again.,CONTRIBUTOR
58451,bhack,1306004049,2022-11-07 18:14:06,"Yes it seems we have a segmentation fault on nightly:
```gdb
#0 0x00007f623573d7d3 in mlir::quant::QuantizedType::getExpressedType() const () from /usr/local/lib/python3.9/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#1 0x00007f623573e1ac in mlir::quant::QuantizedType::castFromExpressedType(mlir::Type) ()
from /usr/local/lib/python3.9/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
```",gdb #0 0x00007f623573d7d3 in mlir::quant::QuantizedType::getExpressedType() const () from /usr/local/lib/python3.9/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so #1 0x00007f623573e1ac in mlir::quant::QuantizedType::cast,CONTRIBUTOR
55495,infil00p,1306001108,2022-11-07 18:11:47,I'm still getting the issue on 2.10 when I use the precompiled libraries provided in the AAR on Gradle.,I'm still getting the issue on 2.10 when I use the precompiled libraries provided in the AAR on Gradle.,CONTRIBUTOR
58451,nyadla-sys,1305969908,2022-11-07 17:46:25,"@bhack I changed first cell of colab to **!pip install tf-nightly**, but I still see the crash while converting to quantized int8 model","I changed first cell of colab to **!pip install tf-nightly**, but I still see the crash while converting to quantized int8 model.",MEMBER
58451,bhack,1305954625,2022-11-07 17:32:43,I have some memory limit running your Colab. Have you really tried to run it with TF nightly as it seems it was mispelled at the first cell of your colab.,I have some memory limit running your Colab. Have you really tried to run it with TF nightly as it seems it was mispelled at the first cell of your colab.,CONTRIBUTOR
57826,njzjz,1305269963,2022-11-07 8:44:44,"Same error, so I roll back to 2.9.2.","Same error, so I roll back to 2.9.2.",CONTRIBUTOR
58448,tilakrayal,1305176643,2022-11-07 7:09:03,"@dasmehdix,
Code shared is full of [indentation errors](https://colab.research.google.com/gist/tilakrayal/bdc11b8f1dee2af32af25afdf3e56ff0/untitled731.ipynb), Kindly share a colab gist with issue reported or indented code with all dependencies such that we can replicate the issue reported. Thank you!",Code shared is full of [indentation errors](https://colab.research.google.com/gist/tilakrayal/bdc11b8f1dee2af32af25afdf3e56ff0/untitled731.ipynb),CONTRIBUTOR
58457,mohantym,1305063090,2022-11-07 4:20:43,Closing due to Pylint conflict.,Closing due to Pylint conflict.,CONTRIBUTOR
58375,bhack,1304367211,2022-11-05 1:18:26,"> These should be equivalent?
In this formulation I think that this currently will not work for https://github.com/bazelbuild/bazel/issues/14848. Right?",I think that this currently will not work for https://github.com/bazelbuild/bazel/issues/14848. Right?,CONTRIBUTOR
32122,mihaimaruseac,1303605714,2022-11-04 14:04:07,"My bad, I misread the `+` meaning.",I misread the + meaning.,COLLABORATOR
58433,bhack,1303556355,2022-11-04 13:42:40,"> Unfortunately for your first code snippet, you mention it works for you. Indeed it works but if you read carefully my initial comment
Yes sorry is that it seems, without the output, that it was empty. But reading again the ""text"" it is clear. So the pointer to the stateless version is correct.
P.s. the autograph problem mentioned for the non stateless impl at https://github.com/tensorflow/tensorflow/issues/58433#issuecomment-1302627410 is still valid.","""Unfortunately for your first code snippet, you mention it works for you.""",CONTRIBUTOR
58030,smit-hinsu,1303160977,2022-11-04 9:20:59,Comparison is undefined for complex types so CategoricalAccuracy doesn't make sense for it. Later TensorFlow versions will remove support for complex types to avoid this crash and have an explicit failure.,Comparison is undefined for complex types so CategoricalAccuracy doesn't make sense for it.,CONTRIBUTOR
58427,bhack,1302866025,2022-11-04 2:02:03,Generally it could still happen a rollback after the merge (https://github.com/tensorflow/community/issues/425),Generally it could still happen a rollback after the merge.,CONTRIBUTOR
58368,freedomtan,1302760738,2022-11-03 22:43:23,"FYR, mostly, this is a bug of Xcode 14.x (or more specifically ld? There were some new changes in ld. It failed to build gcc with some early Xcode 14 versions). I ran into this problem since early Xcode 14 betas.
I could build TensorFlow with Xcode 13.x on macOS Monterey (12.x) without problems. When I tried to build it with Xcode 14 on macOS 12.x, it failed.","""I ran into this problem since early Xcode 14 betas.""",CONTRIBUTOR
58159,kaixih,1302643249,2022-11-03 20:44:45,"Noticed a couple of newly added unit tests of bf16 failed on CPU. Just skip these tests on CPU, since they are not focus of this PR. @reedwm","""A couple of newly added unit tests of bf16 failed on CPU""",CONTRIBUTOR
58433,bhack,1302627410,2022-11-03 20:27:18,"Basically It is like
```python
@tf.function
def f(X):
if X == (0,0):
return True
return False
print(f(1))
print(f(tf.constant(1))) # This will not work
```","python @tf.function def f(X): if X == (0,0): return True return False print(f(1)) print(f(tf.constant(1))) # This will not work ",CONTRIBUTOR
32122,mihaimaruseac,1302417523,2022-11-03 17:07:22,I think `test_seek_v2` is actually correct. You are opening the file only for writing so of course you should not be able to read from it,I think test_seek_v2 is actually correct. You are opening the file only for writing so of course you should not be able to read from it.,COLLABORATOR
58032,mihaimaruseac,1302414957,2022-11-03 17:05:39,The infra team needs to create jobs for the nightly jobs. CC @learning-to-play @angerson @nitins17,The infra team needs to create jobs for the nightly jobs.,COLLABORATOR
58177,mihaimaruseac,1302414079,2022-11-03 17:05:01,"This is a real issue that used to be considered a vulnerability but due to large number of occurences versus limited impact it was decided to only consider it a bug. Please don't autoclose these types of issues.
However, please do check if these are happening in tf-nightly. If they don't, then the issue can be closed.","""due to large number of occurences versus limited impact it was decided to only consider it a bug""",COLLABORATOR
58180,mihaimaruseac,1302413001,2022-11-03 17:04:13,This is a real issue that used to be considered a vulnerability but due to large number of occurences versus limited impact it was decided to only consider it a bug. Please don't autoclose these types of issues.,"""It was decided to only consider it a bug.""",COLLABORATOR
58086,cantonios,1302385343,2022-11-03 16:44:39,"Marking as closed then. For the slow transpose issue, if it's a blocker, please re-open #13017.",slow transpose issue,CONTRIBUTOR
58055,cantonios,1302294753,2022-11-03 15:35:35,"Commented on the bug, but I think we should instead disable the op definition for non-integer types in [math_ops.cc](https://github.com/tensorflow/tensorflow/blob/c676c85dd4d48956729857fe98981e50bd80e0a5/tensorflow/core/ops/math_ops.cc#L520).
Instead of `.BINARY_MORE()`, that should probably read
```
.Input(""x: T"").Input(""y: T"").Output(""z: T"").Attr(
""T: {uint8, int8, uint16, int16, int32, uint32, uint64, int64}"")
```","""I think we should instead disable the op definition for non-integer types in [math_ops.cc](https://github.com/tensorflow/tensorflow/blob/c676c85dd4d48956729857fe98981e50bd80e0a5/tensorflow/core/ops/math_ops.cc#L520)""",CONTRIBUTOR
48845,tanzhenyu,1302277365,2022-11-03 15:23:25,TF 2.10 + running locally = issue is still here,TF 2.10 + running locally = issue is still here.,CONTRIBUTOR
58428,bhack,1302258199,2022-11-03 15:10:32,As you car read there if you manually access to that sysfs on the host and it is `< 0` it could be a firmware issue. You can use the suggested workaround and report it to your PC/laptop/server vendor as it is probably a firmware bug.,As you car read there if you manually access to that sysfs on the host and it is  0 it could be a firmware issue.,CONTRIBUTOR
58392,bhack,1302241309,2022-11-03 14:58:37,"@chunky This is why I have asked you to try to reproduce the same with a controllable environment like docker so we are for sure exactly on the same page.
If you don't want to use a clean/under control environment please check that your custom env respect the tested build configuration:
https://www.tensorflow.org/install/source#tested_build_configurations
`identification is GNU 11.3.0` is not in the table.","""If you don't want to use a clean/under control environment please check that your custom env respect the tested build configuration: https://www.tensorflow.org/install/source#tested_build_configurations identification is GNU 11.3.0 is not in the table.""",CONTRIBUTOR
58431,bhack,1302236619,2022-11-03 14:55:17,"It is hard to compare results between TF version with you notebook as if you see your code gives different results with the same TF version across multiple runs.
Other then seeds you need probably to remove other random sources for a reproducible experiment. When you will reproduce the same results across multiple run on the same TF version we could check if there is really a numerically relevant difference.","""Hard to compare results between TF version with you notebook as if you see your code gives different results with the same TF version across multiple runs""",CONTRIBUTOR
58431,bhack,1302109196,2022-11-03 13:22:06,"Your code is not accessible.
Please can you try also with the last stable version TF 2.10?","""Your code is not accessible""",CONTRIBUTOR
58379,bhack,1301789598,2022-11-03 8:41:53,"> I use tflite model . IF use the TFlite2.10 to compile benchmark on PC, can also test performance. Im not sure the tflite2.10 is or not exist this problem.
> We always suggest to test the last available version for bug reports. Can you test 2.10 and 2.11rc2?
if still confirmed on the last releases, it would be nice if you can replicate it on the master branch.","I use tflite model . IF use the TFlite2.10 to compile benchmark on PC, can also test performance. Im not sure the tflite2.10 is or not exist this problem. > We always suggest to test the last available version for bug reports. Can you test 2.10 and 2.11rc2? if still confirmed on the last releases, it would be nice if you can replicate it on the master branch.",CONTRIBUTOR
57390,dixr,1301481832,2022-11-02 23:16:54,"@wangpengmit Can you justify why this is really the best solution? Why not change the use of `.ndim` in `kron`?
I mean, none of all the other functions in `np_math_ops.py` require `.ndim`, they all call some alternative rank functions that don't produce this error.
If for some reason `kron` really has to use `.ndim` instead of an alternative, then you should make the error message more helpful.","""Why not change the use of .ndim in kron?""",CONTRIBUTOR
58027,sachinprasadhs,1301251871,2022-11-02 21:03:21,"Negative value in the `seq_axis` is handled in the code and it returns the InvalidArgument Error, below is the code and the output for that.
```
import tensorflow as tf
seq_lengths = [7, 2, 3, 5]
input = [[1, 2, 3, 4, 5, 0, 0, 0], [1, 2, 0, 0, 0, 0, 0, 0],
[1, 2, 3, 4, 0, 0, 0, 0], [1, 2, 3, 4, 5, 6, 7, 8]]
output = tf.reverse_sequence(input, seq_lengths, seq_axis=-1, batch_axis=0)
print(output)
```
`InvalidArgumentError: Invalid seq_dim -1 [Op:ReverseSequence]`",InvalidArgumentError: Invalid seq_dim -1 [Op:ReverseSequence],CONTRIBUTOR
57601,sachinprasadhs,1301072760,2022-11-02 18:47:49,"> Saving it using the newest version of Tensorflow 1? It only occurs when saving a model in Tensorflow 1 and I tried with Tensorflow 1.15 which should be the newest version.
Try saving the model in Tensorflow version 2.5 and above, for migrating your 1.x code to 2.x, you can follow https://www.tensorflow.org/guide/migrate",Saving it using the newest version of Tensorflow 1?,CONTRIBUTOR
57674,kaixih,1300998223,2022-11-02 17:38:01,"> this breaks TF2 tests that previously ran BF16 ops and had pre-Ampere GPUs. Can you elaborate a little more about the TF2 tests? Are they some of your internal tests?
Or are you talking about the unit tests in this PR?
For the conv ops, we didn't have the bf16 kernels before, so how did you test it on pre-ampere gpus? I think I am still a bit confused about why this PR would break the ""TF2 tests"".
@reedwm",This breaks TF2 tests that previously ran BF16 ops and had pre-Ampere GPUs.,CONTRIBUTOR
58334,bhack,1300837992,2022-11-02 16:29:24,"> TF just calls Eigen's matrix sqrt [here](https://github.com/tensorflow/tensorflow/blob/38a677b555140410598603ff15abbe408b252e40/tensorflow/core/kernels/linalg/matrix_square_root_op.cc#L45).
> > I don't think we have that in Eigen either.
So, probably still waiting on https://gitlab.com/libeigen/eigen/-/issues/840","""TF just calls Eigen's matrix sqrt""",CONTRIBUTOR
58334,cantonios,1300809722,2022-11-02 16:17:58,"TF just calls Eigen's matrix sqrt [here](https://github.com/tensorflow/tensorflow/blob/38a677b555140410598603ff15abbe408b252e40/tensorflow/core/kernels/linalg/matrix_square_root_op.cc#L45).
I don't think we have that in Eigen either.",TF just calls Eigen's matrix sqrt [here](https://github.com/tensorflow/tensorflow/blob/38a677b555140410598603ff15abbe408b252e40/tensorflow/core/kernels/linalg/matrix_square_root_op.cc#L45),CONTRIBUTOR
58295,roserg,1300397221,2022-11-02 13:20:24,"""Is there a particular reason why the support for this op on GPU is not included already? Is it bugged?""
The reason is very limited support of parameters, it will work correctly only in specific case. Looks like your case is not supported or smt else is wrong.","""Looks like your case is not supported or smt else is wrong.""",CONTRIBUTOR
57601,muxamilian,1300207077,2022-11-02 11:54:10,Saving it using the newest version of Tensorflow 1? It only occurs when saving a model in Tensorflow 1 and I tried with Tensorflow 1.15 which should be the newest version.,Saving it using the newest version of Tensorflow 1?,CONTRIBUTOR
58379,bhack,1299987507,2022-11-02 10:15:31,"Yes, It seems that some convolutional kernels are slower on the AMD GPU. Just taking the first 3 in the log.
AMD Ryzen 5 5600U
```
convolution_2d 0 linked : relu 1 - 1.043610 ms
convolution_2d 2 linked : relu 3 - 0.409920 ms
convolution_2d 4 linked : relu 5 - 0.205160 ms
```
intel(R) UHD Graphics 620 ```
convolution_2d 0 linked : relu 1 - 0.538500 ms
convolution_2d 2 linked : relu 3 - 0.096083 ms
convolution_2d 4 linked : relu 5 - 0.100500 ms
```
Is the AMD driver updated?
/cc @roserg",Is the AMD driver updated? /cc @roserg.,CONTRIBUTOR
57757,cantonios,1298941374,2022-11-01 18:30:27,"`std::pow(1, NaN) = 1` in c++ [[docs](https://en.cppreference.com/w/cpp/numeric/math/pow)]. Looks like XLA is violating this, returning `NaN`.","std::pow(1, NaN) = 1",CONTRIBUTOR
57794,cheshire,1298542571,2022-11-01 13:51:54,"Hm seems like `tensorflow/compiler/xla/service/gpu:cudnn_fused_conv_rewriter_test` is still failing =/
It fails comparing module equality in `CudnnFusedConvRewriterHloTest.DontFuseSideInputThroughRelu`",seems like tensorflow/compiler/xla/service/gpu:cudnn_fused_conv_rewriter_test is still failing =/,MEMBER
56762,SandSnip3r,1297405409,2022-10-31 17:11:35,"Unfortunately, this item is taking a back seat to some other higher priority issues. Also, I am having trouble getting permissions to update our internal rocm testing version. I am trying to pass this off to someone who has permissions and time to help. Sorry about the delay.",I am having trouble getting permissions to update our internal rocm testing version.,CONTRIBUTOR
58335,mihaimaruseac,1297352302,2022-10-31 16:28:19,"(I no longer work in TF, just doing drive-by reviews on stale items or on work that crosses boundary)","I no longer work in TF, just doing drive-by reviews on stale items or on work that crosses boundary.",COLLABORATOR
58051,bhack,1297350330,2022-10-31 16:26:51,"> here has been no progress on Dynamic Reshape for XLA Literals as far as I'm aware.
Is it something that could be contributed? Do we have any pointer to contribute this?",No progress on Dynamic Reshape for XLA Literals.,CONTRIBUTOR
58051,JoshVarty,1297347085,2022-10-31 16:24:29,There has been progress on support for dynamic shapes within XLA since 2020 but there has been no progress on Dynamic Reshape for XLA Literals as far as I'm aware.,No progress on Dynamic Reshape for XLA Literals.,CONTRIBUTOR
41732,dwyatte,1296293170,2022-10-30 16:12:13,BoostedTreesClassifier and other estimators are deprecated as of TensorFlow 2.9,BoostedTreesClassifier and other estimators are deprecated as of TensorFlow 2.9.,CONTRIBUTOR
58212,mihaimaruseac,1296292692,2022-10-30 16:09:46,Merging manually since Copybara/GitHub integration seems to consider this to be a conflict,Merging manually since Copybara/GitHub integration seems to consider this to be a conflict.,COLLABORATOR
58369,bhack,1296272479,2022-10-30 14:24:48,"We don't have explicit tests for `mod` with non int/unit input types:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/math_ops/cwise_ops_binary_test.py#L335","""We don't have explicit tests for mod with non int/unit input types""",CONTRIBUTOR
55743,stewartmiles,1295477439,2022-10-28 21:19:23,"@gbaned @JunyoungLim hello again, I'm still waiting for the AMD ROCm build to be fixed - it isn't due to this change - and this commit to be merged. As I've mentioned before I don't mind you taking this commit and manually patching in g3 then I can just close this out when it is auto merged to this repo.",I'm still waiting for the AMD ROCm build to be fixed - it isn't due to this change - and this commit to be merged. As I've mentioned before I don't mind you taking this commit and manually patching in g3 then I can just close this out when it is auto merged to this repo.,CONTRIBUTOR
58351,bhack,1295266247,2022-10-28 17:30:12,"It was already protected emitting `UnimplementedError` in our stable release 2.10.0:
```python UnimplementedError: {{function_node __wrapped__DepthwiseConv2dNative_device_/job:localhost/replica:0/task:0/device:CPU:0}} Current kernel implementation does not support dilations, received [1 2 3 1] [Op:DepthwiseConv2dNative]
```",UnimplementedError,CONTRIBUTOR
58283,reedwm,1294393406,2022-10-28 3:12:25,"@pgpetrak can you review? I think merging this will also require updating the cuDNN frontend library internally.
/CC @artem-b",I think merging this will also require updating the cuDNN frontend library internally.,MEMBER
58346,DEKHTIARJonathan,1293820487,2022-10-27 17:03:43,"@bixia1 for review.
Sorry for the large PR, I really don't have a way to split this in small incremental changes",I really don't have a way to split this in small incremental changes.,CONTRIBUTOR
57983,cantonios,1293787191,2022-10-27 16:32:25,"> I did a search and believe there is no uses (other than the definition itself) within public tensorflow code base. However, I am not sure if there is any internal code inside google that is still using `ShapeFromFormat`, it will be a little harder to debug if internal tests are failing.
Let's try to remove it then. If there are any internal failures, I can update those on my end.","I am not sure if there is any internal code inside google that is still using ShapeFromFormat, it will be a little harder to debug if internal tests are failing.",CONTRIBUTOR
57983,yongtang,1293778669,2022-10-27 16:25:12,"@cantonios I did a search and believe there is no uses (other than the definition itself) within public tensorflow code base. However, I am not sure if there is any internal code inside google that is still using `ShapeFromFormat`, it will be a little harder to debug if internal tests are failing.","I am not sure if there is any internal code inside google that is still using ShapeFromFormat, it will be a little harder to debug if internal tests are failing.",MEMBER
58315,bhack,1293352092,2022-10-27 10:58:56,"@tilakrayal Please check internally with the teams how we need to handle assert issues on raw ops after we have declassified these as security bugs.
Cause in the last few days we had many tickets like this one.",Cause in the last few days we had many tickets like this one.,CONTRIBUTOR
58316,bhack,1293344594,2022-10-27 10:51:28,"We had other issues with GCC 11 https://github.com/tensorflow/tensorflow/issues/50303
As the build is only tested with what you see in the table we could have still some compatibility issue (or with our dependencies versions) with other GCC releases.",We had other issues with GCC 11,CONTRIBUTOR
57502,albertz,1293318104,2022-10-27 10:26:46,I assume this is not really fixed? We get the same in `MaxPoolingNoMaskOp`.,I assume this is not really fixed? We get the same in MaxPoolingNoMaskOp.,CONTRIBUTOR
51803,albertz,1293159175,2022-10-27 8:16:00,"For reference, others report (https://github.com/matterport/Mask_RCNN/issues/521#issuecomment-780459992):
> I had accidentally created a Dense layer with zero ""units"" (layers in the output)
The error likely originates from sth like that.","""I had accidentally created a Dense layer with zero ""units"" (layers in the output) The error likely originates from sth like that.""",CONTRIBUTOR
58149,penpornk,1292857419,2022-10-27 1:45:05,"Also cc'ing @nSircombe and @milpuz01:
FYI, since this affects the aarch64 build as well.","cc'ing @nSircombe and @milpuz01: FYI, since this affects the aarch64 build as well.",MEMBER
57779,jxy,1292482771,2022-10-26 19:01:55,"It is an issue when we cast a variable the precision changes depending on whether the variable contains a python float or a TF tensor.
Can we at least leave the issue open until our ""long-standing desire"" gets satisfied?","""Can we at least leave the issue open until our ""long-standing desire"" gets satisfied?""",CONTRIBUTOR
57069,cantonios,1292236294,2022-10-26 15:36:43,"> Could you please check this PR when you have some time? Are there any other header issues which need to be addressed?
Yes, we're still seeing errors of the form:
```
tensorflow/core/framework/tensor.cc:63:10: fatal error: 'tensorflow/tsl/util/byte_swap_array.h' file not found
```
for mobile builds. Once all the tests pass, this should be submitted automatically.","""We're still seeing errors of the form:  tensorflow/core/framework/tensor.cc:63:10: fatal error: 'tensorflow/tsl/util/byte_swap_array.h' file not found  for mobile builds.""",CONTRIBUTOR
57653,nouiz,1292219327,2022-10-26 15:24:09,"I suppose you meant kAfterOptimizationsDumpName instead of kBeforeOptimizationsDumpName.
I did it and created/moved those constant string to the right place.",I suppose you meant kAfterOptimizationsDumpName instead of kBeforeOptimizationsDumpName.,CONTRIBUTOR
58293,tilakrayal,1292016262,2022-10-26 13:13:50,"@xushanthu-2014,
To expedite the trouble-shooting process, could you please provide a complete code you are using. Also as metioned, please don't import keras directly and try to import ```
from tensorflow import keras
from tensorflow.keras.models import load_model
```","""To expedite the trouble-shooting process, could you please provide a complete code you are using.""",CONTRIBUTOR
58247,bhack,1291835270,2022-10-26 10:38:36,"> The Tensorflow build seems to choke on the python3-protobuf package for some reason. Installing protobuf via pip fixed the issue here:
Yes sorry I just looked at the first error on the 2nd log.
I think it is an instance of the old https://github.com/tensorflow/tensorflow/issues/6341
@berndporr Can you try to install protobuf from pip?",The Tensorflow build seems to choke on the python3-protobuf package for some reason.,CONTRIBUTOR
57794,kaixih,1291524970,2022-10-26 5:29:14,"I can repro the failure on my V100 machine. Basically, we need to skip the `CudnnFusedConvRewriterHloTest.FuseElu` test on the pre-Ampere GPUs as well, because the test will expect the fused hlo instruction but the `RunHloPass` will check the GPU generation and skip the fusion when it is lower than Ampere. So, we should skip this test for the pre-Ampere platforms.
The fix has been submitted. PTAL. @cheshire","""Can repro the failure on my V100 machine.""",CONTRIBUTOR
58305,mihaimaruseac,1290912214,2022-10-25 17:37:32,Need to immediately be followed by the revert of the cherrypick,Need to immediately be followed by the revert of the cherrypick.,COLLABORATOR
58304,mihaimaruseac,1290905991,2022-10-25 17:34:23,This PR just complicates matters. The original PR needs to be reverted and the original fix cherrypicked properly.,This PR just complicates matters.,COLLABORATOR
56762,SandSnip3r,1290709957,2022-10-25 15:03:20,All tests are not passing. Still working on this. Sorry it's taking so long.,All tests are not passing.,CONTRIBUTOR
58296,bhack,1290617128,2022-10-25 14:06:22,"It is mainly related to:
https://github.com/keras-team/keras-cv/issues/291
https://github.com/tensorflow/tensorflow/issues/55639
Some warnings were suppressed https://github.com/tensorflow/tensorflow/commit/88a263ee53fdfa172c37090ef245ac0668b6200c but it just reduces messages as it was not going to solve the performance issues.","""It is mainly related to: https://github.com/keras-team/keras-cv/issues/291 https://github.com/tensorflow/tensorflow/issues/55639 Some warnings were suppressed https://github.com/tensorflow/tensorflow/commit/88a263ee53fdfa172c37090ef245ac0668b6200c but it just reduces messages as it was not going to solve the performance issues.""",CONTRIBUTOR
48881,372046933,1290499891,2022-10-25 12:46:54,"CPU memory profiling does not work on https://colab.research.google.com/gist/rmothukuru/db189c30d7acd81df1b2b75423315238/tensorboard_profiling_keras.ipynb#scrollTo=dFWOMyaHkUX5
even with TF 2.9",CPU memory profiling does not work on https://colab.research.google.com/gist/rmothukuru/db189c30d7acd81df1b2b75423315238/tensorboard_profiling_keras.ipynb#scrollTo=dFWOMyaHkUX5 even with TF 2.9.,CONTRIBUTOR
58277,bhack,1290470344,2022-10-25 12:32:07,"It is not the best warning message but the issue is that you have used `set_loop_options` twice so your second entry detect another instruction before and instead `""set_loop_options"" must be the first statement in the loop block`
Can you try with a single entry?:
`tf.autograph.experimental.set_loop_options(shape_invariants=[(y_n, tf.TensorShape([None])), (interior, tf.TensorShape([None]))])`","""set_loop_options"" must be the first statement in the loop block",CONTRIBUTOR
58293,bhack,1290442366,2022-10-25 12:08:22,"I cannot reproduce this but please don't import keras directly cause you are probably going to use an old Keras wheel.
The right import is: `from tensorflow.keras.models import load_model`",I cannot reproduce this but please don't import keras directly cause you are probably going to use an old Keras wheel.,CONTRIBUTOR
55941,reedwm,1289775016,2022-10-24 23:26:50,"Unfortunately this was rolled back in 970c3b44ea8b0db78d92ada624f03aeacf2e4518 because it broke the TF serving build. @learning-to-play, can you debug this or triage?","""Unfortunately this was rolled back in 970c3b44ea8b0db78d92ada624f03aeacf2e4518 because it broke the TF serving build.""",MEMBER
58007,cantonios,1289587686,2022-10-24 20:38:52,"From the above MR, this is a GPU kernel issue. Please specify these details in the bug description - the test in question passes on CPU, but fails on GPU.","""The test in question passes on CPU, but fails on GPU.""",CONTRIBUTOR
58131,cantonios,1289406316,2022-10-24 18:11:42,"`std::pow` always converts to float/double/long-double, so there we can use `inf`. In TF, we want to keep the output type as integer, which doesn't support `inf`. The overflow will result in undefined behavior, which is exactly what we are seeing. We do not need alignment (on CPU/GPU, or between any platform).","std::pow always converts to float/double/long-double, so there we can use inf. In TF, we want to keep the output type as integer, which doesn't support inf. The overflow will result in undefined behavior, which is exactly what we are seeing. We do not need alignment (on CPU/GPU, or between any platform)..",CONTRIBUTOR
58145,bixia1,1289399529,2022-10-24 18:05:32,This line change is already the existing code. Should close this PR.,This line change is already the existing code. Should close this PR.,CONTRIBUTOR
58145,bixia1,1289318961,2022-10-24 16:52:20,trying to manually merge it.,Trying to manually merge it.,CONTRIBUTOR
58282,drivanov,1289318508,2022-10-24 16:51:57,@bixia1 : This is a replacement for [PR#](https://github.com/tensorflow/tensorflow/pull/58256) which was unexpectedly closed when I tried to resolve the merge conflicts.,"""unexpectedly closed""",CONTRIBUTOR
58217,mihaimaruseac,1289310712,2022-10-24 16:45:38,Can you squash these commits please? It doesn't make sense to have 5 commits for a line change and one extra empty line,It doesn't make sense to have 5 commits for a line change,COLLABORATOR
58131,bhack,1289086781,2022-10-24 14:03:33,Probably we could close this. I don't expect that MLIR kernel codegen could/would align on the mul overflow type of the Eigen implementation.,I don't expect that MLIR kernel codegen could/would align on the mul overflow type of the Eigen implementation.,CONTRIBUTOR
58131,bhack,1289071510,2022-10-24 13:53:13,"@creakseek Also please note that more in general you cannot rely on unspecified behavior for an expected output so please consider the right input/tensor type for the values you feed in a specific operation:
https://en.cppreference.com/w/c/language/conversion
> Although signed integer overflow in any arithmetic operator is undefined behavior, overflowing a signed integer type in an integer conversion is merely unspecified behavior.","""Cannot rely on unspecified behavior""",CONTRIBUTOR
58276,penpornk,1288706461,2022-10-24 9:15:05,"Closing this PR as I forgot that I made additional changes to the [original PR](https://github.com/tensorflow/tensorflow/pull/57998) during merge, so we can't just use the original commit. Creating a new one soon.","I made additional changes to the [original PR](https://github.com/tensorflow/tensorflow/pull/57998) during merge, so we can't just use the original commit.",MEMBER
58269,penpornk,1288631704,2022-10-24 8:28:44,"[ARM CI](https://github.com/tensorflow/tensorflow/actions/runs/3309074945/jobs/5461899463) failed, but it seems to be an existing failure since [another run](https://github.com/tensorflow/tensorflow/actions/runs/3308078945) before it (from another PR) also got the same failure.","""It seems to be an existing failure since [another run](https://github.com/tensorflow/tensorflow/actions/runs/3308078945) before it (from another PR) also got the same failure.""",MEMBER
2625,bhack,1288086930,2022-10-23 11:02:59,"@tensorflow/dev-support Can you remove the contribution welcome label and retriage this?
I think we don't have a clear enough contribution path to label this as contribution welcome so we need to re-triage it. This ticket is very old but recently we had a related bug at https://github.com/tensorflow/tensorflow/issues/58133","""I think we don't have a clear enough contribution path to label this as contribution welcome so we need to re-triage it.""",CONTRIBUTOR
58263,bhack,1287831818,2022-10-22 15:46:55,"I am also tracking the same at https://github.com/tensorflow/profiler/issues/503
But the ownership of this issue is still not clear.
/cc @yatbear",I am also tracking the same at https://github.com/tensorflow/profiler/issues/503 But the ownership of this issue is still not clear.,CONTRIBUTOR
56761,SandSnip3r,1287492549,2022-10-21 22:23:23,FYI. Still working on resolving internal build issues.,Still working on resolving internal build issues.,CONTRIBUTOR
35677,MarkDaoust,1286957414,2022-10-21 13:24:00,"There'd be no harm in merging the PR. But there are a bunch of merge conflicts. Since Random seeds are such a common topic in software I'm not sure we need to be explaining it here. So I'm just closing, unless someone wants to fix the conflicts. Sorry!","""But there are a bunch of merge conflicts.""",MEMBER
57990,sachinprasadhs,1286279938,2022-10-20 23:43:33,"Right now, we don't have option to support the formats automatically as explained above.
For any specific OP to support any type, it has to be registered in kernels to perform any operations.","""For any specific OP to support any type, it has to be registered in kernels to perform any operations.""",CONTRIBUTOR
57762,ekuznetsov139,1286044709,2022-10-20 19:36:49,It is not easy to split it up. I will attempt to cut it into two pieces.,I will attempt to cut it into two pieces.,CONTRIBUTOR
58171,rsanthanam-amd,1285526444,2022-10-20 13:18:41,Closing this because it does not work on CUDA. I will open another PR with a better fix.,"""I will open another PR with a better fix.""",CONTRIBUTOR
58078,jpienaar,1284779382,2022-10-20 1:29:42,"Yes, we created an internal copy, modified to pass and then it triggered a test not on presubmit that resulted in it reverting. @not-jenni would be able to add some more details.","""We created an internal copy, modified to pass and then it triggered a test not on presubmit that resulted in it reverting.""",MEMBER
58131,bhack,1284194669,2022-10-19 15:27:38,"> I find that this issue only exists with dtype=np.int64
I think that case is the same without any explicit dtype",I find that this issue only exists with dtype=np.int64 I think that case is the same without any explicit dtype.,CONTRIBUTOR
58133,bhack,1284181763,2022-10-19 15:18:23,"@fuzzyswan Python floats are represented as 64-bit double-precision values:
Can you try to reproduce the same error using `dtype=tf.float64` in both `tf.range` calls?","""Can you try to reproduce the same error using dtype=tf.float64 in both tf.range() calls?""",CONTRIBUTOR
58131,bhack,1284171644,2022-10-19 15:11:47,@mohantym I suppose that you cannot reproduce this with `dtype=np.float32` or `dtype=np.float64` used in both the CPU/GPU inputs right?,I suppose that you cannot reproduce this with dtype=np.float32 or dtype=np.float64 used in both the CPU/GPU inputs right?,CONTRIBUTOR
58022,cheshire,1283785633,2022-10-19 10:31:37,"> only supports the default allocator (backend().memory_allocator()). Is it possible to run these tests with GPUBFCAllocator
Is it possible to pipe through a method which supplies a custom allocator? TF runtime does pipe it through, so the API must be there?","""only supports the default allocator""",MEMBER
57948,rsanthanam-amd,1283147621,2022-10-18 23:49:39,@cheshire I had to fix this up because what I had previously was failing on CUDA.,I had to fix this up because what I had previously was failing on CUDA.,CONTRIBUTOR
56497,bixia1,1283041184,2022-10-18 21:43:45,emailed you the error log again.,emailed you the error log again.,CONTRIBUTOR
57212,cantonios,1283000310,2022-10-18 21:02:49,This is a tf-addons issue. `tfa.image.mean_filter2d` currently needs a constant filter size.,tfa.image.mean_filter2d currently needs a constant filter size.,CONTRIBUTOR
57956,cantonios,1281316057,2022-10-17 18:41:42,"> The compressed keyword could be renamed. Introducing an entirely new datatype might be a substantial effort.
The issue is it's not _really_ a generic sparse tensor anymore - with ""compressed"" on, it's a severely restricted kind of sparse tensor - one with a very specific structure that really only comes up with embeddings. You're really trying to squeeze in a different type into the existing `SparseTensor` class. This should be an embedding-specific type.",The compressed keyword could be renamed.,CONTRIBUTOR
57956,cantonios,1281222402,2022-10-17 17:36:14,"I don't think ""compressed"" is the right term here. A ""compressed"" sparse tensor signals to me something like compressed sparse row/column storage. What we're talking about here is a very special kind of sparse tensor that seems specific to embeddings: (as Rohan mentioned) one which acts more like a ragged tensor.
My suggestion is to either try to use RaggedTensor directly, or create a completely new type for this rather than modify `SparseTensor`.","""compressed"" sparse tensor signals to me something like compressed sparse row/column storage. What we're talking about here is a very special kind of sparse tensor that seems specific to embeddings: (as Rohan mentioned)  one which acts more like a ragged tensor. My suggestion is to either try to use RaggedTensor directly, or create a completely new type for this rather than modify SparseTensor.",CONTRIBUTOR
57069,cantonios,1281179202,2022-10-17 17:03:39,"@kun-lu20 sorry, I've been away for a week.
We still have header issues. Now it's complaining about
```
./tensorflow/core/framework/tensor_util.h:24:10: error: module //tensorflow/core/framework:tensor does not depend on a module exporting 'tensorflow/core/framework/tensor_shape.proto.h'
```
I'm not sure why we're getting this now, since AFAIK we always included that header. Maybe we actually rely on it now though with your additions.","""I'm not sure why we're getting this now, since AFAIK we always included that header.""",CONTRIBUTOR
57805,mihaimaruseac,1281171059,2022-10-17 16:56:42,"Closing as duplicate of #58032 (later issue, but with more content and fewer bots)","Closing as duplicate of #58032 (later issue, but with more content and fewer bots)",COLLABORATOR
57778,jpienaar,1280876459,2022-10-17 13:38:28,"That's an assert failure and I'm not sure what config=v2 sets, but setting ""--copt=-UNDEBUG"" explicitly should enable the asserts.","That's an assert failure and I'm not sure what config=v2 sets, but setting ""--copt=-UNDEBUG"" explicitly should enable the asserts.",MEMBER
58107,tilakrayal,1280549786,2022-10-17 9:21:39,"@sachinprasadhs,
I was able to reproduce the issue on tensorflow-gpu v2.9 and nightly whereas on [cpu](https://colab.research.google.com/gist/tilakrayal/3f5ad2b8c85bc3b85d5ff26e808f0419/cpu.ipynb), the code was able to execute without any issues/error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/56a3bdd6a9e646999d77e49eb8ac4adf/gpu.ipynb).","I was able to reproduce the issue on tensorflow-gpu v2.9 and nightly whereas on [cpu](https://colab.research.google.com/gist/tilakrayal/3f5ad2b8c85bc3b85d5ff26e808f0419/cpu.ipynb), the code was able to execute without any issues/error.",CONTRIBUTOR
58091,mihaimaruseac,1279443821,2022-10-14 20:40:57,"Please send PRs to fix these. Google's internal build is C++17, so we won't notice this.","""Google's internal build is C++17, so we won't notice this.""",COLLABORATOR
56869,rahulbatra85,1279239908,2022-10-14 16:47:00,"@SandSnip3r I noticed feedback/copybara CI failed.
Is this still failing because of this
[tensorflow/tsl/platform/default/rocm_rocdl_path.cc:23] fatal error: rocm/rocm_config.h: No such file or directory
23 | #include ""rocm/rocm_config.h""
| ^~~~~~~~~~~~~~~~~~~~
?","""I noticed feedback/copybara CI failed. Is this still failing because of this [tensorflow/tsl/platform/default/rocm_rocdl_path.cc:23] fatal error: rocm/rocm_config.h: No such file or directory 23 | #include ""rocm/rocm_config.h"" |  ?""",CONTRIBUTOR
57778,jpienaar,1279101181,2022-10-14 14:40:18,tensorflow/compiler/mlir/tosa/tests:tf-to-tosa-pipeline.mlir.test fails with this test: out of range smallvector access in mlir::tosa::convertMirrorPadCommon().,out of range smallvector access in mlir::tosa::convertMirrorPadCommon().,MEMBER
58092,elfringham,1278986614,2022-10-14 13:07:07,Those 10 failures were introduced by this commit https://github.com/tensorflow/tensorflow/commit/720df16242fb5cace4fae147a45688750b660ee4,Those 10 failures were introduced by this commit.,CONTRIBUTOR
56762,SandSnip3r,1278187704,2022-10-13 21:16:24,"I am seeing a ton of failures in our internal CI build of Tensorflow as OSS.
They are all errors that rocm headers can't be found. The offending headers are:
rocm/include/hipfft/hipfft.h
rocm/include/hipsparse/hipsparse.h
rocm/include/rccl/rccl.h
rocm/include/rocsolver/rocsolver.h
Do you have any intuition as to what could be wrong?",I am seeing a ton of failures in our internal CI build of Tensorflow as OSS. They are all errors that rocm headers can't be found.,CONTRIBUTOR
58090,rsanthanam-amd,1278095997,2022-10-13 19:37:26,"> The mentioned commit was actually meant to be fixing multi-manager instantiation for NCCL, not making it worse, but I guess it went wrong with AMD.
> > It's weird that `add_device_filters` is not working for ROCM, do you know why?
I'm not sure, let me investigate.
Is there a way to prevent this PR from landing until I can investigate this?","""I guess it went wrong with AMD""",CONTRIBUTOR
58003,cheshire,1278050364,2022-10-13 18:59:24,"@ezhulenev > xla_gpu_bef_executable to true in amoschenyq-debug and xla_gpu_enable_xla_runtime_executable
I think those are not supported yet, so it's not unexpected it does not work for you right now.","""I think those are not supported yet, so it's not unexpected it does not work for you right now.""",MEMBER
39467,mihaimaruseac,1277758549,2022-10-13 15:02:25,CC @learning-to-play : this is another issue blocked on protobuf/grpc bump,CC @learning-to-play : this is another issue blocked on protobuf/grpc bump.,COLLABORATOR
57915,sachinprasadhs,1276850253,2022-10-12 23:43:24,"Hi, In the above comment I did not see the inclusion of Select tf ops.
You can try the below snippet in your code and let us know the outcome. Thanks.
```
converter.target_spec.supported_ops = [
tf.lite.OpsSet.TFLITE_BUILTINS_INT8, # enable TensorFlow Lite ops.
tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
tflite_model = converter.convert()
```
Note that all ops are right now are not available in `INT8`.",I did not see the inclusion of Select tf ops.,CONTRIBUTOR
57954,bixia1,1276825664,2022-10-12 23:14:12,"Build failed:
third_party/tensorflow/compiler/tf2tensorrt/convert/weights.cc:61:11: error: enumeration value 'kBOOL' not handled in switch [-Werror,-Wswitch]
switch (type_) {
^~~~~
1 error generated.
third_party/tensorflow/compiler/tf2tensorrt/convert/weights.cc:61:11: error: enumeration value 'kBOOL' not handled in switch [-Werror,-Wswitch]
switch (type_) {
^~~~~
1 error generated.","Build failed: third_party/tensorflow/compiler/tf2tensorrt/convert/weights.cc:61:11: error: enumeration value 'kBOOL' not handled in switch [-Werror,-Wswitch] switch (type_)   1 error generated.",CONTRIBUTOR
56333,sachinprasadhs,1276789118,2022-10-12 22:15:47,"As you have mentioned, it throws `InvalidArgumentError` error for very small output value, but it does not have any standard restriction on the range of values to document it.","""It throws InvalidArgumentError error for very small output value, but it does not have any standard restriction on the range of values to document it.""",CONTRIBUTOR
57844,cota,1276729858,2022-10-12 20:57:16,"The fix is not trivial, so here's a possible workaround.
Assuming you don't need XLA, can you please try to build with `--define=with_xla_support=false`?","The fix is not trivial, so here's a possible workaround. Assuming you don't need XLA, can you please try to build with --define=with_xla_support=false?",CONTRIBUTOR
57887,mihaimaruseac,1276729694,2022-10-12 20:57:04,"Also, please don't use ""Update <file>"". Use better commit messages and PR titles: https://cbea.ms/git-commit/","""Update file>""",COLLABORATOR
52030,bhack,1276554142,2022-10-12 18:10:02,"We had a similar error in Keras CV enabling the MLIR bridge for https://github.com/keras-team/keras-cv/issues/895:
`tf.TensorListReserve' op unknown tensor list element shape`
`note: see current operation: %38 = ""tf.TensorListReserve""(%26, %25) {device = """"} : (tensor<i32>, tensor<i32>) -> tensor<!tf_type.variant<tensor<*xi32>>>`",tf.TensorListReserve' op unknown tensor list element shape,CONTRIBUTOR
57663,mihaimaruseac,1276536231,2022-10-12 17:52:55,"> @rishikasinha-tf, would it be possible to include the PR in a TF 2.10.1 release? The log message is confusing as it (incorrectly) implies that an error has occurred that would prevent TF from running correctly.
Please send a cherrypick PR for `r2.10` too to fix the error message. See [go/tf-release/cherrypick](https://goto.google.com/tf-release/cherrypick) (internal link)",The log message is confusing as it (incorrectly) implies that an error has occurred that would prevent TF from running correctly.,COLLABORATOR
57926,jpienaar,1276530379,2022-10-12 17:47:00,The markdown is automatically generated from the .td file. If it is mangled that means someone added something unexpected. In this case it looks like TensorArrayConcatV3's doc.,The markdown is automatically generated from the .td file. If it is mangled that means someone added something unexpected. In this case it looks like TensorArrayConcatV3's doc..,MEMBER
57829,nouiz,1275378710,2022-10-11 22:59:23,"> @nouiz Could you please name the fusions from your HLO which suffer significantly from merging into broadcasts?
I updated the description to have a unit test HLO. I just checkout upstream TF and it was fusion 15 that has the issue.",I updated the description to have a unit test HLO. I just checkout upstream TF and it was fusion 15 that has the issue.,CONTRIBUTOR
56761,philipphack,1275288633,2022-10-11 21:21:36,"> EDIT: I do not see the issue on my local 3090.
The *F16Padded tests verify the padding and slicing that is applied to run GEMMs based on operands with sizes that aren't multiples of 8 on Tensor Cores. The Pascal architecture didn't have Tensor Cores, and therefore these steps are skipped which leads to the HLO mismatch. A possible solution could be to deactivate these tests on older GPUs.",I do not see the issue on my local 3090.,CONTRIBUTOR
56761,SandSnip3r,1275270270,2022-10-11 20:59:47,"I'm seeing it on a P100. I'm seeing some unrelated crash on V100. I haven't yet checked to see what's going on there. Something about ""blas_lt != nullptr"". I suspect that's not related to this PR.
EDIT: I do not see the issue on my local 3090.","I'm seeing it on a P100. I'm seeing some unrelated crash on V100. I haven't yet checked to see what's going on there. Something about ""blas_lt != nullptr"". I suspect that's not related to this PR.",CONTRIBUTOR
58057,vinila21,1275184500,2022-10-11 19:39:17,Not required aymore,Not required aymore.,CONTRIBUTOR
43696,ml-0,1274916477,2022-10-11 15:50:52,"The description requests:
Once, #41860 is merged all lines like
INCLUDES += isystem$(MAKEFILE_DIR)/downloads/cmsis/CMSIS/Core/Include/
should be removed from all Makefiles. This is still required if not already done in another context.
It refers to the Makefiles from tflite-micro which are in a separate repo (https://github.com/tensorflow/tflite-micro), meanwhile.","The description requests: Once, #41860 is merged all lines like INCLUDES += isystem$(MAKEFILE_DIR)/downloads/cmsis/CMSIS/Core/Include/ should be removed from all Makefiles. This is still required if not already done in another context. It refers to the Makefiles from tflite-micro which are in a separate repo (https://github.com/tensorflow/tflite-micro), meanwhile.",CONTRIBUTOR
58005,cheshire,1274442125,2022-10-11 10:00:55,> but PlatformUtil doesn't have a static function can provide the running platform name in the runtime. Do you think you could add one?,"""but PlatformUtil doesn't have a static function can provide the running platform name in the runtime.""",MEMBER
55941,learning-to-play,1274022028,2022-10-11 2:53:49,"Hi @trevor-m, It seems the macOS build is still failing.",It seems the macOS build is still failing.,COLLABORATOR
58005,i-chaochen,1273553466,2022-10-10 16:22:07,"> Could we minimize duplication and ideally make this a runtime switch?
Appricate for the feedback! Now I use a more genric PLATFORM to avoid the duplication.
Yes, you're correct that will be great if it can determine it at runtime instead of preprocessing, but PlatformUtil doesn't have a static function can provide the running platform name in the runtime. IIUIC, it's the StreamExecutor to take charge of the running platform name in the runtime?","""IIUIC, it's the StreamExecutor to take charge of the running platform name in the runtime.""",CONTRIBUTOR
58039,Ingkarat,1273256981,2022-10-10 12:42:02,"For the CLA issue, it shows that there are 2 contributors possibly because the email set for git commits (non-gmail) and the email for CLA were different when creating this PR. I have changed the git email to match the CLA one but the issue still remains. I am not sure how to resolve this. Do I need to re-open the PR?","""I am not sure how to resolve this.""",CONTRIBUTOR
57959,tilakrayal,1273126016,2022-10-10 10:46:51,"@jiannanWang,
Sorry for the delay. I tried to execute the code on tensorflow-gpu and the results are the same in both cases. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/76627395e7ec322a3c5b34bbe586c7fb/untitled660.ipynb).",I tried to execute the code on tensorflow-gpu and the results are the same in both cases.,CONTRIBUTOR
56762,rahulbatra85,1272102067,2022-10-07 21:30:39,"Yeah, we already split it into other PRs. It was much larger. Also, I understand larger commits are not a good idea, but in this case changes really need to go together
Also note, most of the code changes are ROCm specific. About half(may be more) the files touched are only for ROCm. In common files, almost all the changes are again ROCm specific. They are #defines like TENSORFLOW_ROCM","""I understand larger commits are not a good idea, but in this case changes really need to go together""",CONTRIBUTOR
56762,SandSnip3r,1272099176,2022-10-07 21:25:48,"@rahulbatra85 The description says that this will be split. @cheshire asked if it could be split. I suspect the reason this is taking so long is because its so large. Smaller PRs are substantially easy to get in.
Let me check what internal CI looks like.",I suspect the reason this is taking so long is because its so large.,CONTRIBUTOR
58007,wenscarl,1272009805,2022-10-07 19:17:13,"@NeilGirdhar I don't think it's that PR to blame. I will remove your name. There is a non-defined path for rank-1 complex tensor with conjugate required. Should be easy to fix but I am not sure what should be a reasonable behavior, doing a conjugate or treated as no-op?","I don't think it's that PR to blame. I will remove your name. There is a non-defined path for rank-1 complex tensor with conjugate required. Should be easy to fix but I am not sure what should be a reasonable behavior, doing a conjugate or treated as no-op?.",CONTRIBUTOR
57979,i-chaochen,1271561119,2022-10-07 13:00:21,"@cheshire it seems my last changes will fail on cuda side, so I have added the same test for cuda as well.","""I have added the same test for cuda as well.""",CONTRIBUTOR
56088,elfringham,1271505445,2022-10-07 12:05:43,My PR https://github.com/tensorflow/tensorflow/pull/57969 was approved 3 days ago but has not yet been merged. Can anyone please see if there is anything that needs to be addressed?,"""My PR https://github.com/tensorflow/tensorflow/pull/57969 was approved 3 days ago but has not yet been merged.""",CONTRIBUTOR
57744,d0k,1271486086,2022-10-07 11:45:06,"Algebraic Simplifier rewrites `ln(pow(A,B)) => B*ln(abs(A))`. That doesn't look right for negative `B`.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/algebraic_simplifier.cc#L3373","Algebraic Simplifier rewrites ln(pow(A,B)) => B*ln(abs(A)). That doesn't look right for negative B.",MEMBER
57955,rsanthanam-amd,1269266210,2022-10-06 3:38:50,"i'm not quite sure how to craft a specific unit test to check this.
one thing i will mention is that this fix was borne out of several existing unit test failures on gfx90a:
- //tensorflow/compiler/xla/tests:conv_depthwise_test_gpu - //tensorflow/compiler/xla/tests:convolution_test_gpu
- //tensorflow/compiler/xla/tests:convolution_test_gpu_alternative_layout_gpu
because there was no memory fence, we were seeing intermittent mismatches between expected and actual results",i'm not quite sure how to craft a specific unit test to check this.,CONTRIBUTOR
57069,cantonios,1268683284,2022-10-05 16:49:30,"We're still seeing internal build failures related to visibility of `tensor_util.h`, specifically for android builds and tf-lite. I'll need to investigate later today.","""still seeing internal build failures related to visibility of tensor_util.h, specifically for android builds and tf-lite""",CONTRIBUTOR
57954,bixia1,1268566907,2022-10-05 15:03:16,"@DEKHTIARJonathan something is wrong with this PR, it modifies > 350 files.","""something is wrong with this PR, it modifies > 350 files.""",CONTRIBUTOR
57887,bhack,1267683680,2022-10-04 22:44:52,"> We don't support anaconda installation and there is a concern that listing that in the README might result in unmet expectations from developers.
Can we use something like:
https://github.com/tensorflow/build/#community-supported-tensorflow-builds ?","""We don't support anaconda installation and there is a concern that listing that in the README might result in unmet expectations from developers.""",CONTRIBUTOR
56557,ashiqimranintel,1267348428,2022-10-04 17:47:01,"@penpornk , can you re-run the test?",Can you re-run the test?,CONTRIBUTOR
57794,cheshire,1266620067,2022-10-04 8:54:28,"Seems to break `//tensorflow/compiler/xla/service/gpu:cudnn_fused_conv_rewriter_test` when running with `XLA_FLAGS=--xla_gpu_enable_xla_runtime_executable`, judging by the error message the error is with the test. @ezhulenev","""Seems to break""",MEMBER
55639,bhack,1266234528,2022-10-04 0:17:03,"> Will look into a solution for invariant issue for pfor in this quarter and update. Thanks!
With the within the batch augmentation policy we have (imho wrongly) tried to use `vectorized_map` to workaround the limits of the image ops that are working with an images (batch) but with a fixed transformation (whole batch) arg.
If we are still talking about covering the missing converter with invariant I don't think that we will solve this cases.","""If we are still talking about covering the missing converter with invariant I don't think that we will solve this cases.""",CONTRIBUTOR
57751,cantonios,1266147083,2022-10-03 22:40:33,"The CPU behavior is actually intentional to match numpy. Using `==` between tensors returns False if there is a shape mismatch [doc](https://www.tensorflow.org/api_docs/python/tf/Tensor#__eq__). If you use `tf.math.equal`, it will assert on incompatible dimensions.
Looks like the GPU behavior is the buggy one.","""Looks like the GPU behavior is the buggy one.""",CONTRIBUTOR
56088,elfringham,1265752571,2022-10-03 16:58:02,@cantonios your recent commit introducing FP8 is not building on AARCH64. https://github.com/tensorflow/tensorflow/issues/57918,"""Your recent commit introducing FP8 is not building on AARCH64.""",CONTRIBUTOR
57890,gadagashwini,1265345548,2022-10-03 12:07:49,"Hi @surajitkundu-dazn, Still I am unable to access the given link. <img width=""1662"" alt=""Screen Shot 2022-10-03 at 5 36 17 PM"" src=""https://user-images.githubusercontent.com/99852755/193573001-26f0dcf7-bf2a-4fdb-b1b8-5b7bbe38c99e.png"">",Still I am unable to access the given link.,CONTRIBUTOR
57844,namrata-ibm,1265200920,2022-10-03 9:52:15,"@angerson Could you please check?
This issue will be seen on all big endian machines. Also as per [this](https://github.com/tensorflow/tensorflow/blob/9e12018a40b5024f51bbaa4d27dcad047bd8449f/.bazelrc#L686) TFRT integration should be optional as of now. Is there a way to disable it?","""TFRT integration should be optional as of now. Is there a way to disable it?""",CONTRIBUTOR
57052,foxik,1264588517,2022-10-02 8:55:58,"The Keras guys wrote in https://github.com/keras-team/keras/issues/16978 that it looks like a TF issue and that we should report it here.
@elina-israyelyan Could you please reopen the issue?",The Keras guys wrote in https://github.com/keras-team/keras/issues/16978 that it looks like a TF issue and that we should report it here.,CONTRIBUTOR
57879,joker-eph,1263942410,2022-09-30 19:32:40,"`--dynamic=off` was missing I think. It is not longer needed at HEAD, I rebased the change",--dynamic=off was missing,CONTRIBUTOR
57794,cheshire,1263586072,2022-09-30 13:36:14,"We can try to run our benchmarks first, then it's probably simplest to enable it by default in this PR. Having it disabled by default is not great, as we'll essentially get a lot of dead untested code.","Having it disabled by default is not great, as we'll essentially get a lot of dead untested code.",MEMBER
57861,impjdi,1262662236,2022-09-29 18:28:40,"You can't use the OpenGL delegate to run it on Vulkan. We do have a Vulkan implementation internally, but we haven't open sourced it yet due to binary size reason. The Vulkan impl would have to be shipped with shaderc which is another tens of megabytes; we're waiting for Android to ship shaderc as part of the OS so that devs don't have to eat the app size cost, but it's seeing very little progress on that end. So ... no Vulkan impl in the short term.","""We do have a Vulkan implementation internally, but we haven't open sourced it yet due to binary size reason.""",CONTRIBUTOR
57069,cantonios,1262488005,2022-09-29 16:00:39,@kun-lu20 this PR now has a cyclic dependency and fails to build. Can you rebase and address it?,This PR now has a cyclic dependency and fails to build.,CONTRIBUTOR
57906,mohantym,1262110637,2022-09-29 10:53:50,"Hi @Saar-Ken-Ji !
In Colab , Session was crashing in Colab with 2.9 . rsqrt in select ops list. So could you try with below syntax in lite conversion.
```
converter.target_spec.supported_ops = [
tf.lite.OpsSet.TFLITE_BUILTINS_INT8, # enable TensorFlow Lite ops.
tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
converter.experimental_enable_quantizer = True
```",crashing in Colab with 2.9 . rsqrt in select ops list. So could you try with below syntax in lite conversion.,CONTRIBUTOR
56762,SandSnip3r,1261361775,2022-09-28 19:18:10,"I see that the Internal CI build (Py+CPP Test Suite - Ubuntu GPU, Python 3.9) fails. I think you can see the results. The failures are in the same files that you have changed. Have you tried resolving this issue?","I see that the Internal CI build (Py+CPP Test Suite - Ubuntu GPU, Python 3.9) fails.",CONTRIBUTOR
57087,hawkinsp,1260976244,2022-09-28 14:11:23,"For anyone else reading this, ""Bus error"" was from running in a docker container without enough SHM space for NCCL.","""Bus error""",MEMBER
57801,cheshire,1260621202,2022-09-28 9:14:59,"CHECK really isn't great as it crashes the entire process which potentially runs other things as well.
However I agree that reaching parity with CUDA would be sufficient.
How about: use CHECK if CUDA does so, and propagate bad Status/do logging if CUDA does so?",CHECK really isn't great as it crashes the entire process which potentially runs other things as well.,MEMBER
44670,adriangb,1260280162,2022-09-28 1:37:04,Theyre kinda different issues and this has been open for 2 years now so Id rather leave this open until it gets fixed,Theyre kinda different issues and this has been open for 2 years now,CONTRIBUTOR
57855,reedwm,1260192424,2022-09-27 23:55:35,"I confirmed we don't have native Windows GPU builds anymore, and no CI will run for Windows GPUs. So this can be marked as a non-draft PR.","I confirmed we don't have native Windows GPU builds anymore, and no CI will run for Windows GPUs. So this can be marked as a non-draft PR.",MEMBER
57656,nouiz,1259959125,2022-09-27 19:32:49,"> That would be untested code though. Sorry for us lagging behind with Ampere CI. Could you predicate on compute capability at runtime? We do have a number of tests doing that.
I rebased and added a test. I already modified one existing tests, but an older rebase removed it.",untested code though,CONTRIBUTOR
56761,SandSnip3r,1259956719,2022-09-27 19:30:25,"We were waiting on #57648 to be merged before continuing this, right?","Waiting on #57648 to be merged before continuing this, right?",CONTRIBUTOR
57872,ganler,1259932751,2022-09-27 19:06:23,"This seems to be related to:
- https://github.com/tensorflow/tensorflow/issues/55563
- https://github.com/bazelbuild/bazel/issues/15359
I tried all of the mentioned fixes but none of them work...",I tried all of the mentioned fixes but none of them work.,CONTRIBUTOR
40195,harahu,1259375847,2022-09-27 11:39:37,"@mohantym I agree that this issue probably belongs in the keras repo. However, it was created before the split was made. I would have expected the organization to have made an effort in porting over any keras-related issues, including this one, if you feel community-created issues are valuable.
I take it this was not done. Would you like this issue to be ported over now? How do you suggest we go about that?",I take it this was not done.,CONTRIBUTOR
14798,bhack,1259347323,2022-09-27 11:12:18,"@ganler As you see I cannot make progress on my PR at https://github.com/tensorflow/tensorflow/pull/56510
/cc @cheshire @theadactyl",I cannot make progress on my PR at https://github.com/tensorflow/tensorflow/pull/56510 /cc @cheshire @theadactyl.,CONTRIBUTOR
32085,MarkDaoust,1258750396,2022-09-26 23:10:58,"> since 2 years ago we no longer use ./configure / configure.py for the standard builds
Oh! So I guess we need an update to the build from source instructions. To say... ""only `configure` the build if you need to""?","""since 2 years ago we no longer use ./configure / configure.py for the standard builds""",MEMBER
56493,advaitjain,1258712066,2022-09-26 22:30:50,"@Tombana, please note that the fix had to be reverted with https://github.com/tensorflow/tensorflow/commit/db1dd64675e4cb471390ba2aacdedf27b842399a due to some internal regressions. We're going to see how we can reconcile the different moving pieces but its going to take some time.","""The fix had to be reverted with https://github.com/tensorflow/tensorflow/commit/db1dd64675e4cb471390ba2aacdedf27b842399a due to some internal regressions""",MEMBER
57536,MarkDaoust,1258557854,2022-09-26 20:07:06,"> #PROBLEM!!! NEEDS INDENTATION!
It doesn't actually. Why do you think it does?
Closing the tape is ""stop writing new gradient ops to the tape"", not ""throw out the tape"".","""It doesn't actually. Why do you think it does?""",MEMBER
57747,ganler,1258339183,2022-09-26 16:51:52,"Notably, the result is only wrong on XLA-GPU. It is correct on Eager-CPU/GPU and XLA-CPU, as well as native Python, PyTorch and NumPy. We suspect it might be due to some miss-casting. This bug is also confirmed by OSS VRP but was not regarded as a security vulnerability as the exploit can be non-trivial.","Notably, the result is only wrong on XLA-GPU. It is correct on Eager-CPU/GPU and XLA-CPU, as well as native Python, PyTorch and NumPy. We suspect it might be due to some miss-casting. This bug is also confirmed by OSS VRP but was not regarded as a security vulnerability as the exploit can be non-trivial.",CONTRIBUTOR
40195,harahu,1258099170,2022-09-26 14:09:19,@mohantym There's nothing in the documentation you linked that indicates this issue having been addressed.,Nothing in the documentation you linked that indicates this issue having been addressed.,CONTRIBUTOR
55941,learning-to-play,1257440580,2022-09-26 3:57:52,"@angerson @nitins17 MacOS presubmits failed, but the ""Details"" link to see the logs is missing. Can you help fix this issue?","""Can you help fix this issue?""",COLLABORATOR
57762,cheshire,1256934825,2022-09-24 10:33:03,"@sergeykozub @SandSnip3r @ekuznetsov139 due to merge conflict, landing this all at once might be very hard. It would be much easier to split it up.","""due to merge conflict, landing this all at once might be very hard.""",MEMBER
22710,Keno,1256910605,2022-09-24 8:35:38,This code path is obsolete - I don't think anyone is using it.,This code path is obsolete - I don't think anyone is using it.,CONTRIBUTOR
57759,ganler,1256837137,2022-09-24 2:25:04,"@mohantym @sachinprasadhs It seems this bug only occurs when one of the arguments in max is `tf.zeros`. When setting it to `tf.ones`, the inferred shape will be correct.","""It seems this bug only occurs when one of the arguments in max is tf.zeros.""",CONTRIBUTOR
57779,sachinprasadhs,1256620856,2022-09-23 20:00:03,"Update: With the changes, tests are failing again in multiple files.",tests are failing again in multiple files.,CONTRIBUTOR
57779,sachinprasadhs,1256618229,2022-09-23 19:56:31,"The above PR was automatically rolled back, since the tests were failing.
I have submitted the changes again. I will update once the test results are completed.","""The tests were failing""",CONTRIBUTOR
57645,sachinprasadhs,1256547299,2022-09-23 18:40:13,"Apologies for the delayed response,
`tf.compat.v1.disable_eager_execution()` would force the entire code to run in graph mode and results in faster execution as compared to Tensorflow eager mode where only model logic part is wrapped in `tf.function` and runs in graph mode when `run_eagerly` is set to `False`.",delayed response,CONTRIBUTOR
57808,mihaimaruseac,1255537891,2022-09-22 20:53:22,"We no longer patch 2.7, so only 2.8 to 2.10 need to be cherrypicked","We no longer patch 2.7, so only 2.8 to 2.10 need to be cherrypicked.",COLLABORATOR
57800,i-chaochen,1255045918,2022-09-22 13:43:24,I closed it and opened another one as this is from the wrong branch.,I closed it and opened another one as this is from the wrong branch.,CONTRIBUTOR
57735,mihaimaruseac,1254322912,2022-09-21 23:08:22,"TF does not generate VEX statements, so the only option would be to read the entire codebase / send PR to update and then use `tf-nightly` / patched version when it would be released","TF does not generate VEX statements, so the only option would be to read the entire codebase / send PR to update and then use tf-nightly / patched version when it would be released.",COLLABORATOR
57672,cantonios,1253904532,2022-09-21 15:54:22,"Those are two separate things. Adding an option to `tf.pad` is relatively straightforward. Adding to all convolutions is a whole other beast which requires changing our APIs - our current convolution API doesn't have separate `padding` and `padding_mode` options, it assumes we always pad with zeros and only controls the size.","Those are two separate things. Adding an option to tf.pad is relatively straightforward. Adding to all convolutions is a whole other beast which requires changing our APIs - our current convolution API doesn't have separate padding and padding_mode options, it assumes we always pad with zeros and only controls the size.",CONTRIBUTOR
56207,aliencaocao,1253145650,2022-09-21 2:59:46,"Yes mine triggers 100%, until i remove a specific preprocessing function. Its not even the model itself that is causing the issue, it is the specific preprocessing function of this specific model (all other preprocessor have no issue)","""specific preprocessing function of this specific model""",CONTRIBUTOR
57663,mihaimaruseac,1252866174,2022-09-20 20:17:25,"Unless there is a cherrypick for 2.10 (and a 2.10.1 patch release), #56691 will only be included in 2.11 and later","Unless there is a cherrypick for 2.10 (and a 2.10.1 patch release), #56691 will only be included in 2.11 and later.",COLLABORATOR
57735,mihaimaruseac,1252773332,2022-09-20 18:49:32,"Unfortunately there is no SBOM generated by TF during build, so you have to check the source code
Main branch has https://github.com/tensorflow/tensorflow/blob/92d57e61d41f8e6fca50d582ac7a56046a391591/tensorflow/workspace2.bzl#L568-L576
So using vulnerable zlib.","""Using vulnerable zlib..""",COLLABORATOR
57763,edwardyehuang,1252603588,2022-09-20 16:29:19,"> Thanks for reporting this issue. As a short cut for now (to have a determined initializer for your test), you can use `tf.keras.utils.set_random_seed()` to set the seed for python/numpy/tf/keras.
Hi. I think you misunderstand it. Even set random seed using `tf.keras.utils.set_random_seed()`, the generated weights is different between 2.10 and 2.9/2.8 (Stateless vs Stateful rng)","""Even set random seed using tf.keras.utils.set_random_seed(), the generated weights is different between 2.10 and 2.9/2.8 (Stateless vs Stateful rng)""",CONTRIBUTOR
56050,Nyrio,1252117312,2022-09-20 9:55:33,"@bixia1 I haven't received the test logs, can you please try to send them again?","I haven't received the test logs, can you please try to send them again?",CONTRIBUTOR
56918,sirakiin,1251623617,2022-09-19 22:12:56,"The PR file changes LGTM and already approved / reviewed, please see the checks for the reason merging is blocked. Some of the CI failures seems to be transient issues and shall resolve after re-triggering those. For CLA please see [this](https://github.com/tensorflow/tensorflow/pull/56918/checks?check_run_id=7534827657) on how to resolve the check.","""The PR file changes LGTM and already approved / reviewed, please see the checks for the reason merging is blocked.""",MEMBER
57087,nouiz,1251408805,2022-09-19 18:45:07,"> @nouiz We could provide HLO dump, but I expect this situation to be quite frequent in the future, as JAX tends to have better test coverage than we do.
> > I think it would be worthwhile to figure out how to run JAX tests with a given revision, do you think this is a reasonable process?
It makes sense. But we need to find a way that doesn't slow down the workflow.","""I expect this situation to be quite frequent in the future, as JAX tends to have better test coverage than we do.""",CONTRIBUTOR
57734,mihaimaruseac,1251303918,2022-09-19 17:08:37,TF 2.6 is no longer in the window of supported versions,TF 2.6 is no longer in the window of supported versions.,COLLABORATOR
57735,mihaimaruseac,1251303491,2022-09-19 17:08:15,TF 2.6 is no longer in the windows of supported versions.,TF 2.6 is no longer in the windows of supported versions.,COLLABORATOR
57738,aliencaocao,1251000115,2022-09-19 13:10:42,"We probably have a different issue here because I am using cuda 11.7 with cudnn 8.5, while you are using 8.1. If you have cuda 11.7 installed but cudnn 8.1, it will not work and will give the PTXAS error.","I am using cuda 11.7 with cudnn 8.5, while you are using 8.1.",CONTRIBUTOR
54390,abattery,1250582711,2022-09-19 5:22:33,I don't have permissions to approve the TOSA related changes. I will manually merge this change without the TOSA comment updates. Could you send a separate change for the TOSA related one and get approvals from the right owner instead?,I don't have permissions to approve the TOSA related changes.,CONTRIBUTOR
56207,aliencaocao,1250187863,2022-09-18 4:22:07,"Just an update, I have been busy with work recently so only got time to try again now. My display was already at 100%. I however cannot reproduce the issue now due to another issue that made it impossible to train any model: https://github.com/tensorflow/tensorflow/issues/57738
Therefore, I can only continue to test on this bug after I manage to resolve that.
Sorry for the stall.","""I have been busy with work recently so only got time to try again now.""",CONTRIBUTOR
56850,lu-wang-g,1250157806,2022-09-17 23:58:46,"Seems like I can't approve this CL. Khanh, can you?","I can't approve this CL. Khanh, can you?",MEMBER
57674,reedwm,1249897478,2022-09-16 22:13:59,"Do you plan on supporting BF16 in all or most kernels so that it is usable outside XLA? I am concerned about the binary size increase that would be caused by instantiating many kernels with BF16, as well as the kernel maintenance cost.","I am concerned about the binary size increase that would be caused by instantiating many kernels with BF16, as well as the kernel maintenance cost.",MEMBER
57711,sachinprasadhs,1249679156,2022-09-16 18:29:56,"This is due to the very large input which is causing OOM/ memory overflow with large input, when you try large value like tf.int32.max, you will get the error output.
Below is the error output.
```
import tensorflow as tf
tf.eye(2147483647)
ResourceExhaustedError: OOM when allocating tensor with shape[2147483647,2147483647] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:MatrixDiagV3] name: diag ```",OOM/ memory overflow with large input,CONTRIBUTOR
56088,elfringham,1249460532,2022-09-16 14:45:44,"> CD on master branch failing on grappler remapper tests after this commit [9d4e950](https://github.com/tensorflow/tensorflow/commit/9d4e950687b25f78fe30a74a8b40ba16f12c9fd1) , is this a known issue and is the fix WIP? @nitins17
tensorflow.python.framework.errors_impl.UnimplementedError: Fusion is not implemented: [BiasAdd,GeluExact]","""UnimplementedError: Fusion is not implemented: [BiasAdd,GeluExact]""",CONTRIBUTOR
56155,anotheruserofgithub,1249323510,2022-09-16 12:50:53,"@nutsiepully @gbaned Could you merge this now, please? It has been a very long time since this PR has been opened.
If you really need more explanations about this PR, I can certainly provide you with a detailed manuscript.","""It has been a very long time since this PR has been opened.""",CONTRIBUTOR
57586,elfringham,1249067913,2022-09-16 8:21:27,Made moot by https://github.com/tensorflow/tensorflow/commit/6343eb47624d27fb04c219093db9d6e8a8d15653,"""Made moot by""",CONTRIBUTOR
57087,cheshire,1247921088,2022-09-15 10:41:30,"> I confirmed that this test fail here with upstream/master
Could be both? We only test internally on Volta/Pascal now =/","""We only test internally on Volta/Pascal now =/""",MEMBER
57561,sachinprasadhs,1247656614,2022-09-15 6:49:43,"I do not think we have builtin ops listed anywhere apart from the sources which is already been linked in the issue above.
But, if you want to check if certain op is supported or not during conversion, you can include only `tf.lite.OpsSet.TFLITE_BUILTINS` in your converter, if that op is not available in TFLITE builtin, it will throw error.
```
converter.target_spec.supported_ops = [
tf.lite.OpsSet.TFLITE_BUILTINS]
```","""I do not think we have builtin ops listed anywhere apart from the sources which is already been linked in the issue above.""",CONTRIBUTOR
57514,sachinprasadhs,1247626943,2022-09-15 6:12:16,"This has. to be edge case scenario.
`tf.int32.min` and `tf.int32.max` are not the same values, which are `-2147483648` and `2147483647` respectively.
Since, `tf.abs(tf.int32.min)` value which is `tf.abs(-2147483648)` exceeds the `tf.int32.max` value which is `2147483647`. it throws exception in that case.
When `tf.int32.min` is added with 1, it matches the `tf.int32.max` value and works in that case.",This has. to be edge case scenario.,CONTRIBUTOR
44609,yishuangP,1247368986,2022-09-14 22:34:30,"Hi Dan, really sorry that you have to download the `tar.gz` file and unpack it to use it in Swift package. Hopefully we'll release it as Swift package soon. For `TensorFlowLiteSwift`, we just publish the sources and it depends on other XCFrameworks (like `TensorFlowLiteC`). Eventually we'll release it as Swift package.",really sorry that you have to download the tar.gz file and unpack it to use it in Swift package.,CONTRIBUTOR
57526,cantonios,1247051141,2022-09-14 16:56:30,"Why? I don't think we want it officially part of the API that the axis is ignored when inputing a single tensor. It is still a user error. It just is not worth us checking for it since doing so only adds overhead, and we return the correct result if the specified axis was _any_ valid value.","""I don't think we want it officially part of the API that the axis is ignored when inputing a single tensor.""",CONTRIBUTOR
57671,mihaimaruseac,1246892819,2022-09-14 14:54:44,"Because of the way TF is built with optimizations, we can only target one version of GPU libraries.","Because of the way TF is built with optimizations, we can only target one version of GPU libraries.",COLLABORATOR
56604,namrata-ibm,1246712172,2022-09-14 12:45:22,"@rthadur @gbaned I am not able to view the failures, could you please check?","I am not able to view the failures, could you please check?",CONTRIBUTOR
57656,cheshire,1245903407,2022-09-13 20:10:12,That would be untested code though. Sorry for us lagging behind with Ampere CI. Could you predicate on compute capability at runtime? We do have a number of tests doing that.,That would be untested code though.,MEMBER
57669,mihaimaruseac,1245770991,2022-09-13 18:02:39,TF 2.4 is no longer supported. Please switch to TF 2.10,TF 2.4 is no longer supported. Please switch to TF 2.10,COLLABORATOR
57481,cantonios,1245679426,2022-09-13 16:48:42,"Again, looks simply to be a numerical issue with estimating the gradients. Playing around with `delta` for the numerical causes significant swings in the maximum error. The theoretical gradient of `x * sigmoid(x)` is correct from #57357.","Again, looks simply to be a numerical issue with estimating the gradients.",CONTRIBUTOR
57087,nouiz,1245539444,2022-09-13 14:58:14,"Any update? It was approved 4 days ago, but isn't merged yet.
One CI failed, but the test failure doesn't seem related to my PR. It is a grappler error:
//tensorflow/python/grappler:remapper_test_gpu","""One CI failed, but the test failure doesn't seem related to my PR.""",CONTRIBUTOR
56796,cantonios,1245403322,2022-09-13 13:18:59,"The function `x/norm(x)` is not differentiable at 0, so what do you expect it to be? Without `epsilon`, a straight-forward implementation would produce `NaN`. With `epsilon`, the piecewise definition of the function is
```
l2_normalize(x) = x/sqrt(epsilon), if x < sqrt(epsilon)
x/norm(x), otherwise
```
so for small x, the gradient of this is `1/sqrt(epsilon)`, which is exactly what is returned.","The function x/norm(x) is not differentiable at 0, so what do you expect it to be? Without epsilon, a straight-forward implementation would produce NaN. With epsilon, the piecewise definition of the function is  l2_normalize(x) = x/sqrt(epsilon), if x  sqrt(epsilon)  x/norm(x), otherwise ",CONTRIBUTOR
53767,lgeiger,1245374520,2022-09-13 12:55:50,"I retested the above example with `2.10.0` and the segfault seems to be fixed now, however conversion still fails with:
```
'tfl.transpose' op has mismatched quantized axes of input and output
```
See [here](https://colab.research.google.com/drive/1IXri5HeDc9qTAtDOp-LqZyQTL8CcemGq?usp=sharing).","I retested the above example with 2.10.0 and the segfault seems to be fixed now, however conversion still fails with:  'tfl.transpose' op has mismatched quantized axes of input and output ",CONTRIBUTOR
54458,albertz,1245218462,2022-09-13 10:32:43,"But as you can read above: ""Be aware that the flag may increase the memory usage.""
So this is not really a solution.","""Be aware that the flag may increase the memory usage.""",CONTRIBUTOR
57645,tilakrayal,1245174394,2022-09-13 9:55:27,"@sachinprasadhs,
I was able to reproduce the issue on tensorflow v2.8, v2.9 and [nightly](https://colab.research.google.com/gist/tilakrayal/6ec9204828bcc1a63fc2a70bf16f5747/untitled594.ipynb) with and without the line `tf.compat.v1.disable_eager_execution().` Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/0c33fe9588906eafd35f134f9815e234/untitled581.ipynb).","I was able to reproduce the issue on tensorflow v2.8, v2.9 and [nightly](https://colab.research.google.com/gist/tilakrayal/6ec9204828bcc1a63fc2a70bf16f5747/untitled594.ipynb) with and without the line tf.compat.v1.disable_eager_execution().",CONTRIBUTOR
26182,plopresti,1244480993,2022-09-12 21:11:12,"In case anyone finds this during a search...
I just had this problem with Tensorflow build under CUDA 11.7. The error was coming from the call to cudnnCreate() in stream_executor/cuda/cuda_dnn.c.
The problem was a mismatch between the versions of CUDA (specifically libcublas) and libcudnn8. In my case, I accidentally installed the cuda10.2 variant of the libcudnn8 RPM. Installing the correct cuda11.7 version resolved it.","""The error was coming from the call to cudnnCreate() in stream_executor/cuda/cuda_dnn.c""",CONTRIBUTOR
57653,nouiz,1243822454,2022-09-12 14:25:13,"> All I see is:
> > > [XLA] Add the XLA_FLAGS xla_dump_show_fusion_subcomputations.
> > It allows to not dump the fusion internal. This allows to display bigger graph.
Today it tooks longer for the web site to be updated. Can you refresh the page? Currently I see this text:
`Dumps [or not] the fusion internal in the dot and html output file.`.",Currently I see this text: Dumps [or not] the fusion internal in the dot and html output file.,CONTRIBUTOR
53260,mohantym,1243581649,2022-09-12 11:10:30,"@youchangkim ! @penpornk !
I could replicate this in my mac M1 machine . It is not replicating in [Colab ](https://colab.sandbox.google.com/drive/1c5OMqus-QBVvaETWzMelBi2LFz90GNLy?resourcekey=0-ph0Yv_zdI-SHm9i2Mwcwyg#scrollTo=eu6lvroGVYAu)though.",I could replicate this in my mac M1 machine . It is not replicating in [Colab](https://colab.sandbox.google.com/drive/1c5OMqus-QBVvaETWzMelBi2LFz90GNLy?resourcekey=0-ph0Yv_zdI-SHm9i2Mwcwyg#scrollTo=eu6lvroGVYAu)though..,CONTRIBUTOR
57494,EnricoMi,1242785178,2022-09-10 18:40:14,"As of `20220910`, this is still not fixed.","As of 20220910, this is still not fixed.",CONTRIBUTOR
56606,Flamefire,1242719060,2022-09-10 12:33:21,"Unrelated CI failures, e.g.:
> ERROR: The project you're trying to build requires Bazel 5.3.0 (specified in /workspace/.bazelversion), but it wasn't found in /usr/local/lib/bazel/bin.","Unrelated CI failures, e.g.: > ERROR: The project you're trying to build requires Bazel 5.3.0 (specified in /workspace/.bazelversion), but it wasn't found in /usr/local/lib/bazel/bin..",CONTRIBUTOR
57649,hanbinyoon,1242171746,2022-09-09 16:10:00,"Looping in @anlunx --tf_xla_persistent_cache_directory should only work with the GpuCompiler when using XLA_FLAGS=--xla_gpu_jitrt_executable
@ezhulenev for any plans with the CpuCompiler.",Looping in @anlunx --tf_xla_persistent_cache_directory should only work with the GpuCompiler when using XLA_FLAGS=--xla_gpu_jitrt_executable @ezhulenev for any plans with the CpuCompiler.,MEMBER
21348,ebrevdo,1242163330,2022-09-09 16:02:39,"I expect this bug to be gone in TF2, and TF1 is no longer officially supported by TensorFlow team. Can you provide a TF2-based repro, one that does not use Sessions?","""I expect this bug to be gone in TF2 and TF1 is no longer officially supported by TensorFlow team.""",CONTRIBUTOR
56737,cantonios,1242160005,2022-09-09 15:59:54,You're running into floating-point precision issues because the frame length is too large (representable numbers go out of range in forward accumulation of gradients). Changing to float64 fixes the issue.,running into floating-point precision issues because the frame length is too large,CONTRIBUTOR
57634,elfringham,1240798444,2022-09-08 14:30:10,Found some unexpected unit test failures so will convert to draft for the moment.,Found some unexpected unit test failures so will convert to draft for the moment.,CONTRIBUTOR
57494,joker-eph,1239823690,2022-09-07 20:11:37,"> Could you include headers from the new location?
I don't think that is a good idea: this isn't the final location.",Could you include headers from the new location? I don't think that is a good idea: this isn't the final location.,CONTRIBUTOR
57564,bhack,1239763565,2022-09-07 18:57:25,Too late for my comments... `copybara-service` has arrived to merge this.,Too late for my comments... copybara-service has arrived to merge this.,CONTRIBUTOR
57494,rdzhabarov,1239667997,2022-09-07 17:17:11,"We are about to remove `tensorflow/stream_executor` completely.
Everything has been moved to the `tensorflow/compiler/xla/stream_executor`.
`tensorflow/stream_executor/...` pretty much has only header redirections to the `tensorflow/compiler/xla/stream_executor/...`
Could you include headers from the new location?",tensorflow/stream_executor/...,CONTRIBUTOR
57378,bixia1,1239610540,2022-09-07 16:20:12,"I see there are 8 commits, can you squash them into 1 commit?","I see there are 8 commits, can you squash them into 1 commit?",CONTRIBUTOR
57630,bhack,1239472638,2022-09-07 14:32:41,"> Yeah, TF does not compile on this type of VM in less than 6hrs, even if using a cache.
I really want to be sure about this and it is why I am asking here to collaborate on the same ""public page"". As this GitHub action is extremely simple to be runned also [locally as is](https://github.com/nektos/act) I would like to clearly collaborate to away all doubts concerning a possible reproducibility of the cache/cache misses with this Action before going to investigate other longer term programs.","TF does not compile on this type of VM in less than 6hrs, even if using a cache.",CONTRIBUTOR
57630,mihaimaruseac,1239459518,2022-09-07 14:22:08,"Yeah, TF does not compile on this type of VM in less than 6hrs, even if using a cache. On contrast, the [Linux kernel compiles on GH Actions in **slightly less over 2 hours without any cache**](https://github.com/whokilleddb/build-a-kernel-using-github-actions).
The build system of TF needs a lot of improvements, it should not be at least 3 times as complicated as the Linux kernel.
CC @learning-to-play","""TF does not compile on this type of VM in less than 6hrs, even if using a cache""",COLLABORATOR
53271,mihaimaruseac,1239400795,2022-09-07 13:36:13,"TF's APIs should be used via `tf.`. So `tf.keras.*` is legitimate, `keras.*` not so much.","TF's APIs should be used via tf.. So tf.keras.* is legitimate, keras.* not so much.",COLLABORATOR
56927,edwardyehuang,1239195213,2022-09-07 10:17:36,"@mohantym Can you please reopen this issue? I find I have to address it after upgrading to TensorFlow 2.10
Also, this issue should be with a ""bug"" label.","I find I have to address it after upgrading to TensorFlow 2.10 Also, this issue should be with a ""bug"" label.",CONTRIBUTOR
57193,jakeh-gc,1239113693,2022-09-07 9:03:28,"Any idea what I can do to resolve this?
> feedback/copybara  Google internal checks FAILED for runs with create time 2022-09-06T16:34:12.870440305Z.
I also need to resolve a merge conflict.",I also need to resolve a merge conflict.,CONTRIBUTOR
57405,Jerry-Ge,1238774333,2022-09-07 0:23:30,The ARM CI test failed. Seems due to flaky.,The ARM CI test failed. Seems due to flaky.,CONTRIBUTOR
39833,cantonios,1238667583,2022-09-06 21:27:04,This likely belongs outside of core TF. We are trying to minimize the API surface.,This likely belongs outside of core TF. We are trying to minimize the API surface.,CONTRIBUTOR
57494,joker-eph,1238619021,2022-09-06 20:30:06,Still not right apparently...,Still not right apparently...,CONTRIBUTOR
56761,jlebar,1237428889,2022-09-05 20:07:14,"To over-communicate, I'm waiting for responses to my review comments above.",I'm waiting for responses to my review comments above.,CONTRIBUTOR
56088,elfringham,1236756526,2022-09-05 9:26:49,"There was a new unit test a few days ago on master, //tensorflow/compiler/xla/runtime:custom_call_test that fails on Linaro CI. My testing also shows it failing on x86 so I am not sure why this is not picked up by the main CI testing. I could not find the run that made it pass, it went straight from not existing in the log to having a cached pass.
I have a PR to fix it that also needs an AARCH64 specific part https://github.com/tensorflow/tensorflow/pull/57586","I could not find the run that made it pass, it went straight from not existing in the log to having a cached pass.",CONTRIBUTOR
56088,mseth10,1236594396,2022-09-05 6:41:06,"CD on master branch failing on grappler remapper tests after this commit https://github.com/tensorflow/tensorflow/commit/9d4e950687b25f78fe30a74a8b40ba16f12c9fd1 , is this a known issue and is the fix WIP? @nitins17","""CD on master branch failing on grappler remapper tests after this commit""",CONTRIBUTOR
57494,EnricoMi,1236080677,2022-09-03 9:11:24,"It looks like your change in `tensorflow/BUILD` (702e7a4f)
```
""//tensorflow/stream_executor:stream_executor_headers"",
```
should read
```
""//tensorflow/stream_executor:stream"",
```
or
```
""//tensorflow/stream_executor:headers"",
```"," ""//tensorflow/stream_executor:stream_executor_headers"",  should read  ""//tensorflow/stream_executor:stream"",  or  ""//tensorflow/stream_executor:headers"", .",CONTRIBUTOR
56769,bhack,1235932831,2022-09-02 22:14:12,@cheshire Is there a margin to do something more here? If not can we at least improve the error message in the code? As I think that the PR to `known_issues.md` is a little bit too marginal for the avg developer with the current error string in the code.,Is there a margin to do something more here? If not can we at least improve the error message in the code? As I think that the PR to known_issues.md is a little bit too marginal for the avg developer with the current error string in the code.,CONTRIBUTOR
57087,nouiz,1234634890,2022-09-01 18:26:39,"This test shouldn't be executed on CPU. It execute, so it mean the build on CPU worked.
The BUILD file contains this:
```
tf_cc_test(
name = ""horizontal_loop_fusion_test"",
srcs = [""horizontal_loop_fusion_test.cc""],
tags = tf_cuda_tests_tags(),
```
`tf_cuda_tests_tags()` include the tag `require-gpu`.
So this should be an issue with the CPU builder that include this tests while it shouldn't.
Who can investigate that?","""So this should be an issue with the CPU builder that include this tests while it shouldn't.""",CONTRIBUTOR
52148,bhack,1234105682,2022-09-01 10:54:31,"> Hi, Could you please refer the comment here [#55639 (comment)](https://github.com/tensorflow/tensorflow/issues/55639#issuecomment-1233238644) which explains about the possible reasons for vectorized_map issues with different OPS.
I don't think it is related. It is the impl of the converter:
https://github.com/tensorflow/tensorflow/blob/d8ce9f9c301d021a69953134185ab728c1c248d3/tensorflow/python/ops/parallel_for/pfor.py#L2695-L2746
/cc @ishark",I don't think it is related. It is the impl of the converter.,CONTRIBUTOR
57056,olipinski,1233971416,2022-09-01 8:57:44,"Seems like almost all tests succeeded except for one, but I am not sure what to make of this. I don't know if this PR is what breaks this?
```
tensorflow/c/eager/c_api_test.cc:681: Failure
Expected equality of these values:
orig_ptr
Which is: 0x55ab336cfc00
TF_TensorData(t)
Which is: 0x7f5d90089f40
```","seems like almost all tests succeeded except for one, but I am not sure what to make of this. I don't know if this PR is what breaks this",CONTRIBUTOR
56305,cantonios,1233735596,2022-09-01 4:47:00,"Hmm... seems to be because you are overwriting `a`. Creating an intermediate `b` seems to work around the issue:
```
def test():
a = tf.random.normal([3, 5])
with tf.GradientTape(persistent=True) as tape:
tape.watch(a)
b = a * tf.constant([[1],[0],[0]], dtype=tf.float32)
cos_score = tf.keras.losses.cosine_similarity(b[:,None], b[None,:])
loss = tf.reduce_sum(cos_score)
grad = tape.gradient(loss, a)
print(grad)
```",hmm... seems to be because you are overwriting a. Creating an intermediate b seems to work around the issue,CONTRIBUTOR
56255,yishuangP,1233674970,2022-09-01 2:56:21,"Hi, I just verified that this build issue is resolved in build `0.0.1-nightly.20220831`. Unfortunately the fix won't be in 2.9.1, if you want to use 2.9.1, you can try building the framework using tensorflow/lite/ios/build_frameworks.sh. Besides, make sure you add this build flag `--define=tflite_with_xnnpack=false` when building the flex delegate framework.","""Unfortunately the fix won't be in 2.9.1""",CONTRIBUTOR
56826,kaixih,1233346577,2022-08-31 19:39:53,I see. I should have done less of the force-pushing. I did that mainly because I wanted to make the other PRs that depend on this one more easily to be rebased.,I should have done less of the force-pushing.,CONTRIBUTOR
56826,kaixih,1233336203,2022-08-31 19:27:38,"Sorry, I reopened it because it showed ""closed"" instead of ""merged"". And yes, I just noticed it was actually merged. @reedwm I got a bit confused. Closing this one for now.",I got a bit confused.,CONTRIBUTOR
57353,sachinprasadhs,1233214872,2022-08-31 17:23:00,"@Apprisco, Seems like it is the duplicate of the above linked issue. Could you please close this issue and track the progress in the above linked issue.","""Seems like it is the duplicate of the above linked issue.""",CONTRIBUTOR
57388,yishuangP,1233184121,2022-08-31 16:51:49,Hi this seems like a duplicate of https://github.com/tensorflow/tensorflow/issues/56255 and it should be fixed in the latest nightly build (version 0.0.1-nightly.20220831),"""It should be fixed in the latest nightly build (version 0.0.1-nightly.20220831).""",CONTRIBUTOR
57378,bixia1,1233114185,2022-08-31 15:46:40,Could you please squash the commits?,Could you please squash the commits?,CONTRIBUTOR
57087,reedwm,1232296902,2022-08-31 0:08:46,Ah I missed the fact that this PR relies on #57020. That is probably way this PR is failing.,I missed the fact that this PR relies on #57020.,MEMBER
53816,cantonios,1232268119,2022-08-30 23:20:26,"@bersbersbers yes, it was reverted.
The issue is that the change to always use the `EuclideanNorm` op from within `tf.norm` breaks many workflows that rely on the existing `norm` function, since it introduces a new op that isn't supported on all devices.
The only workable solution is that if you *need* the higher accuracy, then use `tf.math.reduce_euclidean_norm` directly. Internally, it uses a more stable way of computing the norm that less prone to overflow and precision issues.","""The issue is that the change to always use the EuclideanNorm op from within tf.norm breaks many workflows that rely on the existing norm function, since it introduces a new op that isn't supported on all devices.""",CONTRIBUTOR
57370,rsanthanam-amd,1232243743,2022-08-30 22:41:05,"i was trying to figure out a runtime switch, but the PR merged before i could change it.
rint can sometimes raise the inexact flag but nearbyint does not.
for AMDGPU/rocm, they are equivalent but i am not sure about cuda so i thought it prudent to preserve the original behavior.","i was trying to figure out a runtime switch, but the PR merged before i could change it.",CONTRIBUTOR
52045,szutenberg,1231398951,2022-08-30 9:20:24,"Hi @mohantym ,
Sure. Epsilon 1e-07 works but my point is that grappler's constant folding doesn't work correctly.","""My point is that grappler's constant folding doesn't work correctly.""",CONTRIBUTOR
54458,albertz,1231298007,2022-08-30 7:58:57,"@gadagashwini I did.
But @rainwoodman just made a comment on the `output_all_intermediates` usage. However, as long as this issue here is not fixed, I don't see the alternative? Even the error message suggests that:
> Try using tf.compat.v1.experimental.output_all_intermediates(True).
The other suggested workaround to rearrange the code is not possible in my case.
So, as you see, it is important for me that you fix this.","""I did. But @rainwoodman just made a comment on the output_all_intermediates usage. However, as long as this issue here is not fixed, I don't see the alternative?""",CONTRIBUTOR
57056,cantonios,1230827755,2022-08-29 20:29:27,"@olipinski sorry for the delay here.
Looks like you're missing a registration of `SplitVOpGPULaunch`. Register it in `split_lib_gpu.cu.cc`, and add a corresponding `extern template struct` declaration in `split_lib_gpu.h`.","""Looks like you're missing a registration of SplitVOpGPULaunch""",CONTRIBUTOR
57468,mihaimaruseac,1230453635,2022-08-29 15:11:12,"Has to be on your side, or the Pr imported manually.","""Has to be on your side, or the Pr imported manually.""",COLLABORATOR
57012,mihaimaruseac,1229331405,2022-08-27 23:35:53,Please don't make this type of PRs. They block automation. See https://github.com/tensorflow/community/issues/425 https://github.com/tensorflow/tensorflow/pull/56119 and https://github.com/tensorflow/tensorflow/pull/57478 for such a case.,"""They block automation""",COLLABORATOR
57461,talyz,1228211433,2022-08-26 8:28:52,"The cause was indeed that MSVC doesn't support `__attribute__((weak))` or similar hints. Never found out why it's only a problem when building a shared library, though.",MSVC doesn't support __attribute__((weak)) or similar hints.,CONTRIBUTOR
57387,gadagashwini,1228082607,2022-08-26 5:52:22,"Hi @trungnhat-incoder, `ERROR: D:/nghich/tensorflow/tensorflow/BUILD:1035:21: //tensorflow:libtensorflow_framework.so.2.11.0: no such attribute 'shared_lib_name' in 'cc_shared_library' rule`
Tensorflow master branch has `cc_shared_library rule`. Please take a look at this reference [link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl#L2971). Could you build Tf master branch and check. Thank you!","""Error: D:/nghich/tensorflow/tensorflow/BUILD:1035:21: //tensorflow:libtensorflow_framework.so.2.11.0: no such attribute 'shared_lib_name' in 'cc_shared_library' rule""",CONTRIBUTOR
56869,rthadur,1227856932,2022-08-25 23:35:00,@rahulbatra85 could you please resolve conflicts!,"""could you please resolve conflicts!""",CONTRIBUTOR
57116,bixia1,1227680164,2022-08-25 19:33:24,Can you fix the PR description and make sure that you rebase? I am still seeing the change that are done via https://github.com/tensorflow/tensorflow/pull/57139,I am still seeing the change that are done via https://github.com/tensorflow/tensorflow/pull/57139.,CONTRIBUTOR
56604,namrata-ibm,1227335554,2022-08-25 14:26:13,"@haozha111 @gbaned I couldn't see relevant failures in above check, could you please let me know if something is blocking the merge?","I couldn't see relevant failures in above check, could you please let me know if something is blocking the merge?",CONTRIBUTOR
57196,drubinstein,1227214822,2022-08-25 12:50:13,@sachinprasadhs I haven't checked it and didn't think it was worth the effort since all iPhones since 2013 have been 64-bit/arm64. I tried to get an armv7 build running on my M1 laptop but couldn't figure out a way to do it nor do I have a iPhone5 available to me unfortunately.,I haven't checked it and didn't think it was worth the effort since all iPhones since 2013 have been 64-bit/arm64.,CONTRIBUTOR
57052,sushreebarsa,1226616955,2022-08-25 0:06:33,"@elina-israyelyan I tried to replicate the issue using latest TF versions but colab is crashing [here](https://colab.research.google.com/gist/sushreebarsa/e2caffd985951f415afb29b2a8ae3c66/3d_unet_train.ipynb). Could you please find the attached gist and confirm the same?
Thank you!",I tried to replicate the issue using latest TF versions but colab is crashing,CONTRIBUTOR
56939,mihaimaruseac,1226302073,2022-08-24 20:44:37,There is no separate branch cut for RCs,No separate branch cut for RCs.,COLLABORATOR
56653,drivanov,1225991772,2022-08-24 16:58:47,"@bixia1: Please, don't merge that one into master. I have resolved the formal merge conflicts appeared after the merge of [PR#56942](https://github.com/tensorflow/tensorflow/pull/56942), but I need to fix something.","""I have resolved the formal merge conflicts appeared after the merge of [PR#56942](https://github.com/tensorflow/tensorflow/pull/56942), but I need to fix something.""",CONTRIBUTOR
51502,cantonios,1224322132,2022-08-23 16:37:57,This error message is consistent for all ops across TF when you try to run something on a dtype that isn't supported. I suggest we keep as is for reduce max. There are separate efforts for improving error messages across the entire TF codebase.,This error message is consistent for all ops across TF when you try to run something on a dtype that isn't supported.,CONTRIBUTOR
51502,mohantym,1223809463,2022-08-23 9:30:49,It is still replicating in 2.9 and nightly. Attached [gist](https://colab.sandbox.google.com/gist/mohantym/6858ff76dd5f7e35d943683b4d8dd2f1/untitled79.ipynb#scrollTo=rt5eLIgAfLEU) for reference.,It is still replicating in 2.9 and nightly.,CONTRIBUTOR
56255,yishuangP,1223579278,2022-08-23 5:50:42,"Hi really sorry for the late reply. I'm working on a fix. In the meantime, can you try tensorflow/lite/ios/build_frameworks.sh? You might need to add `--config=monolithic` when building TensorFlowLiteSelectTfOps_framework.","I'm working on a fix. In the meantime, can you try tensorflow/lite/ios/build_frameworks.sh? You might need to add --config=monolithic when building TensorFlowLiteSelectTfOps_framework.",CONTRIBUTOR
55746,JunyoungLim,1223201103,2022-08-22 22:09:57,"This AMD ROCm build failure and ""Waiting for status to be reported"" blockers seem to be ubiquitous across all the PRs at the moment. I will try it internally if it works.","This AMD ROCm build failure and ""Waiting for status to be reported"" blockers seem to be ubiquitous across all the PRs at the moment.",CONTRIBUTOR
57020,nouiz,1222522060,2022-08-22 15:30:32,@cheshire any idea why it isn't merged while the other PR got merged?,"""why it isn't merged while the other PR got merged?""",CONTRIBUTOR
57020,nouiz,1220985696,2022-08-19 18:36:57,"> Infra got into a broken inconsistent state somehow.
> > Unfortunately, our external compiler warnings don't have -Werror, so got this:
> > ```
> ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]
> module->RemoveEmbeddedComputation(old_computation);
> ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~
> ```
Fixed.","""Infra got into a broken inconsistent state somehow.""",CONTRIBUTOR
57161,Jerry-Ge,1220944239,2022-08-19 17:46:19,"Alright, it seems the `Py+CPP` passed here. While `Windows Bazel GPU  Internal CI build failed
` is failing everyone else which I believe it's not due to my fault?",while  Windows Bazel GPU  Internal CI build failed  is failing everyone else which I believe it's not due to my fault?,CONTRIBUTOR
57020,cheshire,1220932663,2022-08-19 17:40:08,"Infra got into a broken inconsistent state somehow.
Unfortunately, our external compiler warnings don't have -Werror, so got this:
```
ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]
module->RemoveEmbeddedComputation(old_computation);
^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~
```","""ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]""",MEMBER
57161,Jerry-Ge,1220813138,2022-08-19 15:34:10,"Taking a note here: the large constant values has to be written like `0x5{{.*}}` in the mlir test files, otherwise it causes the pattern match failures.","Taking a note here: the large constant values has to be written like 0x5.* in the mlir test files, otherwise it causes the pattern match failures.",CONTRIBUTOR
56745,sharadmv,1219947305,2022-08-18 20:49:24,I'm going to close this PR since after discussing offline w/ @hawkinsp we should avoid including CUDA in non-GPU builds.,I'm going to close this PR since after discussing offline w/ @hawkinsp we should avoid including CUDA in non-GPU builds.,CONTRIBUTOR
56942,drivanov,1219870810,2022-08-18 19:30:27,"@bixia1 : I fixed the missing virtual destructor issue by excluding the virtual `AllowedDataType` method.
Now the allowed data types are determined by the second input parameter of the constructor:
```
explicit OpConverterBase(const OpConverterParams* params,
const std::vector<DataType>& data_types =
{DataType::DT_FLOAT, DataType::DT_HALF})
```","""I fixed the missing virtual destructor issue by excluding the virtual AllowedDataType method""",CONTRIBUTOR
57197,gzmkl,1219735297,2022-08-18 17:12:17,this workaround is not needed,This workaround is not needed.,CONTRIBUTOR
55746,stewartmiles,1219707646,2022-08-18 16:41:09,"@JunyoungLim I'm now back to waiting on ""Code Check - Changed Files"" and ""Py+CPP Test Suite - Ubuntu CPU, Python 3.9"" to run but they look like they're hung. Is it possible those other two workflows depend upon AMD ROCm completing successfully?","I'm now back to waiting on ""Code Check - Changed Files"" and ""Py+CPP Test Suite - Ubuntu CPU, Python 3.9"" to run but they look like they're hung.",CONTRIBUTOR
55766,Yulv-git,1219645913,2022-08-18 15:39:51,"> Please fix more than just one single letter typos.
Sorry, I have nothing else to fix at the moment.",Please fix more than just one single letter typos.,CONTRIBUTOR
57161,Jerry-Ge,1219630143,2022-08-18 15:24:41,@jpienaar It fails the `ARM CI Extended / build (3.10) (pull_request) ` test where I saw a lot of FLAKY keywords there. Is there anyway to re-run those tests and could I have the permission to trigger the test runs here for efficiency? Tks,I saw a lot of FLAKY keywords there.,CONTRIBUTOR
57192,tilakrayal,1219105493,2022-08-18 6:54:42,"@sachinprasadhs,
I was able to reproduce the issue on tensorflow v2.9 and nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/775dc8ff4e3d8bb2b7c7dfbac78db7d3/untitled502.ipynb) and the issue was reproducible on all int datatype(**int8, int16, int32, int64)**, but was able to execute without any issue on other datatypes (**bfloat16, half, float32, float64, complex64, complex128)**","""I was able to reproduce the issue on tensorflow v2.9 and nightly""",CONTRIBUTOR
55746,JunyoungLim,1218748411,2022-08-18 0:21:57,"Hi Stewart, I just checked that AMD ROCm error has been happening in other PRs as well. Although we might need to fix this eventually, for now the ROCm check failure is not blocking the PR to be submitted.
To unblock this PR from the remaining checks that are stuck, can you try pushing an empty commit to this PR so that we can re-trigger the builds in CI?
https://stackoverflow.com/a/58190576/6096472","""I just checked that AMD ROCm error has been happening in other PRs as well.""",CONTRIBUTOR
56300,JXRiver,1218649702,2022-08-17 23:30:49,"`tape.batch_jacobian` and `tf.vectorized_map` (API of pfor) both don't support RaggedTensor at this moment. We are planning to support both of them. Before that happen, you may try to pad RaggedTensor to tensor.",tf.vectorized_map (API of pfor) both don't support RaggedTensor at this moment.,CONTRIBUTOR
56471,LostBenjamin,1218351901,2022-08-17 18:22:24,"Hi,
Our automatic tool detected this statically. We manually checked it and identified it as a bug, because `self.node_index` is a `dict` (for example, see [this line](https://github.com/tensorflow/tensorflow/blob/87c892726d1ca89d24d4e384482a1c6bbbb4e1aa/tensorflow/python/autograph/pyct/cfg.py#L349)) and thus the correct operator should be `in`. I am afraid that we cannot add a test as we are not familiar with the codebase.
Best,
Jingxuan","""We manually checked it and identified it as a bug, because self.node_index is a dict (for example, see [this line](https://github.com/tensorflow/tensorflow/blob/87c892726d1ca89d24d4e384482a1c6bbbb4e1aa/tensorflow/python/autograph/pyct/cfg.py#L349)) and thus the correct operator",CONTRIBUTOR
57079,mihaimaruseac,1218289448,2022-08-17 17:16:20,"Please sing CLA, please change title to something not ""update <file>"", please revert trivial incorrect changes.
See https://cbea.ms/git-commit/ for how to better title the PR and the commits.","""Please sing CLA, please change title to something not ""update file>"", please revert trivial incorrect changes.""",COLLABORATOR
56127,rdzhabarov,1217268588,2022-08-16 23:24:44,@mdfaijul the test was failing when i fixed that. So i expect it to fail with your change.,"""I expect it to fail with your change.""",CONTRIBUTOR
56127,rdzhabarov,1217253198,2022-08-16 22:58:43,"@mdfaijul can you check tensorflow/core/transforms/remapper/tests/contraction.mlir?
EDIT: I changed `-remapper` to `-tfg-remapper` but it's still failing.",-remapper to -tfg-remapper but it's still failing.,CONTRIBUTOR
57113,mseth10,1217223395,2022-08-16 22:13:27,"@nitins17 looks like it's missing CLA.. I checked and mine is present, looks like it's missing yours.
Also, after this PR is merged we need to enable more runners to share load. Do you have access to this page? https://github.com/tensorflow/tensorflow/settings/actions/runners/new?arch=arm64&os=linux","I checked and mine is present, looks like it's missing yours.",CONTRIBUTOR
57020,nouiz,1217016137,2022-08-16 18:47:04,"The windows build was still failed.
So I rebased by PR. Maybe it will allow a clean run.
@cheshire can you approve again?",The windows build was still failed.,CONTRIBUTOR
55608,Nyrio,1216651700,2022-08-16 13:38:16,"The pylint error is in the existing code, not in my branch. See #57178","The pylint error is in the existing code, not in my branch.",CONTRIBUTOR
57020,cheshire,1216403307,2022-08-16 9:40:31,"> `ERROR: T:/src/github/tensorflow/tensorflow/python/util/BUILD:189:27: Linking tensorflow/python/util/_pywrap_checkpoint_reader.so failed: (Exit 1120): link.exe failed: error executing command`
Windows build has failed. Let me try to rerun it.",Error: T:/src/github/tensorflow/tensorflow/python/util/BUILD:189:27: Linking tensorflow/python/util/_pywrap_checkpoint_reader.so failed: (Exit 1120): link.exe failed: error executing command,MEMBER
56604,namrata-ibm,1216403083,2022-08-16 9:40:19,"@gbaned @haozha111 This is yet to merge, could you please check once?","""This is yet to merge, could you please check once?""",CONTRIBUTOR
57161,eric-k256,1216005246,2022-08-16 0:14:21,"I'd expect test_softmax_qi8 in tensorflow/compiler/mlir/tosa/tests/tfl-to-tosa-pipeline.mlir to fail with this change. The test code itself is fine, but the CHECK lines will need to be updated to match what the new softmax code is generating. The non int8 softmax should continue to work as is.","""The test code itself is fine, but the CHECK lines will need to be updated to match what the new softmax code is generating.""",CONTRIBUTOR
56127,rdzhabarov,1215297300,2022-08-15 16:25:41,"@mdfaijul these are the failures
```
tensorflow/core/transforms/remapper/remapping_helper.h:26 ClangTidy: do not use unnamed namespaces in header files
tensorflow/core/transforms/remapper/remapping_helper.h:65 ClangTidy: unused function 'GetTfgOpName'
```
Let's fix these. If there will be yet another failures I'll take a look and will be fixing internally myself.",let's fix these. If there will be yet another failures I'll take a look and will be fixing internally myself..,CONTRIBUTOR
57013,mihaimaruseac,1214187656,2022-08-13 16:44:14,It did not show as merged here because copybara sync was broken. Fixed in big squash commit 49f97f135a2e1d5d22e60d2a80ec668d53f9708a,"""Copybara sync was broken""",COLLABORATOR
56485,bixia1,1213557911,2022-08-12 22:14:32,I am waiting for the author to address my comments.,I am waiting for the author to address my comments.,CONTRIBUTOR
56127,rdzhabarov,1213432795,2022-08-12 19:09:21,"@mdfaijul i see some trivial failures, could you fix those?
* pass.cc:44 ClangTidy: do not use namespace using-directives; use using-declarations instead
* pass.cc:121 ClangTidy: missing [#include] <string> for 'std::string'
* remapping_helper.h:63 ClangTidy: missing [#include] <string> for 'std::string'","""I see some trivial failures, could you fix those?""",CONTRIBUTOR
57115,reedwm,1213370992,2022-08-12 17:56:38,"An internal test failed with the infamous ""No algorithm worked!"" error. I'll try to get an example to reproduce.
/CC @jlebar FYI (not sure why you weren't automatically CCed)","""No algorithm worked!"" error",MEMBER
57130,mihaimaruseac,1213231897,2022-08-12 15:25:42,I think these translations should go into tensorflow.org website. We cannot create readme files for each language in the world and expect them to be in sync.,I think these translations should go into tensorflow.org website. We cannot create readme files for each language in the world and expect them to be in sync.,COLLABORATOR
40457,carlthome,1213046147,2022-08-12 12:11:10,"Hmm, I also got tripped up on this now in 2.9.1. I'm using tf.data and return minibatches on the format `Tuple[Dict[str, tf.Tensor], Dict[str, tf.Tensor]]` with (features, targets).
Since that format ""just works"" with losses, metrics and optimizers, I expected it to also work for class_weight, but alas I need an additional `tf.data.Dataset.map(lambda features, targets: (features[input_key], targets[output_key]))` to use class_weight. Bit of a gotcha IMO.","""I'm using tf.data and return minibatches on the format Tuple[Dict[str], tf.Tensor], Dict[str, tf.Tensor]]""",CONTRIBUTOR
57116,drivanov,1212701367,2022-08-12 3:45:35,"@bixia1: It is just a first draft. The test doesn't even work. In the meantime, it makes sense to review the refactoring of `tensorflow/python/compiler/tensorrt/test/BUILD`",The test doesn't even work.,CONTRIBUTOR
57092,lzr-official,1212637924,2022-08-12 1:13:01,"It's not only the `DeregisterGraph`, there are also other methods without `CallOptions`.
`GRPC_FAIL_FAST` has also been tried, but it doesn't always work for unknown reason.","""it doesn't always work for unknown reason.""",MEMBER
57092,haoyuz,1212535546,2022-08-11 21:55:52,"> The caller `DeregisterGraphAsync` doesn't have `CallOptions` in its parameters or in the request message.
Yes, but how could a deregister graph RPC fail with timeout? There is no blocking logic in handling `DeregisterGraph` RPCs, so I guess it's timing out because the remote worker is down? Should it set `GRPC_FAIL_FAST` in that case?",The caller DeregisterGraphAsync doesn't have CallOptions in its parameters or in the request message.,MEMBER
56127,rdzhabarov,1212489263,2022-08-11 21:02:39,"@penpornk could you help merging this? I do not see to have a permission to ""merge pull request"".","I do not see to have a permission to ""merge pull request"".",CONTRIBUTOR
57091,bixia1,1212149329,2022-08-11 15:30:50,"@DEKHTIARJonathan Can you double check your PR description, I think you meant
Behavior can be deactivated using export TF_TRT_EXPERIMENTAL_FEATURES= disable_logger_filtering","Can you double check your PR description, I think you meant Behavior can be deactivated using export TF_TRT_EXPERIMENTAL_FEATURES= disable_logger_filtering.",CONTRIBUTOR
56088,mseth10,1211856818,2022-08-11 11:21:38,"@elfringham I see that we have removed `pad_op_test_{cpu/gpu}` from the arm skip-test list. While these tests pass on master branch, they are [failing](https://github.com/tensorflow/tensorflow/actions/workflows/arm-ci.yml?query=branch%3Ar2.10) on r2.10 branch. Is there a fix on master branch that needs to be backported to r2.10 branch for these tests?","""I see that we have removed pad_op_test_cpu/gpu from the arm skip-test list. While these tests pass on master branch, they are [failing]""",CONTRIBUTOR
57056,cantonios,1211049505,2022-08-10 17:49:13,"@olipinski which test is failing in your local install?
For your PR, gradients are only supported for floating-point types, so you'll need to modify the `testGradientsAll` test to something like:
```
def testGradientsAll(self):
for dtype in _TEST_DTYPES:
# Gradients not supported for integer types.
if not dtype.is_integer:
self._testGradientsSimple(dtype)
self._testGradientsSimpleVariable(dtype)
```",# Gradients not supported for integer types,CONTRIBUTOR
57020,nouiz,1210759603,2022-08-10 14:30:40,"> No it was old, are you saying previously you saw PR being committed as multiple separate commits? I don't think that was ever the case?
Check this old PR of mine: https://github.com/tensorflow/tensorflow/pull/51558/
Its merge commit is: https://github.com/tensorflow/tensorflow/commit/f08d120097b070ad972948c7db3544aaaa6296a1
Then do: `git log --graph f08d120097b070ad972948c7db3544aaaa6296a1`
My individual commits are in the log","""I don't think that was ever the case""",CONTRIBUTOR
57010,maxhgerlach,1210520650,2022-08-10 11:06:39,"@gadagashwini, I see the same issue as OP, MacBook Pro 2021 with M1 Pro (aarch64), macOS 12.5.
TensorFlow is installed via the package `tensorflow-macos==2.9.2`, no `tensorflow-metal` needed.
The issue may very well be specific to arm64 (Apple Silicon). In that case you won't see it with an Intel CPU.","I see the same issue as OP, MacBook Pro 2021 with M1 Pro (aarch64), macOS 12.5.",CONTRIBUTOR
57020,cheshire,1210462248,2022-08-10 10:13:55,"> I fixed all the comments in their respective commits and updated each commit description.
When the system imports the PR, it squashes all the commits into one. It squashes the commit descriptions as well. This usually results in very confusing commits messages. Would be best to have a single commit, with a single descriptive message.","""This usually results in very confusing commits messages.""",MEMBER
57056,olipinski,1210341366,2022-08-10 8:32:08,"@cantonios I have added the split test, however it does not seem to pass, even on the release branch on my local install.","I have added the split test, however it does not seem to pass, even on the release branch on my local install.",CONTRIBUTOR
22770,ebrevdo,1210193276,2022-08-10 5:53:02,"(that said, the new tf2 code calls `tf.disable_v2_behavior()`, which drops you back in TF1-mode. I don't know the support story for this anymore...)","""That said, the new tf2 code calls tf.disable_v2_behavior(), which drops you back in TF1-mode.""",CONTRIBUTOR
56982,sachinprasadhs,1209947927,2022-08-09 22:18:24,"`tf.keras.losses.MSE` is doing the casting and it is converting to complex128 in both the cases, but in `grad_backward` when you calculate the gradient with respect to x, which is float32, it casts back to float32, that is the reason you're getting float32 in `grad_backward` and `complex128` in `res_forward`. To maintain the consistency, you can include manual casting to the `dtype` which you require.","""tf.keras.losses.MSE"" is doing the casting and it is converting to complex128 in both the cases, but in grad_backward when you calculate the gradient with respect to x, which is float32, it casts back to float32, that is the reason you're getting float32 in grad_backward and complex128 in res_forward. To maintain the consistency, you can include manual casting to the dtype which you require..""",CONTRIBUTOR
57044,penpornk,1209689295,2022-08-09 17:48:07,"~~It's failing again after everything is done though:~~
![Screen Shot 2022-08-09 at 10 47 01 AM](https://user-images.githubusercontent.com/38085909/183723926-cab57351-065c-43da-9e1a-eed0e4a1ff66.png)
~~I'll try testing it internally.~~
Edited to add: Sorry my page was stale. After refreshing, it also passed for me! :)",It's failing again after everything is done.,MEMBER
56628,cantonios,1209475737,2022-08-09 14:44:34,"@VictoriaGriffith yes, we're trying to decide what to do about that. Thanks for bringing this up. We have the same ""issue"" across all back-ends: that `gather_nd` supports a single index tuple, but `scatter_nd` does not. However, it's not currently a high priority for us, since there is a work-around.","""We have the same ""issue"" across all back-ends: that gather_nd supports a single index tuple, but scatter_nd does not.""",CONTRIBUTOR
56495,EnricoMi,1208248822,2022-08-08 15:04:49,I have removed `__exit__` from dispatcher and worker.,I have removed __exit__ from dispatcher and worker.,CONTRIBUTOR
56918,gbaned,1207965277,2022-08-08 10:49:51,"@chxin66 It still shows CLA is pending, can you please make sure to use same GitHub username and email-id associated with it.","""still shows CLA is pending""",CONTRIBUTOR
56715,penpornk,1206831771,2022-08-05 20:20:04,@snadampal We are having trouble importing this PR. Could you please help rebase it? Sorry for the inconvenience. :(,:(,MEMBER
56939,sachinprasadhs,1205564129,2022-08-04 17:33:58,"Since it is still not merged and the branch cut for 2.10rc0 happened 2 days ago, it will not make it to 2.10 release, but you can use tf-nightly till the next stable release is made","""It will not make it to 2.10 release, but you can use tf-nightly till the next stable release is made.""",CONTRIBUTOR
56785,olipinski,1204920257,2022-08-04 8:15:39,"Sorry, I meant if it is not desirable to add it for a single use case of reducing memory consumption. I guess the main reason to do that would be for any local testing of models, which is what my issue was about. I cannot run my models locally, as they are too big in memory usage. As for casting  I'm using Ray, so all the code where I would have to cast it back and forth is not really accessible, without some major changes.
Hope this is clearer!","""I cannot run my models locally, as they are too big in memory usage.""",CONTRIBUTOR
51799,mihaimaruseac,1204634204,2022-08-04 0:48:55,"@mohantym I just said I left TF, please don't reassign me!","I just said I left TF, please don't reassign me!",COLLABORATOR
56995,philipphack,1204561804,2022-08-03 22:53:45,"It seems that making `selected_algorithm` optional requires expanding the patterns into multiple lines. I previously attempted to use a regex which matches either the string or nothing, but apparently some regex operators are not recognized by FileCheck.","I previously attempted to use a regex which matches either the string or nothing, but apparently some regex operators are not recognized by FileCheck.",CONTRIBUTOR
56995,jlebar,1204547108,2022-08-03 22:27:10,"I'm saying that we do not need to check for the presence of selected_algorithm on pre-Ampere GPUs. Rather than selectively removing it, we can make it optional in the test, or otherwise make it so that the test doesn't check for it.
Removing it with a complex search-and-replace on a string is complicated and not something I can approve, though.","""Removing it with a complex search-and-replace on a string is complicated and not something I can approve, though.""",CONTRIBUTOR
56928,sachinprasadhs,1204292170,2022-08-03 17:54:35,Could you try the option to `setUseNnapiCpu` to `true` in the `NnApiDelegate.Options` object and let us know if it is still crashing.,Could you try the option to setUseNnapiCpu to true in the NnApiDelegate.Options object and let us know if it is still crashing.,CONTRIBUTOR
54390,abattery,1203602581,2022-08-03 7:45:32,Could you sync again? The current sync head wasn't green enough.,The current sync head wasn't green enough.,CONTRIBUTOR
56119,bhack,1203109136,2022-08-02 19:09:02,"@gbaned We could not merge this.
@mdanatg We lost the PR... What you want to do?",We could not merge this.,CONTRIBUTOR
56563,cantonios,1202863929,2022-08-02 15:44:48,"It shouldn't make a difference...
Looks like numpy is generating a `float64` tensor, so we have more precision bits to resolve the sum/mean. The tf version generates a `float32` tensor. Using `float32` with numpy will also lead to the same issue.",Using float32 with numpy will also lead to the same issue.,CONTRIBUTOR
45663,Flamefire,1202749371,2022-08-02 14:55:07,"Test has only moved here: https://github.com/tensorflow/tensorflow/blob/v2.9.1/tensorflow/python/kernel_tests/array_ops/batch_scatter_ops_test.py#L86
And code looks still the same: https://github.com/tensorflow/tensorflow/blob/v2.9.1/tensorflow/core/kernels/scatter_nd_op_gpu.cu.cc#L201
https://github.com/tensorflow/tensorflow/blob/v2.9.1/tensorflow/core/framework/register_types.h#L195-L196
So yes the issue persists.",Test has only moved here: https://github.com/tensorflow/tensorflow/blob/v2.9.1/tensorflow/python/kernel_tests/array_ops/batch_scatter_ops_test.py#L86 And code looks still the same: https://github.com/tensorflow/tensorflow/blob/v2.9.1/tensorflow/core/kernels/scatter_nd_op_gpu.cu.cc#,CONTRIBUTOR
56967,tilakrayal,1202102393,2022-08-02 7:08:24,"@7NoteDancing,
We can see the file was loaded and saved on the drive. As the warning mentioned, please try to compile the file manually and try to load. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/70eae78f09906322deefbabbfe30f88b/untitled473.ipynb).","""Please try to compile the file manually and try to load""",CONTRIBUTOR
56606,Flamefire,1202078152,2022-08-02 6:37:52,"Seems CI wants a slightly different formatting. Added that change (only), the other failures are unrelated","Added that change (only), the other failures are unrelated.",CONTRIBUTOR
56563,cantonios,1202025839,2022-08-02 5:14:02,"Ugh, so I dug into this, and it's because in the first case we're doing a naive linear reduction and eventually run out of precision when adding up elements.
In the second case, when you split the channels, we're doing a more accurate tree reduction.
This is an Eigen ""bug"" - not bug as in it's wrong, but bug as in it could be doing something more accurate.
@rmlarsen it's the `InnerMostDimPreserver` that seems to always do a linear traversal.","Ugh, so I dug into this, and it's because in the first case we're doing a naive linear reduction and eventually run out of precision when adding up elements. In the second case, when you split the channels, we're doing a more accurate tree reduction. This is an Eigen ""bug"" - not bug as in it's wrong, but bug as in it could be doing something more accurate.",CONTRIBUTOR
56575,cantonios,1201767977,2022-08-01 22:02:08,"Closing, as a work-around has been presented with no response.","Closing, as a work-around has been presented with no response.",CONTRIBUTOR
56949,nitins17,1200054392,2022-07-30 0:59:44,Closing this one as it is invalid. https://github.com/tensorflow/tensorflow/pull/56953 has the version changes for rc0,Closing this one as it is invalid.,MEMBER
56941,nfelt,1199992771,2022-07-29 22:27:13,"Sorry, I don't have the expertise in the profiler to review this code. Perhaps @yisitu can recommend someone on the profiler team to review?",I don't have the expertise in the profiler to review this code.,CONTRIBUTOR
56623,psunn,1199271387,2022-07-29 13:13:39,"TESTs would pass when compiling with gcc>=8.1, it fails on `gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0`
code snippet to stand out the issue: https://godbolt.org/z/6o76cTM3W","TESTs would pass when compiling with gcc>=8.1, it fails on gcc (Ubuntu 7.5.0-3ubuntu118.04) 7.5.0",CONTRIBUTOR
56393,Young768,1198731945,2022-07-28 23:50:22,"Hi @srujun This is the memory profile when running with batch_size=1024, using your method to capture.
![1024](https://user-images.githubusercontent.com/7083506/181655381-0d80f7ad-767d-4e49-86b8-9f0c78e73047.png)
Did you try running a larger ```--num_iter``` with batch_size=512. When I ran with batch_size=512., it hit OOM at around step 50-60. And with batch_size=1024., it hit OOM at around step 20-30.","""It hit OOM at around step 50-60.""",CONTRIBUTOR
56653,bixia1,1198434599,2022-07-28 17:27:45,"@drivanov can you take care of this, it say cla/google failure.","""cla/google failure""",CONTRIBUTOR
56673,snadampal,1198386273,2022-07-28 16:40:46,"I rebased yesterday, looks like master moved. Will rebase it again.","I rebased yesterday, looks like master moved. Will rebase it again.",CONTRIBUTOR
56935,hawkinsp,1198320873,2022-07-28 15:37:38,"@nouiz No, it wasn't. That symptom was memory corruption (missing synchronization?)","No, it wasn't. That symptom was memory corruption (missing synchronization?)",MEMBER
56931,psunn,1197901121,2022-07-28 9:33:22,TESTs failures seem caused by recent [update of FlatBuffer](https://github.com/tensorflow/tensorflow/commit/625a4045bc0728c0f3d1b63e05749201f8b401dd).,TESTs failures seem caused by recent [update of FlatBuffer](https://github.com/tensorflow/tensorflow/commit/625a4045bc0728c0f3d1b63e05749201f8b401dd).,CONTRIBUTOR
56747,sirakiin,1197563007,2022-07-28 1:58:14,"Hi @sdkdzq1 sorry for the delay.
For this model, the issue comes from the softmax node with a too large quantization range.
See hexagon [nnlib](https://source.codeaurora.org/quic/hexagon_nn/nnlib/tree/hexagon/ops/src/op_softmax_d32.c#n115) for the restrictions. Could you check your model quantization and see if it's expected?","""For this model, the issue comes from the softmax node with a too large quantization range.""",MEMBER
56903,mohantym,1197504764,2022-07-28 0:06:31,"Hi @NeuroRoboticTech !
Sorry for the late response. I could not find any issue with Python 3.7 and colab environment. It might be an environment issue ( CUDA 11.2 and CudNN 8.1 are tested configs for 2.8.2). Could you share the complete error log to help expedite the issue.","""I could not find any issue with Python 3.7 and colab environment.""",CONTRIBUTOR
56572,reedwm,1197405706,2022-07-27 21:49:13,"This is failing an internal test with
```
error: non-constant-expression cannot be narrowed from type 'dnn::ActivationMode' to 'double' in initializer list [-Wc++11-narrowing]
```
I can get more context if you want.",error: non-constant-expression cannot be narrowed from type 'dnn::ActivationMode' to 'double' in initializer list [-Wc++11-narrowing],MEMBER
44447,kun-lu20,1197374752,2022-07-27 21:13:35,"On TensorFlow v2.9.1, test case `//tensorflow/compiler/mlir/xla/tests/translate:while.hlotxt.test` passes with the Bazel build arg `--copt=-O -c opt` (optimized binary) but still fails with fastbuild binary. Looks like the cause is lacking certain operations in LLVM SystemZ backend.","On TensorFlow v2.9.1, test case //tensorflow/compiler/mlir/xla/tests/translate:while.hlotxt.test passes with the Bazel build arg --copt=-O -c opt (optimized binary) but still fails with fastbuild binary. Looks like the cause is lacking certain operations in LLVM SystemZ backend.",CONTRIBUTOR
56088,nitins17,1196981544,2022-07-27 16:29:12,"Just a FYI, the Arm CD workflow [failed](https://github.com/tensorflow/tensorflow/runs/7535892666?check_suite_focus=true) yesterday because of an LLVM integrate. I believe https://github.com/tensorflow/tensorflow/commit/47c640a961874f644cd071752835c7b792450bb8 should fix it.","""I believe https://github.com/tensorflow/tensorflow/commit/47c640a961874f644cd071752835c7b792450bb8 should fix it.""",MEMBER
29198,mohantym,1196764007,2022-07-27 13:28:25,"I could replicate this issue in 2.8, 2.9 and nightly but was getting a different result when I used tf.shape(q) instead of q.shape. Attached[ gist](https://colab.sandbox.google.com/gist/mohantym/05441f9a8d07e06d17a7752fbdfedcdc/git_29198.ipynb#scrollTo=maKEhgFf0zVG) for reference.","""was getting a different result when I used tf.shape(q) instead of q.shape""",CONTRIBUTOR
56904,zaccharieramzi,1196436020,2022-07-27 8:44:37,"After further investigation, I think that the memory leak actually comes from the use of `shuffle`, because I still have it even with a version of `RandomResizedCrop` not relying on `crop_and_resize`. The `shuffle` memory leak is explained here : https://github.com/tensorflow/tensorflow/issues/44176#issuecomment-783768033
However, I still think that this warning is a problem.","After further investigation, I think that the memory leak actually comes from the use of shuffle, because I still have it even with a version of RandomResizedCrop not relying on crop_and_resize.",CONTRIBUTOR
56791,gaurides,1196279241,2022-07-27 5:24:18,"> Metaoptimizer test failed
I have root caused the issue.. need some tests before i upstream. So, I can only do that tomorrow.","I have root caused the issue.. need some tests before i upstream. So, I can only do that tomorrow.",CONTRIBUTOR
56890,sachinprasadhs,1196037207,2022-07-26 22:19:05,"@shkarupa-alex , It can be due to the large input given, could you please confirm if you face the same issue when the small input is given.","""It can be due to the large input given, could you please confirm if you face the same issue when the small input is given.""",CONTRIBUTOR
55645,PatriceVignola,1196027905,2022-07-26 22:05:26,"I originally moved the files because the ""Android Demo"" check couldn't find them. This target was somehow depending on `kernels_experimental`, which in turn needs those files. Are there any reasons that Android build should depend on `kernels_experimental` at all, assuming this library's purpose is to be used by pluggable devices?
I'm going to try something else locally to fix this ""Android Demo"" build error, but I'm flying relatively blind if I can't repro the other failures locally :/","""Android Demo"" check couldn't find them.",CONTRIBUTOR
56900,nitins17,1195874302,2022-07-26 19:07:58,Closing this one as #56901 has disabled the `quantization_ops_test`,Closing this one as #56901 has disabled the quantization_ops_test.,MEMBER
56898,sachinprasadhs,1195815710,2022-07-26 18:08:38,"Hi, Could you please try the below select TF options which will allow the failed ops to fallback to TF Ops and let us know if that works, since the Rsqrt is not supported in the mentioned select ops in your code. ```
converter.target_spec.supported_ops = [
tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
```","""Rsqrt is not supported in the mentioned select ops in your code.""",CONTRIBUTOR
56088,mseth10,1195711555,2022-07-26 16:32:04,"> It looks like #56828 should fix this but we need to wait for it to be reviewed and merged so let's skip the test for now. We can remove it from the skip list after #56828 is merged.
@nitins17 removing the test from skip-list and adding a new one as it's been failiing because of a recent update. Please help merge this PR https://github.com/tensorflow/tensorflow/pull/56901","""It looks like #56828 should fix this but we need to wait for it to be reviewed and merged so let's skip the test for now.""",CONTRIBUTOR
50304,sogartar,1195413070,2022-07-26 12:22:25,"> I am trying to add linting pre-commit in our developer container at #48371
> > If you use master you can already run the plain `pylint` command.
> > I think that you can also use `yapf` for autoformatting if you set the style as python Google style.
I want to resurrect this issue because
```
yapf --style google ...
```
is not the correct solution. For example the default indentation is 4 spaces where Tensorflow uses 2 spaces.",I am trying to add linting pre-commit in our developer container at #48371,CONTRIBUTOR
55645,PatriceVignola,1194918547,2022-07-26 2:29:45,"@mihaimaruseac I'll remove `tensor_list.cc`, but obviously I won't know if it breaks something else by just running the OSS tests or fixes anything. What command can I run that could reproduce the errors above? I apologize for all the back and forth, I'm running the test suites that I'm aware of locally but it seems like there are a lot more tests than what the CI shows publicly.","I'll remove tensor_list.cc, but obviously I won't know if it breaks something else by just running the OSS tests or fixes anything. What command can I run that could reproduce the errors above?",CONTRIBUTOR
56088,mihaimaruseac,1194644658,2022-07-25 21:20:52,"I think it's a little bit more, but that issue should be deduplicated here","I think it's a little bit more, but that issue should be deduplicated here.",COLLABORATOR
56785,cantonios,1194594596,2022-07-25 20:34:15,"@olipinski why do you need `int8` support?
We can easily add it, but doing so increases TF's already massive binary size. For this reason, we try to keep registrations minimal. Will casting to one of the supported types work for you, or are you actually dealing with very large datasets that `int8` is actually required?","""why do you need int8 support?""",CONTRIBUTOR
56887,elfringham,1193947620,2022-07-25 11:48:03,"@d0k on AARCH64 the availability of _Float16 depends on if the target CPU includes the support or not. The documentation is sadly lacking in explaining this. So the default compile settings of '--copt=-march=armv8-a' result in the above failure. But if targetting a CPU with the needed support by '--copt=-march=armv8-a+fp16' then _Float16 is available and the code would compile, but then generate an illegal instruction trap if run on a CPU without the support.",The documentation is sadly lacking in explaining this.,CONTRIBUTOR
56482,roserg,1193843194,2022-07-25 10:05:14,"Your unrolled model should be executed fully on gpu after this commit: https://github.com/tensorflow/tensorflow/commit/6bca0adc394a29eed9cd6e0a7d2143c46ba8e5b4
But this model still much slower on gpu, comparing to cpu
Not all models faster on gpu, gpu needs very big models to be efficient and faster than cpu","""But this model still much slower on gpu, comparing to cpu""",CONTRIBUTOR
56167,SeeForTwo,1192867076,2022-07-22 19:18:01,"Sorry for the inconvenience, but could please update the commit description to reflect what it currently is, i.e. this is now a fix for TF_TString_GetDataPointer method for the OffsetType, and fix for TEST(TF_CTStringTest, OffsetType)?","""This is now a fix for TF_TString_GetDataPointer method for the OffsetType, and fix for TEST(TF_CTStringTest, OffsetType)?""",MEMBER
55645,mihaimaruseac,1192713882,2022-07-22 15:53:03,"Please make sure all header inclusions are also paired with the corresponding BUILD rules. Internal tests are now failing because of
```
.../tensorflow/c/kernels_experimental.cc:30:10: error: module //.../tensorflow/c:kernels_experimental does not depend on a module exporting '.../tensorflow/core/kernels/tensor_list.h'; to fix run:
build_cleaner //.../tensorflow/c:kernels_experimental
#include "".../tensorflow/core/kernels/tensor_list.h""
```"," .../tensorflow/c/kernels_experimental.cc:30:10: error: module //.../tensorflow/c:kernels_experimental does not depend on a module exporting '.../tensorflow/core/kernels/tensor_list.h'; to fix run: build_cleaner //.../tensorflow/c:kernels_experimental #include "".../tensorflow/core/kernels/tensor_list",COLLABORATOR
56847,edwardyehuang,1192667389,2022-07-22 15:05:28,"Note that, I also tried
```
export PYTHONHASHSEED=0
```
and
```
tf.config.threading.set_intra_op_parallelism_threads(1)
tf.config.threading.set_inter_op_parallelism_threads(1)
```
However, none of them worked. I think some ops in TensorFlow are nondeterministic on TPU.","""I think some ops in TensorFlow are nondeterministic on TPU.""",CONTRIBUTOR
56835,roserg,1192223899,2022-07-22 6:17:58,"Metal delegate supports only ""static"" batch. It means that batch size must be specified in the model. Dynamic batch(different batch sizes) with the same model not supported.","""Metal delegate supports only ""static"" batch. It means that batch size must be specified in the model. Dynamic batch(different batch sizes) with the same model not supported.""",CONTRIBUTOR
55645,mihaimaruseac,1191983011,2022-07-21 22:08:19,"It needs to be imported again, sorry, I didn't get a chance to look at it before (since I'm no longer in TF, reviewing TF PRs is a lower priority for me)","I'm no longer in TF, reviewing TF PRs is a lower priority for me.",COLLABORATOR
56714,elfringham,1191321764,2022-07-21 10:32:32,@cantonios I'm guessing that the rollback was because of the pylint error? If so please merge #56730 first then resubmit this patch. The pylint error was pre-existing in one of the files touched by this change and was not caused by this change.,The pylint error was pre-existing in one of the files touched by this change and was not caused by this change.,CONTRIBUTOR
56482,roserg,1191266583,2022-07-21 9:35:51,"Hi, can you share .tflite model? Empty/random weights are fine. If not, at least part of the tflite model with unpack reshape that do not supported on gpu.
Not all models faster on GPU delegate, especially when used cpu->gpu->cpu pipeline.","Not all models faster on GPU delegate, especially when used cpu->gpu->cpu pipeline.",CONTRIBUTOR
56832,abattery,1190794246,2022-07-20 21:48:57,"I suspect even for the TFLite floating point conversion, these kind of small numerical differences can be found.
There are many reasons to introduce such behaviors due to 1) conversion logic change 2) kernel logic change and so on. It is really impossible to guarantee the numerical exactness. For example, the numerical exactness is hard to achieve in (a+b) * c != a*c + b*c.
Those differences seem minor and looks like the behavior is working as intended.","I suspect even for the TFLite floating point conversion, these kind of small numerical differences can be found.",CONTRIBUTOR
56302,nSircombe,1190690146,2022-07-20 19:50:33,"I'm getting the same error in ManyLinux2014 builds with Python 3.10, but not 3.8.","I'm getting the same error in ManyLinux2014 builds with Python 3.10, but not 3.8.",CONTRIBUTOR
56464,mihaimaruseac,1190399727,2022-07-20 15:02:28,"This needs to be handled internally, after @learning-to-play finishes the migration to `cc_shared_library` and migrates protobuf to newer releases, both in these files and in `workspace?.bzl` ones.","This needs to be handled internally, after @learning-to-play finishes the migration to cc_shared_library and migrates protobuf to newer releases, both in these files and in workspace?.bzl ones.",COLLABORATOR
56815,chunduriv,1190055159,2022-07-20 9:38:05,"@celestinoxp,
>tensorflow 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.2 which is incompatible.
Protobuf 4.0 is incompatible with TF. For more details please take a look at https://github.com/tensorflow/tensorflow/issues/56815#issuecomment-1189277185 by @mihaimaruseac.
Thank you.","""Protobuf 4.0 is incompatible with TF""",CONTRIBUTOR
56088,nitins17,1189679338,2022-07-20 0:37:27,"@mseth10, @elfringham, @nSircombe Looks like there were no uploads to the `tf-nightly-cpu-aws` project on PyPI for the last few days as the Arm CD workflow has been [failing](https://github.com/tensorflow/tensorflow/actions/workflows/arm-cd.yml) due to a test failure (`tensorflow/python/kernel_tests/nn_ops:pooling_ops_3d_test_cpu`). fyi: @yarri-oss, @rishikasinha-tf","""Looks like there were no uploads to the tf-nightly-cpu-aws project on PyPI for the last few days as the Arm CD workflow has been [failing]""",MEMBER
56815,mihaimaruseac,1189277185,2022-07-19 16:14:26,"For future: please use descriptive titles.
Regarding this issue: there is a big ABI/API breakage with protobuf 4 and TF cannot upgrade due to segfaults in Windows caused by ODR violations. @learning-to-play is working on `cc_shared_library` support and migration past these to then upgrade protobuf","""There is a big ABI/API breakage with protobuf 4 and TF cannot upgrade due to segfaults in Windows caused by ODR violations.""",COLLABORATOR
55766,mihaimaruseac,1189272297,2022-07-19 16:12:28,"Brian does not work in TF for more than a year, removing assignment.","""Brian does not work in TF for more than a year, removing assignment.""",COLLABORATOR
55766,mihaimaruseac,1189271389,2022-07-19 16:12:07,Please fix more than just one single letter typos.,"""Please fix more than just one single letter typos.""",COLLABORATOR
55935,mihaimaruseac,1189267606,2022-07-19 16:10:39,"Please use better commit messages and PR titles, not ""update < file>"".","""update  file>""",COLLABORATOR
56804,mohantym,1187020234,2022-07-18 10:17:41,"Hi @derrkater !
Could you put select ops syntax in TFLite conversion and let us know whether it resolves.
```
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.target_spec.supported_ops = [
tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)
```","""put select ops syntax in TFLite conversion and let us know whether it resolves""",CONTRIBUTOR
55534,elfringham,1187005524,2022-07-18 10:02:56,"@snadampal You need to address the buildifier failure on your PR. You have an added line that is not in alphabetical order in the list. This is why the required presubmit test ""Code Check - Changed Files"" is failing. See #56673 for the details on that.","""You have an added line that is not in alphabetical order in the list.""",CONTRIBUTOR
52913,johan-gras,1186586515,2022-07-17 18:35:44,"@gbaned unfortunately I am not working at Arm anymore, so I am unable to modify this PR. Someone from Arm would have to take a look at it. Maybe @Tessil?","""I am not working at Arm anymore, so I am unable to modify this PR.""",CONTRIBUTOR
56789,keith,1185838101,2022-07-15 19:21:35,"Added the patch for now, this actually broke when https://github.com/tensorflow/tensorflow/pull/56446 merged, which is why I didn't initially have this issue","Added the patch for now, this actually broke when https://github.com/tensorflow/tensorflow/pull/56446 merged, which is why I didn't initially have this issue.",CONTRIBUTOR
56787,keith,1185672582,2022-07-15 15:49:07,AMD ROCm failure appears unrelated,AMD ROCm failure appears unrelated.,CONTRIBUTOR
56707,PatriceVignola,1185635202,2022-07-15 15:05:34,"@gbaned @rohan100jain Could we get more eyes on this? This is an issue many users have faced since we released the preview of our plugin since most people use the `tensorflow` package, not `tensorflow-cpu`.","This is an issue many users have faced since we released the preview of our plugin since most people use the tensorflow package, not tensorflow-cpu.",CONTRIBUTOR
56769,bhack,1184408781,2022-07-14 12:49:35,As we could see also on this last ticket we will have a more generic issue with the current Keras-cv logic and XLA `jit_compile` /cc @qlzh727,As we could see also on this last ticket we will have a more generic issue with the current Keras-cv logic and XLA jit_compile,CONTRIBUTOR
55645,mihaimaruseac,1183821041,2022-07-14 0:42:16,"Finally,
```
comparison of different enumeration types ('tensorflow::DataType' and 'TF_DataType') [-Werror,-Wenum-compare]
if (cc_a.dtype() == TF_VARIANT) {
~~~~~~~~~~~~ ^ ~~~~~~~~~~
1 error generated.
```","Finally,  comparison of different enumeration types ('tensorflow::DataType' and 'TF_DataType') [-Werror,-Wenum-compare] if (cc_a.dtype() == TF_VARIANT)    1 error generated. ",COLLABORATOR
56664,rsanthanam-amd,1183568404,2022-07-13 18:55:04,"I finally found the correct problem: //tensorflow/core/grappler/optimizers:generic_layout_optimizer.
In this case, I have to make a change to the ROCm TF fork, so I will close this PR without merging.
Sorry for the confusion.","I finally found the correct problem: //tensorflow/core/grappler/optimizers:generic_layout_optimizer. In this case, I have to make a change to the ROCm TF fork, so I will close this PR without merging. Sorry for the confusion.",CONTRIBUTOR
56687,sachinprasadhs,1183478453,2022-07-13 17:13:05,"May be it is because of the flag mentioned here.
```
# Flag, indicating that test should be run only with partial_pivoting=True
FLAG_REQUIRES_PIVOTING = ""FLAG_REQUIRES_PIVOT""
```","# Flag, indicating that test should be run only with partial_pivoting=True",CONTRIBUTOR
56119,bhack,1182553641,2022-07-12 22:24:22,"/cc @learning-to-play Just a side note. Formally I am not changing an API here as in the signature. I am just changing the behaviour of an API with autograph to be aligned with the expected output we have in python.
Then I could not help to fix failures that are internal only as contributor cause I have no visibility.","""I could not help to fix failures that are internal only as contributor cause I have no visibility.""",CONTRIBUTOR
54299,lgeiger,1182021452,2022-07-12 17:06:04,"@aaudiber To check whether this could somehow be a problem with Python's GC not cleaning up `data_iterator`, I also tested
```python
for epoch in range(3):
for batch in iter(dataset):
pass
```
and
```python
for epoch in range(3):
for batch in dataset:
pass
```
but both snippets show the same increasing memory usage as the original reproduction (tested on `f-nightly==2.10.0.dev20220711`).","""I also tested python for epoch in range(3): for batch in iter(dataset): pass  and python for epoch in range(3): for batch in dataset: pass  but both snippets show the same increasing memory usage as the original reproduction (tested on f-nightly==2.10.0.dev20220711).""",CONTRIBUTOR
54299,lgeiger,1181881473,2022-07-12 15:09:22,"@aaudiber were you able to reproduce this issue on your end?
I am still seeing the same behaviour in TF 2.9.1 which makes proper shuffling of datasets that do not fit into memory pretty much impossible together with Keras `model.fit`.
Let me know if I can provide you with more information about this problem.",I am still seeing the same behaviour in TF 2.9.1 which makes proper shuffling of datasets that do not fit into memory pretty much impossible together with Keras model.fit.,CONTRIBUTOR
56731,cantonios,1180892766,2022-07-11 21:34:22,"@bhavani-subramanian can you try hacking this at the top of `tensorflow/core/kernels/eigen_spatial_convolutions.h`, before including Eigen's `Tensor` header?
```
// Hack to disable breaking AVX512 special GemmKernel.
// There is a conflicting specialization there causing build breakages.
#define GEMM_KERNEL_H disabled
```
I *think* this should solve it temporarily to unblock you, but I'm having trouble actually building this on our end.","I *think* this should solve it temporarily to unblock you, but I'm having trouble actually building this on our end.",CONTRIBUTOR
56497,bixia1,1180578064,2022-07-11 15:49:09,waiting for author to fix the test failure.,waiting for author to fix the test failure.,CONTRIBUTOR
54479,bhack,1179579586,2022-07-09 17:39:36,"Some of these `Dynamic reshape is not implemented` are:
https://github.com/tensorflow/tensorflow/blob/262777e9f9304c7df6b694934af819c820954ef5/tensorflow/compiler/xla/literal.cc#L963-L965",Dynamic reshape is not implemented,CONTRIBUTOR
56714,elfringham,1179501204,2022-07-09 8:09:28,"Sorry, found some new failures that I need to fix.",Found some new failures that I need to fix.,CONTRIBUTOR
55779,rahulbatra85,1179241017,2022-07-08 18:03:00,"@ezhulenev Hi Eugene,
This is passing all the CIs except ARM CI which is failing a conv_3d_ops. The failure looks to be completely unrelated to this commit as it is grappler/ROCm specific and adds some dependencies to the BUILD file. Also, for Windows Bazel GPU I can't see any details. Seems to me the CIs might be broken.
Given this can we merge this commit?",The failure looks to be completely unrelated to this commit as it is grappler/ROCm specific and adds some dependencies to the BUILD file.,CONTRIBUTOR
56609,rsanthanam-amd,1178889609,2022-07-08 11:41:44,Our Ubuntu-sanity test failed (the bazel query subtest specifically) if I didn't remove those dependencies. The conv_buffer_1x1 stuff was removed in some commit but it appears that commit failed to remove those [stale] refences in tf-lite.,"""It appears that commit failed to remove those [stale] refences in tf-lite.""",CONTRIBUTOR
56655,sachinprasadhs,1178287302,2022-07-07 22:14:32,"This is because currently there is no Fill kernel for `GPU` for `dtype` bool, even though the ctx manager has both `CPU` and `GPU`, it uses `CPU` and only in case if it is required it will be copied to `GPU`.
Below comment and code explains the case where it is forced to run `with ops.device(""/device:CPU:0""):`.
https://github.com/tensorflow/tensorflow/blob/a113dd02168a01aba7aea99d8266630233d9269c/tensorflow/python/framework/constant_op.py#L318-L323","This is because currently there is no Fill kernel for GPU for dtype bool, even though the ctx manager has both CPU and GPU, it uses CPU and only in case if it is required it will be copied to GPU. Below comment and code explains the case where it is forced to run with ops.device(""/device:CPU:0""):.",CONTRIBUTOR
26842,cantonios,1178186341,2022-07-07 20:26:44,"Yes, this is still a problem - and it's an Eigen issue: [#2491](https://gitlab.com/libeigen/eigen/-/issues/2491)","Yes, this is still a problem - and it's an Eigen issue: [#2491](https://gitlab.com/libeigen/eigen/-/issues/2491).",CONTRIBUTOR
9234,cantonios,1178184891,2022-07-07 20:25:23,"> I started having this error ""/tensorflow/core/kernels/linalg/svd_op_impl.h:110] Eigen::BDCSVD failed with error code "" on Intel CPUs, in 2.7 and in 2.9.1 as well
That would be a different issue. Perhaps this one: #26842","I started having this error ""/tensorflow/core/kernels/linalg/svd_op_impl.h:110] Eigen::BDCSVD failed with error code "" on Intel CPUs, in 2.7 and in 2.9.1 as well",CONTRIBUTOR
56679,mihaimaruseac,1178029032,2022-07-07 18:15:03,"Yes, but this discussion confused @gadagashwini into thinking OP wanted to install via Conda, when Conda is just an orthogonal issue here.
To debug this further, we need the output of the two commands mentioned in https://github.com/tensorflow/tensorflow/issues/56679#issuecomment-1177881334:
```
pip show tensorflow
python -m pip show tensorflow
```","This discussion confused @gadagashwini into thinking OP wanted to install via Conda, when Conda is just an orthogonal issue here. To debug this further, we need the output of the two commands mentioned in https://github.com/tensorflow/tensorflow/issues/56679#issuecomment-1177881334:  pip show tensorflow python -m pip show tensorflow .",COLLABORATOR
56679,bhack,1178023873,2022-07-07 18:10:50,"Yes installing in a conda env doesn't mean install with coda.
I was always talking about preparing a fresh env with conda as it is our official doc now and not installing with conda.
Also I think that the first command/section is very confusing as It install something that It is used only in the metioned step-by-step section.",I was always talking about preparing a fresh env with conda as it is our official doc now and not installing with conda.,CONTRIBUTOR
56679,bhack,1177946474,2022-07-07 17:09:02,"Not anymore (I meant to be sure you have a fresh env).
https://www.tensorflow.org/install/pip
For CPU It is only suggested to skip step 3 (Linux) and 4 (win).",Not anymore (I meant to be sure you have a fresh env),CONTRIBUTOR
56679,mihaimaruseac,1177932547,2022-07-07 16:58:31,No. Conda is used only to install the GPU libs (as that's easier than using the operating system's utilities). But TF official releases and guides are still pip based.,No. Conda is used only to install the GPU libs (as that's easier than using the operating system's utilities). But TF official releases and guides are still pip based.,COLLABORATOR
56698,guanxinq,1177667566,2022-07-06 15:02:47,"@pindinagesh, could you redirect this question to TF repo? It looks more like a TF issue.","""It looks more like a TF issue.""",MEMBER
56609,rsanthanam-amd,1177624151,2022-07-07 13:34:30,This PR is failing to merge because of CI failures but I am unable to ascertain what the problem is. Can someone take a look?,unable to ascertain what the problem is.,CONTRIBUTOR
56591,Flamefire,1177202308,2022-07-07 7:39:54,"Is this good to go or is there anything I need to do? The CI seems to be buggy (not caused by this PR) as all I see is
> tensorflow/tools/ci_build/pylintrc:1:0: E0015: Unrecognized option found: profile, cache-size, files-output, comment, zope, required-attributes, bad-functions, disable-report, no-space-check, ignore-iface-methods, short-func-length, deprecated-members, ignore-exceptions, copyright (unrecognized-option)","The CI seems to be buggy (not caused by this PR) as all I see is > tensorflow/tools/ci_build/pylintrc:1:0: E0015: Unrecognized option found: profile, cache-size, files-output, comment, zope, required-attributes, bad-functions, disable-report, no-space-check, ignore-iface-methods, short-func-length, deprecated-members, ignore-exceptions, copyright",CONTRIBUTOR
56660,sachinprasadhs,1176433673,2022-07-06 16:31:52,Could you try again saving the model with `model.save` instead of `tf.saved_model.save` and follow the other steps as usual.,Could you try again saving the model with model.save instead of tf.saved_model.save and follow the other steps as usual.,CONTRIBUTOR
56664,hawkinsp,1176193381,2022-07-06 13:00:21,"@chr1sj0nes is the author of that change.
Something looks off here.
a) XLA shouldn't be depending on anything much in tensorflow/core/kernels.
b) in general targets should depend directly on what they need. Something is usually off if a top-level target is having dependencies added because an underlying target is missing them.",Something looks off here.,MEMBER
56674,tilakrayal,1174901316,2022-07-05 10:32:16,"@sachinprasadhs,
While I was trying to reproduce the provided code, the execution was taking longer than expected. Kindly find the [gist](https://colab.research.google.com/gist/tilakrayal/c6b64b5740e18886f8f9edc52417dc33/2-8dog_classifier.ipynb) of it [here](https://colab.research.google.com/gist/tilakrayal/dcfb4b657c010a678648c3d744498fc5/2-9dog_classifier.ipynb).","""The execution was taking longer than expected.""",CONTRIBUTOR
56535,benkli01,1174835928,2022-07-05 9:28:09,"Hi @abattery. Thanks for looking into this. The issue here is that there is no ""strict mode"". We can not rely on the converter to produce a TFLite graph that only contains int8 operations (which might be required by the hardware).","""We can not rely on the converter to produce a TFLite graph that only contains int8 operations (which might be required by the hardware"")""",CONTRIBUTOR
56535,rino20,1174818719,2022-07-05 9:12:29,"Hi, @abattery It seems you are already assigned to this issue. Could you take a look at this issue? - This is a blocker of ARM side.",- This is a blocker of ARM side.,CONTRIBUTOR
56673,elfringham,1174766638,2022-07-05 8:24:29,"@gadagashwini Unfortunately the colab gists referenced above by @mohantym are not relevant, you cannot build with --config=mkl_aarch64 on x86.
Full log from an AARCH64 build is here
https://ci.linaro.org/job/ldcg-python-manylinux-tensorflow-onednn-nightly/108/consoleText","""Unfortunately the colab gists referenced above by @mohantym are not relevant, you cannot build with --config=mkl_aarch64 on x86.""",CONTRIBUTOR
56585,gadagashwini,1173450475,2022-07-04 7:26:40,"Hi @wangzy0327,
Delete the tensorflow repo, re-clone and try again, it starts compiling:
```
bazel clean --expunge
rm -rf /packages/tensorflow
!git clone tensorflow
cd /packages/tensorflow
./configure
bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
```","Delete the tensorflow repo, re-clone and try again, it starts compiling.",CONTRIBUTOR
54390,abattery,1172788271,2022-07-01 23:51:33,Enabling both at once is really hard. You might want to land the new features without enabling by default and then you might be able to carefully turn on in the separate PRs for each repository.,Enabling both at once is really hard.,CONTRIBUTOR
56484,bixia1,1172624504,2022-07-01 18:49:58,this is currently waiting on https://github.com/tensorflow/tensorflow/pull/56497,This is currently waiting on https://github.com/tensorflow/tensorflow/pull/56497.,CONTRIBUTOR
33975,Flamefire,1172166333,2022-07-01 9:56:06,"Hi @mohantym I just got around testing this with the latest Bazel and TF but the issue still persists.
Possible solutions is our long-term patch at https://github.com/easybuilders/easybuild-easyconfigs/blob/439c0f2cb41bcb8f11071457550a2786eabe76a8/easybuild/easyconfigs/t/TensorFlow/TensorFlow-2.1.0_fix-cuda-build.patch or https://github.com/tensorflow/tensorflow/pull/56360",I just got around testing this with the latest Bazel and TF but the issue still persists.,CONTRIBUTOR
56088,elfringham,1172100494,2022-07-01 8:47:16,"I submitted the PR https://github.com/tensorflow/tensorflow/pull/56620 to remove uses of no_oss_py2, which has been approved but not merged yet, though not sure why.
So it seems that the ARM_CI and ARM_CD workflows are only running the pip tests. Are there plans to also run the nonpip tests on the source code directly?
The current test flags for these ARM workflows include ""-requires-gpu,-gpu,-tpu,"" which are not used in the x86 pip tests.","""I submitted the PR https://github.com/tensorflow/tensorflow/pull/56620 to remove uses of no_oss_py2, which has been approved but not merged yet, though not sure why.""",CONTRIBUTOR
55534,d0k,1171237194,2022-06-30 13:42:52,I don't have a setup to debug that build configuration and I'm still busy resolving breakages from your batch matmul change. What is broken for mkl_aarch64?,I don't have a setup to debug that build configuration and I'm still busy resolving breakages from your batch matmul change.,MEMBER
55534,snadampal,1171198029,2022-06-30 13:08:03,"I'm testing on aarch64 system, with https://github.com/tensorflow/tensorflow/commit/d74695c1097b181e4fe6b68e47b620053ce4a073 checked out, the default config builds fine, but not the '--config=mkl_aarch64'. would you be able to make sure mkl_aarch64 build is also tested before merging the PR. Recently we've setup aarch64 ci/cd for tensorflow
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/Dockerfile.cpu.arm64
Is there something I can help with?","""I'm testing on aarch64 system, with https://github.com/tensorflow/tensorflow/commit/d74695c1097b181e4fe6b68e47b620053ce4a073 checked out, the default config builds fine, but not the '--config=mkl_aarch64'. would you be able to make sure mkl_aarch64 build is also tested before merging the PR.""",CONTRIBUTOR
49944,Flamefire,1171019026,2022-06-30 10:02:19,"No update on such a fundamental issue after more than 1 year?
Note that we have to fully disable the AVX512 optimization in order to use TensorFlow on such systems: https://github.com/easybuilders/easybuild-easyconfigs/blob/develop/easybuild/easyconfigs/t/TensorFlow/TensorFlow-2.5.0_disable-avx512-extensions.patch",No update on such a fundamental issue after more than 1 year?,CONTRIBUTOR
56624,tilakrayal,1170989660,2022-06-30 9:33:22,"@gowthamkpr,
While I was trying to reproduce the issue on tensorflow v2.8, v2.9 and nightly, the code execution time was longer than expected. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/5560b8fd2223ffb0a584f4d2d253eefd/56624.ipynb).","""The code execution time was longer than expected""",CONTRIBUTOR
55966,kaixih,1170854444,2022-06-30 7:14:30,I can see some tests are failed but the log shows the failure is unrelated to this PR. Can you help check? @reedwm,I can see some tests are failed but the log shows the failure is unrelated to this PR.,CONTRIBUTOR
55959,DEKHTIARJonathan,1170200639,2022-06-29 16:20:25,"Depends on https://github.com/tensorflow/tensorflow/pull/56497
I need to wait on #56497 to rebase everything properly",Depends on https://github.com/tensorflow/tensorflow/pull/56497 I need to wait on #56497 to rebase everything properly.,CONTRIBUTOR
27298,mohantym,1169876881,2022-06-29 11:40:19,It is still replicating in [2.9 ](https://colab.sandbox.google.com/gist/mohantym/45b195bfa177a64af960f530308f8ad6/untitled56.ipynb#scrollTo=659SZpY7a5eu)and [2.10.0dev ](https://colab.sandbox.google.com/gist/mohantym/e4c752c98c44fda71e1514a91b4aa441/untitled56.ipynb#scrollTo=S-9S7X1kKXCH).,It is still replicating in [2.9](https://colab.sandbox.google.com/gist/mohantym/45b195bfa177a64af960f530308f8ad6/untitled56.ipynb#scrollTo=659SZpY7a5eu)and [2.10.0dev ](https://colab.sandbox.google.com/gist/mohantym/e4c752c98c,CONTRIBUTOR
56356,penpornk,1169250299,2022-06-28 21:14:58,"@kulinseth There seems to be an issue finding the `Expm1` op on M1 GPU. Could you please help take a look? ```
No registered 'Expm1' OpKernel for 'GPU' devices compatible with node {{node sub}}
. Registered: device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]
```
@AlbertoSinigaglia Have you installed the [`tensorflow-metal` plugin](https://developer.apple.com/metal/tensorflow-plugin/)? It is required for GPU usage.","No registered 'Expm1' OpKernel for 'GPU' devices compatible with node node sub . Registered: device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF] ",MEMBER
56276,bhack,1168877901,2022-06-28 15:32:47,"About failing copybara:
```bash
bazel test --test_env=TF_XLA_FLAGS=--tf_xla_auto_jit=2 --test_env=XLA_FLAGS=""--xla_dump_to=/tmp/generated"" --distinct_host_configuration=True tensorflow/python:bincount_ops_test_gpu --flaky_test_attempts=1
//tensorflow/python:bincount_ops_test_gpu PASSED in 11.0s
```
```bash ls /tmp/generated/*.ptx | wc -l
13
```","""Failing copybara: bash bazel test --test_env=TF_XLA_FLAGS=--tf_xla_auto_jit=2 --test_env=XLA_FLAGS=""--xla_dump_to=/tmp/generated"" --distinct_host_configuration=True tensorflow/python:bincount_ops_test_gpu --flaky_test_attempts=1""",CONTRIBUTOR
55941,learning-to-play,1168184114,2022-06-28 3:43:44,"As I shared with @trevor-m, multiple presubmits are failing including Windows GPU, Windows Bazel GPU, and so on as you can see in the table above. You need to pass all the presubmits. Furthermore, because the change touches many different directories, multiple owners have to approve the code. Feel free to sync and make sure all the presubmits pass. I can check if all the internal tests pass and also help add all the reviewers here (currently some of the comments are only visible internally).",multiple presubmits are failing,COLLABORATOR
55959,bixia1,1167485341,2022-06-27 15:17:09,Can you squash the commits?,Can you squash the commits?,CONTRIBUTOR
55600,cantonios,1166330079,2022-06-25 17:28:02,Potentially. The user hasn't responded back to report that it works for them.,The user hasn't responded back to report that it works for them.,CONTRIBUTOR
55522,impjdi,1165773296,2022-06-24 17:13:56,"Sorry, I'm not too familiar with lib.so setup; I only use statically linked binaries. In fact, I never got dynamically linked libraries to work with Bazel; I remember I gave up after a couple weeks of trying.
`undefined symbol: glFenceSync` is a linker error, so linking is failing due to not finding EGL / GLES libraries.","""I never got dynamically linked libraries to work with Bazel; I remember I gave up after a couple weeks of trying.""",CONTRIBUTOR
56231,qlzh727,1165719345,2022-06-24 16:07:24,"The latest nightly should include this fix already. It should reach tf 2.10 release.
For auto complete ""from tensorflow.keras import layers"", I think it worked before 2.7 since keras was a python package located under tensorflow/python/keras. After we split the keras into a separate PIP package, i think we have some difficulty to build such hard deps in the tf/keras __init__ files. The import itself should still work as always, but the auto completion part is not.","""After we split the keras into a separate PIP package, i think we have some difficulty to build such hard deps in the tf/keras __init__ files.""",MEMBER
56558,mohantym,1165439752,2022-06-24 10:32:17,"@FengMu1995 ! Few reasons not detecting GPU delegate be from below. 1. If you have not declared Gpu [dependencies](https://www.tensorflow.org/lite/performance/gpu#step_2_edit_appbuildgradle_to_use_the_nightly_gpu_aar) in build.gradle file.
2. If there is no Gpu on device Could you move this to closed status if it is resolved.","""Few reasons not detecting GPU delegate be from below.""",CONTRIBUTOR
55534,d0k,1164847919,2022-06-23 20:33:31,You should be able to see wrong results on `bazel test -c opt //tensorflow/compiler/xla/tests:dot_operation_test_cpu` https://source.cloud.google.com/results/invocations/e217cb81-ca26-408d-ac6f-b4f8da0e081c/log,"""should be able to see wrong results on bazel test -c opt //tensorflow/compiler/xla/tests:dot_operation_test_cpu https://source.cloud.google.com/results/invocations/e217cb81-ca26-408d-ac6f-b4f8da0e081c/log""",MEMBER
56381,cantonios,1164774308,2022-06-23 19:12:35,"Duplicate of #33131 - yes, it is on the roadmap, but will not allow direct assignment like that since tensors are immutable.","Duplicate of #33131 - yes, it is on the roadmap, but will not allow direct assignment like that since tensors are immutable.",CONTRIBUTOR
56452,meena-at-work,1164673814,2022-06-23 17:19:57,"> This is currently failed with some existing code and new checks :-( Basically, we will need to replace three absl::make_unique in this file with its std:: correspondent.
@bixia1 , I've made a separate commit with the absl replacements. Can you please review?",:-(,CONTRIBUTOR
56276,bhack,1164672142,2022-06-23 17:18:09,"Honestly I don't know why we cannot align some these flag in our OSS public build (`-Werror,-Wunused-result`).","-Werror,-Wunused-result",CONTRIBUTOR
55959,DEKHTIARJonathan,1163793852,2022-06-23 0:20:32,"@bixia1 I blocked my change for TRT < 8.0.0. Can you relaunch the tests internally ?
I'm still not able to reproduce with TRT 7.1.3 on my side",I blocked my change for TRT  8.0.0. Can you relaunch the tests internally ? I'm still not able to reproduce with TRT 7.1.3 on my side.,CONTRIBUTOR
56537,impjdi,1163739383,2022-06-22 22:58:40,"As mentioned in the tflite_convert's error & warning messages, 6D tensors are not really supported in TFLite; only through Flex delegate which pulls in (non-mobile) TF code.
My team mates were able to get away with a hack folding / unfolding tensors into lower ranks to be able to use the GPU delegate. See whether you can do that. When you do it naively, you will indeed end up with 6D tensors which is not TFLite-friendly, especially not TFLite GPU.","""6D tensors are not really supported in TFLite; only through Flex delegate which pulls in (non-mobile) TF code.""",CONTRIBUTOR
55608,bixia1,1163391194,2022-06-22 17:05:42,Please squash the commits.,"""Please squash the commits.""",CONTRIBUTOR
55534,d0k,1162886571,2022-06-22 9:47:00,"The merge conflict is in xla.proto, when another flag is added, I added another one yesterday, but it has been rolled back for now.
I don't seem to have access to the other CI builds either, it's just 404. Is there a way to retrigger them?","The merge conflict is in xla.proto, when another flag is added.",MEMBER
56372,hubingallin,1162180989,2022-06-21 18:45:22,"Hi @lightingghost, it's a known issue that DistributedIterator cannot be checkpointed. Support on this may require further discussion and design.","""It's a known issue that DistributedIterator cannot be checkpointed.""",MEMBER
44160,MarkDaoust,1162086258,2022-06-21 17:45:50,"> Preprocessing layers take long time to adapt the entire dataset, is there any solutions?
Sometimes people forget to batch the dataset before passing it to adapt. Using a large batch size instead of individual records can make a huge difference. If the problem is instead that you actually have a **large** dataset then you shouldn't be using `.adapt` you should use tensorflow_transform.","""Preprocessing layers take long time to adapt the entire dataset, is there any solutions?""",MEMBER
55959,bixia1,1161976188,2022-06-21 16:21:38,This PR have some test failure that Jonathan is having problems to reproduce.,"""Having problems to reproduce""",CONTRIBUTOR
46779,CuiYifeng,1161136666,2022-06-21 2:34:37,Close this PR due to similar fixes in https://github.com/tensorflow/tensorflow/pull/56139.,"""Close this PR due to similar fixes in https://github.com/tensorflow/tensorflow/pull/56139..""",CONTRIBUTOR
56465,tfeher,1159780356,2022-06-19 17:25:43,"@DEKHTIARJonathan Could you remove the last point in the description (""Cache device memory size""). That was separately added in #56268, so it is not part of this PR anymore.","""That was separately added in #56268, so it is not part of this PR anymore.""",CONTRIBUTOR
50705,thierryherrmann,1159303295,2022-06-17 23:34:30,"@sachinprasadhs I'm sorry to contradict you. `tf.saved_model.save()` is absolutely not deprecated.
It's not deprecated even as of TF2.9: https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/saved_model/save
And it is also described in details in the guide (at least as of today): https://www.tensorflow.org/guide/saved_model#saving_a_custom_model",I'm sorry to contradict you.,CONTRIBUTOR
44061,sachinprasadhs,1159135159,2022-06-17 18:31:08,"`tf.contrib` module has been deprecated, as the error message suggests, use the OP which does not make use of OP from deprecated tf.contrib.","tf.contrib module has been deprecated, as the error message suggests, use the OP which does not make use of OP from deprecated tf.contrib.",CONTRIBUTOR
56479,rsanthanam-amd,1158791154,2022-06-17 11:45:54,This has always passed on Nvidia. The enablement of this test on ROCm was done several weeks ago but needed the modification in this PR because of some other change. The same modification was done for Nvidia recently and that change needs to be replicated for CUDA.,The enablement of this test on ROCm was done several weeks ago but needed the modification in this PR because of some other change.,CONTRIBUTOR
56386,EnricoMi,1158680325,2022-06-17 9:20:42,"You can call private method `_stop()` of `DispatchServer` and `WorkerServer`, which will stop the underlying servers.
The `__del__` method is calling that private method, so eventually, `_stop` gets called anyway.
There is no public / documented way to shut them down gracefully. And there is no explanation why.","""And there is no explanation why.""",CONTRIBUTOR
54390,abattery,1158477137,2022-06-17 4:25:28,"For example,
```
[lut.h:47]:25: error: use of undeclared identifier 'optimized_ops'
uint8x16_t output = optimized_ops::aarch64_lookup_vector(table, input);
^
1 error generated.
```",[lut.h:47]:25: error: use of undeclared identifier 'optimized_ops',CONTRIBUTOR
55808,wenscarl,1158449159,2022-06-17 3:22:50,"> I would expect something along the lines of https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/compiler/jit/mark_for_compilation_pass_test.cc;l=180-197
Test fail with
`bazel test -test_env=TF_XLA_FLAGS=--tf_xla_cluster_exclude_ops=Where --config=cuda -c opt //tensorflow/compiler/jit:compilation_passes_test `
and pass with `bazel test --config=cuda -c opt //tensorflow/compiler/jit:compilation_passes_test `
which are as expected. @cheshire Could you review the changes?","""Test fail with bazel test -test_env=TF_XLA_FLAGS=--tf_xla_cluster_exclude_ops=Where --config=cuda -c opt //tensorflow/compiler/jit:compilation_passes_test  and pass with bazel test --config=cuda -c opt //tensorflow/compiler/jit:compilation_passes_test  which are as expected.""",CONTRIBUTOR
56458,mohantym,1157198543,2022-06-16 3:43:01,@geraldstanje ! It is not crashing just skipping the statements from TF_SessionRun onwards.,It is not crashing just skipping the statements from TF_SessionRun onwards.,CONTRIBUTOR
55808,wenscarl,1157125063,2022-06-16 1:09:09,"> Forking a process and writing to a global directory for a test is really not great. Maybe look at existing C++ tests for autoclustering?
Could you give me a pointer where to add the C++ test? How about `compilability_check_util_test.cc` ?",Forking a process and writing to a global directory for a test is really not great.,CONTRIBUTOR
56408,mihaimaruseac,1157090005,2022-06-16 0:10:20,"Known internal breakage. Test disabled.
At the moment, once required builds passed and import/copybara is also green, you are waiting on internal review. Reviewer with approval rights has not yet acted on the internal version of this PR. You are in the bottom left corner here:
![life_of_pr](https://user-images.githubusercontent.com/323199/173962445-08056cd9-db00-49ed-a7f4-11c81eea556b.png)","Known internal breakage. Test disabled. At the moment, once required builds passed and import/copybara is also green, you are waiting on internal review. Reviewer with approval rights has not yet acted on the internal version of this PR. You are in the bottom left corner here: ![life_of_pr](https://user-images.githubusercontent.com/323199/173962445-08056cd9-db00-49ed-a7f4-11c81eea556b.png)",COLLABORATOR
56408,jamiecook,1157071986,2022-06-15 23:29:20,"Again CI doesn't seem to be building in an unrelated component (python //py_test_dir/tensorflow/python/distribute/failure_handling:failure_handler_test failed in 75% of cases) all other python tests passed: `Executed 821 out of 821 tests: 820 tests pass and 1 fails locally.`
![image](https://user-images.githubusercontent.com/151124/173958663-5b87f88a-052c-47f5-83ef-202410d36dee.png)",Again CI doesn't seem to be building in an unrelated component (python //py_test_dir/tensorflow/python/distribute/failure_handling:failure_handler_test failed in 75% of cases) all other python tests passed.,CONTRIBUTOR
56157,impjdi,1157068996,2022-06-15 23:24:06,"Unassigned myself, because I'm not familiar with the Python code path =/","Unassigned myself, because I'm not familiar with the Python code path =/",CONTRIBUTOR
56393,Young768,1156834329,2022-06-15 19:16:31,"@srujun Thanks for the fix. The code is good now. Right now I can increase the global batch size up to 1024 for 8 v100 GPUs.
However, after 30-40 steps, it hits OOM again. Would you be able to use this standard [script](https://github.com/tensorflow/models/blob/archive/research/inception/inception/data/build_imagenet_data.py) to generate TFRecord and reproduce the issue?
Btw, this OOM issue never happens when I use the very small dataset like mnist.","""After 30-40 steps, it hits OOM again.""",CONTRIBUTOR
44264,shelper,1156831994,2022-06-15 19:13:41,"This is annoying , and causes issues
for instance, i am running ` tensorflow_datasets.load('cats_vs_dogs')`, and it downloads the data, not to my home directory, but to something like `project_root/~/tensorflow_datasets/cats_vs_dogs`, this is definitely not desirable.....","This is annoying , and causes issues for instance, i am running  tensorflow_datasets.load('cats_vs_dogs'), and it downloads the data, not to my home directory, but to something like project_root//tensorflow_datasets/cats_vs_dogs/, this is definitely not desirable.",CONTRIBUTOR
56393,srujun,1156797264,2022-06-15 18:31:15,"Ah my apologies, that particular line should be:
```
children = [_split(child, splits, axis + 1) for child in children]
```
We want to recursively split on each dimension of the input tensor. Does this fix make it work?"," children = [_split(child, splits, axis + 1) for child in children] ",CONTRIBUTOR
56457,mohantym,1155951226,2022-06-15 3:48:02,"Hi @MM2932 ! I tracked the error [here](https://developer.android.com/ndk/reference/group/neural-networks#aneuralnetworksdevice_wait:~:text=ANEURALNETWORKS_DEAD_OBJECT) Here are a few things I need to know.
1. Could you confirm whether Android studio is detecting your device in a live state. 2. Commands to implement qti-dsp delegate specificially.","""I tracked the error [here](https://developer.android.com/ndk/reference/group/neural-networks#aneuralnetworksdevice_wait::text=ANEURALNETWORKS_DEAD_OBJECT)""",CONTRIBUTOR
56072,DEKHTIARJonathan,1155773990,2022-06-14 22:32:57,"@bixia1 description added. I tried to find the source of this double logging, I was not able. Can be reproduced doing: `python tensorflow/python/compiler/tensorrt/test/topk_test.py 2>&1 | tee double_log_debug.log`","I tried to find the source of this double logging, I was not able. Can be reproduced doing: python tensorflow/python/compiler/tensorrt/test/topk_test.py 2>&1 | tee double_log_debug.log.",CONTRIBUTOR
37284,Ir1d,1155614934,2022-06-14 19:10:59,"@mohantym I'm not aware of this HTTPS url in the doc. There are HTTP URLs and they work fine. However, note that the cert for https://download.tensorflow.org is still misconfigured.","I'm not aware of this HTTPS url in the doc. There are HTTP URLs and they work fine. However, note that the cert for https://download.tensorflow.org is still misconfigured.",CONTRIBUTOR
56422,svenstaro,1154907852,2022-06-14 8:58:36,Why highlight me even?,Why highlight me even?,CONTRIBUTOR
51499,muxamilian,1154429037,2022-06-13 20:53:46,"It's probably not a bug in Tensorflow but Apple's tensorflow metal plugin. See for example the following discussion https://developer.apple.com/forums/thread/689299 or this one https://developer.apple.com/forums/thread/697057?answerId=704830022#704830022
The solution is to run operations from `tf.random` and `tf.sort` and `tf.argsort` on the CPU like this:
```python
with tf.device('/cpu:0'):
tf.random.uniform((10,))
```","""It's probably not a bug in Tensorflow but Apple's tensorflow metal plugin.""",CONTRIBUTOR
56408,mihaimaruseac,1154123913,2022-06-13 16:21:06,MLIR breakage at the point this PR was created.,MLIR breakage at the point this PR was created.,COLLABORATOR
55808,cheshire,1153817106,2022-06-13 11:48:07,Forking a process and writing to a global directory for a test is really not great. Maybe look at existing C++ tests for autoclustering?,Forking a process and writing to a global directory for a test is really not great.,MEMBER
56242,LukeWood,1153568489,2022-06-13 7:25:19,reopened this because we should probably fully trace back the root cause of this issue,reopened this because we should probably fully trace back the root cause of this issue.,CONTRIBUTOR
56328,roserg,1152436734,2022-06-10 14:43:34,"From your model(resnext14_16x4d.tflite), I can see that you probably need grouped convolutions. We added support with this change(https://github.com/tensorflow/tensorflow/commit/8c2c575ab1090373873716afcd09ddd50dbce830).
So you probably need to fix your model and split will disappear at all.","""I can see that you probably need grouped convolutions.""",CONTRIBUTOR
56279,hubingallin,1151621844,2022-06-09 21:03:30,The error is because of the [multiprocessing Threadpool](https://github.com/tensorflow/tensorflow/blob/9eb5fdf99053625f6e870e895a7cce6d1d3ed752/tensorflow/python/distribute/cross_device_ops.py#L1104) created is not closed properly before the program exits. A [fix](https://github.com/tensorflow/tensorflow/commit/3022b93691e00d82380cc3af9e1299feaae24432) has already been submitted,The error is because of the [multiprocessing Threadpool](https://github.com/tensorflow/tensorflow/blob/9eb5fdf99053625f6e870e895a7cce6d1d3ed752/tensorflow/python/distribute/cross_device_ops.py#L1104) created is not closed properly before the program exits.,MEMBER
54481,sirakiin,1151426447,2022-06-09 17:51:27,"Hi @MM2932 ,
The depthwise conv issue is identified and it's a defect in Hexagon's depthwiseSupernode implementation. Unfortunately we can not modify hexagon nn lib. I'm trying to provide a workaround so we can avoid disabling it, but it requires some refactoring on the hexagon delegate code base. Will update in the thread next week.","""Unfortunately we can not modify hexagon nn lib.""",MEMBER
50108,chunduriv,1151074074,2022-06-09 12:44:45,"@dbl001,
Sorry for the late response. I tried with the latest version on MacOS Monetery 12.3.1 and it is working. Could you try with the latest tensorflow (TF2.9) as shown below ```
pip3 install tensorflow-macos
import tensorflow as rf
```",I tried with the latest version on MacOS Monetery 12.3.1 and it is working. Could you try with the latest tensorflow (TF2.9) as shown below  pip3 install tensorflow-macos import tensorflow as rf .,CONTRIBUTOR
56391,mohantym,1149566250,2022-06-08 7:33:04,"@CORRELU ! It should be tf.TensorSpec instead of TensorSpec. Please let us know if it works.
```
tf_train = tf.data.experimental.load(
""D:/data/train_w512_small"",
element_spec=(tf.TensorSpec(shape=(None, None, 5), dtype=tf.float32), tf.TensorSpec(shape=(None, None, 5), dtype=tf.float32)),
compression=None,
reader_func=None,
)
```","""It should be tf.TensorSpec instead of TensorSpec.""",CONTRIBUTOR
56373,SnoopJ,1149538689,2022-06-08 7:00:37,"As described in #56399, this failure goes beyond the numpy-specific case.","As described in #56399, this failure goes beyond the numpy-specific case.",CONTRIBUTOR
56393,Young768,1148988616,2022-06-07 17:50:17,"Btw, I turned off mixed precision training and disabled fusion for BN layers, since it seems DTensor does not support these yet.","""DTensor does not support these yet.""",CONTRIBUTOR
54379,gzmkl,1148859514,2022-06-07 15:56:05,"@penpornk We have a better solution: skip the related test if it is running on AVX2 platform (when TestCPUFeature(...AVX512F) returns false) because oneDNN fused pad conv op is not supported on AVX2 or earlier instruction set. After our internal full-validation and code review, I will update this PR.","""We have a better solution: skip the related test if it is running on AVX2 platform (when TestCPUFeature(...AVX512F) returns false) because oneDNN fused pad conv op is not supported on AVX2 or earlier instruction set.""",CONTRIBUTOR
56358,bhack,1148846609,2022-06-07 15:44:41,"@sachinprasadhs These metrics are third_party/deprecated.
You could use TF Addons if you don't want to wait Keras-cv porting:
https://www.tensorflow.org/addons/api_docs/python/tfa/metrics/F1Score","""these metrics are third_party/deprecated""",CONTRIBUTOR
40075,impjdi,1148838175,2022-06-07 15:38:07,"One thing I wanted to mention before we close the bug is that we cut corners and rely on undefined behavior in certain cases. If device-specific bug, there's a chance that this is not working out. As OpenGL is not relying on those features, I would try enforcing OpenGL for that particular device.","""If device-specific bug, there's a chance that this is not working out.""",CONTRIBUTOR
55941,joker-eph,1148039973,2022-06-06 23:40:26,"It seems that this configuration will quickly get obsolete by the migration to cc_shared_library (as far as I understand), is this correct?
If so: - Is this PR gonna help the migration in any way?
- Otherwise, I'm concerned about the added complexity and how it'll get in the way of the current migration (by having more select/if and an extra config to maintain).
-",- I'm concerned about the added complexity and how it'll get in the way of the current migration (by having more select/if and an extra config to maintain).,CONTRIBUTOR
56298,qlzh727,1147735122,2022-06-06 18:08:30,"dtensor API is only available at tf 2.9 or later, so it is expected to fail in tf 2.6.","dtensor API is only available at tf 2.9 or later, so it is expected to fail in tf 2.6.",MEMBER
56347,SnoopJ,1147727959,2022-06-06 18:00:36,"Note: title change and force-push are because the bug has been refiled as #56373 to comply with a request for using the issue template. Branch name is now slightly misleading, but semantics of the changes are unaffected by this name-juggling.","""Note: title change and force-push are because the bug has been refiled as #56373 to comply with a request for using the issue template.""",CONTRIBUTOR
54379,ashiqimranintel,1147579948,2022-06-06 15:28:20,"@penpornk, could you re-run the test again?",Could you re-run the test again?,CONTRIBUTOR
56312,tilakrayal,1147165122,2022-06-06 8:03:44,"@gowthamkpr,
I was able to reproduce the issue on tensorflow-gpu [v2.8](https://colab.research.google.com/gist/tilakrayal/41ccd32091cfba1720c93fc30cbf306f/2-8_gpu.ipynb) and v2.9. In tensorflow-cpu [v2.8](https://colab.research.google.com/gist/tilakrayal/6ad3790641f539bedd86ab5338617585/2-8-56312.ipynb), i was not able to find the issue. Please find the gist.","""I was able to reproduce the issue on tensorflow-gpu [v2.8](https://colab.research.google.com/gist/tilakrayal/41ccd32091cfba1720c93fc30cbf306f/2-8_gpu.ipynb) and v2.9. In tensorflow-cpu [v2.8](https://colab.research.google.com/gist/tilakrayal/6ad",CONTRIBUTOR
54146,joker-eph,1146156429,2022-06-03 16:30:40,This is missing tests I think.,This is missing tests I think.,CONTRIBUTOR
56347,SnoopJ,1146029619,2022-06-03 14:36:43,"CI failure does not look related to these changes, seeing the same failure on #56345 (which has no code changes) so I assume this is noise.
```
//bazel_pip/tensorflow/python/kernel_tests/nn_ops:conv_ops_3d_test_cpu FLAKY, failed in 3 out of 33 in 300.3s
```",CI failure does not look related to these changes,CONTRIBUTOR
56276,bhack,1145841517,2022-06-03 10:52:11,"On GPU I have a build error:
Do you understand what It means?
```
ERROR: /tf/tensorflow/tensorflow/compiler/tests/BUILD:2190:15: //tensorflow/compiler/tests:bincount_op_test_gpu: missing input file '//tensorflow/compiler/tests:bincount_op_test.py'
```","""I have a build error: Do you understand what it means?""",CONTRIBUTOR
56276,bhack,1145821512,2022-06-03 10:24:05,"As you see with the few performance tests I've added on CPU we are passing 10% gain for medium and large and small input (int64).
It cannot achieve a 10% gain with small input (int32).
The pointer to the log is:
https://source.cloud.google.com/results/invocations/eb6fa396-382b-464b-847d-10a4dfdd3658/targets/pkg%2Fpip_and_nonpip_tests/tests","""Cannot achieve a 10% gain with small input (int32)""",CONTRIBUTOR
56183,khanhlvg,1145516446,2022-06-03 2:06:31,"TVM isn't part of the TensorFlow ecosystem so unfortunately we don't have anyone familiar with it at Google. If you want to run inference with TF models, I'd suggest checking out:
1. TF Serving if you want to run inference on the server.
2. TFLite if you want to run inference on edge devices.","""TVM isn't part of the TensorFlow ecosystem so unfortunately we don't have anyone familiar with it at Google.""",MEMBER
54155,karimnosseir,1144226450,2022-06-01 23:02:12,"@joker-eph The model shared here fails during saved model to TF importing, looks like coming from a composite tensor.
Mehdi can you please have a look ?
Thanks","""The model shared here fails during saved model to TF importing, looks like coming from a composite tensor.""",CONTRIBUTOR
56321,bjacob,1144013024,2022-06-01 18:51:49,I wasn't able to add @sjarus as reviewer so here's a FYI.,I wasn't able to add @sjarus as reviewer so here's a FYI.,CONTRIBUTOR
54374,Nyrio,1143963435,2022-06-01 18:00:29,"We decided that we don't need this functionality at the moment, so let's close this PR instead of reworking it using the function names.","""let's close this PR instead of reworking it using the function names.""",CONTRIBUTOR
56076,Nyrio,1143462342,2022-06-01 11:07:16,@gbaned The other PRs in the series need to be reviewed first (cf description),The other PRs in the series need to be reviewed first (cf description),CONTRIBUTOR
56309,awf,1143459469,2022-06-01 11:04:13,"This is a feature for tensorflow developers following the instructions at https://www.tensorflow.org/install/source#cpu-only
Currently, as a developer, I cannot debug tensorflow using gdb inside the developer docker container.","Currently, as a developer, I cannot debug tensorflow using gdb inside the developer docker container.",CONTRIBUTOR
55919,muxamilian,1143221863,2022-06-01 7:34:59,"@Petros626 One thing you can do is to train Spaghettinet normally and then use post-training quantization. This should always work. But training-aware quantization (another quantization method) doesn't work without this patch. However, I'm not the author of Spaghettinet, I just reported the bug. @hermitman, who wrote this patch originally, is a coauthor of the Spaghettinet, so I guess it's better to ask him.","""training-aware quantization (another quantization method) doesn't work without this patch""",CONTRIBUTOR
56281,drivanov,1143098857,2022-06-01 4:15:56,"I am closing that one because [PR#56295](https://github.com/tensorflow/tensorflow/pull/56295) contains these three (`>`, `<`, `==`) and two more (`>=`, `<=`) operations.","I am closing that one because [PR#56295](https://github.com/tensorflow/tensorflow/pull/56295) contains these three (>, , ==) and two more (>=, ==) operations.",CONTRIBUTOR
54173,impjdi,1142648249,2022-05-31 21:17:32,"14dd3a808ded246021a110ee7cf8b174214ed6c2 should have fixed it, right?","""should have fixed it, right?""",CONTRIBUTOR
56165,Nyrio,1142524382,2022-05-31 18:46:50,"@bixia1 Is there a way to see the details of the test failures in the ""ARM CI"" job? I can only see the bazel logs but not the `test.log` files, and cannot reproduce these failures.","I can only see the bazel logs but not the test.log files, and cannot reproduce these failures.",CONTRIBUTOR
56259,kushanam,1142520198,2022-05-31 18:41:30,"> @sachinprasadhs,
> While I was trying to reproduce the issue in google colab, the code was executing more than 3 hrs which was longer than expected. Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/85a9bd43309f44ed92cd4611c13233f5/56259.ipynb).
Made a smaller dataset. could you have another look please?","""The code was executing more than 3 hrs which was longer than expected.""",CONTRIBUTOR
56302,elfringham,1141887483,2022-05-31 9:20:35,"@mohantym @gadagashwini The referenced gist above does not match what I am seeing, not sure why that would be, possibly a different default in your environment. A full build log can be seen at
https://ci.linaro.org/view/All/job/ldcg-python-manylinux-tensorflow-onednn-nightly/68/consoleText","not sure why that would be, possibly a different default in your environment",CONTRIBUTOR
56258,tilakrayal,1140611442,2022-05-30 2:15:16,"@hkvision,
I was facing different error while trying to execute the mentioned code. Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/477ac53b13c581843b3da03b4b6745b4/untitled357.ipynb).","""I was facing different error while trying to execute the mentioned code.""",CONTRIBUTOR
56088,mihaimaruseac,1138701954,2022-05-26 15:29:18,"Segfaults arising from C++17 switch are mostly due to `std::string_view(nullptr)`. I'm currently hunting for these and fixing, but will take a while to get to all of them","""Segfaults arising from C++17 switch are mostly due to std::string_view(nullptr)""",COLLABORATOR
56259,tilakrayal,1138599489,2022-05-26 13:48:54,"@sachinprasadhs,
While I was trying to reproduce the issue in google colab, the code was executing more than 3 hrs which was longer than expected. Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/85a9bd43309f44ed92cd4611c13233f5/56259.ipynb).","""The code was executing more than 3 hrs which was longer than expected.""",CONTRIBUTOR
56088,nSircombe,1138547357,2022-05-26 12:56:26,"I think this is a bit of a mangling of `-no_oss_py2`, it's set as `-no_oss_py${py_ver}` - I don't think it does anything and is just ignored, but I may be wrong.","I think this is a bit of a mangling of -no_oss_py2, it's set as -no_oss_py$py_ver - I don't think it does anything and is just ignored, but I may be wrong.",CONTRIBUTOR
54479,bhack,1138544904,2022-05-26 12:53:24,"Just another info. I see that iterating the development over this TF2XLA kernels is quite slow also on a GCP instance with a modern CPU. E.g. with every single edit in `unique_op.cc`:
```bazel test -k tensorflow/python/eager:def_function_xla_jit_test --flaky_test_attempts=1 --test_filter=*testUniqueCompilability*```
```Elapsed time: 48.398s, Critical Path: 40.65s```",I see that iterating the development over this TF2XLA kernels is quite slow also on a GCP instance with a modern CPU. E.g. with every single edit in unique_op.cc: bazel test -k tensorflow/python/eager:def_function_xla_jit_test --flaky_test_attempts=1 --test_filter=*testUniqueCompilability* Elapsed time: 48.398,CONTRIBUTOR
56088,elfringham,1138337442,2022-05-26 9:22:51,"Hi @milpuz01 those unit tests started to fail with commit https://github.com/tensorflow/tensorflow/commit/8ea5ed0c392b329a3e0481a3f1f7b0ca86821b84
I can only reproduce them inside the manylinux2014 docker container at the moment. Outside of that those tests pass.",I can only reproduce them inside the manylinux2014 docker container at the moment.,CONTRIBUTOR
56210,sachinprasadhs,1138011983,2022-05-26 0:27:40,"@harrylincoln , To make it more generic changing it to `models_file_path` from `models_path` would be more relevant.
Also, have you tried the below option with `model_file_path`?
`# classifier = audio.AudioClassifier.create_from_file(model_path)`",# classifier = audio.AudioClassifier.create_from_file(model_path),CONTRIBUTOR
56088,mseth10,1137810255,2022-05-25 20:19:33,"The nightly pipeline seems to be broken today. Earlier only some tests were failing as mentioned by @milpuz01 , but today the build seems to be failing as well. Does anyone have pointers as to what might have caused it (should be one of yesterday's commits) and how to fix it? Thanks! @mihaimaruseac @learning-to-play @nitins17 https://github.com/tensorflow/tensorflow/actions/runs/2383006475","""The nightly pipeline seems to be broken today.""",CONTRIBUTOR
48056,bhack,1137779594,2022-05-25 19:43:10,"I think this was merged with https://github.com/tensorflow/tensorflow/commit/299cb76dd913e7bb0349a13c1165459dac4ea81e but the PR is not closed and put in a merged status.
We have collected this case in https://github.com/tensorflow/community/issues/413",I think this was merged with https://github.com/tensorflow/tensorflow/commit/299cb76dd913e7bb0349a13c1165459dac4ea81e but the PR is not closed and put in a merged status.,CONTRIBUTOR
41448,qlzh727,1136121755,2022-05-24 16:09:22,Reassign to myself since Tom has left the team.,Tom has left the team.,MEMBER
40075,impjdi,1136108548,2022-05-24 15:57:18,"Hm, we don't have a Samsung Galaxy Tab S6 to take a look at that device-specific issue :( As sushreebarsa suggested, I would start with a TF upgrade.","""We don't have a Samsung Galaxy Tab S6 to take a look at that device-specific issue""",CONTRIBUTOR
56242,bhack,1136042836,2022-05-24 15:00:13,"@kaitolucifer You need to open this type of tickets in the [Keras repository](https://github.com/keras-team/keras/). But I suppose it could be caused by the fallbacks introduced at:
https://github.com/keras-team/keras/commit/dbc4526978865b372e794accbaa7c584f9c86a0f
It is already partially tracked at:
https://github.com/tensorflow/tensorflow/issues/55639 (/cc @wangpengmit @rohan100jain)
https://github.com/keras-team/keras-cv/issues/291 (/cc @LukeWood @qlzh727)","""But I suppose it could be caused by the fallbacks introduced at: https://github.com/keras-team/keras/commit/dbc4526978865b372e794""",CONTRIBUTOR
56121,tilakrayal,1134757898,2022-05-23 14:34:47,"@gowthamkpr ,
I was able to reproduce the issue in tensorflow [v2.8](https://colab.research.google.com/gist/tilakrayal/1e3b0be6e3530f5bd0a50ce0e88da509/text_generation.ipynb), and in [v2.7](https://colab.research.google.com/gist/tilakrayal/8016913e447c36af168fbf85f98e8d5e/text_generation.ipynb) and nightly the code was failing with different error. Please find the gist.","I was able to reproduce the issue in tensorflow [v2.8](https://colab.research.google.com/gist/tilakrayal/1e3b0be6e3530f5bd0a50ce0e88da509/text_generation.ipynb), and in [v2.7](https://colab.research.google.com/gist/tilakrayal/8016913e447c36af168fbf85f98e",CONTRIBUTOR
30418,ppham27,1134698389,2022-05-23 13:43:54,"I am not too familiar with that repository, unfortunately. I don't see a lot of value in rewriting it, though. If I were to keep pursuing this, I'd be more inclined to merge this into TF Core as a fix for https://www.tensorflow.org/api_docs/python/tf/recompute_grad.","I am not too familiar with that repository, unfortunately.",CONTRIBUTOR
56207,aliencaocao,1134319225,2022-05-23 8:01:00,"The complete code is very long as it involves loading of datasets which I cannot make public, and various preprocessing functions. What I have provided is a line that when removed, solves the issue.","The complete code is very long as it involves loading of datasets which I cannot make public, and various preprocessing functions. What I have provided is a line that when removed, solves the issue.",CONTRIBUTOR
54498,njzjz,1133490000,2022-05-21 1:07:27,"I got the same issue when building TensorFlow 2.9 with CUDA 10.1, among a lot of `.cu.cc` files. Finally, I decided to use CUDA 10.2 instead, and the errors disappeared. It's still quite confusing why 10.2 doesn't have this issue.","I got the same issue when building TensorFlow 2.9 with CUDA 10.1, among a lot of .cu.cc files.",CONTRIBUTOR
56119,bhack,1133374790,2022-05-20 21:05:01,"To summarize I think that all is working correctly and the code in master was working cause the old local scope of the lambda args but it was ""wrong"".","""wrong""",CONTRIBUTOR
56180,sachinprasadhs,1133222249,2022-05-20 19:06:26,"Hi, Currently NNAPI acceleration is not supported when dynamic sized tensors are used, you can find the details from the tensorflow document [here](https://www.tensorflow.org/lite/android/delegates/nnapi#use_supported_models_and_ops).
The only way now would be to have workaround solution like you're doing.",Currently NNAPI acceleration is not supported when dynamic sized tensors are used,CONTRIBUTOR
55971,sachinprasadhs,1133217461,2022-05-20 18:59:18,"Hi, It can be due to some of the High level TF APIs which doesn't support dispatching which means you cannot pass `KerasTensors`. For more details you can look at the detail here.
https://github.com/keras-team/keras/blob/4098cefe29aea05ba80faff878cec464f1b3a2e9/keras/engine/keras_tensor.py#L77-L93","""It can be due to some of the High level TF APIs which doesn't support dispatching which means you cannot pass KerasTensors.""",CONTRIBUTOR
56169,bhack,1133004351,2022-05-20 14:54:42,P.s. take care if/when you will work in graph mode with your initial syntax. We are working on fixing/align it https://github.com/tensorflow/tensorflow/issues/56089,P.s. take care if/when you will work in graph mode with your initial syntax. We are working on fixing/align it https://github.com/tensorflow/tensorflow/issues/56089.,CONTRIBUTOR
56137,kulinseth,1132455690,2022-05-20 4:43:56,"> @sachinprasadhs / @kulinseth I see that `tensorflow-macos` 2.9.0 was released, but there is not a 3.10 wheel:
> > * https://pypi.org/project/tensorflow-macos/2.9.0/#files
We have planned for py38/py39 releases so far. We can look into adding 3.10 support.","""I see that tensorflow-macos 2.9.0 was released, but there is not a 3.10 wheel:""",CONTRIBUTOR
56081,impjdi,1132306528,2022-05-19 23:51:15,"TFLite GPU is not 100% compatible with all the ops & options. As the error message says:
> Slice does not support shrink_axis_mask parameter.
You can look for that string in `//tensorflow/lite/delegates/gpu/common/model_builder.cc`.",TFLite GPU is not 100% compatible with all the ops & options.,CONTRIBUTOR
56019,tilakrayal,1131272510,2022-05-19 6:25:27,"@JamesJacquesDiego ,
I was facing different error while executing the mentioned code.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/f7d41bd80ea5b9b250d4e357bb0e3694/untitled346.ipynb).","""I was facing different error while executing the mentioned code.""",CONTRIBUTOR
55966,kaixih,1130380196,2022-05-18 18:42:15,Force pushed the PR to resolve the conflict.,Force pushed the PR to resolve the conflict.,CONTRIBUTOR
55812,cantonios,1130307106,2022-05-18 17:41:51,"I think you want a combination of `tf.gather_nd` and `tf.tensor_scatter_nd_update`:
```
# Get the list of indices for some condition mask = tf.where(y_true > 5)
# Extract the values from another tensor at those indices
vals = tf.gather_nd(error, mask)
# Update values at the supplied indices based on the old values
new_error = tf.tensor_scatter_nd_update(error, mask, 5 * vals)
```","I think you want a combination of tf.gather_nd and tf.tensor_scatter_nd_update:  # Get the list of indices for some condition mask = tf.where(y_true > 5) # Extract the values from another tensor at those indices vals = tf.gather_nd(error, mask) # Update values at the supplied indices based on the old values new_error = tf.tensor_sc",CONTRIBUTOR
55931,mihaimaruseac,1129001773,2022-05-17 15:18:39,"Sadly, I don't know what error occurred during that fetch. Might have been just a transient error and retrying would work.","Sadly, I don't know what error occurred during that fetch.",COLLABORATOR
56099,chunduriv,1128948526,2022-05-17 14:33:12,"@zurgeg, Thank you for reporting the issue. Unfortunately I am unable to replicate the issue using `Firefox 96.0.3`. Please refer to the screenshot in below
![image](https://user-images.githubusercontent.com/74177924/168835495-0b7f0788-c88c-4335-9462-06bc55c89c9c.png)
Generally `503 service unavailable` caused by a high number of requests sent to the server, which exhausts available resources.
Please can you confirm if you are facing any issues using `Firefox 96.0.3`? Thank you.",unable to replicate the issue using Firefox 96.0.3,CONTRIBUTOR
56119,bhack,1128187727,2022-05-16 22:11:39,"P.s. I think that there was already your TODO:
https://github.com/tensorflow/tensorflow/blob/79eee8e0493fb443a3fe6f247c3f55cec40b11e7/tensorflow/python/autograph/operators/control_flow.py#L196",I think that there was already your TODO.,CONTRIBUTOR
55231,cantonios,1128111543,2022-05-16 20:36:37,I think a test or two timed out. Doesn't look related.,I think a test or two timed out.,CONTRIBUTOR
55808,cheshire,1127753715,2022-05-16 14:33:01,"Do you think we could do something much simpler, without a flag, and just disabling tf.where?","Do you think we could do something much simpler, without a flag, and just disabling tf.where?",MEMBER
56116,EnricoMi,1127537255,2022-05-16 11:11:25,"Same for Ubuntu 20.04.
Last version that worked was `tf_nightly_gpu-2.10.0.dev20220427`.
First version this fails is `tf_nightly_gpu-2.10.0.dev20220514`.
https://buildkite.com/horovod/horovod/builds/7748#c5793a34-ec06-4ca8-aa26-b4491ea3fbca/224-470",Same for Ubuntu 20.04,CONTRIBUTOR
56073,Maratyszcza,1126368630,2022-05-13 19:16:57,@gbaned Could you merge the PR?,Could you merge the PR?,CONTRIBUTOR
42424,mohantym,1126005848,2022-05-13 12:30:36,Sorry @fsx950223 ! I thought disabling eager execution in v2 code will give more accurate results. But it is still replicating in [2.8](https://colab.sandbox.google.com/gist/mohantym/649e6467c6caa1b404ac258eae79a378/git_42424.ipynb#scrollTo=HnkBabLwXllb) and [nightly](https://colab.sandbox.google.com/gist/mohantym/8df6343819ea8e8b8be1b9d626afb913/git_42424.ipynb#scrollTo=HnkBabLwXllb) version. Thanks for pointing out.,I thought disabling eager execution in v2 code will give more accurate results. But it is still replicating in [2.8](https://colab.sandbox.google.com/gist/mohantym/649e6467c6caa1b404ac258eae79a378/git_42424.ipynb#scrollTo=HnkBabLwXllb) and [nightly](https://colab.sandbox.google.,CONTRIBUTOR
55960,joker-eph,1125950529,2022-05-13 11:22:59,The error is one of binary getting to big to link in this code model. Any commit can trigger it really when we're close to the limit.,The error is one of binary getting to big to link in this code model.,CONTRIBUTOR
44539,mihaimaruseac,1125525936,2022-05-12 23:53:17,This stalled at the moment as we;re trying to find a best way for steering non-python language bindings.,stalled at the moment as we;re trying to find a best way for steering non-python language bindings.,COLLABORATOR
56073,lgeiger,1125297365,2022-05-12 18:25:44,"> Needs unit tests
@Maratyszcza Sorry about that. I wasn't sure whether there is a concrete reason why reshapes are not yet delegated to XNNPACK before adding tests. I updated the PR to add tests for signed and unsigned reshapes similar to the floating point tests.",I wasn't sure whether there is a concrete reason why reshapes are not yet delegated to XNNPACK before adding tests.,CONTRIBUTOR
53767,lgeiger,1124162232,2022-05-11 18:35:56,@JunyoungLim I retested the above example `2.10.0-dev20220427` and the converter still segfaults.,I retested the above example 2.10.0-dev20220427 and the converter still segfaults.,CONTRIBUTOR
56060,yongtang,1123887381,2022-05-11 14:55:35,@mihaimaruseac There a quite a few vulnerabilities in curl 7.83.0. I don't know if the update has already been applied internally in google or not. Please feel free to close the PR if it has been taken care of.,I don't know if the update has already been applied internally in google or not.,MEMBER
56021,kobrineli,1123855406,2022-05-11 14:34:53,"@rockspring I've got the way to resolve problem.
Try not to build with `--copt='-UNDEBUG'` or build with `--copt='-DNDEBUG'`.
But while building with -UNDEBUG there is still a problem since the moment when LsbMask in xla/util.h became `constexpr`.","""But while building with -UNDEBUG there is still a problem since the moment when LsbMask in xla/util.h became constexpr.""",CONTRIBUTOR
56025,nSircombe,1123214421,2022-05-11 5:53:53,Could they not simply be listed explicitly in TF_TEST_TARGETS with the other test exclusions?,Could they not simply be listed explicitly in TF_TEST_TARGETS with the other test exclusions?,CONTRIBUTOR
56025,mseth10,1123074239,2022-05-11 1:15:15,"> This is not the right way to temporarily exclude these tests. It hides the exclusion away in multiple places and also prevents these tests from being run for the default Eigen build where they are passing.
Hi @elfringham , I agree with your concern. We can introduce a new tag like `no_aarch64_onednn_acl` and use it to exclude these tests. Do you think that's an acceptable solution?",This is not the right way to temporarily exclude these tests. It hides the exclusion away in multiple places and also prevents these tests from being run for the default Eigen build where they are passing.,CONTRIBUTOR
55807,duncanriach,1121593282,2022-05-09 21:19:59,"The four tests that compared the gradients between the CPU and GPU were being run even when there was no GPU available, which would have compared the CPU gradients with themselves, always passing. The tests were being run under both `depthwise_conv_op_test.py` and `depthwise_conv_op_d9m_test.py`, so the most recent commit removes eight tests when there is no GPU available.","The four tests that compared the gradients between the CPU and GPU were being run even when there was no GPU available, which would have compared the CPU gradients with themselves, always passing.",CONTRIBUTOR
55972,mohantym,1121340670,2022-05-09 16:51:01,@Sanjay2802 ! This approach won't work on TFlite model maker . @sachinprasadhs ! Could please you look at this issue? Thanks,"""This approach won't work on TFlite model maker""",CONTRIBUTOR
56025,elfringham,1121303627,2022-05-09 16:14:59,"I think the use of no_aarch64 tag to exclude tests that fail when TF_ENABLE_ONEDNN_OPT=1 is not the best thing to do. The flag is currently used to exclude tests that can never work on AARCH64. This would add a group of tests that we hope are only temporarily broken and so would have their no_aarch64 tag removed at some later point. It also stops those tests being run at all on AARCH64 even for the Eigen build, where they currently pass. I think some other mechanism would be better.","""I think the use of no_aarch64 tag to exclude tests that fail when TF_ENABLE_ONEDNN_OPT=1 is not the best thing to do.""",CONTRIBUTOR
55940,yimeisun123,1119137754,2022-05-05 23:31:21,"Two test are failed, I am trying to look at the details, but not sure if the failure is related to this PR change. For the Code Check-Changed Files, I couldn't see what exactly failed.
For the Py+CPP Test Suite, I see //tensorflow/c/eager:c_api_test_gpu failed, not sure if it is from this PR change.
Please let me know if I miss anything. Thanks.","""I am trying to look at the details, but not sure if the failure is related to this PR change.""",CONTRIBUTOR
55807,duncanriach,1119116020,2022-05-05 23:01:38,"> This needs to get fixed, as we are not testing depthwise convolutions currently. @duncanriach, do you want to fix this? A quick fix would be to add `@unittest.skip(""Test currently fails"")` for the two failing test methods.
Yes, I'll work on this. It's weird that these tests are failing because I thought I ran them successfully for PR [55657](https://github.com/tensorflow/tensorflow/pull/55657).","""It's weird that these tests are failing because I thought I ran them successfully for PR [55657](https://github.com/tensorflow/tensorflow/pull/55657)...""",CONTRIBUTOR
55645,PatriceVignola,1118148892,2022-05-05 4:26:27,Is there anything I can do to satisfy the failing checks or are they infra issues? They don't seem related to my changes.,Failing checks or are they infra issues? They don't seem related to my changes.,CONTRIBUTOR
55849,drivanov,1118118765,2022-05-05 3:01:01,"@bixia1 : I fixed it. In my previous check-in I was using ```
#if IS_TRT_VERSION_GE(8, 2, 0, 0)
return ValidateImpl(....);
#else
return errors::Unimplemented(""Boolean op: "", params_->node_def.op(),
"" is not supported in TRT version < 8.2"");
#endif
```
for the wrong class (`ConvertBinary` instead of `ConvertBooleanBinary`)","""I fixed it. In my previous check-in I was using  #if IS_TRT_VERSION_GE(8, 2, 0, 0) return ValidateImpl(....); #else return errors::Unimplemented(""Boolean op: "", params_->node_def.op(), "" is not supported in TRT version  8.2""); #endif  for the wrong class (ConvertBinary instead of ConvertBooleanBin",CONTRIBUTOR
55919,mihaimaruseac,1117973352,2022-05-04 21:57:59,"We no longer patch 1.15. We don't have access to VMs on which to build it, our end-of-life-policy is that each version is available for ~1 year. We are currently releasing TF 2.9 so this means versions TF 2.6, TF 2.7, TF 2.8 and TF 2.9 are the only versions that we support.","""We don't have access to VMs on which to build it, our end-of-life-policy is that each version is available for 1 year.""",COLLABORATOR
55817,bani-intelaipg,1117600382,2022-05-04 17:12:44,"@penpornk I believe this has been broken for a while. I tried to go back to find a passing build with ""--config=mkl"" setting, but haven't found one with my limited rebuild effort. I can use some guidance on a potential analysis of the why this import is potentially failing in Windows. Do we have to explicitly set some PATH/PYTHONPATH env var to help the build find the module? The pipeline you mentioned was a recent temporary effort and not a mainline pipeline.","""I believe this has been broken for a while""",CONTRIBUTOR
55495,impjdi,1117526194,2022-05-04 15:58:23,"Uh, I'm not familiar with the Maven repository. When I go to search.maven.org, I only get 3 search results and neither libs are there =/",I'm not familiar with the Maven repository.,CONTRIBUTOR
55840,bhack,1117200361,2022-05-04 11:27:02,"> `tf2.8 reports error: Segmentation Fault (core dumped)` I cannot reproduce this on Colab GPU (TF 2.8)
Please note also that usually the fix in the older versions (<=1 year) will be done if that is a security bug or if the bug is critical and impacting larger community. In this case it is working fine or not reproducible with Tensorflow 2.8 you need to continue using this version.",tf2.8 reports error: Segmentation Fault (core dumped),CONTRIBUTOR
55817,mihaimaruseac,1116764755,2022-05-03 23:43:06,"Penporn (@penpornk), can you bisect this if this still happens? I couldn't find a cc_shared_library last week last week that could have cause this",I couldn't find a cc_shared_library last week last week that could have cause this.,COLLABORATOR
55550,bhack,1116447133,2022-05-03 18:50:52,"> tf.image.rgb_to_grayscale(img) also does not yield the same result as the decode_image version.
Are you sure that the weights are exactly the same?
As `decode_image` in your case is `libpng`:
https://github.com/glennrp/libpng/blob/master/pngrtran.c#L1013-L1049
Instead `tf.image.rgb_to_grayscale(img)` is:
https://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850e/tensorflow/python/ops/image_ops_impl.py#L2514",tf.image.rgb_to_grayscale(img) also does not yield the same result as the decode_image version. Are you sure that the weights are exactly the same?,CONTRIBUTOR
55752,hawkinsp,1116414281,2022-05-03 18:19:14,I think you actually need to fix the `std::string` references before merging: they cannot appear in a C API.,I think you actually need to fix the std::string references before merging: they cannot appear in a C API.,MEMBER
55628,kaixih,1116347799,2022-05-03 17:16:18,"IIRC, the `AMD ROCm -- Community CI Build` will fail if those TENSORFLOW_USE_ROCM is there. It usually was some dependence issue but I didn't dive deep into the issue. @rahulbatra85","IIRC, the AMD ROCm -- Community CI Build will fail if those TENSORFLOW_USE_ROCM is there.",CONTRIBUTOR
46342,Anmol-Sharma,1116134190,2022-05-03 13:59:37,"Hi @sushreebarsa I don't this is still an issue. At that time when I opened this issue, it was present and I raised a fix as well [Here](https://github.com/tensorflow/estimator/pull/59) but due to very large merge conflicts (when the time to merge came) the changes were skipped.
So, I think this ticket can be closed, as mixing of Estimators with TF2 may not be a very good idea, so I'm closing this.",I don't this is still an issue.,CONTRIBUTOR
55505,372046933,1115560018,2022-05-03 2:45:50,"Copy `genrule` from the above comment. https://github.com/tensorflow/tensorflow/issues/55505#issuecomment-1114333283
Build results in the following error.
```
ERROR: /xxx/yyy/BUILD:330:8: in cmd attribute of genrule rule //:tf_wheel: label '//:build_pip_package' in $(location) expression is not a declared prerequisite of this rule
ERROR: /xxx/yyy/BUILD:330:8: Analysis of target '//:tf_wheel' failed
ERROR: Analysis of target '//:tf_wheel' failed; build aborted: ```",genrule:,CONTRIBUTOR
55829,Nayana-ibm,1115218742,2022-05-02 18:24:12,"Tried with installing typing, typing-extensions but not resolved","Tried with installing typing, typing-extensions but not resolved.",CONTRIBUTOR
55222,mihaimaruseac,1115086903,2022-05-02 16:20:11,Since https://github.com/tensorflow/tensorflow/pull/55222#issuecomment-1084771301 showed up in #53828: the comment here is to help in solving the issue properly: you need to subclass the methods for the ragged tensor and attach docstring to the new methods.,Since https://github.com/tensorflow/tensorflow/pull/55222#issuecomment-1084771301 showed up in #53828: the comment here is to help in solving the issue properly: you need to subclass the methods for the ragged tensor and attach docstring to the new methods.,COLLABORATOR
55814,bhack,1114964710,2022-05-02 14:32:21,Yes sorry I supposed to have a `tf.print` also in eager.. probably there was some copy-paste last minute change. But can you see that with `tf.float64` we have the same results in eager and graph mode?,tf.print also in eager..,CONTRIBUTOR
55817,bani-intelaipg,1114542170,2022-05-02 6:35:16,Same error is there with commit# b5fa6a4eecdcf69408708e98ba4de6debb880596,Same error is there with commit# b5fa6a4eecdcf69408708e98ba4de6debb880596.,CONTRIBUTOR
55817,mihaimaruseac,1114334113,2022-05-01 21:01:43,"We were using a non-LTS Bazel at that commit, to work around some other `cc_shared_library` issues. Can you try with 3637a0245a2b44f2de2aa84b6f8f40cac5600109 (the commit after the one you tested, which reverts back to LTS Bazel)? This way we'd know if the issue is Bazel or some `cc_shared_library` changelist.","""We were using a non-LTS Bazel at that commit, to work around some other cc_shared_library issues.""",COLLABORATOR
55817,mihaimaruseac,1114332227,2022-05-01 20:50:14,"Rostam, this looks like another cc_shared_library issue",This looks like another cc_shared_library issue.,COLLABORATOR
55810,mihaimaruseac,1114083662,2022-05-01 0:30:57,2022-04-30 06:07:54.980435: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.,W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries.,COLLABORATOR
55797,elfringham,1113521556,2022-04-29 16:50:36,This should have been resolved by https://github.com/tensorflow/tensorflow/commit/a969f2bf9196b534f2acc47ca11e963c729bb80d,This should have been resolved by https://github.com/tensorflow/tensorflow/commit/a969f2bf9196b534f2acc47ca11e963c729bb80d.,CONTRIBUTOR
55505,mihaimaruseac,1113457426,2022-04-29 15:37:24,"If you are building a target from an external repo you have to use `@` syntax.
I think this is more and more becoming an issue for Bazel, not TF. There's no breakage that can be root caused to TF itself.","I think this is more and more becoming an issue for Bazel, not TF. There's no breakage that can be root caused to TF itself.",COLLABORATOR
55752,jpienaar,1113259844,2022-04-29 12:32:49,"SG, I didn't see any of those failing here (I mentioned the other set yesterday and those you seem to have fixed but kokoro pending).",I mentioned the other set yesterday and those you seem to have fixed but kokoro pending.,MEMBER
55752,lipracer,1113253902,2022-04-29 12:26:20,"I have resubmitted jax's [patch](https://github.com/google/jax/pull/10482), but it has not been merged yet.Integration testing may also be affected.","I have resubmitted jax's [patch](https://github.com/google/jax/pull/10482), but it has not been merged yet.",CONTRIBUTOR
55752,lipracer,1112856521,2022-04-29 4:04:54,"Yes. I just run with command:
```bazelisk test tensorflow/compiler/mlir:all --cache_test_results=no```
and all pass, It seem some test no run.","I just run with command: bazelisk test tensorflow/compiler/mlir:all --cache_test_results=no and all pass, It seem some test no run.",CONTRIBUTOR
55792,kaixih,1112640428,2022-04-28 20:47:39,Initial investigation shows that [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/remapper.cc#L401): the returned `input_props` of the conv2d node is an empty vector and further [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/costs/graph_properties.cc#L2833): the member `input_properties_` is also an empty map. vis. @nluehr @wenscarl,Initial investigation shows that [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/remapper.cc#L401): the returned input_props of the conv2d node is an empty vector and further [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/costs/graph_,CONTRIBUTOR
55655,bixia1,1112521031,2022-04-28 18:17:29,"Many tests failed, will email a log",Many tests failed.,CONTRIBUTOR
55610,wenscarl,1111404340,2022-04-27 19:35:55,"@cheshire > Actually do you want to send a patch disabling tf.where autoclustering?
Yes. `tf_xla_ops_to_cluster` serves a similar purpose but not quite the granularity we want. I would like to propose a `tf_xla_ops_cluster_blacklist` under `TF_XLA_FLAG` which takes comma separated op names like where or unique. Does that sound good?",tf_xla_ops_cluster_blacklist,CONTRIBUTOR
54374,bixia1,1111317104,2022-04-27 18:00:50,We discussed this at meetings. This solution doesn't work because we currently allow TF-TRT to perform conversion on functions.,This solution doesn't work because we currently allow TF-TRT to perform conversion on functions.,CONTRIBUTOR
55754,haozha111,1111245927,2022-04-27 16:59:49,"The Loss value from the initial training on Google Colab seems to end at 0.241, but it starts at 0.0003855.
Hi,
From your C++ code, I didn't see you load the initial checkpoint from the previous training in colab. This will cause the model to use random weights. Could you try calling the restore signature as described in this article?
https://www.tensorflow.org/lite/examples/on_device_training/overview#restore_the_trained_weights","""The Loss value from the initial training on Google Colab seems to end at 0.241, but it starts at 0.0003855.""",CONTRIBUTOR
55561,Yulv-git,1111187769,2022-04-27 16:08:59,"> This seems to have trouble landing. Can you split it into multiple PRs, one per top-level directory, please?
I have created multiple new PRs based on multiple branches to fix typos.",I have created multiple new PRs based on multiple branches to fix typos.,CONTRIBUTOR
55725,mihaimaruseac,1111103091,2022-04-27 14:54:08,We had some issues hitting the size limit on the `tensorflow-cpu` project. We asked for a size limit bump at https://github.com/pypa/pypi-support/issues/1870,We had some issues hitting the size limit on the tensorflow-cpu project.,COLLABORATOR
55561,Yulv-git,1110637654,2022-04-27 7:21:54,"> This seems to have trouble landing. Can you split it into multiple PRs, one per top-level directory, please?
I don't know how to split this PR into multiple PRs.
Perhaps, I can create multiple branches based on the current official latest master branch to create multiple new PRs.",I don't know how to split this PR into multiple PRs.,CONTRIBUTOR
55296,wangpengmit,1110437154,2022-04-27 1:35:48,"Hi @LJKS, thanks for reporting! I think there are two issues here: (1) TF doesn't support differentiating through variable writes; (2) your workaround doesn't work for its own reason. About (2), because `copy_gradient`'s first return value isn't used, `var_up` in `grad` will also be zeros.
I can't think of a working workaround though. So the current situation is that differentiating through variable writes is impossible in TF. We are exploring some ways to properly support it.",TF doesn't support differentiating through variable writes; (2) your workaround doesn't work for its own reason.,MEMBER
55730,mihaimaruseac,1110291230,2022-04-26 21:59:10,"So, windows GPU failure:
```
T:\src\github\tensorflow>CALL tensorflow\tools\ci_build\windows\gpu\pip\run.bat --enable_remote_cache ""T:\src\gfile\bazel_wrapper.py"" 1>tensorflow/tools/ci_build/builds/win.out 2>&1 ERROR: Aborting VM command due to timeout of 10800 seconds
```
This can be ignored, I see the other one passed, so we should be good.","So, windows GPU failure: ",COLLABORATOR
54650,christopherbate,1110264047,2022-04-26 21:24:59,"I fixed the issue. I also rebased on top of tree, but GitHub is showing extra commits from main branch","I also rebased on top of tree, but GitHub is showing extra commits from main branch.",CONTRIBUTOR
55655,bixia1,1110104681,2022-04-26 18:12:37,@[meena-at-work](https://github.com/meena-at-work) Would you please make the PR description more descriptive and put the examples outside the PR description?,"""Would you please make the PR description more descriptive and put the examples outside the PR description?""",CONTRIBUTOR
55561,mihaimaruseac,1110007779,2022-04-26 16:33:09,"This seems to have trouble landing. Can you split it into multiple PRs, one per top-level directory, please?","""Trouble landing""",COLLABORATOR
51429,mizabrik,1109913463,2022-04-26 15:06:31,"We faced an issue with tiled mean too; unfortunately, I can not provide a model either :(","We faced an issue with tiled mean too; unfortunately, I can not provide a model either.",CONTRIBUTOR
55752,jpienaar,1109838796,2022-04-26 14:03:26,"We don't expect it to be different with same input state no:
""The output is guaranteed to be a deterministic function of the initial state but it is not guaranteed to be deterministic between backends and different compiler versions.""
https://www.tensorflow.org/xla/operation_semantics#rngbitgenerator
If you wanted different numbers you have to use returned updated state from output of one as input to the other.","""The output is guaranteed to be a deterministic function of the initial state but it is not guaranteed to be deterministic between backends and different compiler versions.""",MEMBER
55747,tilakrayal,1109825492,2022-04-26 13:51:54,"@sachinprasadhs ,
I was facing different error while trying to execute the given code.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/238a0b0ad41c64e1592fc42511f932d2/55747.ipynb).","""I was facing different error while trying to execute the given code.""",CONTRIBUTOR
55657,reedwm,1109123881,2022-04-25 23:07:02,"The CI complains when a test with a timeout of ""long"" is added, but gives us the option of ignoring it when merging. Since you just split a test and didn't add a new test with a timeout of ""long"", we should ignore it. It does say that tests with a timeout of ""long"" are not automatically run by the CI, so it turns out `depthwise_conv_op_test` was not and is not run by the CI. We should probably fix this later.
There's also a merge conflict, but I'll manually resolve it.","The CI complains when a test with a timeout of ""long"" is added, but gives us the option of ignoring it when merging.",MEMBER
55745,stewartmiles,1109113289,2022-04-25 22:46:11,"Looks like some C99 (which, AFAIK, didn't appear until C++20) has crept into TF Lite.
https://github.com/tensorflow/tensorflow/blob/0cdb708bf66dde960e4ff9d6e8bf304d7d2b6d7c/tensorflow/lite/delegates/external/external_delegate.cc#L157","Looks like some C99 (which, AFAIK, didn't appear until C++20) has crept into TF Lite.",CONTRIBUTOR
55745,stewartmiles,1109111463,2022-04-25 22:42:24,"After fixing that issue, there is another problem:
```
external_delegate.cc
src\tensorflow\tensorflow\lite\delegates\external\external_delegate.cc(158,11): error C7555: use of designated initializers requires at least '/std:c++20' [src\tensorflow\tensorflow\lite\cmakebuild\tensorflow-lite.vcxproj]
```
has TF _really_ moved to C++20? It doesn't seem great for compatibility.","after fixing that issue, there is another problem",CONTRIBUTOR
55696,sachinprasadhs,1108959641,2022-04-25 19:33:08,"`embedding_lookup_sparse` is not registered with XLA, `XLA_TPU_JIT` and `XLA_GPU_JIT` as well.
As the error message suggests you can perform the unsupported OPs compilation in CPU by setting `tf.config.set_soft_device_placement(True)`.
In your case, since you are using only one operation inside tf.function, it makes sense if you use only eager mode, also `experimental_get_compiler_ir` is limited to XLA operations only.","embedding_lookup_sparse is not registered with XLA, XLA_TPU_JIT and XLA_GPU_JIT as well. As the error message suggests you can perform the unsupported OPs compilation in CPU by setting tf.config.set_soft_device_placement(True). In your case, since you are using only one operation inside tf.function, it makes sense if you use only eager mode, also experimental_get",CONTRIBUTOR
55725,EnricoMi,1108495403,2022-04-25 12:16:25,"@tilakrayal you probably run macOS with Python 3.7 or 3.8, or Linux with Python 3.7. All other Python ad OS versions do not work.","""All other Python ad OS versions do not work.""",CONTRIBUTOR
55664,lu-wang-g,1107007416,2022-04-22 23:09:08,"I think it's the same issue as https://github.com/tensorflow/tensorflow/issues/47595, where something went wrong during conversion. The model should only have 4 outputs instead of 8. Circle back to @karimnosseir to double check the conversion step.","I think it's the same issue as https://github.com/tensorflow/tensorflow/issues/47595, where something went wrong during conversion.",MEMBER
55686,bixia1,1106031966,2022-04-22 5:44:07,@DEKHTIARJonathan It is nice to show an example of the graph here. But would you please move the graph example out of the PR description and put it in a comment block?,"""It is nice to show an example of the graph here. But would you please move the graph example out of the PR description and put it in a comment block?""",CONTRIBUTOR
55207,sachinprasadhs,1105451967,2022-04-21 16:37:12,"It is mentioned as failed to dump `encapsulate_xla_computations` for before, halfway and after in the warning log, it means it is not making use of XLA. You can ignore these warnings.","Failed to dump encapsulate_xla_computations for before, halfway and after in the warning log.",CONTRIBUTOR
55700,nSircombe,1105038785,2022-04-21 10:37:21,"@penpornk - here's a PR to avoid the...
```
//tensorflow/core/kernels:no_mkldnn_contraction_kernel
Multiple matches are not allowed unless one is unambiguously more specialized.
```
...error when running `bazel test` on a build with `--config=mkl_aarch64`.
Is there any chance this could be cherry-picked for the next TF 2.9 RC?",...error when running bazel test on a build with --config=mkl_aarch64.,CONTRIBUTOR
55562,bhack,1104558310,2022-04-20 23:47:22,"@LukeWood too late, It was already merged. If @wangpengmit is ok with this additional API change I could eventually open a new one.","too late, It was already merged.",CONTRIBUTOR
55382,penpornk,1104231514,2022-04-20 17:40:48,"@gbaned The TF_AssignRefVariable PR just went in (https://github.com/tensorflow/tensorflow/pull/55640). As for this PR, it can't be merged until we investigate the failure of https://github.com/tensorflow/tensorflow/pull/53561. So I'm temporarily closing it until we have time to revisit.","The TF_AssignRefVariable PR just went in (https://github.com/tensorflow/tensorflow/pull/55640). As for this PR, it can't be merged until we investigate the failure of https://github.com/tensorflow/tensorflow/pull/53561. So I'm temporarily closing it until we have time to revisit.",MEMBER
55670,cantonios,1104100192,2022-04-20 15:56:11,"Hi @maxiwell, I just want to confirm this is actually what IBM wants. It was removed by default because it's broken on system-installed compilers on many systems (e.g. ubuntu), since it requires a specific ld version that we cannot detect from within Eigen. The thinking was that build systems might be able to detect this and turn on the flag if desired.","""It was removed by default because it's broken on system-installed compilers on many systems (e.g. ubuntu), since it requires a specific ld version that we cannot detect from within Eigen.""",CONTRIBUTOR
55621,gadagashwini,1103760271,2022-04-20 10:13:54,"@OrazioLombardi,
```
ERROR: C:/users/priva/tensorflow/tensorflow/core/kernels/BUILD:1222:18: C++ compilation of rule '//tensorflow/core/kernels:inplace_ops_gpu' failed (Exit 1): python.exe failed: error executing command
cd C:/users/priva/_bazel_priva/5lyc5377/execroot/org_tensorflow
```
Looks issue with MSVC2019.
Make sure you followed steps mentioned [here](https://www.tensorflow.org/install/source_windows) and install required packages. Thanks !",ERROR: C:/users/priva/tensorflow/tensorflow/core/kernels/BUILD:1222:18: C++ compilation of rule '//tensorflow/core/kernels:inplace_ops_gpu' failed (Exit 1): python.exe failed: error executing command cd C:/users/priva/_bazel_priva/5lyc5377/execroot/org_tensorflow  Looks issue with MS,CONTRIBUTOR
53396,apivovarov,1102976222,2022-04-19 18:50:18,Can it be backported to r2.7? (since the issue was introduced in v2.7.0 first),Can it be backported to r2.7? (since the issue was introduced in v2.7.0 first),CONTRIBUTOR
48084,mihaimaruseac,1102820907,2022-04-19 15:55:54,https://github.com/tensorflow/tensorflow/pull/48084#pullrequestreview-679861204 is still not addressed. Reviewing it now means giving the exact same comment and just wasting time.,"""still not addressed""",COLLABORATOR
46356,lu-wang-g,1102096708,2022-04-19 5:36:05,"Sorry that I just noticed this issue. Seems like it's due to that Sceneform repackaged FlatBuffers, which conflicts with the official Flatbuffer Java time. I'll loop in the Sceneform team to take a look at this issue.","""Seems like it's due to that Sceneform repackaged FlatBuffers, which conflicts with the official Flatbuffer Java time.""",MEMBER
55613,chsigg,1101586303,2022-04-18 17:25:18,Wouldn't this break [this](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/tools/toolchains/remote_config/rbe_config.bzl;drc=af542fe44674dd8c86159cd2f51782e75140ae8b;l=40) config running on e.g. sm_52?,Wouldn't this break [this](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/tools/toolchains/remote_config/rbe_config.bzl;drc=af542fe44674dd8c86159cd2f51782e75140ae8b;l=40) config running on e.g. sm_52?,CONTRIBUTOR
54484,edloper,1101519096,2022-04-18 15:55:56,"I'm not familiar with this code, so I'm probably not the right person to review it. But it looks like the original change was automatically rolled back because it caused a test in this file to fail: https://github.com/tensorflow/tensorflow/blob/v2.8.0/tensorflow/compiler/tests/image_ops_test.py. (Unfortunately, since it was rolled back several months ago, the logs from the failed test aren't available anymore.)","""I'm not familiar with this code, so I'm probably not the right person to review it.""",CONTRIBUTOR
55563,gadagashwini,1101187984,2022-04-18 7:50:02,"@ayaka14732,
Can you try to clear the bazel cache before building Tensorflow
`bazel clean --expunge`",Can you try to clear the bazel cache before building Tensorflow bazel clean --expunge,CONTRIBUTOR
55524,gadagashwini,1101042660,2022-04-18 2:53:53,"@zhucan, Tensorflow v2.8 has no `--config==verbs`.
Tensorflow v2.0 supports `--config==verbs`","""Tensorflow v2.8 has no --config==verbs.""",CONTRIBUTOR
55364,karimnosseir,1100233006,2022-04-15 17:04:42,"One more thing for pixel phones DSP access is disabled so it will not work.
If you have the Pixel phone rooted you can run these commands first
```
adb root
adb shell setenforce 0
```",DSP access is disabled so it will not work.,CONTRIBUTOR
55610,cheshire,1099596510,2022-04-14 20:32:19,"@wenscarl OK it might make sense to not compile tf.where in autoclustering environment altogether then. I can make this change.
> Say in a training loop, XLA will trigger compilation multiple times every time seeing a data with different size
Do you have a repro I could try? XLA compiles up to the upper bound of the `tf.where` output, not to the concrete bound seen at runtime, so it seems bizarre it would recompile.","""XLA compiles up to the upper bound of the tf.where output, not to the concrete bound seen at runtime, so it seems bizarre it would recompile.""",MEMBER
55610,wenscarl,1099523367,2022-04-14 18:46:30,"Thanks @cheshire. Making `tf.where` XLA compilable in autoclustering may results into a perf regression. Say in a training loop, XLA will trigger compilation multiple times every time seeing a data with different size. See the snapshot from profiler.
![recompile](https://user-images.githubusercontent.com/25590028/163456405-41e50e98-69b6-4d07-9410-9867cd067fd2.PNG)
Reverting this commit resolve this. I would prefer to have a switch flag to turn it off.","""I would prefer to have a switch flag to turn it off.""",CONTRIBUTOR
54771,jayfurmanek,1099196977,2022-04-14 13:38:36,"Ugh, note to self - just use buildifier even on small little format things that are easy to fix manually.
Sanity *should* pass now.
Sorry for the churn.","Ugh, note to self - just use buildifier even on small little format things that are easy to fix manually.",CONTRIBUTOR
55505,372046933,1099142065,2022-04-14 12:37:55,"@gadagashwini If I'm not mistaken, https://github.com/tensorflow/tensorflow/issues/21461#issuecomment-430449910 configures a local installed TF. Which is installed by pip. In my scenario, I was building TF wheel. Since TF is not installed, `cc_tf_configure` will fail.","""I'm not mistaken, https://github.com/tensorflow/tensorflow/issues/21461#issuecomment-430449910 configures a local installed TF. Which is installed by pip. In my scenario, I was building TF wheel. Since TF is not installed, cc_tf_configure will fail.""",CONTRIBUTOR
54973,leondgarse,1098681739,2022-04-14 3:50:38,"Just verified still exists in `TF 2.9.0-rc0`. Testing results updated in [tf_280_xla_test.ipynb](https://colab.research.google.com/drive/1LTVJ7jRRzsODzMuPB-svocV4jcbx1SYY?usp=sharing). For other versions, `TF 2.7.1` works, and `TF 2.8.0-rc0` throws error.",Just verified still exists in TF 2.9.0-rc0.,CONTRIBUTOR
55567,rthadur,1098507408,2022-04-13 21:31:29,"> Build is still failing. I don't think the fix is in Python, since the argument can be a list or a tensor or anything that can be converted to a tensor.
I agree ,any idea how we can fix this issue ?",Build is still failing.,CONTRIBUTOR
55567,mihaimaruseac,1098466303,2022-04-13 20:39:47,"Build is still failing. I don't think the fix is in Python, since the argument can be a list or a tensor or anything that can be converted to a tensor.",Build is still failing.,COLLABORATOR
52142,leandro-gracia-gil,1097561011,2022-04-13 5:02:30,"@MeghnaNatraj That limitation is absurd in the case where you have multiple signatures and some of them act as properties returning information about the model. It forces you to pass a tensor even if it's completely ignored inside.
Would it be possible to consider adding support for this?","""That limitation is absurd in the case where you have multiple signatures and some of them act as properties returning information about the model.""",CONTRIBUTOR
55567,rthadur,1097280979,2022-04-12 22:14:06,"> Please use a proper commit message / PR title, not `Update <file>`. See https://cbea.ms/git-commit/
Sorry , updated description.","""Please use a proper commit message / PR title, not Update file>.""",CONTRIBUTOR
55415,sshahrokhi,1097237814,2022-04-12 21:29:08,"@michaelbanfield could you please review this PR? It is changing where JAX is loading the libtpu, and removing executor file which was the light weight version of tpu_api_dlsym_initializer.","""It is changing where JAX is loading the libtpu, and removing executor file which was the light weight version of tpu_api_dlsym_initializer..""",CONTRIBUTOR
55562,bhack,1096789258,2022-04-12 14:16:44,`feedback/copybara` is failing again.,feedback/copybara is failing again.,CONTRIBUTOR
55505,372046933,1095835413,2022-04-12 2:13:48,"@gadagashwini Added `--symlink_prefix=/` to `bazel build`. But nothing changed. My build command is
```bash
bazel build --config=native_arch_linux --config=cuda --symlink_prefix=/ @org_tensorflow//tensorflow/tools/pip_package:build_pip_package
```
By the way, did you notice that I am building TF as an external dependency. i.e. `@org_tensorflow//tensorflow/tools/pip_package:build_pip_package` not `//tensorflow/tools/pip_package:build_pip_package`",--symlink_prefix=/,CONTRIBUTOR
55582,mihaimaruseac,1095757707,2022-04-12 1:04:29,"TF 1.15 is too old. When it was released there was no python 3.8.
This is not a TF issue.",TF 1.15 is too old.,COLLABORATOR
55332,DEKHTIARJonathan,1095494767,2022-04-11 19:48:03,"> @DEKHTIARJonathan I'm not sure that `_experimental_feature_scope` should be in `tf_trt_integration_test_base.py`. I need it in `trt_convert_test.py`, so it would make sense to put it in a separate `test_utils.py` or similar file.
Done
@bixia1: all comments have been addressed. Please review","""I need it in trt_convert_test.py, so it would make sense to put it in a separate test_utils.py or similar file.""",CONTRIBUTOR
55530,awf,1095023202,2022-04-11 13:03:04,"Still present in 55645c from 2 days ago.
I'll take a look, but I don't see anything more recent that might have fixed it.",Still present in 55645c from 2 days ago.,CONTRIBUTOR
55564,tilakrayal,1094654733,2022-04-11 7:36:04,"Hello @NathFarinha237 ,
On running the given code snippet, I am facing an error stating `OSError: File does not exist. Received: {filename}`. Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/fb61ab0f893d14efb33aa26335ed11ea/untitled297.ipynb).","""On running the given code snippet, I am facing an error stating OSError: File does not exist. Received: filename.""",CONTRIBUTOR
55546,tilakrayal,1094574485,2022-04-11 5:41:19,"Hello @markub3327 ,
I tried to execute the given code snippet, I am facing different error stating. Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/773ef1a7f6105767925cd885691e4c21/untitled296.ipynb).",I am facing different error stating.,CONTRIBUTOR
55530,awf,1094062446,2022-04-09 15:05:37,"Update: test still fails on commit `55645ca964508507890529a71591f51a344a6356` April 9
test passes with ``--config=opt``","""test still fails on commit 55645ca964508507890529a71591f51a344a6356 April 9 test passes with --config=opt""",CONTRIBUTOR
55554,reedwm,1093353354,2022-04-08 20:50:36,"Calling `event_mgr->ThenExecute` effectively causes a GPU synchronization to occur before the op after SparseToDense can execute, which perhaps is causing some race condition to not occur, fixng the segfault. I don't think SparseToDense is the cause though.
If you give us an example to reproduce, we might be able to find where the segfault is coming from. But if the example is too large, there's a good chance we won't have time to debug it, unfortunately.","Calling event_mgr->ThenExecute effectively causes a GPU synchronization to occur before the op after SparseToDense can execute, which perhaps is causing some race condition to not occur, fixng the segfault.",MEMBER
55554,nluehr,1093336849,2022-04-08 20:36:15,Prior to this fix we observed intermittent CUDA segfaults during training when validate_indices was false. (this was in a TF1 model with identical SparseToDense GPU implementation),"""CUDA segfaults during training when validate_indices was false""",CONTRIBUTOR
55553,mihaimaruseac,1093015809,2022-04-08 15:38:33,Please fix more than one typo per file. There are several hours of CI that run due this one letter addition.,"""There are several hours of CI that run due this one letter addition.""",COLLABORATOR
55505,372046933,1092476009,2022-04-08 6:11:43,"@tilakrayal No, it's not resolved. Cannot execute ```
./bazel-bin/external/org_tensorflow/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
```",Cannot execute  ./bazel-bin/external/org_tensorflow/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg .,CONTRIBUTOR
54173,impjdi,1092156041,2022-04-07 20:09:33,"> Qualcomm's OpenCL can be more lax than NVidia's when it comes undefined behaviors at runtime
This is absolutely true. We do cut corners and rely on undefined behavior of the supported platforms, e.g. out of bound reads etc.",Qualcomm's OpenCL can be more lax than NVidia's when it comes undefined behaviors at runtime,CONTRIBUTOR
55507,mihaimaruseac,1091964167,2022-04-07 16:35:41,"Yes, but these are just placeholders, they don't mean anything in the rich view","""these are just placeholders, they don't mean anything in the rich view""",COLLABORATOR
55507,mihaimaruseac,1091839228,2022-04-07 14:52:04,"This is just a snippet. The space doesn't bring anything.
When making typo fixes PRs please fix through the entire file/directory.",This is just a snippet. The space doesn't bring anything. When making typo fixes PRs please fix through the entire file/directory.,COLLABORATOR
55363,bhack,1091809458,2022-04-07 14:27:07,`feedback/copybara` is failing again,feedback/copybara is failing again.,CONTRIBUTOR
55200,bhack,1091764964,2022-04-07 13:50:09,"> Apparently that one is not triggered by the kokoro label but by gitHub buttons
Yes but I don't have the permission on this repo",Apparently that one is not triggered by the kokoro label but by gitHub buttons,CONTRIBUTOR
55425,bhack,1091643474,2022-04-07 11:54:21,@gadagashwini This is something else and it has no associated PR. Please check the mentioned @wangpengmit's comment at https://github.com/tensorflow/tensorflow/pull/55192#issuecomment-1081306349,This is something else and it has no associated PR.,CONTRIBUTOR
55475,foxik,1091104181,2022-04-07 5:45:27,Closing as a duplicate of #46635 .,Closing as a duplicate of #46635 ..,CONTRIBUTOR
55475,foxik,1091103965,2022-04-07 5:44:54,"Oh, I was just pointed to me (by djoshea) that this is a duplicate of 46635, so closing.","I was just pointed to me (by djoshea) that this is a duplicate of 46635, so closing.",CONTRIBUTOR
53239,plopresti,1091049257,2022-04-07 3:48:20,"I encountered the same issue. I suspect it is the same problem the Chromium folks had:
https://codereview.qt-project.org/c/yocto/meta-qt6/+/364505/2/recipes-qt/qt6/chromium-gn.inc#38
Removing ""-g"" from the build options fixed it for me.",I encountered the same issue.,CONTRIBUTOR
55517,mihaimaruseac,1090939714,2022-04-06 23:59:59,"Please use a better title and commit message: https://cbea.ms/git-commit/
The commas are instances of what is called an oxford comma, they are ok as they are.
When fixing typos, please fix all in a file/directory, instead of just a few. We need hours of CI to test each change, so let's try to minimize the CI hours/letters changed metrics :)
Thank you for your contribution, but we cannot accept this one.","""We need hours of CI to test each change, so let's try to minimize the CI hours/letters changed metrics :"")",COLLABORATOR
55365,psobot,1090431667,2022-04-06 15:53:32,+1 to @f90's comment - this patch would still result in misleading documentation and would not adequately resolve https://github.com/tensorflow/tensorflow/issues/55290.,+1 to @f90's comment - this patch would still result in misleading documentation and would not adequately resolve https://github.com/tensorflow/tensorflow/issues/55290.,CONTRIBUTOR
55509,mihaimaruseac,1090423880,2022-04-06 15:45:57,Please don't spam with bad PRs. We never merge release branches back into main branch,Spam with bad PRs.,COLLABORATOR
55200,mihaimaruseac,1089434944,2022-04-05 22:17:40,Apparently that one is not triggered by the kokoro label but by gitHub buttons,Apparently that one is not triggered by the kokoro label but by gitHub buttons.,COLLABORATOR
55281,jurahul,1089361506,2022-04-05 21:16:39,"Yes, I am looking into this. There are build failures from this change in internal code, which we need to fix on our side before this can be merged.",build failures from this change in internal code,CONTRIBUTOR
55435,bhack,1089245338,2022-04-05 19:40:53,"I don't know if you can minimize your example a bit but the output names are
`print(my_model.output_names)`
`['model_1', 'model_2', 'model_2_1']`","I don't know if you can minimize your example a bit but the output names are print(my_model.output_names) ['model_1', 'model_2', 'model_2_1'].",CONTRIBUTOR
55351,SandSnip3r,1089227166,2022-04-05 19:25:03,@philipphack It was again confirmed that this change caused drastically increased memory usage (out-of-memory errors). We are investigating.,"""This change caused drastically increased memory usage (out-of-memory errors).""",CONTRIBUTOR
54491,yaochengji,1089042941,2022-04-05 17:01:36,"Hi @joker-eph, The `AMD ROCm CI` complained about ```
ERROR: The project you're trying to build requires Bazel 5.1.0 (specified in /workspace/.bazelversion), but it wasn't found in /usr/local/lib/bazel/bin.
```
I don't think it is related to my fix. Could you help take a look at it?","""I don't think it is related to my fix.""",CONTRIBUTOR
55200,bhack,1088985662,2022-04-05 16:29:32,"@mihaimaruseac Something is not working. The PyLint Action was executed ( failed <relative-time datetime=""2022-03-30T20:26:16Z"" class=""no-wrap"">Mar 30, 2022</relative-time> in 43s )",Something is not working.,CONTRIBUTOR
55201,cantonios,1088879263,2022-04-05 15:35:18,You can submit a PR for that if you like. This is not unique to clip ops - all tensors are restricted by this limitation.,This is not unique to clip ops - all tensors are restricted by this limitation.,CONTRIBUTOR
55379,penpornk,1088328221,2022-04-05 6:48:25,"@PatriceVignola We only run TensorFlow presubmit tests before merging. After merging, PRs could still be reverted if they broke TensorFlow nightly tests or other internal tests (e.g., unit / integration tests of internal applications that use TensorFlow). In this case, it broke an internal test.","""In this case, it broke an internal test.""",MEMBER
55135,MeghnaNatraj,1088039952,2022-04-04 21:41:04,"> [tensorflow/model-optimization#775 (comment)](https://github.com/tensorflow/model-optimization/issues/775#issuecomment-894908619) this comment mentioned that TF2 converter does not support `uint8` quantization any more. Is this the reason why setting `converter.inference_type=uint8` does not help?
Yes, you're right. TF2 converter does not support `uint8` quantization.",TF2 converter does not support uint8 quantization any more.,MEMBER
55206,wangpengmit,1087977707,2022-04-04 20:22:07,Seems your Python paths are not set up properly. I'll try fixing the CI errors internally.,I'll try fixing the CI errors internally.,MEMBER
55317,bhack,1087873316,2022-04-04 18:23:48,"@mdanatg I suppose this was merged with b462db3b7482ff8e52b13ca686f2bca4508a6910. But I have totally lost my attribution on git:
`git show -s --format=""%ae"" b462db3` `gardener@tensorflow.org`","I have totally lost my attribution on git: git show -s --format=""%ae"" b462db3",CONTRIBUTOR
55306,npanpaliya,1087803472,2022-04-04 17:08:32,"@akuegel - Yes, LLVM fix can go directly into llvm-project too. But adding ppc64le support in LLVM will need more work and verification. For TF to compile, only this change is needed in llvm and hence I added a patch in TF itself (TF follows this approach anyway).
I'm working on llvm-project too but it will take time. Hence raised this PR to TF to fix the builds.","""Adding ppc64le support in LLVM will need more work and verification""",CONTRIBUTOR
55201,cantonios,1087650076,2022-04-04 14:46:12,"@DLFrameworkBug yes, the total number of elements in the tensor must be less than `int64` max. Here when trying to compute the new number of elements to add, it overflows to negative. We just don't support such large shapes.","""We just don't support such large shapes.""",CONTRIBUTOR
48213,mohantym,1087113615,2022-04-04 4:51:46,Yeah. It is replicating when saved without h5 extension.,It is replicating when saved without h5 extension.,CONTRIBUTOR
55470,dansuh17,1086495558,2022-04-02 2:19:31,Duplicate of #55464 .,Duplicate of #55464 ..,MEMBER
55206,philipphack,1086442491,2022-04-02 0:58:48,This fails with `ModuleNotFoundError: No module named 'tensorflow.core.kernels'`.,This fails with ModuleNotFoundError: No module named 'tensorflow.core.kernels'.,CONTRIBUTOR
55453,bhack,1086335007,2022-04-01 21:20:47,I've just edited this file on github directly. Who have renamed the file? copybara?,"""Who have renamed the file?""",CONTRIBUTOR
55453,mihaimaruseac,1086040515,2022-04-01 15:28:24,"Oh, this accidentally renames the file :(",:(,COLLABORATOR
54650,christopherbate,1085919792,2022-04-01 13:44:20,"This pr says changes were requested, but nothing has been requested as far as I can see. @bixia1 ?","""Nothing has been requested as far as I can see.""",CONTRIBUTOR
55464,pshiko,1085900361,2022-04-01 13:28:15,"my pr is failed because this problem. https://github.com/tensorflow/tensorflow/pull/54455
@gbaned",my pr is failed because this problem.,CONTRIBUTOR
55375,gadagashwini,1085369917,2022-04-01 3:12:28,"@nirnayr, `tf.sysconfig.get_link_flags()`. returns ```
['-L/usr/local/lib/python3.7/dist-packages/tensorflow',
'-l:libtensorflow_framework.so.2'] ```
Here, `library directory` is `-L` and `library` is `-l`. Now replace the `-L` and `-l` with `library_dirs` and `libraries `arguments to your Extension object in `setup.py`.
Finally, set your `LD_LIBRARY_PATH` environment variable to point to the directory with the `libtensorflow_framework.so` library is located.","tf.sysconfig.get_link_flags(). returns ['-L/usr/local/lib/python3.7/dist-packages/tensorflow', '-l:libtensorflow_framework.so.2'] ",CONTRIBUTOR
55206,wangpengmit,1085270878,2022-04-01 0:41:47,There are some `api_compatibility_test` CI failures. Please run `bazel run //tensorflow/tools/api/tests:api_compatibility_test -- --update_goldens True` to update the API golden files.,api_compatibility_test CI failures.,MEMBER
55379,PatriceVignola,1085081161,2022-03-31 20:37:34,"The build failed again without a ""Details"" link. Is there anything I can do to expedite this PR? I successfully built it and ran the tests locally but maybe I'm missing something. Is there a way to confirm whether it is an infra issue or not?","The build failed again without a ""Details"" link.",CONTRIBUTOR
46833,wangpengmit,1084960115,2022-03-31 18:31:58,"Hi @miguelusque, you've hit a long-running issue internally known as ""the int32 problem"". Basically TF carves out int32 as ""the shape dtype"", to workaround some design problems of the device-placement system. We're designing potential alternatives but it's hard to fix. At the moment just don't use int32 for your normal computation (use e.g. int64 or uint32 instead).","""the int32 problem""",MEMBER
54455,pshiko,1084631863,2022-03-31 14:03:30,"When I merged the latest master again and pushed it, it is error with lint.
The parts pointed out in this lint are different from my changes, should I fix them?","The parts pointed out in this lint are different from my changes, should I fix them?",CONTRIBUTOR
55441,reedwm,1084044136,2022-03-31 3:49:45,"A somewhat hacky way to get the device is to create an empty tensor and immediately query its device: ```
device = tf.constant([], dtype=tf.float32).device
```
@rohan100jain do you know of a better way?
I don't think the fact opening a graph scope clears the device scope is a bug, since each graph has a different device scope, although I admit this behavior is unintuitive. In any case, since this only affects the TF1 API (graphs), I doubt this behavior would ever get fixed.","""I don't think the fact opening a graph scope clears the device scope is a bug, since each graph has a different device scope""",MEMBER
55381,penpornk,1084011388,2022-03-31 2:38:27,This PR got reverted because it broke internal tests. Will add this to the list of `DEVICE_DEFAULT` ops to revisit / investigate later.,"""PR got reverted because it broke internal tests""",MEMBER
55379,PatriceVignola,1083528851,2022-03-30 19:16:26,"I'm not sure why the Windows Bazel GPU build is failing. I built it locally yesterday and it completed just fine, and the Linux GPU build completed successfully. Is it possible to look at the failures or is it unrelated to this PR?",I'm not sure why the Windows Bazel GPU build is failing.,CONTRIBUTOR
55431,DEKHTIARJonathan,1083455019,2022-03-30 18:09:07,"Original Error:
```bash
2022-03-29 19:38:35.717118: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger 4: [network.cpp::validate::2647] Error Code 4: Internal Error (Repeated layer name: TRTEngineOp_000_002/StatefulPartitionedCall/mrcnn/multilevel_crop_and_resize/stack-slice_2:SLICE (layers must have distinct names))
```",bash 2022-03-29 19:38:35.717118: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40],CONTRIBUTOR
55428,drivanov,1083383690,2022-03-30 16:52:50,"> [fail1.log](https://github.com/tensorflow/tensorflow/files/8374305/fail1.log)
FIXED
Sorry, while resolving a merge conflict online, I didn't notice that the line `template <typename T>` disappeared from ```
template <typename T>
class OpConverter_BinaryTest
```",template typename T> disappeared from template typename T> class OpConverter_BinaryTest .,CONTRIBUTOR
54650,bixia1,1083319422,2022-03-30 15:54:00,"I understand how this PR may speed up the BUILD of multiple models, but for inference, we actually want to get the best performance for the engines and sharing a single auto tuning cache can hurt performance (because except for one engine, the rest engines aren't using their true input for auto tune?).
What would you said about this?","""Because except for one engine, the rest engines aren't using their true input for auto tune?""",CONTRIBUTOR
55432,tilakrayal,1083096366,2022-03-30 12:46:38,"@chunduriv ,
I was able to reproduce the issue in tf v2.8 and [nightly](https://colab.research.google.com/gist/tilakrayal/cb4fb69f9096a4a2260acae5fd5fb9e7/nigtlytf_issues_fails_to_load_mobilenet_weights_in_2_8_but_not_2_7.ipynb), whereas in [v2.7](https://colab.research.google.com/gist/tilakrayal/182544e20eeb1d103f8d2cea85bdd781/tf_issues_fails_to_load_mobilenet_weights_in_2_8_but_not_2_7.ipynb) i was able to execute without facing any issue.Please find the gist.","""I was able to reproduce the issue in tf v2.8 and [nightly](https://colab.research.google.com/gist/tilakrayal/cb4fb69f9096a4a2260acae5fd5fb9e7/nigtlytf_issues_fails_to_load_mobilenet_weights_in_2_8_but_not_2_7.ipynb), whereas in [v2.7](https://",CONTRIBUTOR
55332,Nyrio,1082868033,2022-03-30 9:47:53,"@DEKHTIARJonathan I'm not sure that `_experimental_feature_scope` should be in `tf_trt_integration_test_base.py`.
I need it in `trt_convert_test.py`, so it would make sense to put it in a separate `test_utils.py` or similar file.","I'm not sure that _experimental_feature_scope should be in tf_trt_integration_test_base.py. I need it in trt_convert_test.py, so it would make sense to put it in a separate test_utils.py or similar file.",CONTRIBUTOR
55332,bixia1,1082455931,2022-03-29 23:09:52,@DEKHTIARJonathan there are still unaddressed comments.,unaddressed comments,CONTRIBUTOR
55067,mihaimaruseac,1082320476,2022-03-29 20:01:46,"WSL is slow in reading files. There are a lot of files that need to be loaded when `import tensorflow as tf` gets executed.
This is likely not a TF issue.",WSL is slow in reading files.,COLLABORATOR
54714,mihaimaruseac,1082013875,2022-03-29 15:25:07,"Linking large binaries is supposed to take a lot of time. Please also watch memory usage, in case it uses swap (which results in even more slowdown).",Linking large binaries is supposed to take a lot of time.,COLLABORATOR
54279,edwardyehuang,1081993755,2022-03-29 15:10:03,"> Is it just the gradient that's slow, or the entire convolution?
> > IIRC we simply extend the filter (filled with zeros) to match the dilation, so a large dilation would lead to large filters/gradients. How ""large"" are we talking about?
Slow gradient only",Slow gradient.,CONTRIBUTOR
51765,mihaimaruseac,1081981909,2022-03-29 15:00:54,Still needs a unit test before we can take it,Still needs a unit test before we can take it.,COLLABORATOR
55220,gadagashwini,1081347001,2022-03-29 2:55:20,"@algoteam5,
Seems like issue is with custom loss function. Since evaluate method takes x_test and y_test
`results = a.evaluate([x_test_1, x_test_2], labels_test)`
Change this to `results = a.evaluate([x_test_1,x_test_2])`","""Seems like issue is with custom loss function.""",CONTRIBUTOR
55317,bhack,1081295319,2022-03-29 1:03:31,Copybara again,"""Copybara again""",CONTRIBUTOR
55382,PatriceVignola,1081272514,2022-03-29 0:14:17,"@penpornk Since it doesn't look like the `TF_AssignRefVariable` PR will be able to make it in TF 2.9, is there a possibility to prioritize the review of this PR for early 2.10 nightly builds, maybe by the end of the week? We want to at least be able to point users to nightly builds if they want to run the benchmarks that still use those ops.","TF_AssignRefVariable PR will be able to make it in TF 2.9, is there a possibility to prioritize the review of this PR for early 2.10 nightly builds, maybe by the end of the week?",CONTRIBUTOR
55413,skye,1081269293,2022-03-29 0:10:29,"Marking ""ready to pull"" so I can run internal tests, not actually ready for submission.","""ready to pull"" so I can run internal tests, not actually ready for submission.",MEMBER
54591,Nyrio,1081017098,2022-03-28 18:50:55,Closing due to CLA issues.,Closing due to CLA issues.,CONTRIBUTOR
54591,bixia1,1080855525,2022-03-28 16:22:59,"@Nyrio Could you please recreate this PR? I am trying to manually merge this, but got error "" WARN: Cannot migrate http://github.com/tensorflow/tensorflow/pull/54591 because the following check runs have not been passed: [cla/google]"".
Here is what I was told:
Mihai Maruseac, 2 min
Hi. CLA process changed since february
I think best is for author to recreate the PR
I cannot seem to be able to trigger the new CLA process on the PR","""I cannot seem to be able to trigger the new CLA process on the PR.""",CONTRIBUTOR
55272,mihaimaruseac,1080772220,2022-03-28 15:07:02,"Currently blocked on ""import/copybara Pending  Waiting for internal safe review approval"". Needs another eye on review there.
Windows builds were broken last week due to LLVM updates, but now they should be ok.","Currently blocked on ""import/copybara Pending  Waiting for internal safe review approval""",COLLABORATOR
55206,mihaimaruseac,1080770849,2022-03-28 15:05:53,"Rerunning the pylint job, seems to have been using an old version?
Anyway, if still broken, please ignore.","""Rerunning the pylint job, seems to have been using an old version""",COLLABORATOR
55399,mohantym,1080494426,2022-03-28 10:48:54,"Hi Sachin! Could you please look at this issue? It is replicating in [2.7](https://colab.sandbox.google.com/gist/mohantym/b6d40ef800f27a5853c515fbf9104629/github_55399.ipynb#scrollTo=HBHgb24w3V3l) ,[ 2.8](https://colab.sandbox.google.com/gist/mohantym/69634a06b55281b56cb5f105e6152ab8/github_55399.ipynb#scrollTo=-DZ4o0WaCDIZ) and[ nightly](https://colab.sandbox.google.com/gist/mohantym/48c3236fb6c45083d7ebc6fb56de68df/github_55399.ipynb#scrollTo=sGje8x42Frfn).","""It is replicating in [2.7](https://colab.sandbox.google.com/gist/mohantym/b6d40ef800f27a5853c515fbf9104629/github_55399.ipynb#scrollTo=HBHgb24w3V3l) ,[ 2.8](https://colab.sandbox.google.com/gist/mohantym/69634a06b55",CONTRIBUTOR
55377,gadagashwini,1080361415,2022-03-28 8:41:02,"@klorinczi,
> RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd
This looks like numpy version 1.19.5 and TensorFlow 2.4 not playing well together. Use `numpy ~= 1.19.2`",RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd,CONTRIBUTOR
52222,gadagashwini,1080125441,2022-03-28 2:53:15,"@mrnucleation, This issue can be due to several reasons, 1. out of memory: Limit the GPU memory growth
2. configuration Mismatch with CUDA/cuDNN
3. remove the cache folder ~/.nv/","""Can be due to several reasons, 1. out of memory: Limit the GPU memory growth 2. configuration Mismatch with CUDA/cuDNN 3. remove the cache folder /.nv/.""",CONTRIBUTOR
54379,penpornk,1080014024,2022-03-27 20:35:00,"It seems we still got the [Ubuntu CPU](https://source.cloud.google.com/results/invocations/87b01a5b-8b87-415d-bf05-864eeff01ac6) error
```
tensorflow/core/kernels/mkl/mkl_fused_ops_test.cc:794
Expected equality of these values:
::tensorflow::Status::OK()
Which is: OK
(RunOpKernel())
Which is: ABORTED: Operation received an exception:Status: 3, message: could not create a primitive descriptor iterator, in file tensorflow/core/kernels/mkl/mkl_conv_ops.cc:893
```",,MEMBER
55382,penpornk,1079962736,2022-03-27 16:14:34,@PatriceVignola I don't think `TF_AssignRefVariable` will make it. It's too short notice and it touches C API (but at least it's in experimental). Branch cut is this coming Monday (3/28) or Tuesday (3/29) if Monday's nightly isn't clean. I can check with @wangpengmit though.,"""It's too short notice and it touches C API (but at least it's in experimental). Branch cut is this coming Monday (3/28) or Tuesday (3/29) if Monday's nightly isn't clean.""",MEMBER
55272,Gelesh,1079742977,2022-03-26 17:52:16,"@mihaimaruseac , any thought on the internal CI fail Internal",CI fail,CONTRIBUTOR
55206,philipphack,1079524099,2022-03-26 0:00:32,"I can't reproduce the PyLint error, and I don't see any lines that are longer than 80 characters in the diff.","I can't reproduce the PyLint error, and I don't see any lines that are longer than 80 characters in the diff.",CONTRIBUTOR
51439,bhack,1079339288,2022-03-25 19:04:03,"Note: with the Codespace ""free tier"" available disk space (individual account) you will have a `no space left on device` cause with our manylinux/docker refactory effort we still have only a CUDA embedded image that is very large to run on the ""free tier"". See more at https://github.com/tensorflow/build/pull/47",No space left on device,CONTRIBUTOR
55342,mihaimaruseac,1079170507,2022-03-25 16:01:41,"Can you try to fix multiple typos in the same file/directory please? As you see, the CI time is significant (and this is ~1/3rd of the total CI if we take into account the internal CI too). We're trying to minimize the hours_of_ci/delta_of_change metric",Can you try to fix multiple typos in the same file/directory please?,COLLABORATOR
55132,mihaimaruseac,1079157621,2022-03-25 15:47:06,I already changed the affected versions on GitHub but it didn't seem to propagate.,I already changed the affected versions on GitHub but it didn't seem to propagate.,COLLABORATOR
55363,bhack,1079114483,2022-03-25 15:00:13,"Sorry but my local `bazel` cache in the container was invalidated again. I could do some ""blind edit"" if you want but we need to abuse the CI in the meantime.","""I could do some ""blind edit"" if you want but we need to abuse the CI in the meantime.""",CONTRIBUTOR
55334,bhack,1078549950,2022-03-25 0:54:44,"@piEsposito This is a very well old know issue and `decode_image`, `decode_jpg` and `decode_gif` are ""unfied"" you need to use `expand_animations = False`.
As you can see, the thread is active since 2017 https://github.com/tensorflow/tensorflow/issues/9356#issuecomment-298469582","""unfied""",CONTRIBUTOR
55138,SeeForTwo,1078063227,2022-03-24 19:48:25,"> Provide an argument to choose the DCT method in tf.image.adjust_jpeg_quality
Do you have any interest in contributing a PR for this?
> at least mention this caveat in the method's documentation.
I can add something to the documentation and add an example without clipping.
> Use the accurate integer DCT by default.
Changing the default (which would change behavior for existing users) is unlikely to be accepted.","""Changing the default (which would change behavior for existing users) is unlikely to be accepted.""",MEMBER
55353,DEKHTIARJonathan,1076968160,2022-03-24 1:08:57,"Currently TF-TRT may reject Slice OPs during the conversion phase, which obviously leads to:
```bash
2022-03-24 01:00:01.700857: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:1055] TF-TRT Warning: Engine creation for TRTEngineOp_0_2 failed. The native segment will be used instead. Reason: INTERNAL: tensorflow/compiler/tf2tensorrt/convert/ops/slice_ops.cc:116 TRT_ENSURE failure
```
CC: @tfeher","Currently TF-TRT may reject Slice OPs during the conversion phase, which obviously leads to: bash 2022-03-24 01:00:01.700857: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:1055] TF-TRT Warning: Engine creation for TRTEngineOp_0_2 failed. The native segment will be used instead. Reason: INTERNAL: tensorflow/compiler/t",CONTRIBUTOR
55350,bhack,1076821487,2022-03-23 21:07:28,"@mdanatg I've tried to insert `f = autograph.tf_convert(f, autograph_ctx.control_status_ctx())` there but it is failing with the same error","I've tried to insert f = autograph.tf_convert(f, autograph_ctx.control_status_ctx()) there but it is failing with the same error.",CONTRIBUTOR
55306,npanpaliya,1076373925,2022-03-23 13:26:23,I don't see an option to re-trigger the checks. Could someone please do it for me?,I don't see an option to re-trigger the checks.,CONTRIBUTOR
50744,gadagashwini,1075857993,2022-03-23 2:44:36,"@MisRight,
> ERROR: G:/tensorflow-r2.4.0-gpu/tensorflow-r2.4/tensorflow/compiler/xla/BUILD:423:1: C++ compilation of rule '//tensorflow/compiler/xla:literal' failed (Exit 2): python.exe failed: error executing command
This issue is related to Python and Numpy version compatibility issue. Tensorflow v2.4.1 source incompatible with Numpy version >=1.20.1. Tensorflow 2.4 support numpy ~= 1.19.5.
`pip install numpy==1.19.5`",ERROR: G:/tensorflow-r2.4.0-gpu/tensorflow-r2.4/tensorflow/compiler/xla/BUILD:423:1: C++ compilation of rule '//tensorflow/compiler/xla:literal' failed (Exit 2): python.exe failed: error executing command,CONTRIBUTOR
55305,mihaimaruseac,1075477045,2022-03-22 18:23:50,"Reproduced
```
In [6]: tf.strings.unsorted_segment_join(inputs=['123'],segment_ids=[0],num_segments=-1)
F0322 11:23:25.727334 1107260 tensor_shape.cc:396] Check failed: size >= 0 (-1 vs. 0) ```","In [6]: tf.strings.unsorted_segment_join(inputs=['123'],segment_ids=[0],num_segments=-1) F0322 11:23:25.727334 1107260 tensor_shape.cc:396] Check failed: size >= 0 (-1 vs. 0) ",COLLABORATOR
55318,mihaimaruseac,1075436905,2022-03-22 17:43:56,Please stop spamming with github actions or we might need to ban you from contributing to this repository.,Spamming with github actions.,COLLABORATOR
53065,mohantym,1075085088,2022-03-22 11:56:10,Hi @mihaimaruseac ! Shall we close this pull request? it has been going through a lot of loops for a long time now. I am afraid that it might disturb the peace of processes happening here.,"""it has been going through a lot of loops for a long time now.""",CONTRIBUTOR
55206,wangpengmit,1074569394,2022-03-22 0:59:16,I don't like that `ReadVariableOp` does this COR-to-COW conversion as a side effect (even if it only happens when `no_copy` is true). How about adding a new op `TurnOffCopyOnRead` to do this job?,I don't like that ReadVariableOp does this COR-to-COW conversion as a side effect (even if it only happens when no_copy is true). How about adding a new op TurnOffCopyOnRead to do this job?,MEMBER
55132,mihaimaruseac,1074458792,2022-03-21 21:54:13,"I was OOO for the entire period this issue existed
Seems there was a typo when the GH advisories were created",I was OOO for the entire period this issue existed,COLLABORATOR
55303,API92,1074178717,2022-03-21 17:15:19,"@tilakrayal , you should enter path to your bucket in Google Cloud Storage, because tensorflow doesn't support storing checkpoints locally on TPU. This folder may be empty. For example, I used this gs://dc795ab9-90a2-4cb9-b1dd-badcf556350b/init_scope_replica_id , but it deleted now. There is no other code dependencies for notebooks.","""should enter path to your bucket in Google Cloud Storage, because tensorflow doesn't support storing checkpoints locally on TPU""",CONTRIBUTOR
55278,bhack,1074026370,2022-03-21 15:15:30,"> I admit it's frustrating :( Most PRs rely on the internal build system, making it hard to get traction to improve the external one.
It is one of the ""chicken or the egg"" problems we have for the community growing.
I will ping you when the compilation is done.","""It is one of the ""chicken or the egg"" problems we have for the community growing.""",CONTRIBUTOR
55262,tilakrayal,1073884908,2022-03-21 13:16:50,"@DagonArises ,
I do not have access to the link you have provided. Could you please provide the required permissions to view the files.",I do not have access to the link you have provided.,CONTRIBUTOR
55278,bhack,1073853525,2022-03-21 12:49:13,"> Yes. When evaluating `a < b < c` Python calls something in the lines of `(a < b).__bool__()`, which only works in eager mode.
Yes honestly it is so hard to explain this kind of underline machinery to our users/developers. I needed to point another user to the `Cpython` source for a [""similar"" case](https://discuss.tensorflow.org/t/you-cannot-build-your-model-by-calling-build-if-your-layers-do-not-support-float-type-inputs/8356/6)","""When evaluating a  b  c Python calls something in the lines of (a  b).__bool__(), which only works in eager mode.""",CONTRIBUTOR
55292,tilakrayal,1073529368,2022-03-21 6:45:48,"@kanghj ,
I have tried in colab with v 2.8 version and noticed that session is being crashed. Please, find the gist [here](https://colab.research.google.com/gist/tilakrayal/39587d8c4222dbbb35d876cb316973be/untitled255.ipynb). Also please find the errorlog of the crashed colab which stated as below.
`Check failed: ndims_byte() < MaxDimensions() (unsigned char value 254 vs. 254)Too many dimensions in tensor`","""Check failed: ndims_byte()  MaxDimensions() (unsigned char value 254 vs. 254)Too many dimensions in tensor""",CONTRIBUTOR
41639,jhetherly,1073359163,2022-03-20 22:12:46,Running the [gist](https://colab.research.google.com/gist/amahendrakar/697836288a6d02f048d11e08b7044079/41639-tf-nightly.ipynb) linked above with the latest nightly (`2.9.0-dev20220320`) fails with the exact same error.,Running the [gist](https://colab.research.google.com/gist/amahendrakar/697836288a6d02f048d11e08b7044079/41639-tf-nightly.ipynb) linked above with the latest nightly (2.9.0-dev20220320) fails with the exact same error.,CONTRIBUTOR
12843,sachinprasadhs,1072887155,2022-03-18 23:33:44,"scatter_update has been changed to tensor_scatter_nd_update in Tensorflow 2 and the error message has been modified as well. Below is the code and the output.
```
testVar = tf.Variable(tf.zeros([5,1]))
ind = tf.constant([0,3])
data = tf.constant([5,7], dtype=tf.float32)
up = tf.tensor_scatter_nd_update(testVar, ind, data )
InvalidArgumentError: Inner dimensions of output shape must match inner dimensions of updates shape. Output: [5,1] updates: [2] [Op:TensorScatterUpdate]
```","[5,1]",CONTRIBUTOR
53767,lgeiger,1072474624,2022-03-18 14:39:19,"This is still an issue in `2.9.0-dev20220318`. Are there any updates on this?
Being able to trigger a converter segfault seems to be quite problematic.",still an issue in 2.9.0-dev20220318. Are there any updates on this? Being able to trigger a converter segfault seems to be quite problematic.,CONTRIBUTOR
55232,rsanthanam-amd,1072307348,2022-03-18 11:09:27,@gbaned An import/copybara error occurred. please let me know how i can help fix this.,An import/copybara error occurred.,CONTRIBUTOR
55240,Gelesh,1071161228,2022-03-17 18:01:02,"This issue is from the URL , noting is being fetched from URL, even when we hit the url from browser. I am sorry, I dont understand how is it even related to Visual Studio C++. However, I do have that in my pc","I am sorry, I dont understand how is it even related to Visual Studio C++. However, I do have that in my pc.",CONTRIBUTOR
55245,drivanov,1071118208,2022-03-17 17:22:26,"> @drivanov I edited the PR description, would you please check?
To make it simpler, my intention was to was to submit two separate PRs: - first: for refactoring of converter for Binary operations
- second: for `LogicalAnd` and `LogicalOr`
I will remove the code related `LogicalAnd` and `LogicalOr` from `ops/binary_ops.cc` and I will change PR description.","I edited the PR description, would you please check?",CONTRIBUTOR
55177,drivanov,1069793740,2022-03-17 0:58:54,"> @drivanov this test fail, see [log](https://github.com/tensorflow/tensorflow/files/8265934/boolpr.log)
Fixed.","""this test fail, see [log](https://github.com/tensorflow/tensorflow/files/8265934/boolpr.log) Fixed."")",CONTRIBUTOR
54923,wangpengmit,1069717867,2022-03-16 22:48:32,"Looks like if https://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850e/tensorflow/python/eager/context.py#L497 fails, https://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850e/tensorflow/python/eager/context.py#L489 should also be reverted.","Looks like if https://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850e/tensorflow/python/eager/context.py#L497 fails, https://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850",MEMBER
55177,bixia1,1069333532,2022-03-16 16:38:31,"@drivanov this test fail, see
[log](https://github.com/tensorflow/tensorflow/files/8265934/boolpr.log)","""this test fail, see [log](https://github.com/tensorflow/tensorflow/files/8265934/boolpr.log)""",CONTRIBUTOR
55211,Gelesh,1069130133,2022-03-16 13:28:45,"Is the plot, talking about training loss or validation loss. ?
Early stopping is required when, validation loss is increasing to answer 'what is the time Early stopping is required to prevent overfitting ? '.
Im sorry I dont understand the bug/issue being raised over here","Is the plot, talking about training loss or validation loss. ?",CONTRIBUTOR
55240,Gelesh,1069123831,2022-03-16 13:22:52,"this is not duplicate of #55218, this issue is about error 404, file not found. Where as 55218 is about , bazelisk is not attempted to build when bazel is not installed. Some are even attempting to set the path of bazel == path of bazelisk as a work arround","This is not duplicate of #55218, this issue is about error 404, file not found. Where as 55218 is about , bazelisk is not attempted to build when bazel is not installed. Some are even attempting to set the path of bazel == path of bazelisk as a work arround.",CONTRIBUTOR
54434,akuegel,1068860669,2022-03-16 8:28:16,"Unfortunately I am not the one who can accept this change.
@rohan100jain, can you please decide whether this change is ok? Or find someone else to approve?",I am not the one who can accept this change.,MEMBER
55194,smit-hinsu,1068826527,2022-03-16 7:42:50,We don't have a plan to support ImageProjectiveTransform at the moment so for now the best option will be to put this op outside of the XLA cluster.,"""We don't have a plan to support ImageProjectiveTransform at the moment so for now the best option will be to put this op outside of the XLA cluster.""",CONTRIBUTOR
55248,thaink,1068704020,2022-03-16 3:53:49,I think you need to change MismatchBiasSizeTest in the test as well.,I think you need to change MismatchBiasSizeTest in the test as well.,MEMBER
55192,bhack,1068514622,2022-03-15 22:05:54,"Can we give a better self-explainable string to the ""avg"" developer?
I think that the current strings are a little bit cryptic and we don't have any hints on what the user need to check/change to workaround the fallback.","Can we give a better self-explainable string to the ""avg"" developer?",CONTRIBUTOR
55244,Gelesh,1068399191,2022-03-15 19:46:16,"@mihaimaruseac , The CICD build failed, on this patch seems to be from some other issue. Because this patch is just to fix the code alignment and spacing. Nothing functionally different.","The CICD build failed, on this patch seems to be from some other issue.",CONTRIBUTOR
50414,gadagashwini,1067870895,2022-03-15 11:21:21,"@ctrouillefou,
Try changing Python file path
`
python_path=C:/Program Files/Python3/python.exe -> this to C:/Python3/python.exe`
Check msys64 is `C:\msys64\usr\bin` mast be present in your PATH variable.",python_path=C:/Program Files/Python3/python.exe -> this,CONTRIBUTOR
55038,gadagashwini,1067840401,2022-03-15 10:52:17,"@adam1brownell,
You can suppress all debugging logs using below code snippet. Try setting log level before importing tf
```
import os
os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""3""
import tensorflow as tf
```
OR
```
import logging
logging.getLogger('tensorflow').disabled = True
```","""Can suppress all debugging logs using below code snippet.""",CONTRIBUTOR
55190,gadagashwini,1067617376,2022-03-15 6:34:56,"@HimGautam,
There are the Deprecation Warnings, which we can suppress using one of the below code snippets ```
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' import tensorflow as tf
```
OR
```
import warnings
warnings.filterwarnings(""ignore"")
import tensorflow as tf
```",Deprecation Warnings,CONTRIBUTOR
47174,duncanriach,1067322592,2022-03-14 21:39:08,"@edwardyehuang, to trigger the determinism-unimplemented exception in `tf.nn.depthwise_conv2d` (in TensorFlow version 2.8) your model must use the back-prop paths through the GPU implementation of the op. It's possible to run GPU inference only or train on CPU using the op without the exception being thrown. Please confirm that you're definitely _training_ your models on _GPU_.","""to trigger the determinism-unimplemented exception in tf.nn.depthwise_conv2d (in TensorFlow version 2.8) your model must use the back-prop paths through the GPU implementation of the op""",CONTRIBUTOR
51759,penpornk,1067064532,2022-03-14 17:00:03,I'm sorry I get to this PR so late. Is this required for TF-DirectML for TF 2.9? Asking because we are having an API freeze starting tomorrow.,I'm sorry I get to this PR so late.,MEMBER
53367,bhack,1067049384,2022-03-14 16:44:53,"> This has landed but GitHub fails to notice that.
If it fails the detection do we need to manually close the linked issue https://github.com/tensorflow/tensorflow/issues/53300 ?",GitHub fails to notice that.,CONTRIBUTOR
55208,tilakrayal,1066310827,2022-03-14 3:47:59,"@GodsNightmare ,
We see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]","""We see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]""",CONTRIBUTOR
55202,tilakrayal,1066310140,2022-03-14 3:45:54,"@shubhambagwari ,
We see that you are using tf version 1.15, 1.x is not actively supported, please update to latest tensorflow v2.8 and let us know if you are facing same issue.","""We see that you are using tf version 1.15, 1.x is not actively supported, please update to latest tensorflow v2.8 and let us know if you are facing same issue.""",CONTRIBUTOR
54868,gadagashwini,1066283781,2022-03-14 2:42:41,"@swapnilsayansaha,
```
import logging
logging.getLogger('tensorflow').disabled = True
```
Take a look at comment on similar issue [#8340](https://github.com/tensorflow/tensorflow/issues/8340#issuecomment-294618310) (`sys.flags.interactive = False`)",import logging logging.getLogger('tensorflow').disabled = True,CONTRIBUTOR
44266,jvishnuvardhan,1065698279,2022-03-12 0:11:29,@daniel-pp @dakshvar22 Sorry for late response. Can you please open this issue with the standalone code in Keras-team/keras repository? I am not able to move this to keras repo as it is outside of TF repository. Is it possible for one of you to open it in keras-team/keras repo? Keras team is focussed on resolving issues in Keras repo. Sorry for the inconvenience. Thanks,"""I am not able to move this to keras repo as it is outside of TF repository.""",CONTRIBUTOR
54788,bixia1,1065262403,2022-03-11 16:13:46,@DEKHTIARJonathan would you please rebase?,would you please rebase?,CONTRIBUTOR
54104,bschnurr,1064599814,2022-03-10 23:09:11,"tf.data doens't have an issue
![image](https://user-images.githubusercontent.com/1946977/157770196-0556c2b9-3fd3-45f2-8843-0c7dec089907.png)
i'm just looking at anything that is loaded with `_LazyLoader`
and adding a `if _typing.TYPE_CHECKING:` only normal import
```
losses = _LazyLoader(""losses"", globals(), _keras_package + ""losses"")
metrics = _LazyLoader(""metrics"", globals(), _keras_package + ""metrics"")
```",tf.data doens't have an issue ![image](https://user-images.githubusercontent.com/1946977/157770196-0556c2b9-3fd3-45f2-8843-0c7dec089907),CONTRIBUTOR
54482,mihaimaruseac,1064400098,2022-03-10 19:10:42,"Yes, probably broken by c6531b8bcb83d1314acc96360156b549afecb377 LLVM/MLIR","""probably broken by c6531b8bcb83d1314acc96360156b549afecb377 LLVM/MLIR""",COLLABORATOR
54482,yimeisun123,1064366282,2022-03-10 18:28:49,"I checked the logs for ""Ubuntu CPU - Internal CI build failed"" and ""Intel oneDNN -- Community CI Build"", they have the same two test case failures and the failures are not related to this PR change.","I checked the logs for ""Ubuntu CPU - Internal CI build failed"" and ""Intel oneDNN -- Community CI Build"", they have the same two test case failures and the failures are not related to this PR change.",CONTRIBUTOR
23803,mohantym,1064321272,2022-03-10 17:35:03,Hi @ClaudioCimarelli ! You are using older versions(1.x versions) of Tensorflow which is not supported any more. Have you checked this [thread ](https://its.tntech.edu/display/MON/Installing+TensorFlow+in+Your+HPC+Account)on using Tensorflow on HPC cluster though?,"""You are using older versions(1.x versions) of Tensorflow which is not supported any more.""",CONTRIBUTOR
55123,sheepmaster,1063883429,2022-03-10 10:09:33,"I don't have the model that you've been using, but you will definitely also have to change the size in native code (https://github.com/tensorflow/examples/blob/master/lite/examples/super_resolution/android/app/src/main/cc/SuperResolution.h#L35).","I don't have the model that you've been using, but you will definitely also have to change the size in native code (https://github.com/tensorflow/examples/blob/master/lite/examples/super_resolution/android/app/src/main/cc/SuperResolution.h#L35).",CONTRIBUTOR
54476,sachinprasadhs,1063427827,2022-03-09 22:17:04,"All the OPS in TFLite is not having support for `int8`, you can use the flag for OPS which supports TFLite quint8 in their input and output [here](https://www.tensorflow.org/mlir/tfl_ops).
To avoid error, like you have tried you can mention like below `converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]`
Which would apply `TFLITE_BUILTINS_INT8` if OP is supported, else it will apply `SELECT_TF_OPS`","""All the OPS in TFLite is not having support for int8, you can use the flag for OPS which supports TFLite quint8 in their input and output [here](https://www.tensorflow.org/mlir/tfl_ops). To avoid error, like you have tried you can mention like below converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.O",CONTRIBUTOR
53812,deven-amd,1063125163,2022-03-09 16:41:22,"@cheshire, please re-review....I have updated this branch with a subset of the changes you reviewed for the PR to introduce `RocComputeCapability` in the rocm fork.
Also I see that the `Code Check - Changed Files` is failing, but am unable to determine what exactly is causing the failure...please let me know if that is something I need to look into","I have updated this branch with a subset of the changes you reviewed for the PR to introduce RocComputeCapability in the rocm fork. Also I see that the Code Check - Changed Files is failing, but am unable to determine what exactly is causing the failure...please let me know if that is something I need to look into.",CONTRIBUTOR
53760,bixia1,1063048013,2022-03-09 15:35:10,TensorRT7_2 still fails with the same message.,"""TensorRT7_2 still fails with the same message.""",CONTRIBUTOR
55123,sheepmaster,1062916491,2022-03-09 13:22:44,"Hi @douzaikongcheng, if you change the size of the input image, you will also need to retrain/re-convert the model (see https://github.com/tensorflow/examples/blob/master/lite/examples/super_resolution/ml/super_resolution.ipynb) and change the corresponding values in the native code. I suspect the mismatch between these is what is causing the crash.","""I suspect the mismatch between these is what is causing the crash.""",CONTRIBUTOR
55148,elfringham,1062857226,2022-03-09 12:10:44,"This can be avoided by using ""--build_tests_only"" on the command line.","""-- build_tests_only"" on the command line.",CONTRIBUTOR
28601,bhack,1062240389,2022-03-08 21:41:38,@seanpmorgan I think that as we cannot have `keras_tensor` exposed this could be closed.,I think that as we cannot have keras_tensor exposed,CONTRIBUTOR
54657,rmlarsen,1062220146,2022-03-08 21:16:37,@mihaimaruseac Why was this closed?,Why was this closed?,MEMBER
41460,StanislawAntol,1062081806,2022-03-08 18:32:46,"I also ran into this issue on TF 2.5, which is using an outdated version of keras-preprocessing (i.e., v1.1.2). This issue was fixed via this [PR](https://github.com/keras-team/keras-preprocessing/pull/318), but had not made its way into the current TF versions. Though it seems like it will arrive soon based on [this commit](https://github.com/keras-team/keras/commit/373ad97c72ed1ac4b6898e85b2cfd7b016e4b469) from a month ago.","I also ran into this issue on TF 2.5, which is using an outdated version of keras-preprocessing (i.e., v1.1.2).",CONTRIBUTOR
55133,elfringham,1061894297,2022-03-08 15:23:01,Made moot by https://github.com/tensorflow/tensorflow/commit/214d2ed3ff4228e92f246873d3ff535bdb7de35a,"""Made moot by""",CONTRIBUTOR
54382,rsanthanam-amd,1061704026,2022-03-08 12:01:00,Closing this and opening a new PR due to git rebase error.,Closing this and opening a new PR due to git rebase error.,CONTRIBUTOR
9968,mohantym,1061666594,2022-03-08 11:12:57,Hi @singlasahil14 ! It seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. Attaching relevant [thread](https://stackoverflow.com/questions/45193238/why-out-channels-must-be-greater-then-channel-multiplier-in-channels-in-pointw?noredirect=1&lq=1) for reference though.,"""It seems you are using older versions(1.x versions) of Tensorflow which is not supported any more.""",CONTRIBUTOR
54764,chsigg,1061590952,2022-03-08 9:46:21,"No, this PR now touches 200 files.","No, this PR now touches 200 files.",CONTRIBUTOR
54382,chsigg,1061590558,2022-03-08 9:45:56,"No, this PR now touches 484 files.","No, this PR now touches 484 files.",CONTRIBUTOR
54463,njzjz,1061343477,2022-03-08 2:31:17,"It may be a bug of cublas. [cublas 11.4](https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cublas-11.4.0) resolved an issue:
> Some gemv cases were producing incorrect results if the matrix dimension (n or m) was large, for example 2^20.
In your case, m=1638400>2^20. As cublas is not open-source, it's unclear what versions of cublas have this issue.","""It may be a bug of cublas.""",CONTRIBUTOR
55082,drivanov,1061074134,2022-03-07 19:53:46,"@bixia1 : This PR is a replacement for [PR#54230](https://github.com/tensorflow/tensorflow/pull/54230).
I created it because there are a lot of merge conflicts with some current tensorflow files.",I created it because there are a lot of merge conflicts with some current tensorflow files.,CONTRIBUTOR
54868,gadagashwini,1060114539,2022-03-07 2:03:41,"@swapnilsayansaha, Try this ```
import tensorflow as tf
tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)
```",import tensorflow as tf,CONTRIBUTOR
47268,mihaimaruseac,1059656519,2022-03-05 2:34:50,"These are deprecated and will be removed in TF 2.9.
There is no owner for the code, which is why this PR stalled for so long. Apologies.","No owner for the code, which is why this PR stalled for so long.",COLLABORATOR
54973,leondgarse,1059091785,2022-03-04 11:45:37,"@mohantym Uh, right, we can run scripts in colab... Try this [tf_280_xla_test.ipynb](https://colab.research.google.com/drive/1LTVJ7jRRzsODzMuPB-svocV4jcbx1SYY?usp=sharing). Just setting `CUDA_VISIBLE_DEVICES='1'` leaves it no GPU to use in yours, my bad. Updated commands.","""Just setting CUDA_VISIBLE_DEVICES='1' leaves it no GPU to use in yours, my bad.""",CONTRIBUTOR
54868,gadagashwini,1059014332,2022-03-04 9:59:10,"@swapnilsayansaha, You can suppress all debugging logs using below code snippet. Try setting log level before importing tf ```
import os
os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""2""
import tensorflow as tf
```","""Can suppress all debugging logs using below code snippet.""",CONTRIBUTOR
54231,yongtang,1058938692,2022-03-04 8:19:04,The issue is actually in PyObject conversion side I believe. It should have been fixed in: https://github.com/tensorflow/tensorflow/pull/54441,The issue is actually in PyObject conversion side I believe. It should have been fixed in: https://github.com/tensorflow/tensorflow/pull/54441.,MEMBER
17417,mohantym,1058817232,2022-03-04 4:30:32,"It is still replicating in[ 2.8](https://colab.sandbox.google.com/gist/mohantym/a6746a2aa7a9988ed10c8cde68c8f996/github_17417.ipynb), [2.9](https://colab.sandbox.google.com/gist/mohantym/c936dafa04dd9e4cad6dbd7644dbf90b/github_17417.ipynb#scrollTo=E4Z4rqTEK1G5) and [nightly](https://colab.sandbox.google.com/gist/mohantym/32e8e81e1700fc16a1bcd21b5d83f65d/github_17417.ipynb#scrollTo=E4Z4rqTEK1G5) (2.10.0dev) version.","It is still replicating in[ 2.8](https://colab.sandbox.google.com/gist/mohantym/a6746a2aa7a9988ed10c8cde68c8f996/github_17417.ipynb), [2.9](https://colab.sandbox.google.com/gist/mohantym/c936dafa04dd9e4cad6dbd7644db",CONTRIBUTOR
53437,SandSnip3r,1058671022,2022-03-03 23:39:18,I am currently working to resolve internal conflicts,I am currently working to resolve internal conflicts.,CONTRIBUTOR
54104,bschnurr,1058383268,2022-03-03 18:57:57,@qlzh727 @gbaned I think the ios build was missing python3.. not sure how to retrigger,I think the ios build was missing python3.. not sure how to retrigger.,CONTRIBUTOR
54856,rthadur,1058339668,2022-03-03 18:11:49,"I am not able to reproduce the same if i run as below ```
import tensorflow as tf
print(tf.__version__)
import numpy as np
print(tf.experimental.numpy.floor_divide(0,0 )) # InvalidArgumentError: Integer division by zero [Op:FloorDiv]
```
can you please check","""InvalidArgumentError: Integer division by zero""",CONTRIBUTOR
54687,gadagashwini,1057615637,2022-03-03 2:56:36,"@SohaK, When i tried to save the Fine_tune_bert model with same configuration as you mentioned, it has only one .pb file `saved_model.pb`. Size of the model ~3.4MB. Use keras load model to load your model.
```
from tensorflow import keras
new_model = keras.models.load_model(""save_model_keras"")
new_model.summary()
```","""It has only one .pb file saved_model.pb. Size of the model 3.4MB""",CONTRIBUTOR
54249,akuegel,1056802519,2022-03-02 11:10:02,"I think we have seen segfaults also in other cases when using the gcc 8.3 compiler. Can you please try a newer version? gcc-9.3.1 should work for example:
https://github.com/tensorflow/build/blob/nitin/manylinux2014/tf_sig_build_dockerfiles/builder.devtoolset/build_devtoolset.sh#L115",I think we have seen segfaults also in other cases when using the gcc 8.3 compiler.,MEMBER
54249,yaochengji,1056049533,2022-03-02 1:47:00,"I'm using gcc 8.3.0.
```
gcc (Debian 8.3.0-6) 8.3.0
Copyright (C) 2018 Free Software Foundation, Inc.
This is free software; see the source for copying conditions. There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
```",gcc (Debian 8.3.0-6) 8.3.0,CONTRIBUTOR
53938,awpr,1055739316,2022-03-01 18:36:08,Still trying to figure out how to get it to retry.,Still trying to figure out how to get it to retry.,CONTRIBUTOR
54301,christopherbate,1055611455,2022-03-01 16:15:07,"rebased, squashed fixes for the failing test fix. Issue was the graph_def being modified by the MaybeRewriteCast() function was a copy of the input graph def, when really it needed to be a reference.","rebased, squashed fixes for the failing test fix.",CONTRIBUTOR
54719,mpcallanan,1054832030,2022-03-01 0:04:23,"@bhack Hey, sorry, I just joined TF and I'm still learning this github integration (and am mortified that my placeholder text was apparently public!). I did not see either comment internally. I only saw your second comment, only from a Github email.","I did not see either comment internally. I only saw your second comment, only from a Github email.",CONTRIBUTOR
53659,christopherbate,1054617601,2022-02-28 20:07:02,"I've been pulling individual commits out as separate PRs. The last one, which is still awaiting merge, is #54301","I've been pulling individual commits out as separate PRs. The last one, which is still awaiting merge, is #54301.",CONTRIBUTOR
54301,bixia1,1054514515,2022-02-28 17:55:25,"I got this test failur
[casttest.log](https://github.com/tensorflow/tensorflow/files/8155552/casttest.log)
e",I got this test failur,CONTRIBUTOR
54572,akuegel,1054174522,2022-02-28 11:45:25,"> I got below error message.
> > ```
> The specified bucket does not exist.
> ```
I guess it is an infrastructure problem. I have notified our infrastructure team.",I got below error message.,MEMBER
54572,lipracer,1054165182,2022-02-28 11:33:32,"I got below error message.
```
The specified bucket does not exist.
````",I got below error message.,CONTRIBUTOR
54582,alanpurple,1053487364,2022-02-27 10:48:00,"@mohantym I think I did have some misconcept, as soon as the program load cuda context, it takes almost all of gpu's memory, it's normal I guess....
sorry for misunderstanding( of cudnn )","I think I did have some misconcept, as soon as the program load cuda context, it takes almost all of gpu's memory, it's normal I guess...",CONTRIBUTOR
54612,sampathweb,1051293201,2022-02-25 21:43:29,"@serdarakyol - Did you downgrade the CUDA to 11.2? Looking at Nvidia docs it looks like the display driver and cuda driver do not match - https://docs.nvidia.com/deploy/cuda-compatibility/#check-for-compatibility-support
I think TensorFlow should work for 11.2+ so you may not need to downgrade the driver or Cuda version. I have not verified this, but believe 11.2+ are compatible with 11.2.","""Looking at Nvidia docs it looks like the display driver and cuda driver do not match""",CONTRIBUTOR
54508,DEKHTIARJonathan,1051137821,2022-02-25 19:35:27,@bixia1 : sorry you're gonna need to re-approve. PyLint was failing I had to fix a few indentations,"""I had to fix a few indentations""",CONTRIBUTOR
54249,yaochengji,1051081983,2022-02-25 18:13:00,"> Is this still happening? I just tried it, and for me it seems to work now.
@akuegel Thanks. But this bug still appears on my side. I'm using the latest commit `3362b358bbad2e6d`.","""But this bug still appears on my side.""",CONTRIBUTOR
53710,penpornk,1051080984,2022-02-25 18:11:38,"Cc'ing @jsimsa for the answer. IIUC, `tf.data` is meant to be run only on TF native devices be design.","IIUC, tf.data is meant to be run only on TF native devices be design.",MEMBER
54582,alanpurple,1050916201,2022-02-25 14:46:57,"@mohantym so almost all 24GB of GPU memory being taken right after load pretrained BERT-LARGE is normal?
In that case, this is should be closed","almost all 24GB of GPU memory being taken right after load pretrained BERT-LARGE is normal? In that case, this is should be closed.",CONTRIBUTOR
54505,bixia1,1050180388,2022-02-24 19:17:04,"Remove this from PR description to here
@bixia1 : for review
@tfeher @Nyrio: CC","""Remove this from PR description to here""",CONTRIBUTOR
54499,sampathweb,1050103601,2022-02-24 17:45:45,"@ebonat - `pip install tensorflow` will install a version thats compatible with GPU and CPU. So it gives you that warning messages. If you don't want to see warning messages and want to install CPU only version, you could - `pip install tensorflow-cpu` that's a smaller wheel file for CPU only version","""So it gives you that warning messages""",CONTRIBUTOR
54498,sampathweb,1050101952,2022-02-24 17:43:35,"CUDA version. needs to be 11.2+, from your config it looks like its 10.1. So, please see if you can upgrade your local CUDA version to 11.x. 11.2 builds should be compatible with anything 11.x","CUDA version. needs to be 11.2+, from your config it looks like its 10.1. So, please see if you can upgrade your local CUDA version to 11.x. 11.2 builds should be compatible with anything 11.x.",CONTRIBUTOR
54498,JueonPark,1049542650,2022-02-24 6:41:19,"@sampathweb I tried with Python 3.9.7, yet it fails with the same error.
@mohantym I'll try with the tested configuration and let you know.","I tried with Python 3.9.7, yet it fails with the same error.",CONTRIBUTOR
54155,MeghnaNatraj,1049126404,2022-02-23 19:17:17,"Hello @raminmohammadi, this will take a longer to debug, please give us a few days to take a look and get back to you.","""this will take a longer to debug, please give us a few days to take a look and get back to you.""",MEMBER
53367,mihaimaruseac,1048977002,2022-02-23 16:37:52,This has landed but GitHub fails to notice that.,GitHub fails to notice that.,COLLABORATOR
53834,tilakrayal,1048560468,2022-02-23 8:50:53,"@chunduriv ,
While executing the given code in colab, i was facing different error and noticed session is being crashed.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/16f3ccd62f8b78bcc2e57746e847a423/untitled233.ipynb).","""I was facing different error and noticed session is being crashed.""",CONTRIBUTOR
54298,karimnosseir,1048458321,2022-02-23 5:21:40,"@peyer np.
Sadly no, Snapdragon 888 doesn't leverage Hexagon NN API. They had a new APIs for executing on the DSP, and hexagon delegate doesn't work with it.","Sadly no, Snapdragon 888 doesn't leverage Hexagon NN API.",CONTRIBUTOR
14798,bhack,1048082485,2022-02-22 18:20:07,Yes when we don't have or we want to have the orchestration/environment with public visibility we need to have exrta docs on how to test this locally if we want to collect community contribution. As also the TFlite markdown is on hold since 2020 we could ping also the TFlite team,"""When we don't have or we want to have the orchestration/environment with public visibility we need to have exrta docs on how to test this locally if we want to collect community contribution.""",CONTRIBUTOR
14798,bhack,1048067267,2022-02-22 18:01:50,I meant could it be tested locally when we have no visibility of the CI logs?,I meant could it be tested locally when we have no visibility of the CI logs?,CONTRIBUTOR
53938,kaixih,1048057446,2022-02-22 17:49:56,"Same with this one. I see ""Windows Bazel GPU"" fails but the details are not accessible. @awpr @jurahul","""Windows Bazel GPU"" fails but the details are not accessible.",CONTRIBUTOR
14798,bhack,1048055756,2022-02-22 17:48:00,"> t's an internal tool that runs those.\nThey're run from the target version's github branch, with bazel available, so just calling that bazel command and Thanks so probably It Is a little bit hard to contribute a PR with only the OSS/Github visibilty.","t's an internal tool that runs those.nThey're run from the target version's github branch, with bazel available, so just calling that bazel command and Thanks so probably It Is a little bit hard to contribute a PR with only the OSS/Github visibilty.",CONTRIBUTOR
53843,kaixih,1048055710,2022-02-22 17:47:57,It seems the PR has been hanging for a while. I see some tests fail but they seem unrelated to the PR. Can you help check? @awpr @jurahul,I see some tests fail but they seem unrelated to the PR.,CONTRIBUTOR
53710,mihaimaruseac,1048012705,2022-02-22 17:01:57,"See https://github.com/tensorflow/tensorflow/pull/50605#issuecomment-919453092
It seems `tf.data` kernels should not use `DEVICE_DEFAULT`",tf.data kernels should not use DEVICE_DEFAULT,COLLABORATOR
45081,jvishnuvardhan,1047967886,2022-02-22 16:21:30,"@eaedk Sorry for the late response. Can you please share a simple standalone code to reproduce the issue or share the data? When you run a model in a `for` loop, keras will take more memory to keep those models in the memory. Did you try releasing the memory https://www.tensorflow.org/api_docs/python/tf/keras/backend/clear_session. Thanks","""When you run a model in a for loop, keras will take more memory to keep those models in the memory. Did you try releasing the memory https://www.tensorflow.org/api_docs/python/tf/keras/backend/clear_session. Thanks.""",CONTRIBUTOR
54478,aliencaocao,1047893952,2022-02-22 15:09:39,Many pages especially keras related pages all have this issue. Changing browsers does not work.,Many pages especially keras related pages all have this issue. Changing browsers does not work.,CONTRIBUTOR
14798,bhack,1047872509,2022-02-22 14:48:20,At least can we reopen this ticket adding also the XLA label?,can we reopen this ticket adding also the XLA label?>,CONTRIBUTOR
54463,gadagashwini,1047646586,2022-02-22 10:26:12,"@arvindrajan92, Given configurations [Tested build configuration](https://www.tensorflow.org/install/source#gpu) were tested on different platforms. This error is due to - OOM error -GPU is running out of memory
- Doesn't have enough compute capacity
- There's a driver issue.
Can you verify the memory usage with nvidia-smi? If you have any other
processes using the GPU. And also check CUDA compute capability for the given nvidia drivers.",OOM error - GPU is running out of memory - Doesn't have enough compute capacity - There's a driver issue.,CONTRIBUTOR
54466,lipracer,1047378902,2022-02-22 3:00:18,"It seem like infra tool error.
```
[31m[1mERROR: [0mAn error occurred during the fetch of repository 'local_config_rocm':
```",Error: [0mAn error occurred during the fetch of repository 'local_config_rocm':,CONTRIBUTOR
54407,itmo153277,1047377486,2022-02-22 2:56:45,"I think you forgot to put MSYS2 to your `%PATH%`. Check if this command prints a path to `realpath.exe`:
```cmd
for /F %a in (""realpath.exe"") do @echo.%~f$PATH:a
```
If it prints an empty string, your `%PATH%` is not configured correctly","I think you forgot to put MSYS2 to your %PATH%. Check if this command prints a path to realpath.exe: cmd for /F %a in (""realpath.exe"") do @echo.%f$PATH:a  If it prints an empty string, your %PATH% is not configured correctly.",CONTRIBUTOR
54336,tilakrayal,1046793350,2022-02-21 11:49:05,"@markub3327 ,
Looks like code is incomplete. Request you to provide tensorflow version you are using and complete code to reproduce the issue in our environment. It helps us in localizing the issue faster.",Looks like code is incomplete.,CONTRIBUTOR
54386,tilakrayal,1046762263,2022-02-21 11:12:25,"@charlielam0615 ,
The code provided is not complete hence it would be difficult for us to pinpoint the issue in the tensorboard. Please share complete stand alone code to replicate the issue or a colab gist with the error reported.?",The code provided is not complete.,CONTRIBUTOR
53632,elfringham,1046665936,2022-02-21 9:41:43,This seems to have been caused by the use of BAZEL_LINKLIBS to add '-l%:stdc++' to the build. This was introduced in our initial CI scripts and has since just been carried forward. I am not sure of the initial need for this usage but it is not needed now.,This seems to have been caused by the use of BAZEL_LINKLIBS to add '-l%:stdc++' to the build.,CONTRIBUTOR
48040,gadagashwini,1046656136,2022-02-21 9:32:02,"@ZoeZhang91,
Tensorflow v2.4.1 source incompatible with Numpy version 1.20.1. Downgrade numpy ~= 1.19.5 will resolve the issues.
`pip install numpy==1.19.5`","""Tensorflow v2.4.1 source incompatible with Numpy version 1.20.1""",CONTRIBUTOR
52014,thaink,1046623190,2022-02-21 9:01:30,I saw the tflite-micro PR is closed. Can this PR be submitted at current state?,I saw the tflite-micro PR is closed. Can this PR be submitted at current state?,MEMBER
54360,mihaimaruseac,1046298002,2022-02-20 18:46:47,The WSL2 usage should be handled by Microsoft or other documentation. There is nothing special about TF for this.,WSL2 usage should be handled by Microsoft or other documentation.,COLLABORATOR
54383,mihaimaruseac,1046297855,2022-02-20 18:45:49,"No template filled. Rant issues are not productive, they just waste the time of everyone.",No template filled.,COLLABORATOR
54438,mihaimaruseac,1046159935,2022-02-20 4:19:00,"Hi @gadagashwini , please don't use a ""Update <file>"" commit message. Instead, please try to follow [proper commit etiquette](https://cbea.ms/git-commit/)","""Update file>"" commit message.",COLLABORATOR
54423,mohantym,1046020340,2022-02-19 13:36:32,"Please try with ""conda install -c anaconda cudatoolkit "" command . If that does not resolve the issue please check with instructions from [here ](https://www.tensorflow.org/install/pip) with TF 2.8 after activating Conda environment?","""conda install -c anaconda cudatoolkit "" command . If that does not resolve the issue please check with instructions from [here ](https://www.tensorflow.org/install/pip) with TF 2.8 after activating Conda environment?""",CONTRIBUTOR
54439,tilakrayal,1044387647,2022-02-18 11:43:38,"@lhy2749 ,
We see that you are using tf version 1.15, 1.x is not actively supported, please update to latest stable version 2.8 and let us know if you are facing same issue.","""We see that you are using tf version 1.15, 1.x is not actively supported, please update to latest stable version 2.8 and let us know if you are facing same issue.""",CONTRIBUTOR
54388,elfringham,1044197213,2022-02-18 9:18:20,@gadagashwini I disagree. Cleaning the cache is no help. That issue does not seem to relate in any way.,I disagree. Cleaning the cache is no help. That issue does not seem to relate in any way.,CONTRIBUTOR
54442,edwardyehuang,1044101120,2022-02-18 8:02:05,@mohantym I think it is a bug,I think it is a bug.,CONTRIBUTOR
53437,cheshire,1043553697,2022-02-17 22:37:19,Now it seems like GemmRewriteTest is failing?,"""It seems like GemmRewriteTest is failing""",MEMBER
54405,mohantym,1042849228,2022-02-17 11:25:21,"Hi @chunduriv! Could you please look at this issue? It is replicating in 2.7 ,[2.8 ](https://colab.sandbox.google.com/gist/mohantym/0e54856c2c952ab3d5c9763384fc6131/github_54405.ipynb#scrollTo=iQvBGhKm-p7v)and nightly. Size of tflite file came 400 mb after conversion as mentioned in template.","""It is replicating in 2.7 ,[2.8 ](https://colab.sandbox.google.com/gist/mohantym/0e54856c2c952ab3d5c9763384fc6131/github_54405.ipynb#scrollTo=iQvBGhKm-p7v)and nightly""",CONTRIBUTOR
54388,elfringham,1042814696,2022-02-17 10:47:40,"No, that does not help at all.","No, that does not help at all.",CONTRIBUTOR
45770,mohantym,1042619971,2022-02-17 6:34:11,It is still replicating in CPU mode for [TF 2.8](https://colab.sandbox.google.com/gist/mohantym/0f41a7f797bf930813d8383dc1d955a3/github_45770.ipynb#scrollTo=F8ttmtF-X2zd).,It is still replicating in CPU mode for [TF 2.8],CONTRIBUTOR
54342,itmo153277,1042496298,2022-02-17 2:08:59,"I believe the issue itself has not been resolved, If the tensorflow team has decided that the issue is not worth fixing, feel free to close it.","I believe the issue itself has not been resolved, If the tensorflow team has decided that the issue is not worth fixing, feel free to close it.",CONTRIBUTOR
54170,Kh4L,1042450813,2022-02-17 0:44:16,"> Some tests are failing because of error message mismatch, convert_nodes_test.cc: Value of: status Expected: has a status code that is equal to UNIMPLEMENTED, and has an error message that has substring ""Conversion for Fill is not implemented inimplicit batch mode"" Actual: UNIMPLEMENTED: Op type Fill is not supported. (of type tensorflow::Status), whose error message is wrong
I didn't add the version check in the tests. I just pushed the fix.","""Error message mismatch""",CONTRIBUTOR
54170,bixia1,1041832572,2022-02-16 16:13:34,"Some tests are failing because of error message mismatch, convert_nodes_test.cc:
Value of: status
Expected: has a status code that is equal to UNIMPLEMENTED, and has an error message that has substring ""Conversion for Fill is not implemented inimplicit batch mode""
Actual: UNIMPLEMENTED: Op type Fill is not supported. (of type tensorflow::Status), whose error message is wrong","""Error message mismatch""",CONTRIBUTOR
52973,bhack,1041411302,2022-02-16 11:54:47,"Just a side note also having a Linux aarch64 image on M1 we are not going to be able to use the AMX2 instruction set:
https://github.com/tensorflow/tensorflow/issues/52845#issuecomment-974559363
https://nod.ai/comparing-apple-m1-with-amx2-m1-with-neon/",Having a Linux aarch64 image on M1 we are not going to be able to use the AMX2 instruction set.,CONTRIBUTOR
53718,MarkDaoust,1040980677,2022-02-16 1:12:13,It might have been a wrong-way merge or something like that. At this point it's usually easier to just close it and make a new PR from scratch.,"""Wrong-way merge""",MEMBER
53414,wraveane,1040721554,2022-02-15 19:45:46,@bixia1 I had to resolve another merge conflict that came up in the last few days. Does the PR need to be re-approved?,I had to resolve another merge conflict that came up in the last few days.,CONTRIBUTOR
53718,drivanov,1040612173,2022-02-15 18:11:46,"@bixia1 : I rebased before the squash, therefore I see **Conflicting files**. Is it OK?
OR, perhaps I need to create a new PR.","I rebased before the squash, therefore I see **Conflicting files**.",CONTRIBUTOR
54329,smuzaffar,1040336575,2022-02-15 14:26:23,I have signed CLA but no idea why it still shows missing CLA.,I have signed CLA but no idea why it still shows missing CLA.,CONTRIBUTOR
53718,drivanov,1039408081,2022-02-14 18:21:18,"@bixia1: Sorry, I have no idea how this happened. Some of these files are not even from Tensorflow but from my MXNET projects. I just fixed it.
> I see a bunch a unrelated files were uploaded, and an existing file op_convert.h is a copied to a different directory and shown as ""new""? Can you please fix those?",I have no idea how this happened.,CONTRIBUTOR
52605,tilakrayal,948522091,2021-10-21 11:33:37,"@sanatmpa1 ,
I was able to reproduce the issue in tf v2.6 and nightly.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/a699c96140c7e85086d89e4bee47306b/untitled100.ipynb).
[EDIT] This is still an issue with `tf-nightly`(2.9.0-dev20220309)",I was able to reproduce the issue in tf v2.6 and nightly. Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/a699c96140c7e85086d89e4bee47306b/untitled100.ipynb). [EDIT] This is still an issue with tf-nightly(2.9.0-dev20220309).,CONTRIBUTOR
37245,Saduf2019,922485171,2021-09-19 14:44:51,This issue is still reproducible on tf nightly-2.13.0-dev20230305 [gist](https://colab.sandbox.google.com/gist/tilakrayal/161482a5a0ee8e079bd1ac19ec1a3c87/untitled52.ipynb) version.,This issue is still reproducible on tf nightly-2.13.0-dev20230305 [gist](https://colab.sandbox.google.com/gist/tilakrayal/161482a5a0ee8e079bd1ac19ec1a3c87/untitled52.ipynb) version..,CONTRIBUTOR
50765,tilakrayal,880039748,2021-07-14 16:33:14,"@jvishnuvardhan ,
I was able to reproduce the issue in tensorflow [2.11.0-dev20220829](https://colab.research.google.com/gist/tilakrayal/39ee242dcd8f745fe98f4d46cacf3caf/2_4memory_leak-1.ipynb) it is giving error.Please find the gist here.",I was able to reproduce the issue in tensorflow [2.11.0-dev20220829](https://colab.research.google.com/gist/tilakrayal/39ee242dcd8f745fe98f4d46cacf3caf/2_4memory_leak-1.ipynb) it is giving error.Please find the gist here.,CONTRIBUTOR
42680,ymodak,850017495,2021-05-28 0:00:35,"TF 2.11 raises `ValueError: num_tokens must be set to use this layer. If the number of tokens is not known beforehand, use the IntegerLookup layer instead.` Switching to `IntegerLookup layer` results in same reported behavior. See [gist](https://colab.research.google.com/gist/pjpratik/91214b91019d90518d530e52671d7b3b/42680.ipynb)","TF 2.11 raises ValueError: num_tokens must be set to use this layer. If the number of tokens is not known beforehand, use the IntegerLookup layer instead.",CONTRIBUTOR
43043,bhack,734288390,2020-11-26 13:07:02,"Mine was not a solution it was just to check that when it is not constant anymore the dtype is working correctly.
Your problem is related that in the default case `unconnect_gradients='null'` we are not handling any specific `dtype` on the source.
https://github.com/tensorflow/tensorflow/blob/31f0b2159751642a146d70561ac5095a3c4722ff/tensorflow/python/eager/pywrap_tfe_src.cc#L2804-L2814
/cc @edloper","""mine was not a solution it was just to check that when it is not constant anymore the dtype is working correctly.""",CONTRIBUTOR
